{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split, Dataset\n",
    "from collections import Counter\n",
    "import os\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images Shape: (232365, 28, 28)\n",
      "Train Labels Shape: (232365,)\n",
      "Test Images Shape: (38547, 28, 28)\n",
      "Test Labels Shape: (38547,)\n"
     ]
    }
   ],
   "source": [
    "# Define paths to downloaded files\n",
    "\n",
    "train_images_path = \"data/KMNIST/raw/k49-train-imgs.npz\"\n",
    "train_labels_path = \"data/KMNIST/raw/k49-train-labels.npz\"\n",
    "test_images_path = \"data/KMNIST/raw/k49-test-imgs.npz\"\n",
    "test_labels_path = \"data/KMNIST/raw/k49-test-labels.npz\"\n",
    "\n",
    "# Load the dataset\n",
    "train_images = np.load(train_images_path)['arr_0']\n",
    "train_labels = np.load(train_labels_path)['arr_0']\n",
    "test_images = np.load(test_images_path)['arr_0']\n",
    "test_labels = np.load(test_labels_path)['arr_0']\n",
    "\n",
    "print(f\"Train Images Shape: {train_images.shape}\")  # (270912, 28, 28)\n",
    "print(f\"Train Labels Shape: {train_labels.shape}\")  # (270912,)\n",
    "print(f\"Test Images Shape: {test_images.shape}\")    # (45792, 28, 28)\n",
    "print(f\"Test Labels Shape: {test_labels.shape}\")    # (45792,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 232365\n",
      "Total testing samples: 38547\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize pixel values\n",
    "])\n",
    "\n",
    "# Custom dataset class\n",
    "class Kuzushiji49Dataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PIL image and apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = Kuzushiji49Dataset(train_images, train_labels, transform=transform)\n",
    "test_dataset = Kuzushiji49Dataset(test_images, test_labels, transform=transform)\n",
    "\n",
    "# Create DataLoaders for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Total training samples: {len(train_dataset)}\")\n",
    "print(f\"Total testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Extract train and test data\n",
    "train_data, train_labels = next(iter(train_loader))\n",
    "test_data, test_labels = next(iter(test_loader))\n",
    "\n",
    "# Flatten images from (N, 1, 28, 28) -> (N, 784)\n",
    "train_data = train_data.view(train_data.shape[0], -1).to(device)  # (60000, 784)\n",
    "test_data = test_data.view(test_data.shape[0], -1).to(device)  # (10000, 784)\n",
    "\n",
    "train_labels = train_labels.to(device)\n",
    "test_labels = test_labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>codepoint</th>\n",
       "      <th>char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>U+3042</td>\n",
       "      <td>あ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>U+3044</td>\n",
       "      <td>い</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>U+3046</td>\n",
       "      <td>う</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>U+3048</td>\n",
       "      <td>え</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>U+304A</td>\n",
       "      <td>お</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>U+304B</td>\n",
       "      <td>か</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>U+304D</td>\n",
       "      <td>き</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>U+304F</td>\n",
       "      <td>く</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>U+3051</td>\n",
       "      <td>け</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>U+3053</td>\n",
       "      <td>こ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>U+3055</td>\n",
       "      <td>さ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>U+3057</td>\n",
       "      <td>し</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>U+3059</td>\n",
       "      <td>す</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>U+305B</td>\n",
       "      <td>せ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>U+305D</td>\n",
       "      <td>そ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>U+305F</td>\n",
       "      <td>た</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>U+3061</td>\n",
       "      <td>ち</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>U+3064</td>\n",
       "      <td>つ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>U+3066</td>\n",
       "      <td>て</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>U+3068</td>\n",
       "      <td>と</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>U+306A</td>\n",
       "      <td>な</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>U+306B</td>\n",
       "      <td>に</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>U+306C</td>\n",
       "      <td>ぬ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>U+306D</td>\n",
       "      <td>ね</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>U+306E</td>\n",
       "      <td>の</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>U+306F</td>\n",
       "      <td>は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>U+3072</td>\n",
       "      <td>ひ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>U+3075</td>\n",
       "      <td>ふ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>U+3078</td>\n",
       "      <td>へ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>U+307B</td>\n",
       "      <td>ほ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>U+307E</td>\n",
       "      <td>ま</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>U+307F</td>\n",
       "      <td>み</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>U+3080</td>\n",
       "      <td>む</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>U+3081</td>\n",
       "      <td>め</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>U+3082</td>\n",
       "      <td>も</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>U+3084</td>\n",
       "      <td>や</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>U+3086</td>\n",
       "      <td>ゆ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>U+3088</td>\n",
       "      <td>よ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>U+3089</td>\n",
       "      <td>ら</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>U+308A</td>\n",
       "      <td>り</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>U+308B</td>\n",
       "      <td>る</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>U+308C</td>\n",
       "      <td>れ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>U+308D</td>\n",
       "      <td>ろ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>U+308F</td>\n",
       "      <td>わ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>U+3090</td>\n",
       "      <td>ゐ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>U+3091</td>\n",
       "      <td>ゑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>U+3092</td>\n",
       "      <td>を</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>U+3093</td>\n",
       "      <td>ん</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>U+309D</td>\n",
       "      <td>ゝ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index codepoint char\n",
       "0       0    U+3042    あ\n",
       "1       1    U+3044    い\n",
       "2       2    U+3046    う\n",
       "3       3    U+3048    え\n",
       "4       4    U+304A    お\n",
       "5       5    U+304B    か\n",
       "6       6    U+304D    き\n",
       "7       7    U+304F    く\n",
       "8       8    U+3051    け\n",
       "9       9    U+3053    こ\n",
       "10     10    U+3055    さ\n",
       "11     11    U+3057    し\n",
       "12     12    U+3059    す\n",
       "13     13    U+305B    せ\n",
       "14     14    U+305D    そ\n",
       "15     15    U+305F    た\n",
       "16     16    U+3061    ち\n",
       "17     17    U+3064    つ\n",
       "18     18    U+3066    て\n",
       "19     19    U+3068    と\n",
       "20     20    U+306A    な\n",
       "21     21    U+306B    に\n",
       "22     22    U+306C    ぬ\n",
       "23     23    U+306D    ね\n",
       "24     24    U+306E    の\n",
       "25     25    U+306F    は\n",
       "26     26    U+3072    ひ\n",
       "27     27    U+3075    ふ\n",
       "28     28    U+3078    へ\n",
       "29     29    U+307B    ほ\n",
       "30     30    U+307E    ま\n",
       "31     31    U+307F    み\n",
       "32     32    U+3080    む\n",
       "33     33    U+3081    め\n",
       "34     34    U+3082    も\n",
       "35     35    U+3084    や\n",
       "36     36    U+3086    ゆ\n",
       "37     37    U+3088    よ\n",
       "38     38    U+3089    ら\n",
       "39     39    U+308A    り\n",
       "40     40    U+308B    る\n",
       "41     41    U+308C    れ\n",
       "42     42    U+308D    ろ\n",
       "43     43    U+308F    わ\n",
       "44     44    U+3090    ゐ\n",
       "45     45    U+3091    ゑ\n",
       "46     46    U+3092    を\n",
       "47     47    U+3093    ん\n",
       "48     48    U+309D    ゝ"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_path = 'data/KMNIST/raw/k49_classmap.csv'\n",
    "mapping = pd.read_csv(mapping_path)\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyEAAABqCAYAAACrmV7BAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAACXUElEQVR4nO19aZScV3nmU/u+71vv3erW1tq9G4MFMtiEZYBAQmACSUxgQjKHJLMkMyQBhpxJJjnJzCEk7CQZAiRgINgGY8tG0WLtckvd6n2r6tr3ff3mh+e9+qrVkrqlVndV63vO6SO7urrqq1v3u/e+7/u8zyPiOI6DAAECBAgQIECAAAECBGwQxJt9AQIECBAgQIAAAQIECLi3IAQhAgQIECBAgAABAgQI2FAIQYgAAQIECBAgQIAAAQI2FEIQIkCAAAECBAgQIECAgA2FEIQIECBAgAABAgQIECBgQyEEIQIECBAgQIAAAQIECNhQCEGIAAECBAgQIECAAAECNhRCECJAgAABAgQIECBAgIANhRCECBAgQIAAAQIECBAgYEOxpiDk61//OkQiEc6ePbsuby4SifAf/sN/WJfX4r/mH/3RH9323//hH/4hnnrqKXg8HohEIvz7f//vV3zeH/3RH0EkEl33o1QqV/1e98J48vGzn/2MjVMsFruj17pXxu7y5ct473vfC5vNBoVCga6uLnz84x9f1d/eK2NEWMv8+uAHPwiRSISnnnpqTe9xr4ypMO9ujtXuE9/61rfw6KOPwuFwQKFQwO124+1vfztOnDix6ve6F8aTj9vdJ7b6OJ07dw6f+MQnsGvXLuh0OjgcDhw+fBgvvfTSis/nOA5f+9rXcOjQIWg0Guj1euzbtw8/+MEPrnvuVh87YPX3LAD8y7/8Cx566CGYzWYYjUYcOnQIf//3f7/q97oXxrNareKP//iP0dXVBYVCgcHBQfzv//2/1/w6QiVkGf7yL/8S8Xgcv/ALvwC5XH7L5z///PM4efIk+/n5z3++AVfZfsjlcvj1X/91uN3uzb6UtsHRo0dx6NAhZDIZfPGLX8RPf/pTfOYzn1lToHuvYC3z68c//jGeeeYZ6PX6Dbiy9oMw726N1e4T8XgcDz30EL7whS/gpz/9Kf7iL/4C4XAYjz76KF555ZUNvOL2gLBP3Bjf+ta3cPr0aXzkIx/BD37wA3z5y1+GQqHA448/jm9+85vXPf83f/M38Zu/+Zt4/PHH8cMf/hDf/e538Uu/9EsoFAqbcPWbj9Xes1/96lfxnve8By6XC//4j/+If/qnf0Jvby8+9KEP4S//8i838IpbGx//+Mfx+c9/Hp/4xCfwk5/8BO9617vw27/92/gf/+N/rOl1pHfp+toW2WwWYvHrsdlqIt/9+/fDarXe7ctqe/zn//yfYTKZ8OSTT+Kzn/3sZl9Oy6NQKOCXf/mX8aY3vQk/+tGPIBKJ2O9+5Vd+ZROvrDWx2vmVTqfx9NNP4zOf+Qz+6q/+agOvsD0gzLvVYbX7xErZy7e+9a2w2Wz4yle+gje84Q137RrbEcI+cWP8/u//Pv78z/+86bG3ve1t2LdvH/7kT/4EH/rQh9jjzzzzDP72b/8W3/72t/G+972PPX7kyJENu95Ww2rv2a9+9avo7OzEd77zHfb8I0eO4OLFi/j617+O//gf/+OGXG8r48qVK/jKV76Cz33uc/i93/s9AMBjjz2GeDyOz372s/jYxz4Gs9m8qtda90pIqVTCpz71KezZswcGgwFmsxkPPPDAiiVAwt/+7d9iYGAACoUC27dvxz/90z9d95xQKISnn34aXq8Xcrkc3d3d+OM//mPUarV1vX6adK2Cdh9PADh27Bj+7u/+Dl/+8pchkUjW/fVvhHYeu+9+97sIBoP4vd/7vaaD4HqjnceIsJb59alPfQoulwuf/OQn1/06CO08psK8Wx3uZJ/Q6XRQKpWQStcvB9ju4wlszD7RzuNkt9uve0wikWD//v1YXFxsevyv/uqv0NXV1RSA3CnaeeyA1d+zMpkMWq226fkikQh6vX5dq8HtPJ7PPPMMOI7Dr/7qrzY9/qu/+qsoFot4/vnnV/1a614JKZfLSCQS+N3f/V14PB5UKhX87Gc/w7vf/W587Wtfa4rWAeCHP/whjh49ij/5kz+BRqPBF77wBXzgAx+AVCrFe97zHgCvD+qhQ4cgFovx3//7f0dvby9OnjyJz372s5ibm8PXvva1m15TV1cXAGBubm69Py527dqFSCQCq9WKI0eO4LOf/Sw6OjrW7fXbfTyLxSI++tGP4nd+53ewb98+/PCHP7ytcbgdtPPYEa2vXq/j4YcfxunTp6HRaPDEE0/gf/2v/7VudIV2HiNgbfPrZz/7Gb75zW/izJkzdzUYbucxFebd3dkn6vU6Go0GAoEAPv/5z4PjOHziE59Yt9dv9/HcqH2i3cdpOWq1Go4dO4YdO3Y0PXby5Em87W1vw1/8xV/gr/7qr+D3+9HZ2YmPf/zj+NSnPnVbCYatNnY3wm/91m/hve99Lz73uc/hN37jNyASifD1r38d586dw7e+9a11e592Hs/Lly/DZrPB6XQ2Pb579272+1WDWwO+9rWvcQC4M2fOrPpvarUaV61WuY9+9KPc3r17m34HgFOpVFwoFGp6/uDgINfX18cee/rppzmtVsvNz883/f2f//mfcwC4K1euNL3mpz/96abn9fb2cr29vau+ZoJGo+E+/OEPr/i7b37zm9znPvc57tlnn+Veeukl7k//9E85s9nMORwOzu/3r+r174Xx/NSnPsX19PRwhUKB4ziO+/SnP80B4KLR6Kr+/kbY6mN35MgRDgBnNBq53//93+deeukl7otf/CJnsVi4vr4+Lp/P3/I1tvoYcdzq51c2m+W6urq4//Jf/gt7rLOzk3vyySdX9T6ErT6mwrxb332CsG3bNg4AB4BzuVzcv/3bv6369e+F8VyPfeJeGKfl+IM/+AMOAPfMM8+wx4LBIAeA0+v1nNfr5b7xjW9wL774Ivexj32MA8D91//6X697nXtt7G51zz7zzDOcwWBg96xKpeL+4R/+YdWvv9XH881vfjO3bdu2FX8nl8u53/iN37jla7DrWPUzudUP7He+8x3uwQcf5DQaDfsSAXBKpbL5zQHuqaeeuu7vaQFaXFzkOI7jPB4P9/a3v52rVqtNP1euXOEAcF/4wheaXnP5wN4uVrO58PHqq69yYrGY++QnP7mq52/18Xz11Vc5iUTCvfDCC9ddy0YFIe06dm9+85s5ANzTTz/d9PgzzzzDAeC+9KUv3fI1tvoYrWV+feITn+D6+/u5YrHIHrubQUi7jqkw79aO1ewTly9f5l599VXuu9/9Lvf4449zOp2OO3r06Kpef6uP53rtE1t9nJbjS1/6EgeA+9SnPtX0eCAQYJ/p5MmTTb975zvfySmVSi6bzTY9fq+N3c3u2eeee47TarXcr/7qr3LPPfcc98ILL3C/9Vu/xUmlUu6rX/3qql5/q4/nm9/8Zm5wcHDF38nl8uv2j5th3Rsgvve97+F973sfPB4P/uEf/gEnT57EmTNn8JGPfASlUum65y8v5/Afi8fjAIBwOIwf/ehHkMlkTT9UgrxTudf1wqFDhzAwMIBTp06t22u283h+5CMfwbvf/W4cOHAAqVQKqVSKXXMmk0E2m12X97kR2nnsLBYLgOsbCY8cOQKRSITz58+vy/u08xitdn6dPn0aX/jCF/A//+f/RKlUYs9tNBqo1WpIpVIol8vrck1Ae4+pMO/uDnbs2IFDhw7hPe95D55//nl0dnbit3/7t9ft9dt5PDdyn2jnceLja1/7Gp5++mn8xm/8Bv7sz/6s6Xcmk4n1MNx///1Nv3vrW9+KUqmE0dHRNb/nVhm7m4HjOHzkIx/Bo48+iq9+9at44okncPjwYfz1X/81fumXfgm/9Vu/hXw+vy7v1c7jabFY2Hvykc/nUalUVt2UDtyFnpB/+Id/QHd3N7797W838Q5vtMmHQqEbPkYbotVqxe7du/G5z31uxddoJTk/juPWtbm9ncfzypUruHLlCr773e9e97ve3l4MDw/j4sWL6/JeK6Gdx2737t0rNp0R1muOtfMYrXZ+jY6OguM4vOtd77rueYuLizCZTPjLv/xL/M7v/M66XFc7j6kw7+4+pFIp9u3bh+985zvr9prtPJ4buU+08zgRvva1r+HXfu3X8OEPfxhf/OIXr+vvUKlU6O/vX/HaX0+A3959vBXG7lYIh8MIBoN4+umnr/vdwYMH8c1vfhNzc3NNPTi3i3Yez127duGf/umfEAqFmoKjkZERAMDOnTtX/VrrHoSIRCLI5fKmQQ2FQjfs+H/xxRcRDofhcDgAvN7A9+1vfxu9vb3wer0AgKeeegrPPvssent7YTKZ1vuS1w2nTp3C5OTkuirvtPN4Hj169LrHvv71r+Mb3/gGnnnmGXg8nrv23kB7j9273vUu/MEf/AGee+65psPzc889B47jrstw3S7aeYxWO7+eeOKJFZ/7/ve/H93d3fj85z+Pvr6+dbuudh5TYd7dfZRKJZw6dUqYc/8fG7lPtPM4Aa+Py6/92q/hgx/8IL785S/fsMH83/27f4fPf/7zOHHiBB588EH2+LPPPgutVntbh+h2H7vVwGQyQalUrshmOXnyJMRiMVwu17q8VzuP5zve8Q784R/+Ib7xjW/gP/2n/8Qe//rXvw6VSoUnnnhi1a91W0HISy+9tGL3/Nve9jY89dRT+N73voePf/zjeM973oPFxUV85jOfgcvlwuTk5HV/Y7Va8aY3vQn/7b/9N9bxf/Xq1aZs3J/8yZ/ghRdewIMPPohPfvKT2LZtG0qlEubm5vDss8/ii1/8IvsSVgIt9lNTU7f8bK+88gqi0SiA17/k+fl5/PM//zMA4A1veANsNhsAYHh4GB/84AcxNDQEpVKJ06dP48/+7M/gdDrx+7//+7d8Hz626ng+9thj1z328ssvAwAeeuihdfFX2apjNzg4iE984hP4whe+AJ1Oh7e+9a2YmJjAH/7hH2Lv3r1rkl7cqmO02vnldDpXLGUrlUpYLJYVX+dW2KpjKsy79d0nHnzwQfzCL/wChoaGYDAYMDc3h7/5m7/B9PQ0vv/979/yffjYquO53vvEVh2n7373u/joRz+KPXv24Omnn8bp06ebfr93714oFAoAwO/+7u/iH//xH/He974Xn/nMZ+D1evHP//zP+OEPf4g///M/h0qluqfGDljdPatQKPDxj38cf/EXf4EPfehD+MVf/EVIJBI888wz+L//9//iox/96JqoRlt1PHfs2IGPfvSj+PSnPw2JRIKDBw/ipz/9Kf7u7/4On/3sZ9c0RrfVmH6jn9nZWY7jOO5P//RPua6uLk6hUHBDQ0Pcl770JdZAwwcA7hOf+AT3hS98gevt7eVkMhk3ODjI/eM//uN17x2NRrlPfvKTXHd3NyeTyTiz2czt37+f+4M/+AMul8s1vebyZpvOzk6us7NzVZ/xDW94ww0/H7+R8P3vfz/X19fHaTQaTiaTcZ2dndzHPvYxbmlpaVXvw3H3xngux3o3pm/lsavVatyf/umfcn19fZxMJuNcLhf3m7/5m1wymRTG6AZYy/y6k8b0rTymwry7NVa7T3zqU5/ihoeHOYPBwEmlUs7pdHLvete7uOPHj6/qfTju3hjP5biTxvStOk4f/vCHV/X5CAsLC9z73/9+zmQycXK5nNu9e/cNG6u3+thx3Orv2Xq9zn3pS1/iDhw4wBmNRk6v13N79+7l/s//+T9cpVJZ1XvdC+NZqVS4T3/601xHRwcnl8u5gYEB7q//+q9X9bd8iP7/xQgQIECAAAECBAgQIEDAhqC17MEFCBAgQIAAAQIECBCw5SEEIQIECBAgQIAAAQIECNhQCEGIAAECBAgQIECAAAECNhRCECJAgAABAgQIECBAgIANhRCECBAgQIAAAQIECBAgYEMhBCECBAgQIECAAAECBAjYUAhBiAABAgQIECBAgAABAjYUt+WYzgffcr5VYDAY4HA4IJfLYbVaIZPJMDExgfn5+bv2nrdjt9KKY7cZWOvYrfe4SaVSdHR0wGg0QiwWQyKRIJvNYmZmBqVSaV3faz0hzLnbR6uNnUgkYq/faDQAABKJBGKxGC6XC93d3XA4HHj88cdht9tht9uh1+vxwgsv4K//+q+RTqeRzWZRq9Xu2jUSWm3s2gmtOnZWqxUHDx6EVqtFLBZDNptFNBrF4uIim4+bjVYdu7sBqVQKm80GtVqNnp4e9PT0IBwO48yZM8jlcigUCqhWq6t+vc3eY+8WpFIpJBIJXC4X9u7dC4vFgkceeQQejwfd3d3o6upCKpXC1NQUYrEYvvvd7+Ly5csIhUIIBAK3HJdWnnMikQhqtRpvf/vbMTw8DLvdDp/Ph/n5efzN3/wNZmdnUSgUUC6XN+R6lmO1Y3fHQUgrQiwWQyaTQalUQqlUQiaTQSaTQSQS3dakErD1IRKJIBaLIZVKIZVKIZfLIZfLr9uAaf7Qv41G47rH7nXQIkwH6+WLMsdxTT8CXt9MZTIZgNfHRyQSsSBEpVKxH5lMBqlUyoJl/r/tcnBoB9BaAFybr41Go2UO5HcDYrGYfW65XA6JRLLZl3TPQiQSsTVBLpdDoVCwn0ql0tLJsY0EzVWlUgmFQgGlUskCE/4+JJFIIJFI2HPkcjnEYnHT/t2u4H8+iUTSNG8oAOGPBaFV9t8tGYS4XC489thjMJlM6O/vh0qlwve+9z0kEglUKhXk8/mWGHwBrQGxWAyDwQCr1Qqv14vu7m7kcjnMzMygWCyy5zUaDdTrddTrdZaJisfjyGazKBaLyGaz9+S8okMx/xBDB2aj0QiNRsM21Vqthkwmg0qlglgshkwm0zKL4WaAgrTt27dj3759UCqV0Ol0kEgkUKvVkMvlMJvNcDgcUKvV8Hg8UKlUUCgUkMvlcDqdcLlckMlkKJVKa8qOCrgeyzOrUqkUsVgMxWIRgUAAfr9/S85VmUwGs9kMs9mMzs5OyGQynD9/vqUqIfcKRCIRlEoldu7cCZ/Ph127dmHXrl0IhUJwuVyIRqM4fvw45ufnt+RcXA3oPt25cye2bduGjo4O3H///dDr9XC73dBqtdDpdKxa0NHRAZvNBpFIhIcffhgvvfQSnnvuORSLReRyOdTr9c3+SLcFkUgEq9WKjo4OVjHX6/V44xvfiMXFRVy4cAFTU1NQKBTQaDSQSCQs2ZVMJpFOp9f8fus957ZkEGIymbBr1y44nU7s3bsXWq0WIyMjOHnyJPL5PAqFwj178wq4HmKxGGq1GgaDAZ2dndi/fz/y+TxMJlNTxqler6NSqaBWqyGRSKBUKkEmk7GsSy6XuyfnFQUelIWRy+UwGo1QKBTweDwwm83s8XK5jFAohEKhwDaArZCNul1QENLR0YHHHnsMWq0WdrudBSP0L1EFCZSZN5lMsFqtqNVqCAaDm/hJtgZok3Y6nTh06BDkcjnm5+eRTqdRKpWwtLS0JeerVCqFVquFyWSCy+WC0WhEOBxumnMCNg4KhQI9PT0YHBzEgQMHcODAAQSDQXAch6WlJVy5cgXz8/P3JLuDWAtyuRzd3d2477770N3djUcffRRqtbqpCgK8PpY2mw0cx8FsNqNcLiMej+PEiRMQi8UoFAptGYRQxVyr1cJms8HhcMDj8UCpVGJ4eBgWiwWBQABzc3NQqVQwGo2QSqVQKpUQi8Uol8trCkL41ZT1nHNbMgip1WqMNykSiSCTyWC1WtHV1YVoNIp0Oi1kdwQwiEQiaDQamEwm2O12uFwuVCoVKJVKVKvVJuoQVUNyuRyq1Sr6+/uRTCYxMTGBEydOoFQqoVKpbPmNgUrASqUSg4ODrAeLKJBmsxlyuRwWi4VlYKRSKaseFQoF6HQ6TE5OIpfLIRaLoV6vb/lxWw76vMFgEGfPnoXFYsGOHTug1+tZeT0ajWJpaQkymQwWiwUymQz5fB7FYhELCwsIBoOIx+OoVCqb/GnaH0TlValUcDgc0Gq1MBgMKJfLjAqXyWSwuLiISqWyZQIS2if5lTeDwSAEIRsMOmDTIZvoRQCgUqnQ2dkJuVwOk8kElUqFarV6z1Q/aQ/RaDQYGBiAxWLBvn37MDg4CLvdDplMdlNaKsdxqFarjM5WKBRQKpXa+izYaDQQj8exsLAAlUqFRqMBpVKJzs5OaLVaFAoFeDweqNVqGI1GNob1eh2vvPIKYrEYO9OsBGIw0PogFotRqVTWtc9kSwYhpVIJ8XgcKpUKwLWbd9++fZiamsLc3Nw9c+MKuDUkEgnMZjPcbje6urqwbdu2Jr5orVZjWQepVNoUkOTzeZRKJfz4xz/G+Pg4UqkU6vX6hjQIbybEYjGUSiVMJhPe9ra3Yd++fawErlKp4HQ6Ga+cf5BpNBooFosoFAqw2WwwGAxYWFhANptFuVy+5wIRmkvj4+MIBoPw+XyQy+VsU5XJZPD7/ZiYmIBGo8GePXug1WqxtLSEeDyOkZERjI+PtzWloJUgkUjYQby/vx9WqxVarRYymQwdHR3o6OjA9PQ00uk00un0lkk4iMViRtnwer3o6emB0+lk9287H9TaDdTnpVarodVqoVAoALwuuLNr1y64XC54PB7o9Xrk8/l75iwjlUqh0+ngdDrxzne+E319fRgaGkJPTw87XN+sL47jOJTLZeTzeeRyOWQymbYPQmq1GgtANBoN9u/fD41Gg71796JWq2HPnj0oFArs3haJRGg0GiiXyygUCrh8+fJNkymUaKT5SEkYIQi5BegQSJsyv2QViUQYh32zJ59EIhEadFsAlCEpFouoVCqo1+ssEyUSiVCv19FoNK47UANgzW5arRZyuZxlC+4FUCBWLpdRLBabNgCqlCgUCsZBBV4PQqj53+FwwOv1olwuw2w2o1gsIpPJsOrTvYRKpYJcLodUKoVgMIhqtQqxWIxsNotQKIRQKASj0YhCocD6FPx+P6LRKKMICrhzcBzH9o9isYhyuQyLxQKVSgWr1Qq3241MJgOVSoVSqcTWhq0AvpDEvXT/UeWBxGxWEtPgc+lLpRJqtRrK5fK6N4hTEKxSqaDT6WAwGFgQIhKJIJfLWeMxUYG3MkQiEet/I9VTt9sNt9sNh8MBvV4PhUKxKlEOCkJIMWqr3Ls0DykYpbOLVCqFwWBgjfg0txuNBkqlEquy3Sp5RfcDCXMIPSGrAMdxKJVKKJfLaDQaEIvF6OjoYAfKV155hS0im5k91Gg0AMAoPAI2B5RNqNVqsNvt6OvrY9x8UtG4UZmXgg6dTger1QqO45DNZrf899loNFCpVJDNZnHy5EnMzc2x4Mvj8eDw4cOw2Wzwer0wm81NjetUIbnvvvvQ09ODq1evwmw2IxqN4syZM4jH46jValtig1gtKPhdWFjA97//fbZp0GGD+mu6u7tRqVRw7NgxnDhxAvF4fNMkGLciyuUyarUaAoEALl68CJfLxRq2BwYG4HA4YDKZcO7cOSiVSgQCgS2Tiab7N5fLIZFIsB63rR6QaDQaqNVq2Gw2bNu2bcVkkkajgdvtRqPRwOTkJKLRKGZnZzE+Pr6u46NUKuFwOODz+TA8PIwDBw5AoVCwwx/9q1QqodfrUalUtmzQSIms3t5eeL1eDA0N4Y1vfCOMRiN6enqg1WqhUqlWrQpYr9cRCoUQDAabkjftPHYUWFF1gj4LnVfUajXrAaH5TPc0VU9EIhGKxeINx4ESM/l8Ho1GY93Xuy0ZhPArITSwVAmhhlm5XL7pB0W1Wr1iv4GAjQXRqpLJJPup1WrQ6/XgOK6pAnIjuVniTJIU9L0ACkQikQijpVDzfiQSgUgkgslkYmVcKpcT75n6RQqFArxeLyv5Us/WvRSEEO2vVquhUCiwx2kM7XY7tFot8vk8lEolwuEwZmdnWaJFwPqAsqPFYhHRaBQKhQK1Wg0SiYRRDYmiRTSFrQK6N+ke5jMJ2vmgdjNQdUGj0cBoNDL1OUqU0Oc2GAzo6uoCx3GoVCqQyWSIx+Pr36T7/5UFqUfRbDY3ZaDpvagaspXmH9BcjaPqk8lkgtPpRHd3N3bt2sUqRPwK+81A41atVpHL5ZBMJlEoFLZMPxfNj+X7AF+6d/njVPmj9U0mk7HzMt9ygM4yNH53o3q0JYOQSqWCZDIJvV7PSubUHNvR0YGBgQFEIhFMTU2tWaJsPfH000+j0WggFAohkUhgfn4ely5d2jKZtXYBZUjS6TRqtRpmZ2eh0WjgdDqh0Wiwfft2eDweWCwWuN1uiEQiVCoVVCoVjIyMYGZmBufPn8fs7CxyudymB7cbAQqaa7UaIpFIk9QucW31ej0GBwfhcrmY8o5CoWCJAIvFAr1ej+7ubohEIgSDQaRSKczNzWFhYQGhUGizP+amgrJZPT09ePzxxyGTyTA9PY2xsTHMz88zVZetsJG2Cmgs4/E4Lly4gFgshkceeQSNRqNp81ar1dBoNKxpeCtBpVJBr9dDr9czmdNisbjlEmR0yN27dy/2798Pp9OJHTt2QKFQNGWOATA6UKPRgEajQTKZRLVaxeXLl1GtVtcto05KTlarlVXh6TDIT4RJpVIoFIotM/+IPmSxWJgcud1uZ31wvb29cDqdTPBkNcEXJXVISCKZTOKnP/0pJicnMTk52bLJGwrCKCAFgGKxyM6FNM9oThBNje7V1cLtdmPPnj1N8vnRaBTFYpGZ31YqFbbPEDtBoGOtApVKBel0mvkR1Ot1ll1wOp3o6elhpfTNDELe9773oV6vY3p6Gn6/H2fOnMHo6KgQhGwwSGECAJaWlnD69GkolUpYLBYYDAY89dRTGB4eRm9vL1wuF8uGUWPXiRMnMD09jaWlpXsiACFQ9j6RSDQ9Hg6HMT09DZlMhh07drAAzufzQavVsn/VajXrC3E4HAgGgxgfH4dMJkMmk7nngxDKZPl8PrzxjW9EOp3Gs88+i6WlJSwtLQmGZXcJHMchnU5jdHQUuVwO2WyWHVhIjlqpVEKlUm25TDTwOiVIq9UymlK9Xt906vJ6g+4tuVyO7du348iRI7DZbBgYGIBcLr/h39XrdZjNZhQKBVy9ehUKhYKtg+sBkjc3Go0s00+HzeVVEOptbXdQNUqhUMDhcGDnzp0wGo3Ytm0bDAYDhoeHWfP5aqsfABjlPplMYmxsDKFQCMeOHcPo6Chrxm5FUBAik8mg1WohEolYQLUcRAW3WCzsubcCBbV2ux2Dg4PQaDSw2+2o1WqYnJxEMpnE0tISgsEgcrkc8vn8dcyi9cSWDEJqtRqy2SxTjqjX6+yLJenFWq3GGr42C9PT0wBeX1S6urqwtLQEu92OTCaDTCZz3aSj7AyptdD/12o1VvHZapvFRoPvjEyZfaVSCYPBAACIxWIolUqYnZ1FKpXC6Ogo5ufnmdSdgGtjWK/XkUwmIRKJkM1mkcvlWBmd9NoJdLjTarXQ6/Wsie5ezfITXU2n00GtViORSCAejyMcDiMSiTRRtu518Hu26FBWrVbv6H7UarXo6emBz+eDWq1uEjLhU2i3yvyke48aocn3h1SzPB4PALBmVhrzarWKYDDIMrXtJJBA9NFMJoNwOAypVIpyudwkS7oSqAJMh7n1auanw7jZbGa+QDSe9Pokj5rP55miYLuAxkmhUEClUkGpVLKKj81mg16vh8vlQk9PD1NpU6vVTK78VgEXfZ/JZBKlUgmhUAjhcBixWAyjo6NIJBJIpVLsTNiqoL3QYrFgz549UCqV7P7iVyLo3ty3bx96e3vhcDhWHZSKRCI4HA5s27aN0S8pgUpqodSy4PF4UK1WEYlEkMvlmBCNTCZjMt7pdJpVS9c6tlsyCCE+r06nQ6FQQKVSYZPYaDRiaGgIWq0W//Zv/7ap1/nSSy9BLpfj8OHD2Lt3L0qlEs6fP49oNIrJyUlks1n2XJLtIw1og8HAMjn5fB6hUIiZ8AgHlNsHLWR87qPFYoHX60WpVMLk5CRCoRDLSE9NTSEQCNwTsrxrAXFIFxYW4Pf72fwl11qPx4OhoSEA1zJ9CoUCTqcTlUoFo6OjTTLJAJoWWD5vdStCKpWip6cH3d3dMBqNmJ6eRiQSwdjYGILBoDDXeCAVNvoXAKMErhV0UHI4HHjkkUfg9XphtVpZJpqfnNhKSQe5XM6UhygQoUSXVqvF0NAQ9Ho9zGYz9Ho9o4rE43G88MILCAQCKybOWhV8FTQy/yuXy+ju7ka9XodWq70h1alSqbAD12oOx6sBBdE6nQ4dHR1wu90Qi8UoFousUb5er6NYLCKbzSIajSIYDLaV8TKNlcVigdPphMPhwP333w+LxYJt27bB5XIx02CiPfKNcG8Guh+LxSLGxsYQiURw5swZnDlzBvl8HpFIBOVyGalUCqVSqaXHjKh2PT09+NCHPgSbzcbGgt83TOPicrlYcm81lVkKsoeGhtDd3Y25uTkcP34cuVwOAJqo0iqVChaLBdVqFSdOnMDi4iLC4TCCwSB0Oh22b98OuVzO9iXyX1nT5137ELU+qCJAClnlcpmVWElrWqfTsS92szJa4XCYRbmkZOByuSAWixGJRNiEI1lThULBnkOmZVKpFLlcDmKxmOlfC0HI7YMyfETHonEmbillV+hfclLeiuDTAG73wLX8UEK8VWr6rNfrqFarTDoRADOL4z+Xn+Wme5qfkSRsheCEPqtGo4Ferwfweo9CIpFgSRUB10CVSrlczsZLoVCwTPFaDh38bC1lpOkwyq/wVSqVO662tBLo0Mc3fKO1UCKRsM9NqoH0//yxaUc0Gg3kcjlEo1GYzWZks1m2/q8UhNBBN5fLMY+J9fjs5HNBqldarZaNO39NI1oOme61Ykaf5g6f8kbrGT/Ytdvt8Hg8MJvNcDqdsNlsjObIl0leDcWoWq2y6lA4HGZ0okgkwnocyNix1ecqP8lB6wyJYvDHlu5Ng8HAVK5uBP5ZktgyJEGeSqWQTCaRz+cZTY1sB0iwoVarwWw2M3qWQqGAUqmEWq2+Y4GELRmE8BvTA4EATCYTK+3pdDr09fWxBjBqXt+Mg+Tx48ehVCrR3d0Nm80GnU6HX/7lX0Y8Hsfzzz+PQCCAZDKJVCoFrVYLq9UKq9WKd73rXejt7WWbRDabxdzcHGKxGL7xjW8gmUxu+GfZKqA50tvbiw984AOw2WxoNBq4cuUKRkZGcOzYMeRyOUZBoPJ9qy9stwPipHIcxxzi7xRKpRJdXV3o6emBVCpFPB5HJBLB3NwcSqUSstksJBIJ+vr62IbsdrsZPYKM/SYmJpDP55mcL21atEFTgNJu4Dt2W61WeL1ejI+P4+zZs8jn85vaw9ZqoO+8t7cXBw8ehNVqxfDwMGQyGS5evIiFhQVMTk7i/Pnzq6ZgUObVaDSiv7+fiSoAYLTXbDaLYDDIMn9bBeSYTvcZjUMul8OlS5cgl8sxMDCAPXv2YGJiAsePH0csFkMwGGSKgu0EOuCPj48z6VZqjN63bx+USmXT80kOdXR0FJOTk5iYmEAul7tjOXGRSASj0QiTyYTu7m5s374dVqsVGo2mKUFKNPNMJoN8Pt8yYgEUbNB/i0QiaDQaRu/r7u6GVqvF9u3bYbPZYLPZYLfboVarGR2LfLZo7vGrjjfz3qLnhkIhnDlzBtFoFC+99BIWFxcRjUYRjUZZ0qBdFBdprZqYmMBXvvIVWK1WPPHEE+jr64PVaoXFYmHjRBS+W1Xj8vk8YrEYstksxsfHkUwmsbi4iGAwiEQigdnZWYjFYnR2drJ+JJvNxhLfjUYD/f39sNls0Gq1qNVqTRK/FJzfzhqwJYMQqoQUCgVks9km3wZq/Mpms1Cr1VAoFJvWCB4IBKBSqRCNRhGPx2GxWNDV1YVEIoGxsTEAYPxb4ks6nU5s374dO3bsYJFvOp2GVqtFMBhkmcC7ieUZCr5aA///Ce10QJfJZNBoNLBYLNi+fTvsdjvGxsawtLSEubk5XLp0iXkJkAfNnVYLWhWUxeI4bl0oB7RZ6XQ6GI1GSCQSdqgLhUJs8RWLxTCbzRCJRLBYLOjt7WXXQXLK4XAYIpEIuVyOlfnFYjHTSidKXTvNPQLJKlKmqVQqYW5urqWbKTcTer0eHR0d8Hg82L9/P5RKJSqVCuRyOdLpNCQSCcsC3mo+UIaR9gmDwcAy4pRNrFarKBQKKBQKbXfwvhEoq0qHyUajwcz7CoUCEokEO2DabDbMzc0hHo8jFoshn8+3VW8CHxzHsUSfyWRq6g1Z6bm1Wg2xWIwlCNerGkYJF6PRCKvVCpPJdF0TNvWEUHZ8Mz0u+Fl3mjv8LD0l8ywWCzo6OmAymbB7925Gb7Tb7Szhwk/i8ddtureWK5XxQXtCLpfDwsICgsEgJiYmsLCwwJgw7da7RcFSMpnElStXYDabsXfvXrjdbtRqNRZ00Ly7WRWC1rxSqYR0Oo1kMonZ2VlEIhFMTEwwRc94PM5oWNQDp1KpmiqjtBaGw2FotVrWO8ZXyryde2FLBiE0IOVyGdFoFH6/Hw6HAwCYF4HBYEBfXx9yuRymp6eRz+c35Tqr1Srjv+/evRs+nw9msxlveMMbkMlkkM1mUSgUoFAoWJnW4XA0LQLZbBajo6N3Xe2LMhcqlQputxtarRalUgnFYrHpwEo0CGqio14VOmS28oLgcrkwPDwMp9OJeDyOXC6H48eP4+rVq1hcXGSVD7PZDJlMxhqHc7kck/iljEC7ZF5uBNJVXw/1F37zZqFQYMGDXq9nY8SfG6lUilVFZDIZyuUyJiYmkEwmMT4+Dr/fD61Wi71790KtVsPj8UCv1yMYDGJhYQHJZBKjo6NtSU2kfporV64gFothfn6+7U217gZoPJaWlnDmzBkkEglWCenp6YHVakU+n8e5c+eYytWt5jGf5kAUGToAESXWbDZjx44dMJlMGB0dZdnWVshKrxb8+5GSDGq1mhnAKRQKWK1WDA0NIZVKQa1WAwBsNhsbExp/ajRut8Z0An0OiUTCFMGWU7FKpRJisRiSySSmp6cxMTGBWCy2LvekWCxm5oRULdDpdGyc+X4lRJmjH1It2ijI5XK4XC7WWE7u7rQfajQayGQyWCwWljWnXg9ao2l86/U6axRfWlpisrCZTAaNRoOt/aRMyadoAa+fn+bn5xEMBjE6OopXXnmFiXfQHtxuAQgftVoNuVwO9XodL774IsbGxjA4OIjh4WEYjUb09fUxCV8+KAlfrVYxMzODUCiEQCCA0dFRZLNZLCwsIJPJsEZ9OqcBYEqX5XIZSqUSqVQK586dY7RW+s5IiYvOPHdCTdySQQgdmkqlEsLhMDQaDQYGBgC8vtCoVCoYDAb09vayL3p+fn7DJysdNkg+TiaT4S1veQtMJhMeeeSRpn4VfvVheVaAghC/339XgxCNRsOyGgcOHIDdbkcqlUI6nYZUKmVyjuFwmKkhZbNZpNNp1kBHWepWhcPhwIEDByCXy5FIJFAsFnHy5EmcPn2aXb9KpYLRaGQLrMViQSwWY9kXKtu3O2ec7o31AH/eLg9CaDyBa83nRDfIZDKsAXN8fByzs7MIBoMIhULo7e3Fnj174HQ6ceDAAbjdbly+fBnnzp3D3NwcZmdn2zoIGRsbw9jYWNse7jYCRMWgXpl0Os0clZVKJfx+P0wmE0Qi0aoqF8uN0ijDC1yrDJpMJmzfvh1Go5F52wBo6XVtOSi7SVlMqVTKKDTUlG6z2TA0NIR0Og21Wo1arQaLxXLDIGQ9pWo3EnzhC7VaDbVafV12mc4SkUgEMzMzmJycRCqVWpczg0gkgs/nw4EDB9DV1cUSfcsTdnzzOQqQN3o/lcvl8Pl8rHFZq9XCaDSiq6uLUUjVajVrPCcBA5pvfFSrVaTTaeRyOYyMjDCVSb/fz95LrVaz16eADLgmLLCwsICLFy8yqnQmk2n5ROdqUa/XkcvlkMvlcPToUYjFYhw6dAiZTIZVfpcHITQuVKl97bXXcOnSJUxOTuLUqVOs9WClMeI4jtGby+Uy66s7ceIE0uk082fRarXQ6XTs+6NEtBCErAAqnapUKhZR0s0gk8ngdruZgo/JZEKlUkE+n9/QCUylsmw2i0AggMuXL8NqtaKvr4815t7IrZui03g8jqWlJYRCobvCUVYqlaxU5/P5YDQaWdOmUqlkzZvEYTUajWyyFwoFZDIZGAwG5HI5zM7OIhqNtlyVgA4cZrMZPp8P5XIZ4XCYqWkQ9YroRDt37oTFYoHdbofRaEQmk4HP50OpVGINxLOzs1haWmrb/oQ7BUliU4ZMr9fD5/PB5/PB7XY3NdORUzPxnKkBLpVKIRgMIpPJMElH4quTmzBfWlCn08HpdILjOOzatQuJRIKZKdZqtZZv6uZzoQmtdJ+0IqiizM98Um+HWq2GyWRCo9FALBa75WtR8LES/YPWYq1Wi+7ubhgMBpRKJXR3dyMSibD1l2g6rXwQosTW8nnGP9CSUlM+n4dGo2Gmv7lcDo1Gg5mO2mw21Ot1zMzMYHFxEdVqFcVisaU/Px+0vxmNRlgslhWpULlcjmWVSYFyPdYSqq6ZTCa4XC6YTKam/goATawCvjKSXC7f8LVBoVCgq6sLXq8XBoOBHUgdDgczdFQoFNBoNEypjpJxJDNbLBaZytf8/Dyy2SwmJiaYYXAsFoNUKmX9mACaghg6M1UqFYRCoSaPrq3igr4c9LkoKadWq9kaw6eykYDO5cuXGa2f6FeUGF0egJCqIDmoS6XS67xoqN+y0WiwniSS6NXpdFhcXIRcLr8tldAtHYQUi0VcuXIFoVAIe/fuRbVaZZuMSqXCwYMHMTQ0hEKhwJrrZmdnN/SgwnEcUqkUMpkMTp8+jWq1Cp/Phw9/+MPo7e1lk2MlJBIJhEIhXL16FWfOnEE4HEYmk1nX6yNevs1mQ39/Px599FHo9Xo4HA5oNBpWliUX4eUZD6Jnzc7OIh6P4xvf+AZOnDjBZA5bAXSwUCqV6O/vx0MPPYTFxUW8+uqr8Pv9rERJ6iUdHR344Ac/iP7+fqZMwVfNoQawb37zm/jXf/1XlEolRmu6V0AygBSkUcOl2+3GY489BofDAYVC0UQLqVQqrOqRTCaRTqcRjUYxNjbGsrBut5tlhyQSCYrFIhKJBLtnyW03n89jYGAA6XQazz33HEZGRpDNZpFIJFr6e6AqyPLHBNwYlLnjZ/jocGe1WtHf3w+tVotAIHDLyhjtDXSA4m/yxE13uVx44oknUKvV8OSTT6JareL48eN46aWXEAqFcPr0aZaRbdUAcnkPG1Vv+QddMhmlta1er2NpaQmBQACNRgO9vb3QaDQ4dOgQDAYDvvWtb+G5555DKpWC3+9vi8SLSCSCyWRivWc7duyAxWKBRqMBcO37D4fDeO655xAIBJgE7J1+t3z6V29vLw4cOMD2UgCs2pTNZhGJRNi1kMcZ7TsbuY/qdDq8+c1vxp49e2A0GqHT6Vhyju/XQ9UaMgsslUpYWFhAKpVCIBDA/Pw8otEoLl26xGjndP/W63XGNCC5Xn5lqlarIZlMIpvN4ty5c/jxj3+MXC7HsvFbEfS5gsEgzp49i3K5jGw2C7PZzNYn6lWbnZ3FV77yFUxNTSEUCiGRSDA1tZXoaUqlEjabDXK5HDqdjvUi0twkN3vqhU0mk4jH4/B6vRgeHoZGo0EkEmGtA3xridVgSwchjUYDhUIBUqkUpVKJqehQpoy0wOmQzXEcK79v5MZPVYFsNoulpSVIpVJEIhEYDAa2IVLmgw53ANgBLJlMNjlbrjf4Uq2kPpTP51nUSwcnviQfNdZSlok4hg6HA1arlbnZt8KiQQoTpPZAcnjLm+PoO6CMvNPpZOV7eh0KdKkxj78w3yuHSZoDcrmcVYucTifcbjccDgejYAHXsjckn0gZP1rMMpkM0uk0OI5jPGkArDGP3KxTqRRSqRRrqNVoNCwgMRqNUKvVbdM8e6N5QvOUEilSqZSNGVUD7pU5xgdREGgMaBxovIxGI/L5PFQqFQqFwnU8e7rP+QIMdEhfaX2itY3mWqPRgMVigdFoRLFYZImjVheroP2O78XATyLR4aPRaKBUKrEesUgkwrjk1F9ps9lgNBqh0WiY5Hy7gNSZ1Go1S6rRPKD1qVAosEb821UBWg6SgqYDn0ajuc4kcbnYC9Gw6OdOpFFvByTaoFKp2A8fNOep+lEoFJhMLiV6/X4/FhcXkUgkEAwGWQDCH1OVSsWy8cs/I70u7Q2pVIpJtm810P1IZ0C1Ws3OVMurtdSCQMItpHx1K0o13f8kREFVLNpj+H1xxDwgOV96Pt03dA5cyz60pYOQWq2GRCLBKEvUYEfldpVKBblcjgMHDkCr1WJkZAR+vx/xeHxTDsjpdBrj4+NYWlpCuVxmcmxGoxEulwu7du1ifQhKpRITExP42c9+hrm5ubt2IxJPkAx/ZmZm2GGcFkSarLSIms1mqNVq7NmzB4cOHYJcLofT6YTFYsG73vUuDA8P4/Tp03j++edbgh4jkUjgcDhgt9tZRkCj0cDtdqPRaGBpaQnZbBZarRYmkwlut5tlasg1mK+nT1WeSqXSpKLFRzs3zN0IFKySIajZbMbhw4exfft2FpRRpo/kjSuVCgqFAmtwo/kQjUYxMzODRCKBpaUl1mRHhwK+5KJcLkcwGITdbkdPTw+2bdvGvr9arQav18tocZFIpKW5+/zmy+X3slarZQ3RAwMD8Hq9mJ+fx8WLF5HJZDA7O7vmLNRWAFUgc7kc/H4/pFIpDAYDDAYDHA4HDh48CI/Hg0QigVgsxg7MdFiiLC2pPBGNNJ1OQ6VSwWazNb0fvU+5XEYul0OlUkE8HofZbEa9Xmdy8EQDbEXQ2kMGhB0dHXC5XLDb7exgyReJOH78OCKRCF599VWMjY2x+9blcsHtdqOnpwfA68IeRHtuB5A8rtfrZZKkfDpeLBZDJBLB1NQU5ubmWP/ReoAM6ah/4kZSq8uTLyqVCiaTCXq9fsMl4ovFIkZHR5lvBF+NkzwoarUa0uk00uk04vE4Ll++jHw+z85i1BNYqVSQSqVWbK5XKBTo6Ohgxsx8FAoFXLp0CYuLi0zdaSvSsEgin85SbrcbHR0d6O3tZYaPSqWSBWnU27ywsIBQKIRYLLaq8xUFjjKZDNu3b4fL5UJvby/0ej0MBgPzCeFXU+r1OjPXNBqNcLvdyOfzWFpaWnOv1JYOQqhUSZFzqVRizVzANaUJj8fDDkYUgd+p9vftgMrhxF+nBi+LxYL+/n44HA5Uq1VYLBYoFArEYjFMTk4iHA6zUubdAHE4KYvBBx2YiI5FAQcdOAcGBlimTCKRMNnbRCIBpVLZEhkzqoqZzWZoNBqWySfJRFKMob4GqpQoFIqmBZSyq3TD0u8oK7ucprbVQIGYSqWCx+OBy+XC3r17sX//fgDN2WVSESsUCqz3g8aJZHjj8Tgru1MigV+y5xumVatVdrCkhkidTgcAMBqN0Ov1iMfj130PrQS6Nn42nj9PSJnG6XRi3759GBoawsjICGKxGKLRKAKBwD1VcSPQpkhrZzqdZpuvVquF2+2GSCSCy+WCTCaDz+eDw+FgcrtE5ZBKpayfiww0lyd2iDYYi8VYoEKNmVQV1el0qNfrLe/XJBKJoFQqWX+fTqdjiRUCqdlNT09jfn4e586dw8jICDv0EVXIbDYDQJOpWjuAxoBotfysL332aDSKWCzGaNPrJekvlUphtVrhcDiYOeFK40ZZcFofqtVqE01mI1Gr1RCJRFhV0Wg0snWdb4BHwRv5d5CJ8mqTjpRIMJvNjBpJqFarCAaDmJ+fb6LibjXQXkq+ZX19fejv78f27duhUChYcMrvHSJGwFpNqylRarPZ4PV62bhT1Ysf7PDFkuj3er0eJpMJ6XR6zff+lg5CgGvNdtFoFNPT03A4HDCZTE1lZ7VaDZvNBo/Hgx07dsBoNLIs7GZdM79EmUgkIJPJEA6H0Wg04Ha7mXoT8Sw3K7tLk5EOAbVaDdFoFJlMBufOnQPHcfD5fDh8+DAr18tkMuzYsQOPP/54SywgFKTSmHMcB41Gg507d8Lr9cJoNCIUCjEJS6fTCYPBcF3WulKpNDXZhUIh5mq71TI1FKhJpVLo9XrWK9Pb28skTClDQlQ+CjAoo3L16lXWmBiLxRi3n8QilpaWWPC7kvQxLYZExyqXy5ienmbGnmKxmElb79q1C41GA3Nzc5vmC3QzkFMwNXeKRCLWqEkgNTGz2QybzQan08mCXb/fj/n5eaTT6ZYTfVhPEB2VlIyIriGVSuH1elnwS9l8rVYLr9fL1Fzy+TzL4PKpNg6HA+l0GleuXGHV3oWFBZTLZSbGQcmY2dlZHD16lPUtEVWJaAqLi4soFoubamQolUoZXZRoM/T5AcBqtUKr1cLj8aCzsxMdHR1MCIXWtGAwiKmpKQQCAZw+fRqBQOC6Xgi6x2juEkWoHYIQqui73W7s3LkTHR0dTQ7w1ANz4cIFTE5OMnW19VLEUigUcLvd6Orqui7bD1yTLKekFjUQE8V3OXVrI0BViIWFBTa3gGYaFl9YhPo9+L1GqwEdcJfTsWg8UqkU4vF4y/SVrhdI/t9kMjHVR5PJhJ07dzKmhl6vb6JJ0RwgGem1ngcpGUPVzUKhwFzTHQ4H7r//fsTjcZw+fRp+v59V4zo6OliC1mq1olQqIZVKCUHIctBiEg6HcfXqVVSrVfT19TGTHADMnr6zsxN79uyBzWZDIpHYtCCkXq+zwwdlb6VSKfbt2wfg9YVArVYjFouxRvrNbAKkMaYFgTa6dDqNsbEx7Ny5E9u2bWM3mEajYWXkVmhe5DgOxWIR6XSaVc60Wi327duHWq2G4eFh5oXCVzPhZwwpO59OpzE6OoqZmRn4/X4W1Kz0nu0MaqpUKpXo7OxkstJHjhxhlEHyjeFLTBN9rVAoYGRkBCdOnEAmk0E4HGZNjJVKpcloCrj5eNXr9aYqR6lUYhQaytC43W5ks1mcP3++JXtDKCOt1WrR2dkJsVjM5iSBsoMWiwUulwterxcmkwkdHR2YmprCiy++CL/f35JB1kq4ncogHXqpl4+SGhQEd3Z2MuU14NraTo6/RN8j6mS9XkepVGJStCqVCqVSCTKZDNPT08hms03rbjwex+TkJH70ox8hHA4jnU43zSea7xstnboccrkcnZ2dsNvtsFqtMJvNKBQKCIfDEIvF2LFjBxwOB3w+H3p6elhvB39NW1xcxIsvvojFxUX8/Oc/Rzgcvq46R5UEMv7djD6F2wEZ5SmVSnR1dWH//v2wWq2sKZzmxsLCAk6dOoVgMLhuPZe0TikUCvh8PgwMDDBzVgLNH6Iakvkm9atQ4/dG097y+TxOnz59nWEx/7qX//ftJET438/yDHy1WmWqh+0owX4zkBnowMAAtm/fjl/6pV+C1WqFwWBgzJEbHfLp/qaE3mpBNDo+VY6CEJfLhcceewypVAqxWAy5XA5GoxEejwdut5v5vhD7IBQKCUHISiBJt0wmg0KhwDIM/MMRLaZUpidDnc3OKvI3NboWygLz/7+VQIsPZQbT6TSCwSCjbFHJnjjUmw2aH0TLKJfLTfJ0dNChDBSfM0wbAnkUkGJZOBxuUsRqpaCDBAQom6ZQKG4qTQqg6fOTSpjFYmG68UajER0dHUxMgd/UuvyH5jMFG0TJooWQHIHXOq/pNYlOQ709RDXxeDzo6+tr6UM6ZZhkMlkT/5zjOJjNZiaLSSIVlUqFiVO0itDDaqBUKlnmndZjoswS+A7MlPlTqVSsb4FkQolW29HRwUQP+DKn9EPN4lRty+VyzAuI+r7K5TIsFgssFgtThyI6CPXFkUQr0UtoPrUa3ZLuaYvFgu7ubpTLZZjNZojFYnR1dTF3bvIGWX7vV6tVZLNZ5PP5m96PtC5QJnSjZe5vB5Rpp8ZwUkekai1VsmhNp4PZeoIa/qlqQBK2FOzkcjlGvaLgg8Z1M2mlG7XG0D3Pb86nfsBiscjGp51B+6lUKoXJZIJKpcLAwACGhobQ1dXVtJ7RGknyuDeiY2Wz2TWdq4j2pVAoGKODfjiOY+c2n8/HHgPAepapUX55ZWa1uGeCkFgshpmZGRgMBkaRWb7omkwm3HfffQiFQjh79izLZG9mWX056EYkWkorNzjTgX5ubg7Hjh2Dx+Nh1Dej0Yj+/v6WuPZ6vc5kdUnSjg6vpISzfPGnQw5l/iKRCMbGxrC4uIgzZ85gamoK2Wy2JT4fH0SVoI2NqgQk00eqP/xFTCaTMU8AapTT6/XweDzMP4YUXvR6PVtYgWsHFL78Jx06yR+HSut8LfzbHTfKBhkMBtjtdvh8PnR0dLAKnN1ub4nq241gtVrx5JNPwmQyse8HeH3jt9ls6OvrYz1XABAOh3H8+HEsLCwgFou1jUKW1WrFzp07mfRjtVrF9PQ0FhcXWfAhFouhVCpZfw8Fkvfffz8sFgv27t0Ll8vFnktGgisp6lAgQ9S1paUlTE5O4vjx48jlcohGo6hWqxgaGsKuXbvg8/lw8OBB6PV65k0TCARw7tw5XL16lSlEtSrNkq9UuGPHDhw5cgQAmEIkJQlo3PhrGoEaTSORyA1ps3xlLZLTpmbjVgZRG/V6PVPvI1O9Wq3GTCgnJycxNTW1KqPL1YKSn5VKBX6/HxKJhDWax2IxTExMIJfLYWFhAYVCAQcOHGB9dUTb4svibqU+MNpfKdFFPbrANQla8soIBAJtL8RB6n16vR6PPPIIPB4P9u7diz179jABAmIPFItFzMzMYHx8HCaTCfv27YNOp2PzIJlMYnx8fM1+cZR81+l0zGeEVMcoAJHJZDhy5AgqlUpTMlKj0TAKfjKZvK0q6D0RhABgfH0yQ6PNgx+1kflKqVRi2dxWWUz5FZEb/X+rgTLfRLNRq9VsM6NFphWum0q8HMexJulGo8GyA/xmYQBNi/5yn4tyucwarlsxS0PZPmrEJKlhpVLJ/l1+sKIsDWWvqYGVghAqFfPliFcCv2q3PGu8vJH/dsHvK6GNmmQEzWYzPB5PS1TfloPoF3Qgoe+FFNo4joPFYmGVEKKNVCoV1puwXnz1jQBV0mQyGfvuqcGef7ClDZDUrkhOnRooPR5PUzX7Rv0IfG49GWD6/X7Mzc2xiki9XkdPTw8Lpq1WKzPUrFQqyGQybLOlRFargirltVqNqezw1zC+0RkJtiyn91BzPj/7uRJofSSFsnYwKqTKmEKhYD+UVW40GsjlcqyqUygU1lV5kh9k01yl/iSqttFBO5/Ps2QWf8+hCg4FkFspECEsHyPaJ6iCfjfFeDYKFITQ3tTR0QGv1wu32w3g2mfO5XIol8uIx+OMFsk3Z6XzB/+Mu1rwExDUH0KsBAIZOdPc5VeaiSp4u9W5eyII4bjXZWbFYjHjhhMFhb/oymQymEwm1Ot1dgCIRCItUV6m0i0dsKgaQv/dyqjX66yMSNdKh8VWoY9QZen8+fP48pe/DKfTiYcffpgdsCm7SnOGPGZI0lGv12NgYAAqlQq9vb2sqTESiWz2R2vCQw89BJlMhn379qG/v58dzonqQmXf5UEWn7JFVDV6Pv2O+MuUyeJTYSjgp+w28Hrl0WazMUf69dhQaMEm5TjqD7Hb7dDr9fB6vZt+L68EOsDxD3EPP/ww60cAwBoFKRgEmhVRWn0dAK4dLBwOBw4dOsQkHsViMV599VWMjIxApVIxqp/BYGBceDooKhSKpjlK3iDLG3f5GyLJ8KbTaTz77LM4deoUEokEAoEAW0cpC07zl1Se5ubmkMlkcOrUKbz44otIpVItVR1fCdVqFXNzc4jH4/B4PMyJWyKRsN9R5lIqlcLtduPw4cMwmUxN9N8bBSF0L1Nzt1wuR7VaZY36rbKu3wgkJU9Vef7aVywWcfnyZczMzGB6eprtW+u1bqhUKiYL/Mgjj2BgYAA2mw0GgwEzMzO4dOkSUqkUWwfEYjGsVitL9CiVShw8eBBWqxUvv/wy/H4/owW24tq2VvCDNH5SgRQS4/F4E02wHUFB/+DgIN73vvfBbrez3iC6V5PJJK5evYpUKoXLly8zem4ul0N/fz/uv/9+1lfLca+7qYdCoTXvBfl8ngkNKJVKJuS0tLQEnU4HmUzGVLCol3p5Unb5d7WmsVjzX7QhOI5jhi3k3rkSf5oyb2RwZjKZbmn0slGgrBapT9ChrRV7QpaDn8Gga+UbGrUCaBzn5+dRKpXQ3d0Nt9sNu93O+MLkiUIBCVEaqGRJEsoOhwPJZLIlJTr7+/shl8vx0EMP4cCBA8x590Z89hstKnRIAa4ZKvEPc3w+P/81+NQZvncIvd6dbqIUnJNhk0wmQ6lUapIjbcWNmoym+Ie+vr6+Jh3+G/1dLpdrm6wgbVZ6vR5dXV2wWq3wer1QKBSsuqDT6eD1elm1hKp3EokE+XweoVCo6TUp+KUDNXA9Z75arSIajSISieC1117D8ePH2XwFriklEU2XAm7gda+IcDiM6elpjI+Ps0RQK4NPMZ2bm8PExATjnpdKJZw/f5716SkUCgwODuKhhx5ikqt0P95M2Y8fiEilUiZO0upjA7w+DykApooCcK2vLBAIYHJyEtFotEkgYz1AfH6S3t+1axebq7VajSle8g2Kif9P493d3Q29Xo/JyUlmmNkO475a8AMRAq3r1Du2VsWtVgL1lrpcLjz66KNwOp2wWq1NamOFQgEzMzOIRCL4+c9/jtnZWRbwq9VqFmgQxZmETNbaE0I+R8VikSXoM5kMkskkOI5jSpM32tP5yca2CkKo0ZWfRSFpsLtxMCVTqmw2i3g8zvhsfA1q2uiUSiXjk2cymZYpdfLVXCgL0ArXdStQc6TVamVZzHg8jitXrqBWq6G/v3+zL5GBJGEB4OWXX2YBCM1X2hiIqrFv3z4MDAwwNQmDwYChoSGo1Wpks1kEAoGWChRTqRTkcvktpZGXUxWXg7/g8JWvZDIZm6NEMaJsHi1gVDWhsc7lcmumYd3qnqxWq4jH45DL5SyjSJr/HMddZ0C32aBDRCgUwvPPPw+Hw4Hu7m6WIaW+BApQ6PCkUCgwMDAApVKJCxcuIBaLtcWaAIAdcGlT7e3tZTRN6sciEAUjm80yqXKJRILp6WnWPGyxWHDgwAHo9XpoNBp2sARenw+RSIQ1oPN9oEjuV6VSwel0oqurC3a7nfUyBQIBzM7OsgNpO1Sc+BTT6elpdoCle4+a63t7ezE4OIiOjg6WDCDqRzqdZlSk5XOKfAq0Wi2TOeaPDa3z/N7FVgCtP1qtFv39/ejq6oLNZoNIJGJnkGQyiUAggPn5+TUbr63lGvhyxul0GrlcDqFQCMlkkvHxpVIpO1iSUhSJpTQaDeYzks1mWUWwncE/6NLnp3WAGAckqGCz2ZDJZJoEYFoZfIPsbdu2obu7G3v27GmifdZqNQQCAeaDcvz4ccTjcSwuLiKVSqGjowM9PT3wer1sfctms8jlckgmk7dFHaR9ulwuIxgMssR7KpWCy+ViaqHLE7FKpRJWqxXAtST57ZxzNiUIIZlFmlQ6nQ7FYhHBYLCJq7qeoAUymUxiaWkJAFiWm0ATX6lUwuv1skPLZqlQ8EEBCGUDKMPbDjcfmax5vV6oVCrmQn7ixAlUKhW8+93v3uxLZCClklAohImJiesO21QOdzqdMBqN+NjHPsbUjEgv+/7770dfXx8WFhZw6dKlTTG+vBEikQiUSuWqeNu3CkSWCzvQBkmmcRT4V6tVaLVaRruhzB1tuul0ek1BCD/rcqNxLZVKzGmdNqlsNgu/3496vY7t27ev6r02CpTNmp+fx9e//nWoVCrs3r0bPp8Pvb292LNnDxqNBvMB6e/vZ0IPe/fuhclkwosvvsikZ1tlvt0I/E2LBA+Gh4exa9cuAGCHQuLFl8tl5PN5JJNJ1rA7OTkJjuMQDAaxuLiIbdu2wWq1wufzsWQBoVwuY3FxEfPz80gmk02HNYlEwnqdurq6sGPHDmYeVy6XMTk5iZGREeb/0Q5rLgWslUoFIyMjGB0dZb+TSqVs73M6nXjLW97CesQ4jkM6nUYymUQ8Hmfr4fLMKiXqLBYLarUa81iqVqusMgygiTLTCuNGFWyDwYB9+/Zhx44d8Hq9rKclHo8jGAxicnISY2Njd0UCltZAfhASj8exsLDAXNnT6TTzySDTVp1OB7VaDYlEwihkbrcbnZ2diMVi183rdgS/urZcopcqo/l8Hm63mwWIlLxuhfl1M9D3ptPpcPjwYRw5cgRWqxUej4cZ7pZKJYyNjeH48eOYnZ3Fz372M2Ze3Wg0MDQ0hD179qCjo4MFLssVOde6RlEQUiwWMTc3xyioGo0GfX19kEqlMBqN7Cyg0WigVqthsVhYfyK/arpWbGgQQlxbmUwGh8MBjUbDmgzz+TxMJhNrAKQmXyrRE++NL6+7FlB5mQxdbpUN5jfftBJoDOinlUFjqFarGdexXC4jEokgHA5jaWmpJcwKl4M/xsshFotRr9fZAhgMBjE7Owuj0diUNajX63A4HHC73cjn84jFYnfcdL0eyGQy7DtYWlpisoB8LXY+VuKCL/893aPFYpHxpwuFAjiOY3QP8mYAwPqDqKx+N+bASjrzcrmcfTetChpHotNQxlSlUrFACrjmTM1xHMsOms1mmM3mNbvlbiT4zdCJRAJyuZyZshL41TKiWiUSCSwuLrLGTKKfcdzryockv36jTZCkJ5e/F70fUYpItpWC6WKxyKgJ7RKALMfytYzjOFZtMhgMTYdbGgvqv1kOuv+pEqLT6dj97XQ6MTg4yAxC+VUVcpWnCsvtHljuFESF0uv1TOyAetRItID8otZbfICSNHq9Hj6fD263m/UdUeKG1k868wBggghU+eAnGvgVg60AfqM1NUnzxUz4vSK3Iwe7GaDr1Wg08Pl8LPCgQzydKSiIWFhYwOLiIsLhMBO4oaQ9eURZrVZIpVK2lubzedaLdSdrVL1eh0gkYj1vlPThU8uJ1UJn6EajwQQUbicxvqEzVy6XMx7m4cOH0dnZicHBQQwODqJYLLIm8CtXrrBD6uLiItuwSB+aPvhaFgj6csLhME6ePAmfz4cdO3bA5XKt+Py7UY25U1AgRQFaq2c7qYHW7XZj7969cDqdWFhYwOXLl3Hy5EkcPXq0JYOQm4GaNaPRKNLpNH76059icXERe/bswVvf+lZG6bDZbHj44YchlUoxNTWFo0ePsoziZs6rq1evQiaT4eWXX0YikcDu3btZ0+PyxX550M9xXFN5nLLZkUgE2WwW09PTGB0dhVwuh81mg1qtxu7du1mmkTj9s7OziMVimJ+fRzgcZgaRa8XNxpEORR6Ph4kIkLldq93XfND8qlaruHr1Kqanp3Hu3Dl2UCL60tve9jbcd9998Hq92LVrFzQaDQ4cOACtVouxsTFMT09v8idZGXT4DIVCePXVV+H1elnlgtZoChbJeycWi+HEiRN45ZVXkM/nEY/HV1RSKxQKN3SSbjQaTA56+ZpDlXmNRgONRsMoRrFYDEtLS5iYmMDY2BgymcyGjNHdhkwmw7Zt29DX14ehoSG4XC4mLiESiZgKm9FovO5vKSg2mUzo7++H2+2Gw+GA3W7HO97xDtx3333QarWM7phIJFAqlTA7O4u5uTn4/X688soryGazm6Iwptfr0d/fj+7ubvT09KCrq4tRstPpNK5cucKC3dtdl1YCyaJThfMXf/EX4XA44HK5IJPJkEgkMDk5iUAgwFgb1J80OzuLU6dOobu7G16vFxKJhB08+SqOG21ceDfApwZRNY6Uogh8OlurUOVvBgrKPR4PPvCBD6C/vx+9vb3w+XwskEqlUvjJT36CyclJXLp0CZcuXUK5XEahUIBEIkFnZydjWbz5zW+GSqVifSF8utZ6zFdKLJJseiKRYAEPAOzcuRP79+9HPp+Hz+eDXC7H7OwsRkdHEQwG13wNGxKE8L0VKMtCvD6v14uenh5UKhVWaisUCqwZkTIotDlTiZyaCvkHpOU/y0GDm0wmodVqb1q6bMUImybsavh3yxuFljce09/ezRuYKEparZbxh4kSEwqFEIvF2i4IAZqz/6FQCAqFgvFTATA5W5vNho6ODmSzWWg0mqYs92Yhm81CJpMhHA5jYWEBTqeTZT3oUMfPrPGNMflzhTLU/OxhMBhkZVy+ZC8doAGwbD5xoG+nyXc1c5YyT2Q0CaCpGtPKoI34RqIYcrkcoVAI0WiUmemp1WrGD19YWGjpzZkUiKLRKBQKBZLJJPR6Pcvu0tpPkpPZbBbRaBQLCwsol8ush4jmJFXbbtYjRz03JOzBh0h0zbiOxCZIl5/UaMjIcCuAhAGo34hfCaXfA810SwoSSaWMKm9USVUoFHA6ndBqtU1BCCmJ0V5ERsB8Se6NBCVC9Xo98zyie6VcLiORSCCVSt2xX9FKoMqsyWRCR0cHLBbLdZUQOuvwzzG5XA7xeBwWi6Vp3240Gk1y1q14Zlkr6HOReuZKlUt63lp7COnvNhokkW0wGNDR0YHe3l7YbDaoVCpW7SkWi6z3zO/3s8oX/b1er2+SJ6f9mQRNqBKyXp+P3ptvIEv3sMfjabIjqFaryGQyt60auCFBCJmx0GDH43G8+uqrmJ6eRjKZRDabhcFggNfrhUajwcGDB1EoFFjmiqJiompRF38ymWQLB/1LHNZkMnlDbvStuvjp4EROsq0AjUYDr9cLo9GIarXK6AHLQQc/vV7PjLuooYgmaDabRSKRYO6sdyM7LxKJ0NnZif3798PpdLIG5OPHj+PChQsIhUJt01h/I1BvSzabZQ7KLpcLTz75JDweD7q7u5kUo1wuRyQSwbFjx1hP0maAmkVnZ2eZDj4lBxKJBGq1GsvS6HQ6OJ1OiEQilrmkwCMWi+HKlSvIZrMsC0NUme7ubtx3331wuVxNPVfA6w3GoVAIgUAAyWTytuffzZ4vEomYozZxZ+m9SW7bYrGsffBaBPV6HaOjo2yN7OzshFgsxt69e9HV1YVEIoHp6Wm2kbciEokELl++jLm5OaRSKRgMBrZe63Q62O121Ot1+P1+ZDIZVokg9TP+IYSCNkowJRIJNu/oeSQfm06nWeKDKn4ajQb9/f3wer1wOByQyWRIJpMsc09yqa1M41stSAymt7cXe/fuhcfjYcEGrQ1+v5/R38hDZNu2bVAqldi5cye6u7vhcDjQ19cHtVoNp9MJhUKBhYUFzM/PM1drsVjM1M0oIePz+ZgXxrlz5zZ8LdRoNOjq6mLqayKRCIVCAZVKBQsLCzh37hyTOV3PvUksFsNut7NDqM/ng16vZ31L5AtCfjV8EBMkm82ywIOctN1uN3p6eiCRSHDp0qV1u97NBCX5yMixt7eXPU5MkFgshlAodEsn++XJ2I3sHSFJ7J6eHjz++ONwu93o6+uD3W5nKljRaBQjIyMIhUI4d+4cJicnEYvF2PdM9MG9e/di586d6O/vb0oONBoNpFIpLC0tIZPJrPtn4we71Jw+NDSEN77xjQCA+fl55HI5vPbaaxgdHWVzdC2460GISCRimZFMJsP6PcbHxzE/P8/K4D6fDz09PdBqtcwUhTalarXKDkGpVAqFQgHxeBxLS0tM45i4dNFoFBKJ5JZfyK2Uf/i6yK0AyrbrdDpkMpkbcumJ9qLT6eBwOFjWSiaTsfEgB1zKNNytzdVmszGqCMmIXr58GWfOnGmLnpZbgeM4JBIJJBIJ5PN5ZDIZ9PT04P7774fL5YLT6YTX64XBYEC1WsXi4iJGRkY2PQihCg5lW0hpZWlpCeVyGffffz+q1SrsdjtrJqfgn4L8mZkZvPDCC0gkEsx/gQ6ISqUSBoOBuRDzQeIQJB+63odk2nDkcjnLHNE18BX42jkIaTQa7HBsMpkQjUah1+vR09ODRqOB48ePs+TJrYzmNgvkiSKVSuH3+5mqGsdxLCEFvC6PWywWWcC8EvhBSD6fRy6Xa2qGpswq9SzR7/hqiF6vF93d3Uyjv1KpIBwOM552O1Zsl4PuDZIG7evrg8FgaMoS1+t1RCIRLC4uIhKJoFQqQaFQwOv1wmw247HHHsP+/ftZJYUEBEgAIhgMIp1OIxAIQCaTYceOHbBYLHA4HBgaGoJWq8Xi4iKTwN3otZACIvKhoevP5XKIRCKYnJxktPD1hEgkYoZ0LpeLUUOBa8kRSqIuv19JFY7WLkqyqFQqmM1mpo7VDlXe1YICDYVCwb4LSiiQqzfZLdwI/ACEDu4bSWEnmW+Xy4X7778fDoeDJZIJqVQKV65cQSAQwNTUFGZnZ9m5iO5VtVqNvr4+7N27F263+zpWS6FQuOn6eKeg+ahSqaDT6eDz+bB7924kEgkcO3YMoVAIc3NzmJ+fv63Xv6tBCDX7DQwMYO/evSy7ROpDxA8Xi8VYXFxEo9GAwWCAx+OBXq+HUqmEWq1mlQkKCMhURaPRMDOycrmM/v5+Vj0Jh8NN5TwqVzkcDuzYsYM1xt8Id9LtfzdA10ObLT94oBKvQqFAZ2cnjEYjHA4HPB4PZDIZNBpNU/ScSqVYM+fk5CSTko3FYnd8nXzlFeKwFotFjI2NMY5nK43reoGMlBQKBc6ePYtUKsU+v1wuh8/nY1zsRqOBeDyOaDS6addLB45UKoXJyUkAYJWQ0dFR5PN5GI1GTE1NMc4qudSWSiVGj6Em82q1CrPZDKvVir6+PhiNRmg0GmYARk70gUAAV69eva7Z7W6AqoL8DNqtMmftAMoU5vN5+P1+vPrqq7Db7di3bx8MBgNcLhcGBgaQSCQwOzvb0oo59L3w6VWkmiYWi1clPUrfL1EDqWoONJttulwulMtlZvpIG7zZbEZfXx/cbjc0Gg1L0PCrLlsBCoWCUfbMZnNTpX85DaZQKMDpdOLIkSNsX9HpdMyfgmhEhUIBly9fRiwWw8WLFzE+Po5isYhEIsGcyNPpNJxOJ4DX6VAmkwnFYrFJvWyjQFWxUqmEZDIJmUyGmZkZ+P1+XL16lQUC650gozlI5xqxWIxGo4FCocDYIdFoFJlM5rr1ia6VKKwk80rJRq/Xi1wuB41Gw5JB7b7GUU+WVCplSQWS5SYZb5VKddPkAP++3ejxEIvFsNlsMJlM6OrqgsfjgdlsZnM+lUohm81iZmYGr732GlNE41PsLBYLduzYwRrZ+X42/M/Fb+C/G2sVnb+3b9/OKnkKhQKNRgPRaBTBYHBFVs5qcdeCEL4h2UMPPYQPfOADiEajuHr1KoLBIL797W8jEAjg7NmzOH/+POx2O0ZHR2GxWPDYY4+hp6cHLpcLPp+P6XqLRCIYDAYWEfP56sC1pkeiY1EDcaFQQDAYRCQSgcViYSZgZrN5xWunaHszGuduBlLMIJ5ytVplRkYkT/mWt7wF/f39sNvtcLlcK6p8EY8wGo3ipZdewtLSEn72s5/dcRBCGeht27ahs7OTSSBOTEzg6NGjLLvWDjr7a0U+n2dmQd///vfhdDrx9re/HUajEUqlErt27YLP58P8/DycTifOnDmDeDy+aZsFvS/15vAf8/v9TI2D+jmKxWKTagsd+vj3n9vtxkMPPYSenh643W6WVaaK0eTkJKanp3Hs2DGWjFhvcNw1WWF+D1W9XkehUFiR7tCOKBaLKBaLuHLlCgqFArq7uzE0NASfz4fBwUGk02mMj4/D7/e3fBCyfAOjQxdw7SBxs82VKubFYhGhUAhyuRwDAwMAmilX27Ztg8lkYpK/Go2GUYUHBweZ9woZod3I1LZdQd4YHo+HqfPwqViU5Mrn80in0xgYGMCTTz7JAjWSjKX+A0pO/PjHP8bY2BiuXr2KmZkZtiao1WoUCgXmeUOqXB6PBwCuq5JuBPg+W0tLS8jlcjh27BjOnj2LQCDAGsPX+zsnCpXVaoVer4dYLEatVkMikUA6ncbS0hLm5+evO3NQD53f74fD4WCKeSTRa7FYsHPnTtTrdRiNRqTTaeTz+bav3JXLZcZwoWooX7mN+kxvdfjdLIEhsViMrq4uDA4OYu/evdi+fTtLypEwx9zcHE6fPo2f/vSn7Lxar9dZlauvrw8f+MAH4HA44PP5WP/WchZPtVq9a/5FFBDpdDq86U1vwv333w+v1wu1Wo16vY6ZmRnMzc3dkWjHXa2EUCaSKFeVSgV2ux21Wg0qlYrdiPV6HZlMBpFIBNVqlTVWVqtVSCQSJgdI8r63KjuSsVy5XG6SG5PL5ayhTq1WXydrx8+oZTIZJBKJlpK65CsUUUbJ6/XCarWiu7sbZrOZyb5RAz9lfkh1hqpT1JhP5T2DwQCFQtEkDbgWkPkjac97PB4oFAokEgkkk0lkMhlGqduK4Kt6JBIJiEQiBINBLC0tsd4KpVIJh8OBYrGI2dlZqFQqFuxuVraVMtF8kN4/qbAA13x2VlKlo3llMpng8Xhgs9mYWgvNJ+I8RyIRpmW+mmCAeLEk9bz8nl3ecEj3iNlsZhlDAIyOk0qltkQQQp+X+uWMRiPLlspkMjidTkQiESgUCnaoatWM/vLrupPrXMnRlx5Xq9VMEpMOM8A1Y0vKzkulUiwtLSEWiyGVSrV0ELdaiEQitv44HI7rmtH5tBW1Wg2j0QiTyQSLxcL2UxKHAcDGj7xr4vE4o2wCYM9Xq9Vsb6HHFQpFk3T2Ro4B36SYGuYTiQRTO7wbVQTqDVjeZ0p7M1kRULP+clDlhpKrJERB1RCVSgWNRgO9Xs96b9t9n6WkAvVaFotFyGQydn+q1Wro9XqkUqnNvtQVQfebTqdj4ih8sRfaD+PxOEuKkEKdxWJh1D2SkCbBBI/HA51O1yRPTJWQ9dzXaD0g7zOr1cp8gWQyGetH5LdY3C7uWhBCCxpftcFkMkGv18Nut8Pr9SIYDDL1k2w2y+Q9Z2ZmoNFo0Nvbi23btsFut+PAgQMwGo1wu90wGAw3fW+FQgGTycQOI/V6Hb29vahWq6yUR+6VfFDvSTwex9mzZ3HmzBn4/f6WyITRYbHRaDA/gMOHD+PgwYOwWq3YvXs3FAoFKpUKarUapqenEQqFkMlkMDMzg2q1ykqCAwMDzFn44YcfRi6Xw9zcHBYWFpDNZllj1FpgMpkwPDwMm82Gt771rRgYGMDVq1fxve99j0ktp1KpLVkF4YPMzYgfOTs7i507d+Id73gHVCoVHn74Yezbtw+1Wg1LS0ssC9ZKBx3KiJJCEXCtoW/5AVEqlcLhcECv1+PAgQN48sknmRKIRCJhggQXL17Es88+i2g0ypynV3PYVKvVcDgc0Ol0GB4eZjr5fPd1umaO45iansvlQm9vL6t2ptNplnmqVCp4+9vfvs6jtjkgSlatVsOxY8cQDAbh9Xpx+PBhKJVKnDhxgvX0tNIcW0/w+4AoybScZiSRSGCz2SCXyxkVIpVKMcWt5RQuqpoRVaadQYdgMib0er2MHkUg/rlEIsHw8DC2bdvGkoeFQgHT09NM1KHRaMDhcDBVy3A4jMXFReZhQ4lD6iHZvXs3+vr62HuYTCaUSqUNpWORgpTdbsfOnTtZgiyfz2NsbAwjIyN3JZtM5wxq4O/u7obNZoNEImHnDeLzUxVk+bpI3iEk1et0Opk8t0Qigdlshtvtxp49e2A2m3HhwoUbKuu1C0j8pFgsYnFxEbOzs7Barejo6IDBYMD27duZYXMoFNrsy70OIpGIqaCZzeYmHy6iPP/4xz9GIBBg90JfXx/MZjMOHTqEAwcOsOR7Pp/Hs88+i4mJCbzzne/EBz/4QaZSB4BRndfTx4gCvt7eXrz97W9nfS1dXV2IRCIYGxtjPyRgcbu4q5WQ5dkokhorl8vMIImqFZQtBV5XipBKpahUKhCJRMjlcujs7ATHcYxrzs92LdfHJioYgDWpW5GEKpUA75biwO2An8kktSta2CwWC1PIIJ5+IpHA/Pw8EokErly5wjjk1GxcLpdZk55Op4PJZGLVkbVI/dH3QFk2u90Op9MJh8OBK1euYG5uDtFotG1KxMtljVcy57vR/9NziZoVDAahVCphtVpRq9WYQkqj0YDdbodOp2NzvJXAryrcKhglqovRaITNZmMVMHJRrVQqrHGOAtG10B1IHYQWdLfbzQ5VfKohUTGpR4VoDyqVimXVqNraDvNwtaC+hUwmg1AoBJVKBZ/PB6fTCavVCq1Wi3w+39SQvVVB6z7fM4E/h6mSThV6UhmkJAz5LgBgvRGtrDC2WvApFdQYTY7mfFBQz2+cpcQDVdkIJGZB9zglyPgmoVqtllGqiUZN17PRRnN800+TyQSxWMySdKlUinks3A2VSGIuUPaeKDVUWaLK8416JakvKZvNIhKJQCKRoFAooF6vs8MiSXQXi0XGMtksKtJ6gMZGLBYziWyqYlKywWKxNEkstxIo4Cb5an7FnuM4Jt6QSqXYZyKRga6uLgwNDTFFv3K5jNnZWbz22ms4ePAgKpUKJBIJ5HJ5kyjHegXQ/Gs3mUysX47Gu16vIx6PIx6Ps4TOneCuBSG08XMcx5QMyKXUaDTigQcegMViwYULF3Dx4kVWcqRBbTQaCIfDqFarrHFMq9Vix44djG9ut9uhVqvh8/nYjbfahY3/PkQZCQQCOH78OMLhMMbHxxGPx+8owlsPUEWJGvGNRiOTeIvH40gkElhaWsLY2BhKpRKmpqYQj8dZEzifB59KpaBSqRAIBOD3++F0OvGmN72J8StdLhc4jluTYonFYmF9No8++ih0Oh3Gx8dx8eJFvPbaa7hy5UrLByAUREmlUjY/lUolKz3SD+liE+2KNmC6+YmCQAui3W6HRqOBVCpFoVCAXC5n/6/RaGCz2VCv19tS1YQWQb1ejwceeACDg4PYtWtXk+kZH/l8nlU+V3MYpjHv6OjAG9/4Rtjtduzfvx92u/2GQSJlmchhlqogU1NTCIfDuHTpEkZGRrbkYbxQKOD8+fOYn5+H3W5Hf38/bDYbnnrqKYRCIfzkJz/B/Px8y23W6wU6aKvV6ibaHvmK+P1+PPvss6z6SMEqVUDS6fR1ilr8/aEdQfthR0cHOjo6MDw8DJ/PB7vd3uTdcyMQK8Dv9+NHP/oRotEohoeH0dPTA47jmGznwYMHYTQaEQwGEY1GYTab0d3dDbvdjsHBQWbIVy6XEQwG8fLLL2NpaQmRSGQDRuH1cbBarbBYLOjq6oLb7Wb9ovPz83ctAAFeX8esVivMZjM6OzvR09PDkqjlcpkxEIh9sNI1UGBCFgVEt6YeA4lEApPJhPvvvx/d3d0IBoNIJpOMMtOuoDNaMplEIBBghrrUJG2z2ZhJHv/82Aog8RBSBeX3MddqNcbWoQqDRqPBrl272Jljbm4OkUgEZ86cYf2UtIeOjY2xe4zvp3Kn+xpV7WQyGfbu3Yt9+/aho6MDe/bsgVarZcEgGU3Tfn6nuGtBCH0JpIyTyWQYn0+n02HPnj1wOp1MoalYLDKKBi36RN0QiUS4dOkSFAoFZmdn0dnZCZ/Px5qxrVYrizbXEoRQFpE4lMFgEMePH2ema3dbvWc1oKwRqVyR9C5l0cmb4dixY4jH4xgbG0M0Gl2xtyMQCAB4vfF4bm4O27Ztw/DwMAtw7HY7stnsqp1XqZeko6MDAwMDOHjwICQSCf7+7/8eFy5cwPz8PGZmZlqCznYzkHKJUqlkWu56vR7d3d3sUKNQKJDL5RhHPJfLMZEAquYB1+YV9eAQ95kCGNo4NBoNTCYTcrlcWzrdUmBM9/IDDzwAp9MJmUy24j1YKBSYSMRqMjaUTaVF2ul0MrlPfoZvJTNO4FpVs1AoYGlpCZcvX8bY2BgmJia2JCWwWCxidHQUSqUSDzzwAEqlEqPDLC0t4dy5c1hYWACwOYZddxP8irhcLm8ypaSDWzAYxLFjx9hmTgmyVl+b7gRU2XA6ndi9ezcLCFZyQl8J1Lw9PT2No0ePYmlpCUajEV1dXUwAQq1WY+fOnbBYLJiZmcHi4iJ8Ph8OHjwIs9mMnp4eWCwW1uwfiUTw6quvYmlpacNobkSN8fl88Hg8cDqdSKVSSCQSzIfmbt0TUqkUZrMZdrsdbrcbPp+P/Y68MKanp2/qS7Jcgjqfz7P1l+a+Xq/H7t27kUwmcerUKaZquJLkb7uA7s9MJoNwOAyr1crEDfr6+pjjvEajaTo/tgrK5TLr96DrokCTTDM9Hg8OHToEk8mE7du3w2g0YmRkBFeuXMH4+Diee+45FlDWajVEIhHMzMygUCjA7XY39RXdqZIfBSEqlQp79uzBO97xDlgsFvT29kIkErGz5cWLF/HCCy+wfp07xYaYFRYKBSSTSahUKtZYbbFYIJFI0NfXhx07diAWi2FiYgKlUum6jYEfQZKsKSlEmc1mcBwHk8nEpHvpcEQlT47j2CTll9mpoaZYLKJUKmF+fh4LCwubXgGhEi7Rd8xmM7q6umA2myGTyXDlyhWUSiX4/X74/X7mmUL+IbeSwKVDNF93nBoNV+OLQs2LxBnct28frFYrAoEAqtUqAoEA09dvpUWBD5FIBI1GwxrGu7q6WDOm1WptkjaWy+XsMy93E+aPNZWFlzuPe71eRj+kv6f5fDeMIlcDMh4i1Ot1ZLPZVR/QNRoNy3Y6nU4m9sA3hSL1F8rMreWzUrMd6fcTPZNf7SA6CL+SBVyrkJAIAzVyklfOVj140pgFAgGMjIwwf6Zqtcoqlvl8vqXENtYD/P5Dku7kVxdpnvBlztuZqrIaUHZcpVIxekdHR8d1wg4rgZqzw+Ewrly5gtnZWZZ0IfdnhULB9nKXy8Xk9F0uF6xWK7xeL7RaLeRyORqNBgKBAObm5jAxMcEcoTdqjyW51J6eHlit1ia5br555XqC39jrcrngdruh0+nY70kAJ5/Pr9ovic4sWq2W0d/ofcioMxaLsdfaCvObktjUwN1oNNiaXq/XWZ8N//etgEajwSjxbrcblUoFUqmU7Us+nw/79u2DxWKB2+2GUqlkEsyTk5N47bXX4Pf7USgUmkydyU6hWq3CarUCAJN2vt37ic46Go0Ge/bsgd1uZwk/uVyOeDyOcrmM1157DXNzc5ienmZO6esxx+56EELynPPz85BKpUyCrKenh20IRqORSfdSdWL5ZKLD3uTkJGZmZhi/12AwYGRkBCaTiR2GnE4nc5YkxQkyZ8vn86zhKRAIsBITmWeFw2GmVLFZoIhUqVRi7969GB4eRm9vL7q6upDP5/GNb3wDr732GiKRCJN5pZ/VZJrJgTWZTLLnEoeXn125ESQSCRwOBwwGAx555BG8+93vRigUwvHjxxGJRHDhwgVMT0+3rCIPVZccDgcOHz4Mu92Ohx9+GJ2dnexAQzzxYrHIGgaNRiN0Oh1zqqWsAfUfLVdqog2CGjXp//l6/JvhGk/Brd1uZ4+VSiVMT0/fsrxKn8FqteKRRx6Bx+PB9u3bmWs3cK0aVCwWMT09jdHRUczPz7PAYTUgjnQsFmNOrDt37oTdbmemcyQ1LRKJYLPZWJBH3yFVUwwGA/t9O9NrbgbKiDUaDVy4cAGFQgH79u3Dr/zKr8BoNKKnp4fRGrZiEELGYEQtpWQKn25L6/pW9ClaDqIyWq1WPPjgg3jyySfZenUzkLhDuVzG2NgYvv/97zMPi3q9zirmOp0O9Xodcrkcu3btaqI3071H92GtVsP58+fxox/9CMFgEGfPnl11RfROQYe+/v5+PPLII3A4HKjX6yy5EQwG70oTN9EDDQYDhoeHWcIGuJagIZn8YDC4KrnyUqnEqNIUFNIYk+caqXy14r57O2g0GohEIpiamoLX60W9XodYLGb78NDQEBKJBMbHx7G4uNgyVFuSr00kEjCbzcjn86w3SKFQ4MCBAxgcHGRCSaVSCRcvXkQkEsELL7yAF198kZlU8s9RkUgE58+fh9lsZvN2fHwcwWDwts9barUaXq8XHo8Hv/7rv47du3dDp9NBq9UilUphYmICkUgE3/72t3HhwgUWOK9XImdDghAyL6KsHOmNS6VSZqyXSqXgcrmgUCiQTCZRLBbZwZr/WmRcxXfBDIVCrIJCkSNx0+kASX4I5ExKjcNUEcjlck2c/824iWUyGRQKBeRyOaxWK9RqNTweD2sKoslKnyGdTiOVSrGFFlidKQ9l9/lNmmsB+baQbnWj0WALKrnNtspisBxUAdFqtXA4HHC73bDb7TAYDFCr1SgWi0x6juYJVc40Gg2q1SpTOyFdb6oA0FgS7ZCv4kRVEjoIEVVoM+R5SUmIvHdow6RMzUrXQ5U5OshQKdzhcECr1TZlWCkrl81mmes0UR5W+1mpYlEqlZBKpaBUKhGJRKDX69m6QJSQlebvcg4uyWAuX1O2GjiOY8IaRPWj5n6DwbAuhqStCrr/+L2BFAyTaW2rJkbWC/zeGPKKIvERqujyQU3lVEUCwNYnolET/Yd/iCL6MknEUuO3RCJhXhxUcaD9l/ZgSuxsBOi6SCpYpVI1BaW0hq83iG5DDdRms5kl+KinsFQqMUrLagIyGtdCocACRVq36TXpMZVKhUKhwPoU2xmlUgm5XI5Rm/h+NXq9Hg6HA5FIhEnet4pZI9GxiP5JhttUuacKFjV3B4NBdo9kMpkV1yvaD6l3lyh3txPQE/OFrB5oP7darcwviBJX4XAYsViMJa7Xc3zvehDSaDQwMTHBDvpUgqJsVUdHB0wmE/r7++H1ehGLxfDss89ifHwc2Wy2SZGDDzpg5HI5jI6OsgM8X5kDuFZBoRuUL3VL1CWixfBL9RsJ2jRdLhe2b98Oq9WK++67D06nkyndqFQqGAwGlMvlJs1pygzciBu/EsjgqK+vDwaDATKZrMlV/lafX6FQYNeuXeju7kahUMC//uu/YmFhAa+88gpzLG5F0CZ56NAhPPjgg/B6vXjwwQehUCgQi8UwNjaGc+fO4cSJEygUCojH44xnWa/X2YIvl8uZ9j35tNDjWq0WQ0NDbPMxGAysfFytVplC1OXLl3H58mUWNG8kRCIRjEYjc3NXqVRIpVLMdGj5wkcyvFqtFrt378bu3bvhdrtx6NAhZsAFXMs6Ly0t4cc//jGWlpZw+vRpVmFZy0JJQUQsFsOrr74KvV6PQqEAl8uFffv2Yc+ePay5j5rk+Q3xdH/ncjlMTU3h4sWLTDluK/aEEDiOY/O2u7sbpVIJUqmUSaSSR81WPYgv7wtMJpMYGxtjvPvV+tO0KzQaDRwOB5xOJz7wgQ+w6iFRS/neAhzHYWFhAVNTUzAajdi5cyeUSiVLJlHfRq1Wg9vtZmqMBoMBFy9exI9//GNwHAefzwedTgeLxQKj0YhUKgW/388q7sViEVevXsX4+Dir/m7keFDSpLOzE8DrVQTyrcrlcnclIPJ4PNi/fz86OjrwwAMPsF4ckUjEZLVnZ2cxOzuLubm5VVVjSqUSgsEg81JbWFhggQ4ltcrlMtxuN+RyOaamppBIJNp6vSNaYKFQgNfrxfT0NPPQUCqVGBgYYPvs5OQkotEo/H7/XTHCXQsoGUTVq6mpKdjtdvT29kKn07Hz6+joKP7lX/4F8XgcMzMzzHPnRt5OxOaRyWQYGRkBgNsSeBCLxejt7UV3dzcGBgbwlre8hfVwyeVyjIyMYGRkBPPz8zh69CgSiQQz8lzvvWNDKiGkPkF0Io1GwzjaWq2W/YjFYkSjUVy4cIHx3m7WNEaVkUQi0fQ4PzvaDhsO0Ql0Oh18Ph87aHk8HuatAoApkiyXn1wuV0yunMvHjZ6jVqths9lgsVigUCgglUqvU3q62bWS4ofb7cbS0hIWFhbg9/uZw2krgp/xdzqd2LVrF/OSEIlECIfDiMfjmJiYwMmTJ1nGaaWIn4zzZDIZuru7mfqLUqlkDsxk9kcSimSeSdJ20WiUKfNsdNaGMjG0QRNXma5zecWCVHCoKXXPnj2w2Wzo6uqCWq2+znWZDv4LCwuYnJxkDdG3A3LBzmQy0Ov1iMVicLvdzMdEpVKtyHOntYEyTXwzsnZYE24XVHkmSc9arcZoIaQmeC+BDsLE/W/nA9lqQBRlm82GgYEB7N69+7pqN92n9Xod6XQafr+/yaOCqsGUlAIAs9kMvV7PegGTySTOnj3LVBfJw8tutyMajeLq1avs3i0UCgiHw6yfc6PHgy+PS0wComnfraSjXq9nqmQulwtOp5N9B6TGlkqlmNnbagIzyk4rFAomLUxCP/R90ntLJBLWS9fOCQeO41iiLplMIplMsqQYCQ5oNBoEAgFYLBZUq1XGxthsEGsnm80ikUhAJpOx+4z6GmOxGM6fP8+C/ltRZYvF4i1d4m8FOguR7H1/fz/27t3LDMGphWJ6ehozMzN47bXXWHLybmBDGtNpU1xcXMTZs2fhdDpx8ODBJv1khULBNsnDhw+jr68PIyMjOHfuHDOMWu0G0k7Np2KxGJ2dnbDb7di9ezeOHDnClDz0en2Tqy/5LdBBliYsjR3RuFQqFTP4oey0RCJBZ2cnrFYr9u7dize96U2sRFwqlRCLxbC4uIhEInHDsaPsuc1mQ29vLzo6OjAzM4ORkRGkUqmWluHlm+r19/djaGiILdalUgnj4+O4fPky6wO5Wa8GVdZqtRqTqaMAR6lUIhQKMSUzokFoNBpmUJjNZnH58mVWidvoTaLRaCAYDEIkEqG/vx87d+6E2WzG9u3bodfrmXwmfSa9Xo89e/ags7MT+/btQ39/P3Md5medSQ41nU4jk8ncdpl4+bXS3J+fn0csFmOGcx6PB/fffz90Ol3TvQK8fvgMBAKIx+NYXFzEwsICkslk26wLdwKau0SH02g0rJJH1aJ2PpjcCPzeOPp8RMUhg9qtDr1ej507dzJvDn71gw6/+Xwe586dQyAQwMLCAmZmZtDX14dt27bBZrOx4J4SVAqFAj6fDyaTCYVCAVevXmX+T0QvVSqVmJiYgEajYX2XtVoNuVyOCUxsBoxGI9RqNet3JMZEPp9HX19fk5DKnYKSOzKZDJ2dnbjvvvtgs9lYgpV65QKBAF555RW2Pq2V3lKv1xEMBjE9PQ2JRMKSlbt372Z9BESHI4+wdgaxEYLBIM6cOcPMNokGLBKJ4PP5cPjwYSwtLaFYLDZRcDcbCwsL+OEPf8gqiDqdDul0GtlsltlP0H1yt6FSqdDd3Q2TyYQ3vvGNOHToEJPsLpfLuHTpEuLxOI4fP45Tp04hFovddenjDQlCiL8YCARw9uxZeL1e9Pf3NzXGyuVypgT1pje9Cel0Gnq9HtFolGWy1krnaAdIJBJ0dHRgx44dOHjwIN7ylrc0ZZcJVBVKpVKIxWKIxWJs0lIQotfrsWPHDtjtdpw6dQrBYJD1HMhkMrbR3H///XjiiScglUqZQgcFITeLeA0GA3bt2gWHw4He3l54vV6USiVcvnyZRf2tCmqmpwBqcHCQjXG5XMbExAROnz7NGndvNn+otwMAy0rwD+MXLlxgfTrUlK7RaACAKbJRJmQz0Gg0WHWBKH+VSgWLi4swGAyoVCqIRqNNXiB79uzBzp07MTAwgP7+/hVpf+VymWX2SPDhTj8jvw9sfn4eIpGILd7bt2+Hx+OB3W6HQqFYMQgJhUJM9a7VJBzvFijLS+ZmWq2WGZtt1YM4XwKTv34RP/5eCUKIVuXxeKDT6Zo+M3ljxGIx/Ou//ivOnz+PRCKBaDSK/fv34+1vfzsz0iO/ABo/n88Hi8WCYrGI8fFxzM3NIRaLsSrHSmiFe40fhFBTPsdxKJfL6O/vZ32k6xGEAGD9gp2dnTh48CBr8KUGfaKrHjt2DJFI5LboUpT8mp6eht1uZ8ZyJGFLnhGRSARGo7HthSioyhMOh9mcJYobUaS9Xi8OHz4Mv9+P1157jfW+tUIQMj8/j3A4DKVSCa/XC41Gw4z+SqXSXa0yLAc18/t8PrzhDW/AG97wBgDXBKTOnTuHyclJnD59GufOndsQIZcNCUJoMcrn8wgEAuA4DqFQCAaDAVqtlpn3ANeUoQCgs7MT+/fvRzQahVgsZodwcgvdKrQKpVLJSt3UzLwc2WwWU1NTWFxcRDabbcqg893mpVIptFoturu7Gf82n88zFZOenh5WGi6VSpibm0MikUAoFLpO05pAdC+dTtek+x4Khdi1tHrDJ1+GmLInlBEmupHBYEAikbitTDH/+fwFhS/fC4AFIK0yd4kSyXEco1eR2aVOp4PdbofNZoPX62WVyhv1HeVyOQSDQdbEdjeqY3zlMTLqoqZ5+j1RShYWFpg5HakitRooU73SfLvdjYkOj9QEuZHO1JsB+s7Jt4cvjEF9Q6Rsp9FomtQPl/fSUcMrPUavy6+ytDKoekn3BnCt+ZwMO4n6kU6nWY8MUSiJ7lGpVJBIJMBxHCQSCTNxzeVyjBZDY9HK6z7RyagBnHr3KDnEN7a8U4jFYhiNRpjNZphMJlYVoSoIyWOT+/xaK8V0zTqdDmazGVartanXZ6vf58ViEdFoFGq1GtFolIn3ECVXq9XCbDZjYGAA9Xodc3NzmJubY9LlmwWq5otEIialS9Wquy1MQ3sjmVxbrVYMDQ3B7XbDYDAwuhjt2TMzM5ibm2MtFBux3m1IEAKANZmeOnUKVqsVfX19rCTa29vLbiTiqhmNRrzxjW/Evn37sLS0hBdeeAHhcBhnzpzB7Ows43a2O+jz+nw+mM3mFd2zicr2gx/8AEtLS1haWmqiCxFVxWg0MtpRX18ffvEXf5FtJgAwMDAAp9PJJmYsFsMPf/hDzM3N4ezZswiHwysGE7Sh+Xw+PPHEE1CpVLhw4QKuXLmC+fn5TelrWCsouCXtev6iLZPJ4PF4MDg4iHw+j8nJyTt6L35wKBKJmJsrgJYJ1uhe02g0cLlcUKlUsFgsKBQKzJyxs7MTDz74IMxmMwYHB2EymZgc8XKQWserr76KmZkZTExMNFXr1hO0GdP3SQaQwDX5y2g0iqNHj2J+fh6zs7MtmQ2k/iqJRHIdhZR/+F3L65H6G9EB+c367URTXQuox6FUKiEcDkOlUrEqu9FoRH9/PwDA5/OhXC6zAJnfK0HjpNfrmagJKRlR8zKJm7Qy+IaN9L0TJ/3ChQv427/9W4RCIYTDYearQOpVP/rRj2A0Gtl6v7CwgGq1ygRk7HY7fv7zn+O1115DIBBoi/6aaDQKlUqFSCSCUCgEvV7PqMt6vR5Go3FV7vGrgVQqRU9PD/r7+9HX1wedTgeFQgGxWMwy+cFgkJk6UlV8tSDGg8ViwdDQEIaHh+HxeK5LXPI9qFplv1kPJBIJvPbaa0ilUhgdHQUAdHV1wev1QqVSwel0Qq/X493vfjei0Sh+9KMfsUB7I6sNy0H9tnQOIJl++m7u1vdDFU2VSoXh4WE8+eSTsNls2LNnD/PXIwnen/zkJwiHwzh+/DiWlpZY78pGYMOCEOCaSZ5CoUAkEmEZA4fDwUqKlOEEAJ1OB6VSiUajAbfbDYlEApvNhkwmg3Q6zXiP7X6TEU+VJIWBa4dVqnik02lEIhFEo9HrqhV89S+ivpFDa7VahVqtBsdxMJvN0Gq1TOYtHo8jFAohGAwySbibXZ9Go4Fer4dMJmMyoLeiLrUaVqryiMVipvJkNpthNBqZ1B/RgWhsbrdC0mpZeP7iR5K9BoMBSqWSSRd7PB54PJ4mf5TlDa5EbajVakgmk4xiQEot6w2+FDIdlvgBCGVySaUtkUjcdU7rnYA+D10ffS7gGg2B/xn53xn/7/njQt8XyXrzlfS2IviVkEqlwnqtADAJWa1WC4vFwqpz5XKZjTXtOTR2pGJEhwWlUskEDij506rziSSJ6adQKCCdTiMWiyESiSAcDq8oo04UzGKxyCoF5FFAgY1CoUC5XGYOzq06BnyQQEihUEA2m23ydKJExo0EOdYCEiHR6XRMEpkfHFCDdTKZRCaTaZIMXw6ae1RFoftfo9Ew01GDwcCCHKA58Mhms00VwXYIFlcDmtu5XA6JRALxeBxOp5NJtVNPps1mYwI0FouFJRFoLDZj3vIl4+82+FRwg8EAg8HAFPOoD5iYRblcDqFQiPWBJpPJDe/f2tAghCLCZDKJ559/HqdOncLBgwdx8OBBuN1uHDx4sEm9hTYIu92Oxx9/HPl8Hr29vQgEAjh37hx+9rOf3VTFqB1AB0CNRoNisYiJiQkAaGroKxQKGB0dxejoKKOj8UHBSqFQwMWLF5FMJvHYY49haGiI8WHpgEautRcuXEA4HMaxY8cQj8dvKIUsEong8Xjg8/nQ2dnJFDrOnj3LSvvtsBnR4kwHZP41K5VKHDp0CIODg+jt7cXAwADy+TwikQgKhQLGx8cRDodRrVa3RF8BBVZ0SMnlcsw1VavV4tFHH8WOHTug0WiYezz1KfElPslX57XXXkMwGMSpU6dw9OhRpqxztyAWi5mfDsly0yZz+fJlnDt3DrOzs5iYmNhQZ+Y7gUwmY4kYp9PJDkYikQhmsxkOhwMAmCEh/Z7kyKVSKTQaDeRyOZNKNZvNTKZ3amqK+S+1+/xdCRSEkOoQJXOIqiESifDe974XsVgMZ86cwfj4OFO04yvn9ff3o6uri9F2iGNfLpfx0ksv4ZVXXkGlUkE+n2/JcYxGo/j5z3/OPLlcLhdGR0cxNjbGGrBzudx1AWk+n8fVq1dZlZwfmPFlr6PRKGZmZlgCsNVB++jU1BTOnTuHvr4+OBwO5lotl8tx5coV6HQ6FsCu9XuVyWTQ6/UwGAw4ePAgHnnkEcY4AK7NzcnJSZw8eZIph610IKaAT6lUoq+vDyaTCVarFXa7nfV9aLVaDA8Pw+v1MufwXC6HmZkZ5HI5jI+PIxQK4eLFiwiFQi3dq7kW0DimUin8/Oc/x/T0NGQyGXw+H6PlSqVSuN1uWK1WHDlyBD6fD/Pz83j55ZeRSqWYkEwr3rvrAZFIBLVaDZ/PB6PRiIceegg7duyAyWRizu0nTpxAKpXC9PQ05ufnkUwmMTc3x/pTNhobGoQA17KVU1NTbOM0GAyo1+vYs2dP03NpISQJX5KbdLvdSKVSOHHiBPMDaIcFcSXQZ6QmcXKmTSQSKJfLTMaP3FBv1OxLB0vSEd+9e3eTf0Kj0WA9NUS/isfjWFhYuOnEI4oC6cPT9xcIBDA9Pd02CxwdJlbyiaDGNsr6UZZgfn6eZREpSFueCW3XxYx4spQVJtqFQqFAV1cXurq6bvr3lFCg3ovp6WmMj49jcnLyrve88I3R6KDEb/okukgsFrulA3wrgNYAkhK1WCxMAloikcDlcqGnpwcA2CGJKDdqtRpGoxEKhQJGo5EJfJB/ilQqZfxeOvhsNfCb0imwps9Jc8RkMmHXrl1MOIEy/nyjUalUiu3bt2NoaIgFhABYpW9ubg6vvvoqy7y24r2fz+eZ3w8JTly9ehUXLlxAKpW6oZEsyYXyodfrYTab2WelvgbqFWnFz78cxLmPx+Pw+/2wWCzMW4vOHaSsR1XdtX4uovrqdDp4PB709fVd1zvXaDQQj8eZqhglE1Z6Lcrok7QvSf0qlUqYTKYm6hG9dqFQQCgUQiKRwNjYGOuHWCngbGfQPr6wsIBsNsvGEgDbC0hyvr+/n/V6Xr16FSKRCIlEgtEQ22H+rgX8CprVaoXNZsOuXbvwwAMPsOdQwBEIBHDx4kVcvXqV7eObNR4bHoQAzXxnkpENh8Os2aqnp4ctfvwbWSwWM2+L4eFhRCIRxGIxjIyMMPOhO9VQ3mjUajVcunQJxWKRLUDEb6byI6maUFnxRgFXo9Fg9JNjx46hWq2ywxplsXK5HPx+P6amplZFmeE4DplMBsFgsKkHJRQKtYwz6WpAixc135PPBJXL6b8tFgu2bduGUqnE1L86OztZRp1czlOpFMrlMit9ZzIZxONxVmFo9YWfyveUEclms9BqtU0KU8ufD4DRCCKRCC5duoREIoFTp04xGdy7NSckEgmMRiNUKhX6+vqwc+dOOJ1OBINBxGIxTE9PIx6PY3R0FCMjI8wXopVA1Q6iTwLXTOP42vEAWFZPLBYjEAhgdnaW9ShwHMfua6qEyOVymEymJt44BTfZbBZnz57FwsLCbRlbtQtyuRyuXr2KVCqFvr4+Rj0g4ROiG+7fvx8ul4vd8+R1UavVWLKFDoIU8FWrVearQ95MrQZyYCaa7JkzZzA5OYn5+XlEIpE1r0tarRZerxcmk4m9JmWR2+kA12g04Pf7IRKJoNVqkclkGG1Hp9Nh586diMfjiMViLCPMr1Tcaj1Tq9Xo6elh6osajYaJQtD7U6CQyWSaKvH8gyNRiHbu3AmTyYT9+/fD6XTCaDTCZDIxpTISnSDFzEQigXA4jNOnTyMWi2FiYoIFJO2yP68F9XqdGfpdvnwZTqcTdrsdg4ODLOkKvD5/qYL85JNPIpVKYWxsjDVgT0xMsL6ZdgbtBSQgY7PZ8OCDD8JqtcJisTCK9MLCAlKpFM6cOcPEKTbLJqDp+jfrjSlTNTk5ienpaczOzkKpVMLj8TBjNH4pGHg9CLHZbLDZbMzcj4yWAoEA/H5/2wUh1WoVp0+fxtmzZ6/7HX9irGYxrNfrzBAqFovh5MmT7CBCvgFUNVpLU1QymWSNxyTHS2aS7QLaBMgXhMaAH4QQ9c9mswFAU/aPSvVEX5qdnUU2m8Xi4iIikQj8fj+uXLnCjJVaPQihhYcO7plMBj09PTAajSs+n7+RZrNZTE5O4plnnkEoFMLo6Cii0ehdVdOQSqWw2WysSX7//v0AwDTWn3/+eUxOTiIejzOBhVbbXORyObRaLaOOraTaIhKJEI1Gr1v3iJrK/wGu9WsRDYt6esxmM4DXg7discj037cqFYHWt0uXLsHv9zOpcpPJxCpKJpMJIpEIDoeDBeGVSgWFQgHT09PI5XJwOp3swCeTyQCAVQpJBKFWq7HgpVVAc6JcLiMcDkMsFiMcDkMqlbL+trUeuPR6PXp6eqBSqRAOh1lvYrvNn3q9jtnZWSwtLcFqtSKdTkMsFsPpdEIikWDfvn0AwAIQ6nekoO1W+6RGo8Hg4CA8Hg9cLhfLxAPX9hCiSxOdmt8DRv4iKpUKvb29eOqpp+BwOLB//37Y7fam+532bUpORiIRpmj0yiuvsAZ8Mnlut+9qNSDGSDKZxLlz58BxHHbs2IGOjg62TlKfp06nY0I9xWIRly9fRigUwk9+8hPMzs4CQFuPE9H5ZTIZurq68PDDD8Pj8eCtb30rLBYLxsfH4ff7MTY2hp///OdIJpOYmppqqfmxaUEIgW7QYrGIpaUl5mFgNBqh1Wqh0+maNmT6b+JH1mo1dHV1QS6XI5vNIh6Pt8TArgXreWCiz06bK7/xlSTh1vp6lN2nTY42tXYCx3Gs7JjJZBCLxVgJnc99XknqUKlUNjVBkwIZ/S01CcpkMmQyGUxMTFznONxqoAWoVCohHo9DrVazKhsFZvV6nR2YE4kEk0iMRqOYnZ1FKBRCPB5nVbu7BT5dhv+Ty+WYykw0GmVZxs3O7NwIWq2WBbh0EKYeJX4VZPnmQAmI5fOTr7JSq9VYZQQAq7SIRCJ2sLrb39NmgyrI6XQaMzMzTLLT6XRCqVSyKvryZn4y5KQgkRqCxWIxKpUKgsEgEwah6nErzi+gmZpGge5KJo6rgVKphNlshlQqZTK/rVZdXC349NNisch6Tyk4pb6CXC6HfD7PmvT5pqt8IRyO46BQKFji1Ov1wu12Mz8o/vtms1lWMacqPPUcUeLAarXCZDJhYGAAbrcbZrOZVT0IfJETotbNz8/j6tWrCAaDSKVSt2RLbBWQqlsymcTi4iK0Wi1mZ2eZ0BFVPykJSxV+i8XC/jUYDCyp2EoJhdWAms9Juc5isaC/v5+ZXFP/H7mez8/PIx6PM0W2Vpofmx6EEOLxOI4dOwaTyQSz2cxK6kNDQyt6Z5DSk8/nY7rRxBXcCIOVVgcpQSznpd4Ocrlckykf0enaCdQTQzfmuXPnYLFYMDg4yLJQK8kjA2CZBgpaOI5ryqZSliuTySAUCuGf//mfWWP0/Pz8Bn/S1YGC/2g0iosXLyKdTuPw4cPsvhGLxSgWiwiHw0in0zhx4gSWlpYwNjbGGivJaOtuHkxosSWPF6VSCblcDqlUinA4jO9973uIxWJs0W3VAEQket3Vd9++fdDpdEy57uWXX8bs7CxSqRSjTyxfu5YHJfx7msQSyMSRr4zC/3s6/LTS5rPeoL6GZDKJ73znO3j22WdhNpvhcrlgtVqZ3DSJGRiNRrhcLshkMnR3dzclGmjMC4UCXnjhBYyPj+PChQvw+/2MOtdKWK52B1wzarvd/jWLxYJdu3ahVCrhwoULiMfjLSl1vRrQPZXL5Rgl0WazQa1WY3BwEF1dXSiVSnjb297GZJzJHX58fByZTIaxLohy6nA4mGjLkSNHGEuDD/LiisfjWFpaYrQ/jUYDo9GIRx99FC6XC9u3b8fAwAB0Oh0z4aOeJAIlBEmoxu/348SJEzh69CgLvlvJg+pugs4gtMfOzMygWCzC7XbjySefRFdXF0skkJCJTCZDf38/uru7MT8/j3PnziGZTGJ6errtxkwqlcJoNEKv1+Nd73oXDh06BJPJBKfTiXQ6jZMnTyIajeLUqVO4evUqo6JTYrGV0DJBSLVaRSqVAse9bmRoNpthMBjgdDqZiolYLGZZKjqYAIDdbmfZLCo/t9ukWm+sZ6mtFaktawU/cEqn0wiFQmg0GsyAr16vM8UhmlfLqVoA2OFuuV9GtVplBxy3241CoYClpaU7ln6826D+FhJAyOVyjP6TSqXYoS4QCGBxcRFzc3OYnZ3dsIMYXQtlfWj8aTMmB2i+LGurgprPNRoNbDYb6vU67HY7crkcO/jSuC6/3/gZbvqXKiQUXNDnb3Uvi7sFusfJJyaRSDAKai6XQ0dHB0qlEgtk6/U6DAYDSy5QczIAFlxTIO73+5FMJlu6CsLHnaw5tN7J5XJGLSqXyzdsam8HUCWxXC4jk8lApVKx5nDyUqjX6zAajahWq9DpdMxcMJfLQavVMp8Hut+cTidcLhdTY6L1n0D3cy6XYz4VCoWCyd2bTCa4XC54vV50dXWhu7ubmREur8ZTAJLL5ZDL5RCNRhEKhdhPO7IT1gNkQhmLxRAIBACA3fPk0QJcE//QaDRoNBrMI6Zara5oDt3KIAU1g8EAs9kMt9vNlN7I5iEcDjPZ3Xg8zvaHVly7WiYIIbWdVCqFo0eP4sKFC7DZbHA6nbBarTh48CDMZjO2bdsGp9PJ/k4ul8PpdEKtVjNvA1JNaPeDs4D1BR1SLly4gFgsBp1Oh46ODqjVatbISpsCbcDUoHorV12iwtjtdhw5cgT79+8Hx3EIBoMol8stq6tfKpWYKs4PfvADnD59GsDrY0VKGrlcDvPz88hms0gmk01GmXcbCoUCZrOZHdxNJhMymQxee+01ZkK4kddzu+A4DrOzs8jn8/B6vVAqlTAYDHjb294GlUqFdDrNxjafzze5dBOVpFarMXU8ovpFo1HMzc0x+mWrB2IbBTookoAEbcaUzJJKpejr68N9990Hm82GBx54oKkXLJFI4OrVq1haWsL58+cxOjraNv0QpN5FlLO10LDI4EyhUDCPgVKpxBIQN5Jyb3XQGCwsLOD5559nDff0eUmqmaqstNZoNBr09fWhUqkgm8020dtItZPWJqLtAtcqlDSPotEourq64Ha7GU1Qo9Ggt7cXBoMBVqv1OnNRQrVaRbVaxcLCAo4ePYpYLIbz58/D7/cjEoncUGnrXkI8Hserr74Kh8OB3t5e5HI5dHZ2oqOjo+l5dF84HA4MDw9jfn4ek5OTbVPhUyqVUKvV6OjowDvf+U44nU709/dDoVDgypUrOH78OJLJJMbGxpDP56HX63Ho0CEmtkD3cit93pYJQvhZrKtXrwJ4ve9Do9Ggo6MDKpUKbrcbbre7KQiRSCTQ6/WQSCQwm80wm81oNBrXyQ0KEEDZwYWFBSwuLkKtVsNqtUKj0SAajcLtdiOfz7MbnShApFxyM1DZV6/XY8eOHSgUCjhx4gTjHreqPwNtro1GAxcuXMD09DTbZOPxOKanp1kpdzMybVKplG321CNGC2kkEmHOrq04tssRi8WYgtquXbugUqmwa9cu9PX1IZvNMkUvomxQRZcvpUy0UzKVUiqViEajEIvFbVEN2ijQoYxM+wBgaWkJwLVMfyKRgMFgQD6fx549e5rmEIlP+P1+zM3NYXFxceM/xB2A47imPre13B9yuZxVB9RqNSQSCRKJBLvf2hG09sfjcYyMjDDPKGJR8HuFyHgQAAwGA7q6uljVcTktkj/GfJCPUj6fRygUQjQaRV9fH9xuNywWC3w+X5Os9nIK5fLXItWzM2fOIBQKYWRkhAlwCPc8mJ9aNpvF7Ows1Go1DAYDfD7fdd8N2Q74fD4UCoVb7u2tBPLscjgceOCBB+D1etkcXFpawgsvvNCUhD906BC6u7tRLBaRTqeRzWYRDoeFIGS1oGYwahSTyWQt2+groH1AG0m1WkU2m0W5XMbk5CRTFpmenmaZQLlcDrfbDZPJBK1WC5PJBLlcDqPRyIKT5fx7ajKm7HUrZ6n4crF+vx8qlapJzYWapjfrM9ChnA6SGo2GZSPT6TSjx7RDEELXmEqlcPHiRfj9fphMJiQSCej1euj1elZNo++AT9Gq1Wqw2+2o1WqsyZVM6eLxOMbGxto2U72RoMOjTqeD1+uFy+VixoTBYBCJRAJTU1M4e/YsIpHIphh4rQfovljLvSGRSOB2u+FwOKBUKjE9PY1AILDp68B6gYQ4xGIxXn75ZUxNTcHr9bJmZovFAplMxgQKCHyBF/5jVGmiagRf9a5SqUAikWB4eBiFQoGptZE3Gp9iulIQQ+vb/Pw8FhcXMTMzg+np6Savi3b/PtYLfKGVy5cvMwU0vV7PvJeWB3l8if5WB11rZ2cn9u7di87OThiNRohEIly5cgXBYBCXLl1COp1mvZG0xjkcDlSrVZhMJqaO1Upo6SCEDnKpVAp+vx+NRmNFJ+Z2OYQIaB1Qsy5l9iKRCOs5Ii4pGcLt3r0bXq8XHR0dGBwchMFgQG9vL7RaLcsUEohWWCqVWBDSypn6er3OsnbU2Ey4nUPMeqNUKiEcDkMkEjGtf/71tVMWkMYyEong5ZdfhlarBcdx6O/vx/DwMA4dOgSVSsV62/h/t/y/6bUuX74MsVgMv9+PQCAgBCGrAN/AkBy0qSdgdnYWly5dwsTEBH7yk58wg9d2xO0cUKVSKXp7e7Fjxw7U63VcvHiRiVO0axWEj0KhgGKxyJSCyH18YGAATqcTO3bsgEajYWpZfJuAG3nD0LmEejb4iVKlUolHH30UCoWCzbvlCncroVqtskb248eP48SJE4hEIhgdHWWHTCEAaUaj0UAul8Px48eZSqDJZILdbodOp2tq9KdDfbsEITR3du7cife///3Q6/UsAXXy5Em88soriMViiEQirGInlUphNpvR2dnJXicYDDLKdaugpYMQAr8Js1UPcwLaE3zZQwoWSKaWAggKVijLtVw7fvmcXH54b4c5u9nBxs3Qytd2O6AmU+KN07ziix+sdnMkA0SpVNqSBnqtCjoIkiEkgd+DQz9bae6tBiQEQWvdVtp3aS0hilOpVGL3IN2Ht7Pe0N9QdYIfaJAc73Lfs9W8Js3F5UktIQBZGbS2isXipv36ZmiXdZMvN0zzCbgmokHVOP7c5Qda9Pet9nlF3FZZXQQIECBAgAABAgQIENAWaI9alAABAgQIECBAgAABArYMhCBEgAABAgQIECBAgAABGwohCBEgQIAAAQIECBAgQMCGQghCBAgQIECAAAECBAgQsKEQghABAgQIECBAgAABAgRsKIQgRIAAAQIECBAgQIAAARsKIQgRIECAAAECBAgQIEDAhkIIQgQIECBAgAABAgQIELChEIIQAQIECBAgQIAAAQIEbCj+H+Tra7JRhTp6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Plot first 5 images\n",
    "fig, axes = plt.subplots(1, 10, figsize=(10, 2))\n",
    "for i in range(10):\n",
    "    axes[i].imshow(images[i].squeeze(), cmap='gray')\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subsets = np.array([75000, 50000, 10000, 5000, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(test_data, prototype_data, prototype_labels, k=1):\n",
    "    # Compute full pairwise distance matrix in one go\n",
    "    print(\"Computing full distance matrix...\")\n",
    "    distances = torch.cdist(test_data, prototype_data)  # Shape: (10000, 60000)\n",
    "\n",
    "    # Get indices of k nearest neighbors\n",
    "    k_indices = torch.topk(distances, k, largest=False).indices  # Shape: (10000, k)\n",
    "\n",
    "    # Retrieve the k nearest labels\n",
    "    k_labels = prototype_labels[k_indices]  # Shape: (10000, k)\n",
    "\n",
    "    # Majority voting for prediction\n",
    "    pred_labels = torch.mode(k_labels, dim=1).values  # Shape: (10000,)\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = (pred_labels == test_labels).float().mean().item()\n",
    "    print(f'{k}-NN accuracy on full test set (no batching): {accuracy:.4f}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.1406\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict = {}\n",
    "for subset in num_subsets:\n",
    "    accuracy_list = []\n",
    "    for _ in range(30):\n",
    "        random_indicies = torch.randperm(train_data.shape[0])[:subset]\n",
    "        prototype_train_data = train_data[random_indicies]\n",
    "        prototype_train_labels = train_labels[random_indicies]\n",
    "        accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels)\n",
    "        accuracy_list.append(accuracy)\n",
    "    accuracy_dict[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000    0.853282\n",
       " 75000     0.845707\n",
       " 50000     0.833377\n",
       " 10000     0.766494\n",
       " 5000      0.727639\n",
       " 1000      0.599655\n",
       " dtype: float64,\n",
       " 100000    0.000897\n",
       " 75000     0.001687\n",
       " 50000     0.002110\n",
       " 10000     0.002243\n",
       " 5000      0.003012\n",
       " 1000      0.006118\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df = pd.DataFrame(accuracy_dict)\n",
    "accuracy_df.mean(), accuracy_df.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means clustering\n",
    "\n",
    "def optimized_kmeans(X, k, num_iters=100, tol=1e-4, batch_size=5000, device='cpu'):\n",
    "    X = X.to(device, dtype=torch.float32)  # Ensure correct dtype\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Initialize centroids randomly\n",
    "    indices = torch.randperm(N)[:k]\n",
    "    centroids = X[indices]\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        cluster_assignments = torch.empty(N, dtype=torch.long, device=device)\n",
    "\n",
    "        # Compute distances in batches to save memory\n",
    "        for j in range(0, N, batch_size):\n",
    "            batch = X[j:j+batch_size]\n",
    "            distances = torch.cdist(batch, centroids)  # Compute distance for this batch\n",
    "            cluster_assignments[j:j+batch_size] = torch.argmin(distances, dim=1)  # Assign cluster\n",
    "\n",
    "        # Compute new centroids\n",
    "        new_centroids = torch.zeros_like(centroids)\n",
    "        counts = torch.zeros(k, device=device)\n",
    "\n",
    "        for c in range(k):\n",
    "            cluster_indices = (cluster_assignments == c).nonzero(as_tuple=True)[0]\n",
    "            if cluster_indices.numel() > 0:\n",
    "                new_centroids[c] = X[cluster_indices].mean(dim=0)\n",
    "                counts[c] = cluster_indices.numel()\n",
    "            else:\n",
    "                # Assign the farthest point to avoid empty clusters\n",
    "                farthest_point = X[torch.argmax(torch.cdist(X, centroids[c].unsqueeze(0)), dim=0)]\n",
    "                new_centroids[c] = farthest_point\n",
    "\n",
    "        # Check for convergence\n",
    "        if torch.allclose(new_centroids, centroids, atol=tol):\n",
    "            print(f'Converged at iteration {i}')\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return cluster_assignments, centroids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to assign labels to centroids\n",
    "def assign_labels(cluster_labels, y_true, k):\n",
    "    \"\"\"\n",
    "    Assigns a label to each K-Means cluster using majority voting.\n",
    "    \n",
    "    Args:\n",
    "    - cluster_labels (Tensor): Cluster assignments for each point\n",
    "    - y_true (Tensor): True MNIST labels\n",
    "    - k (int): Number of clusters\n",
    "\n",
    "    Returns:\n",
    "    - cluster_to_label (list): List where index `i` corresponds to cluster `i`'s assigned label\n",
    "    \"\"\"\n",
    "    cluster_to_label = [-1] * k  # Initialize list with -1 for empty clusters\n",
    "\n",
    "    for cluster in range(k):\n",
    "        # Get all true labels for this cluster\n",
    "        cluster_indices = (cluster_labels == cluster).nonzero(as_tuple=True)[0]\n",
    "        true_labels = y_true[cluster_indices]\n",
    "\n",
    "        # Find the most common label in this cluster\n",
    "        if len(true_labels) > 0:\n",
    "            most_common_label = Counter(true_labels.tolist()).most_common(1)[0][0]\n",
    "            cluster_to_label[cluster] = most_common_label\n",
    "\n",
    "    return cluster_to_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in num_subsets:\n",
    "    if os.path.exists(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\"):\n",
    "        continue\n",
    "    else:\n",
    "        cluster_labels, centroids = optimized_kmeans(train_data, subset)\n",
    "        cluster_to_label = assign_labels(cluster_labels, train_labels, subset)\n",
    "        if os.path.exists('emnist_centroids') == False:\n",
    "            os.mkdir('emnist_centroids')\n",
    "        torch.save(centroids, f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\")\n",
    "        torch.save(cluster_to_label, f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8486\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8454\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8332\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8140\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7510\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels)\n",
    "    accuracy_dict_kmeans[subset] = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.848606</td>\n",
       "      <td>0.845433</td>\n",
       "      <td>0.833221</td>\n",
       "      <td>0.81399</td>\n",
       "      <td>0.75101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      75000     50000     10000    5000     1000 \n",
       "0  0.848606  0.845433  0.833221  0.81399  0.75101"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df_kmeans = pd.DataFrame(accuracy_dict_kmeans)\n",
    "accuracy_df_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying size K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8486\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8454\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8332\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8140\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7510\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8577\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8526\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8297\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8112\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7236\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8594\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8555\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8302\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8116\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7172\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8584\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8539\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8244\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8058\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7055\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8562\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8490\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8204\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.7987\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.6973\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8539\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8468\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8164\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7932\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.6876\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8498\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8440\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8117\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7892\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6764\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8463\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8408\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8069\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7818\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6659\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8439\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8386\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8030\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7761\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6586\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8400\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8346\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7976\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7718\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6537\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8385\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8321\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7951\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7680\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6480\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8354\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8300\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7902\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7643\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6410\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8347\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8274\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7873\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7609\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6379\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8315\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8259\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7846\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7564\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6326\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8293\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8231\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7814\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7529\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6270\n"
     ]
    }
   ],
   "source": [
    "full_accuracy_df_kmeans = pd.DataFrame()\n",
    "\n",
    "num_neighbors = np.arange(1,30,2)\n",
    "\n",
    "for k in num_neighbors:\n",
    "    accuracy_dict_kmean_per_neigbor = {}\n",
    "    for subset in num_subsets:\n",
    "        prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "        prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "        accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels, k)\n",
    "        accuracy_dict_kmean_per_neigbor[subset] = [accuracy]\n",
    "    temp = pd.DataFrame(accuracy_dict_kmean_per_neigbor)\n",
    "    full_accuracy_df_kmeans = pd.concat([full_accuracy_df_kmeans, temp], axis=0)\n",
    "full_accuracy_df_kmeans['k'] = num_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "      <th>k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.848606</td>\n",
       "      <td>0.845433</td>\n",
       "      <td>0.833221</td>\n",
       "      <td>0.813990</td>\n",
       "      <td>0.751010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.857692</td>\n",
       "      <td>0.852644</td>\n",
       "      <td>0.829712</td>\n",
       "      <td>0.811154</td>\n",
       "      <td>0.723558</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.859423</td>\n",
       "      <td>0.855481</td>\n",
       "      <td>0.830240</td>\n",
       "      <td>0.811635</td>\n",
       "      <td>0.717163</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858365</td>\n",
       "      <td>0.853942</td>\n",
       "      <td>0.824423</td>\n",
       "      <td>0.805769</td>\n",
       "      <td>0.705481</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.856250</td>\n",
       "      <td>0.849038</td>\n",
       "      <td>0.820433</td>\n",
       "      <td>0.798702</td>\n",
       "      <td>0.697260</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.853942</td>\n",
       "      <td>0.846779</td>\n",
       "      <td>0.816442</td>\n",
       "      <td>0.793221</td>\n",
       "      <td>0.687644</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.849760</td>\n",
       "      <td>0.843990</td>\n",
       "      <td>0.811683</td>\n",
       "      <td>0.789231</td>\n",
       "      <td>0.676442</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.846298</td>\n",
       "      <td>0.840817</td>\n",
       "      <td>0.806875</td>\n",
       "      <td>0.781779</td>\n",
       "      <td>0.665913</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.843894</td>\n",
       "      <td>0.838558</td>\n",
       "      <td>0.802981</td>\n",
       "      <td>0.776058</td>\n",
       "      <td>0.658558</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.834567</td>\n",
       "      <td>0.797596</td>\n",
       "      <td>0.771827</td>\n",
       "      <td>0.653654</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.832115</td>\n",
       "      <td>0.795144</td>\n",
       "      <td>0.767981</td>\n",
       "      <td>0.648029</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.835433</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.790240</td>\n",
       "      <td>0.764279</td>\n",
       "      <td>0.641010</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.834712</td>\n",
       "      <td>0.827404</td>\n",
       "      <td>0.787308</td>\n",
       "      <td>0.760913</td>\n",
       "      <td>0.637885</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831538</td>\n",
       "      <td>0.825865</td>\n",
       "      <td>0.784567</td>\n",
       "      <td>0.756394</td>\n",
       "      <td>0.632644</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.829327</td>\n",
       "      <td>0.823125</td>\n",
       "      <td>0.781394</td>\n",
       "      <td>0.752885</td>\n",
       "      <td>0.627019</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      75000     50000     10000      5000      1000   k\n",
       "0  0.848606  0.845433  0.833221  0.813990  0.751010   1\n",
       "0  0.857692  0.852644  0.829712  0.811154  0.723558   3\n",
       "0  0.859423  0.855481  0.830240  0.811635  0.717163   5\n",
       "0  0.858365  0.853942  0.824423  0.805769  0.705481   7\n",
       "0  0.856250  0.849038  0.820433  0.798702  0.697260   9\n",
       "0  0.853942  0.846779  0.816442  0.793221  0.687644  11\n",
       "0  0.849760  0.843990  0.811683  0.789231  0.676442  13\n",
       "0  0.846298  0.840817  0.806875  0.781779  0.665913  15\n",
       "0  0.843894  0.838558  0.802981  0.776058  0.658558  17\n",
       "0  0.840000  0.834567  0.797596  0.771827  0.653654  19\n",
       "0  0.838462  0.832115  0.795144  0.767981  0.648029  21\n",
       "0  0.835433  0.830000  0.790240  0.764279  0.641010  23\n",
       "0  0.834712  0.827404  0.787308  0.760913  0.637885  25\n",
       "0  0.831538  0.825865  0.784567  0.756394  0.632644  27\n",
       "0  0.829327  0.823125  0.781394  0.752885  0.627019  29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_accuracy_df_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for subset in num_subsets:\n",
    "    plt.plot(full_accuracy_df_kmeans['k'], full_accuracy_df_kmeans[subset], label=f'{subset}')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('K Nearest Neighbors vs Accuracy (Base K Means)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def parallel_kmeans_plusplus(X: torch.Tensor, k: int) -> torch.Tensor:\n",
    "    N, D = X.shape\n",
    "    device = X.device\n",
    "    centroids = torch.empty((k, D), device=device)\n",
    "\n",
    "    # 1️⃣ Pick first centroid randomly\n",
    "    first_idx = torch.randint(0, N, (1,))\n",
    "    centroids[0] = X[first_idx.squeeze()]  # ✅ Fix indexing\n",
    "\n",
    "    # 2️⃣ Track min distances\n",
    "    min_distances = torch.full((N,), float('inf'), device=device)\n",
    "\n",
    "    for i in range(1, k):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iter {i}\")\n",
    "        # Compute squared distances (ensuring correct shape)\n",
    "        new_distances = torch.sum((X - centroids[i-1].unsqueeze(0)) ** 2, dim=1)  # ✅ Fix broadcasting\n",
    "\n",
    "        # Update min distances\n",
    "        min_distances = torch.minimum(min_distances, new_distances)\n",
    "\n",
    "        # Sample next centroid\n",
    "        probabilities = min_distances / min_distances.sum()\n",
    "        next_idx = torch.multinomial(probabilities, 1)\n",
    "        centroids[i] = X[next_idx.squeeze()]  # ✅ Fix indexing\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimized_kmeans(X, k, num_iters=100, tol=1e-4, batch_size=10000, device='cpu'):\n",
    "    X = X.to(device, dtype=torch.float32)  # Ensure correct dtype\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Initialize centroids K-Means++\n",
    "    centroids = parallel_kmeans_plusplus(X, k)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        cluster_assignments = torch.empty(N, dtype=torch.long, device=device)\n",
    "\n",
    "        # Compute distances in batches to save memory\n",
    "        for j in range(0, N, batch_size):\n",
    "            batch = X[j:j+batch_size]\n",
    "            distances = torch.cdist(batch, centroids)  # Compute distance for this batch\n",
    "            cluster_assignments[j:j+batch_size] = torch.argmin(distances, dim=1)  # Assign cluster\n",
    "\n",
    "        # Compute new centroids\n",
    "        new_centroids = torch.zeros_like(centroids)\n",
    "        counts = torch.zeros(k, device=device)\n",
    "\n",
    "        for c in range(k):\n",
    "            cluster_indices = (cluster_assignments == c).nonzero(as_tuple=True)[0]\n",
    "            if cluster_indices.numel() > 0:\n",
    "                new_centroids[c] = X[cluster_indices].mean(dim=0)\n",
    "                counts[c] = cluster_indices.numel()\n",
    "            else:\n",
    "                # Assign the farthest point to avoid empty clusters\n",
    "                farthest_point = X[torch.argmax(torch.cdist(X, centroids[c].unsqueeze(0)), dim=0)]\n",
    "                new_centroids[c] = farthest_point\n",
    "\n",
    "        # Check for convergence\n",
    "        if torch.allclose(new_centroids, centroids, atol=tol):\n",
    "            print(f'Converged at iteration {i}')\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    return cluster_assignments, centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subset in num_subsets:\n",
    "    if os.path.exists(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\"):\n",
    "        continue\n",
    "    else:\n",
    "        cluster_labels, centroids = optimized_kmeans(train_data, subset)\n",
    "        cluster_to_label = assign_labels(cluster_labels, train_labels, subset)\n",
    "        torch.save(centroids, f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\")\n",
    "        torch.save(cluster_to_label, f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8539\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8472\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8287\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8178\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7526\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict_kmeans_plus = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels)\n",
    "    accuracy_dict_kmeans_plus[subset] = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.853942</td>\n",
       "      <td>0.847212</td>\n",
       "      <td>0.828702</td>\n",
       "      <td>0.817788</td>\n",
       "      <td>0.752596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      75000     50000     10000     5000      1000 \n",
       "0  0.853942  0.847212  0.828702  0.817788  0.752596"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df_kmeans_plus = pd.DataFrame(accuracy_dict_kmeans_plus)\n",
    "accuracy_df_kmeans_plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8485\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8472\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8287\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.8178\n",
      "Computing full distance matrix...\n",
      "1-NN accuracy on full test set (no batching): 0.7526\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8584\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8545\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8309\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.8131\n",
      "Computing full distance matrix...\n",
      "3-NN accuracy on full test set (no batching): 0.7220\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8599\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8545\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8325\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.8123\n",
      "Computing full distance matrix...\n",
      "5-NN accuracy on full test set (no batching): 0.7172\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8594\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8543\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8280\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.8062\n",
      "Computing full distance matrix...\n",
      "7-NN accuracy on full test set (no batching): 0.7053\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8563\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8502\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8242\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.8010\n",
      "Computing full distance matrix...\n",
      "9-NN accuracy on full test set (no batching): 0.6973\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8534\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8479\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.8209\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.7956\n",
      "Computing full distance matrix...\n",
      "11-NN accuracy on full test set (no batching): 0.6866\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8502\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8439\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.8150\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.7879\n",
      "Computing full distance matrix...\n",
      "13-NN accuracy on full test set (no batching): 0.6778\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8480\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8405\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.8102\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.7826\n",
      "Computing full distance matrix...\n",
      "15-NN accuracy on full test set (no batching): 0.6677\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8439\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8377\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.8053\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.7790\n",
      "Computing full distance matrix...\n",
      "17-NN accuracy on full test set (no batching): 0.6620\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8402\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.8354\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7999\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.7740\n",
      "Computing full distance matrix...\n",
      "19-NN accuracy on full test set (no batching): 0.6538\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8392\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.8344\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7969\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.7703\n",
      "Computing full distance matrix...\n",
      "21-NN accuracy on full test set (no batching): 0.6483\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8376\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.8301\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7929\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.7650\n",
      "Computing full distance matrix...\n",
      "23-NN accuracy on full test set (no batching): 0.6451\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8351\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.8258\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7880\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.7606\n",
      "Computing full distance matrix...\n",
      "25-NN accuracy on full test set (no batching): 0.6381\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8342\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.8251\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7849\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.7574\n",
      "Computing full distance matrix...\n",
      "27-NN accuracy on full test set (no batching): 0.6335\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8315\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.8229\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7813\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.7525\n",
      "Computing full distance matrix...\n",
      "29-NN accuracy on full test set (no batching): 0.6270\n"
     ]
    }
   ],
   "source": [
    "full_accuracy_df_kmeans_plus = pd.DataFrame()\n",
    "\n",
    "num_neighbors = np.arange(1,30,2)\n",
    "\n",
    "for k in num_neighbors:\n",
    "    accuracy_dict_kmean_per_neigbor = {}\n",
    "    for subset in num_subsets:\n",
    "        prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "        prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "        accuracy = compute_accuracy(test_data, prototype_train_data, prototype_train_labels, k)\n",
    "        accuracy_dict_kmean_per_neigbor[subset] = [accuracy]\n",
    "    temp = pd.DataFrame(accuracy_dict_kmean_per_neigbor)\n",
    "    full_accuracy_df_kmeans_plus = pd.concat([full_accuracy_df_kmeans_plus, temp], axis=0)\n",
    "full_accuracy_df_kmeans_plus['k'] = num_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "      <th>k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.848462</td>\n",
       "      <td>0.847212</td>\n",
       "      <td>0.828702</td>\n",
       "      <td>0.817788</td>\n",
       "      <td>0.752596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858413</td>\n",
       "      <td>0.854471</td>\n",
       "      <td>0.830913</td>\n",
       "      <td>0.813125</td>\n",
       "      <td>0.722019</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.859904</td>\n",
       "      <td>0.854519</td>\n",
       "      <td>0.832452</td>\n",
       "      <td>0.812308</td>\n",
       "      <td>0.717212</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.854279</td>\n",
       "      <td>0.827981</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>0.705337</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.856346</td>\n",
       "      <td>0.850192</td>\n",
       "      <td>0.824183</td>\n",
       "      <td>0.800962</td>\n",
       "      <td>0.697308</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.853365</td>\n",
       "      <td>0.847885</td>\n",
       "      <td>0.820913</td>\n",
       "      <td>0.795625</td>\n",
       "      <td>0.686635</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.850240</td>\n",
       "      <td>0.843894</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.787885</td>\n",
       "      <td>0.677837</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.847981</td>\n",
       "      <td>0.840529</td>\n",
       "      <td>0.810240</td>\n",
       "      <td>0.782596</td>\n",
       "      <td>0.667740</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.843942</td>\n",
       "      <td>0.837740</td>\n",
       "      <td>0.805337</td>\n",
       "      <td>0.778990</td>\n",
       "      <td>0.661971</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.840240</td>\n",
       "      <td>0.835433</td>\n",
       "      <td>0.799904</td>\n",
       "      <td>0.773990</td>\n",
       "      <td>0.653798</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839231</td>\n",
       "      <td>0.834423</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.770337</td>\n",
       "      <td>0.648269</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.837596</td>\n",
       "      <td>0.830144</td>\n",
       "      <td>0.792933</td>\n",
       "      <td>0.764952</td>\n",
       "      <td>0.645096</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.835144</td>\n",
       "      <td>0.825817</td>\n",
       "      <td>0.788029</td>\n",
       "      <td>0.760625</td>\n",
       "      <td>0.638077</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.834183</td>\n",
       "      <td>0.825096</td>\n",
       "      <td>0.784904</td>\n",
       "      <td>0.757356</td>\n",
       "      <td>0.633462</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831490</td>\n",
       "      <td>0.822933</td>\n",
       "      <td>0.781298</td>\n",
       "      <td>0.752452</td>\n",
       "      <td>0.627019</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      75000     50000     10000      5000      1000   k\n",
       "0  0.848462  0.847212  0.828702  0.817788  0.752596   1\n",
       "0  0.858413  0.854471  0.830913  0.813125  0.722019   3\n",
       "0  0.859904  0.854519  0.832452  0.812308  0.717212   5\n",
       "0  0.859375  0.854279  0.827981  0.806250  0.705337   7\n",
       "0  0.856346  0.850192  0.824183  0.800962  0.697308   9\n",
       "0  0.853365  0.847885  0.820913  0.795625  0.686635  11\n",
       "0  0.850240  0.843894  0.815000  0.787885  0.677837  13\n",
       "0  0.847981  0.840529  0.810240  0.782596  0.667740  15\n",
       "0  0.843942  0.837740  0.805337  0.778990  0.661971  17\n",
       "0  0.840240  0.835433  0.799904  0.773990  0.653798  19\n",
       "0  0.839231  0.834423  0.796875  0.770337  0.648269  21\n",
       "0  0.837596  0.830144  0.792933  0.764952  0.645096  23\n",
       "0  0.835144  0.825817  0.788029  0.760625  0.638077  25\n",
       "0  0.834183  0.825096  0.784904  0.757356  0.633462  27\n",
       "0  0.831490  0.822933  0.781298  0.752452  0.627019  29"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_accuracy_df_kmeans_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for subset in num_subsets:\n",
    "    plt.plot(full_accuracy_df_kmeans_plus['k'], full_accuracy_df_kmeans_plus[subset], label=f'{subset}')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('K Nearest Neighbors vs Accuracy (K Means++)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)  # Fully connected layer\n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # No activation, since CrossEntropyLoss applies softmax\n",
    "    \n",
    "    def init_weights(self):\n",
    "        nn.init.zeros_(self.linear.weight)\n",
    "        nn.init.zeros_(self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=8, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28 * 28\n",
    "num_classes = 27\n",
    "logit_model = LogisticRegression(input_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_sdg = optim.SGD(logit_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.1734\n",
      "Epoch [2/50], Loss: 1.0344\n",
      "Epoch [3/50], Loss: 1.0189\n",
      "Epoch [4/50], Loss: 1.0091\n",
      "Epoch [5/50], Loss: 1.0032\n",
      "Epoch [6/50], Loss: 0.9984\n",
      "Epoch [7/50], Loss: 0.9962\n",
      "Epoch [8/50], Loss: 0.9918\n",
      "Epoch [9/50], Loss: 0.9879\n",
      "Epoch [10/50], Loss: 0.9861\n",
      "Epoch [11/50], Loss: 0.9848\n",
      "Epoch [12/50], Loss: 0.9851\n",
      "Epoch [13/50], Loss: 0.9815\n",
      "Epoch [14/50], Loss: 0.9806\n",
      "Epoch [15/50], Loss: 0.9793\n",
      "Epoch [16/50], Loss: 0.9783\n",
      "Epoch [17/50], Loss: 0.9781\n",
      "Epoch [18/50], Loss: 0.9778\n",
      "Stopping early at epoch 18 (Loss improvement < 0.001 for 3 epochs)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "previous_loss = float('inf')  # Track the best loss\n",
    "epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    logit_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = logit_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer_sdg.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if previous_loss - current_loss < tolerance:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "            break\n",
    "    else:\n",
    "        epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "    previous_loss = current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Base Logit: 71.25%\n"
     ]
    }
   ],
   "source": [
    "logit_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        outputs = logit_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy Base Logit: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.6129\n",
      "Epoch [2/50], Loss: 1.5576\n",
      "Epoch [3/50], Loss: 1.5813\n",
      "Epoch [4/50], Loss: 1.5550\n",
      "Epoch [5/50], Loss: 1.5188\n",
      "Epoch [6/50], Loss: 1.5362\n",
      "Epoch [7/50], Loss: 1.5241\n",
      "Epoch [8/50], Loss: 1.5394\n",
      "Epoch [9/50], Loss: 1.5186\n",
      "Epoch [10/50], Loss: 1.5239\n",
      "Epoch [11/50], Loss: 1.5152\n",
      "Epoch [12/50], Loss: 1.5130\n",
      "Epoch [13/50], Loss: 1.5249\n",
      "Epoch [14/50], Loss: 1.5295\n",
      "Epoch [15/50], Loss: 1.5067\n",
      "Epoch [16/50], Loss: 1.4989\n",
      "Epoch [17/50], Loss: 1.5026\n",
      "Epoch [18/50], Loss: 1.5074\n",
      "Epoch [19/50], Loss: 1.5150\n",
      "Stopping early at epoch 19 (Loss improvement < 0.001 for 3 epochs)\n"
     ]
    }
   ],
   "source": [
    "# Define Adam Optimizer\n",
    "logit_model_lipschitz = LogisticRegression(input_dim, num_classes)\n",
    "optimizer_sdg = optim.SGD(logit_model_lipschitz.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Early stopping parameters\n",
    "tolerance = 1e-3  # Minimum change in loss to continue training\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "previous_loss = float('inf')\n",
    "epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 50  # Maximum epochs\n",
    "for epoch in range(num_epochs):\n",
    "    logit_model_lipschitz.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(-1, 28 * 28)  # Flatten images\n",
    "\n",
    "        outputs = logit_model_lipschitz(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        logit_model_lipschitz.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        L = max(param.grad.abs().max().item() for param in logit_model_lipschitz.parameters() if param.grad is not None)\n",
    "        adaptive_lr = min(0.1, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "        # Update optimizer's learning rate\n",
    "        for param_group in optimizer_sdg.param_groups:\n",
    "            param_group['lr'] = adaptive_lr\n",
    "\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if previous_loss - current_loss < tolerance:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "            break\n",
    "    else:\n",
    "        epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "    previous_loss = current_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Newtonian Logit: 61.73%\n"
     ]
    }
   ],
   "source": [
    "logit_model_lipschitz.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        outputs = logit_model_lipschitz(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy Newtonian Logit: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMNIST_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EMNIST_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)  # Corrected input size\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Adjust output layer\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, start_dim=1)  # Dynamically flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = EMNIST_CNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_sdg = optim.SGD(cnn_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(train_set))\n",
    "val_size = len(train_set) - train_size\n",
    "train_set, val_set = random_split(train_set, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_cnn = DataLoader(\n",
    "    train_set, batch_size=256, shuffle=True, \n",
    "    num_workers=8, pin_memory=True\n",
    ")\n",
    "val_loader_cnn = DataLoader(\n",
    "    val_set, batch_size=256, shuffle=False, \n",
    "    num_workers=8, pin_memory=True\n",
    ")\n",
    "test_loader_cnn = DataLoader(\n",
    "    test_set, batch_size=256, shuffle=False, \n",
    "    num_workers=8, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.9479 Val Loss: 0.4327\n",
      "Epoch [2/50], Train Loss: 0.3451 Val Loss: 0.3116\n",
      "Epoch [3/50], Train Loss: 0.2644 Val Loss: 0.2814\n",
      "Epoch [4/50], Train Loss: 0.2261 Val Loss: 0.2284\n",
      "Epoch [5/50], Train Loss: 0.1990 Val Loss: 0.2249\n",
      "Epoch [6/50], Train Loss: 0.1804 Val Loss: 0.2195\n",
      "Epoch [7/50], Train Loss: 0.1647 Val Loss: 0.2126\n",
      "Epoch [8/50], Train Loss: 0.1523 Val Loss: 0.2042\n",
      "Epoch [9/50], Train Loss: 0.1407 Val Loss: 0.2111\n",
      "Epoch [10/50], Train Loss: 0.1318 Val Loss: 0.2052\n",
      "Epoch [11/50], Train Loss: 0.1206 Val Loss: 0.2088\n",
      "Epoch [12/50], Train Loss: 0.1148 Val Loss: 0.2090\n",
      "Epoch [13/50], Train Loss: 0.1048 Val Loss: 0.2123\n",
      "Stopping early at epoch 13 (No improvement in 5 epochs)\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "\n",
    "    for images, labels in train_loader_cnn:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = cnn_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer_sdg.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader_cnn)\n",
    "\n",
    "    # Validation loop\n",
    "    cnn_model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_cnn:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = cnn_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "\n",
    "    val_loss /= len(val_loader_cnn)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 92.66%\n"
     ]
    }
   ],
   "source": [
    "cnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_cnn:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = cnn_model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print(f\"Final Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Lipschitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model_lipschitz = EMNIST_CNN(num_classes).to(device)\n",
    "optimizer_sdg = optim.SGD(cnn_model_lipschitz.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.8132 Val Loss: 0.3126\n",
      "Epoch [2/50], Train Loss: 0.2581 Val Loss: 0.2363\n",
      "Epoch [3/50], Train Loss: 0.2081 Val Loss: 0.2281\n",
      "Epoch [4/50], Train Loss: 0.1767 Val Loss: 0.2166\n",
      "Epoch [5/50], Train Loss: 0.1541 Val Loss: 0.2209\n",
      "Epoch [6/50], Train Loss: 0.1391 Val Loss: 0.2208\n",
      "Epoch [7/50], Train Loss: 0.1261 Val Loss: 0.2133\n",
      "Epoch [8/50], Train Loss: 0.1139 Val Loss: 0.2247\n",
      "Epoch [9/50], Train Loss: 0.1098 Val Loss: 0.2379\n",
      "Epoch [10/50], Train Loss: 0.1016 Val Loss: 0.2385\n",
      "Epoch [11/50], Train Loss: 0.0947 Val Loss: 0.2322\n",
      "Epoch [12/50], Train Loss: 0.0886 Val Loss: 0.2481\n",
      "Stopping early at epoch 12 (No improvement in 5 epochs)\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model_lipschitz.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader_cnn:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = cnn_model_lipschitz(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "    \n",
    "        # Backward pass\n",
    "        optimizer_sdg.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "\n",
    "        L = max(param.grad.abs().max().item() for param in cnn_model_lipschitz.parameters() if param.grad is not None)\n",
    "        adaptive_lr = min(0.1, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "        # Update optimizer's learning rate\n",
    "        for param_group in optimizer_sdg.param_groups:\n",
    "            param_group['lr'] = adaptive_lr\n",
    "\n",
    "        optimizer_sdg.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader_cnn)\n",
    "\n",
    "    # Validation loop\n",
    "    cnn_model_lipschitz.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader_cnn:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader_cnn)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Newtonian Logit: 92.91%\n"
     ]
    }
   ],
   "source": [
    "cnn_model_lipschitz.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_cnn:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = cnn_model_lipschitz(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy Newtonian Logit: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1857\n",
      "Epoch [2/100], Loss: 2.5398\n",
      "Epoch [3/100], Loss: 1.8729\n",
      "Epoch [4/100], Loss: 1.4576\n",
      "Epoch [5/100], Loss: 1.2176\n",
      "Epoch [6/100], Loss: 1.0457\n",
      "Epoch [7/100], Loss: 0.9313\n",
      "Epoch [8/100], Loss: 0.8341\n",
      "Epoch [9/100], Loss: 0.7457\n",
      "Epoch [10/100], Loss: 0.6811\n",
      "Epoch [11/100], Loss: 0.6174\n",
      "Epoch [12/100], Loss: 0.5703\n",
      "Epoch [13/100], Loss: 0.5288\n",
      "Epoch [14/100], Loss: 0.4922\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4331\n",
      "Epoch [17/100], Loss: 0.4085\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3662\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3163\n",
      "Epoch [23/100], Loss: 0.3020\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2129\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1888\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1564\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1356\n",
      "Epoch [53/100], Loss: 0.1335\n",
      "Epoch [54/100], Loss: 0.1320\n",
      "Epoch [55/100], Loss: 0.1283\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1011\n",
      "Epoch [71/100], Loss: 0.0990\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0937\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0887\n",
      "Epoch [81/100], Loss: 0.0877\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0846\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0757\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Stopping early at epoch 98 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [98/100], Loss: 0.0725\n",
      "Test Accuracy Base Logit: 51.78%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1865\n",
      "Epoch [2/100], Loss: 2.5245\n",
      "Epoch [3/100], Loss: 1.8751\n",
      "Epoch [4/100], Loss: 1.4651\n",
      "Epoch [5/100], Loss: 1.2073\n",
      "Epoch [6/100], Loss: 1.0404\n",
      "Epoch [7/100], Loss: 0.9256\n",
      "Epoch [8/100], Loss: 0.8296\n",
      "Epoch [9/100], Loss: 0.7461\n",
      "Epoch [10/100], Loss: 0.6788\n",
      "Epoch [11/100], Loss: 0.6204\n",
      "Epoch [12/100], Loss: 0.5719\n",
      "Epoch [13/100], Loss: 0.5277\n",
      "Epoch [14/100], Loss: 0.4913\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4328\n",
      "Epoch [17/100], Loss: 0.4085\n",
      "Epoch [18/100], Loss: 0.3858\n",
      "Epoch [19/100], Loss: 0.3669\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3189\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2067\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1676\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1356\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1025\n",
      "Epoch [70/100], Loss: 0.1016\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Stopping early at epoch 83 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [83/100], Loss: 0.0854\n",
      "Test Accuracy Base Logit: 51.84%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1909\n",
      "Epoch [2/100], Loss: 2.5318\n",
      "Epoch [3/100], Loss: 1.8811\n",
      "Epoch [4/100], Loss: 1.4622\n",
      "Epoch [5/100], Loss: 1.2109\n",
      "Epoch [6/100], Loss: 1.0472\n",
      "Epoch [7/100], Loss: 0.9266\n",
      "Epoch [8/100], Loss: 0.8329\n",
      "Epoch [9/100], Loss: 0.7521\n",
      "Epoch [10/100], Loss: 0.6753\n",
      "Epoch [11/100], Loss: 0.6215\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5265\n",
      "Epoch [14/100], Loss: 0.4917\n",
      "Epoch [15/100], Loss: 0.4590\n",
      "Epoch [16/100], Loss: 0.4307\n",
      "Epoch [17/100], Loss: 0.4070\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3470\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2788\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2408\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1281\n",
      "Epoch [56/100], Loss: 0.1251\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1217\n",
      "Epoch [59/100], Loss: 0.1196\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1087\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1025\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [88/100], Loss: 0.0805\n",
      "Test Accuracy Base Logit: 51.96%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1862\n",
      "Epoch [2/100], Loss: 2.5427\n",
      "Epoch [3/100], Loss: 1.8829\n",
      "Epoch [4/100], Loss: 1.4641\n",
      "Epoch [5/100], Loss: 1.2122\n",
      "Epoch [6/100], Loss: 1.0451\n",
      "Epoch [7/100], Loss: 0.9236\n",
      "Epoch [8/100], Loss: 0.8316\n",
      "Epoch [9/100], Loss: 0.7498\n",
      "Epoch [10/100], Loss: 0.6760\n",
      "Epoch [11/100], Loss: 0.6173\n",
      "Epoch [12/100], Loss: 0.5699\n",
      "Epoch [13/100], Loss: 0.5271\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4616\n",
      "Epoch [16/100], Loss: 0.4328\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3849\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3048\n",
      "Epoch [24/100], Loss: 0.2905\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1639\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1531\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0856\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0809\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [88/100], Loss: 0.0809\n",
      "Test Accuracy Base Logit: 51.76%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1938\n",
      "Epoch [2/100], Loss: 2.5256\n",
      "Epoch [3/100], Loss: 1.8815\n",
      "Epoch [4/100], Loss: 1.4547\n",
      "Epoch [5/100], Loss: 1.2036\n",
      "Epoch [6/100], Loss: 1.0447\n",
      "Epoch [7/100], Loss: 0.9230\n",
      "Epoch [8/100], Loss: 0.8307\n",
      "Epoch [9/100], Loss: 0.7477\n",
      "Epoch [10/100], Loss: 0.6795\n",
      "Epoch [11/100], Loss: 0.6210\n",
      "Epoch [12/100], Loss: 0.5699\n",
      "Epoch [13/100], Loss: 0.5292\n",
      "Epoch [14/100], Loss: 0.4927\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4315\n",
      "Epoch [17/100], Loss: 0.4080\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3189\n",
      "Epoch [23/100], Loss: 0.3043\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2700\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1801\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1636\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0971\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0923\n",
      "Epoch [78/100], Loss: 0.0909\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0849\n",
      "Epoch [85/100], Loss: 0.0839\n",
      "Stopping early at epoch 85 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [85/100], Loss: 0.0839\n",
      "Test Accuracy Base Logit: 51.92%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1806\n",
      "Epoch [2/100], Loss: 2.5336\n",
      "Epoch [3/100], Loss: 1.8838\n",
      "Epoch [4/100], Loss: 1.4694\n",
      "Epoch [5/100], Loss: 1.2103\n",
      "Epoch [6/100], Loss: 1.0472\n",
      "Epoch [7/100], Loss: 0.9226\n",
      "Epoch [8/100], Loss: 0.8309\n",
      "Epoch [9/100], Loss: 0.7505\n",
      "Epoch [10/100], Loss: 0.6779\n",
      "Epoch [11/100], Loss: 0.6206\n",
      "Epoch [12/100], Loss: 0.5720\n",
      "Epoch [13/100], Loss: 0.5270\n",
      "Epoch [14/100], Loss: 0.4910\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4333\n",
      "Epoch [17/100], Loss: 0.4068\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3487\n",
      "Epoch [21/100], Loss: 0.3307\n",
      "Epoch [22/100], Loss: 0.3164\n",
      "Epoch [23/100], Loss: 0.3053\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2132\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1674\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1529\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1282\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1195\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1149\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0846\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0800\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [93/100], Loss: 0.0763\n",
      "Test Accuracy Base Logit: 51.79%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1764\n",
      "Epoch [2/100], Loss: 2.5398\n",
      "Epoch [3/100], Loss: 1.8848\n",
      "Epoch [4/100], Loss: 1.4669\n",
      "Epoch [5/100], Loss: 1.2246\n",
      "Epoch [6/100], Loss: 1.0443\n",
      "Epoch [7/100], Loss: 0.9249\n",
      "Epoch [8/100], Loss: 0.8302\n",
      "Epoch [9/100], Loss: 0.7479\n",
      "Epoch [10/100], Loss: 0.6779\n",
      "Epoch [11/100], Loss: 0.6199\n",
      "Epoch [12/100], Loss: 0.5712\n",
      "Epoch [13/100], Loss: 0.5308\n",
      "Epoch [14/100], Loss: 0.4939\n",
      "Epoch [15/100], Loss: 0.4622\n",
      "Epoch [16/100], Loss: 0.4338\n",
      "Epoch [17/100], Loss: 0.4078\n",
      "Epoch [18/100], Loss: 0.3895\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3329\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3042\n",
      "Epoch [24/100], Loss: 0.2922\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2595\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2281\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2129\n",
      "Epoch [34/100], Loss: 0.2069\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1638\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1312\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1237\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1149\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1096\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1033\n",
      "Epoch [69/100], Loss: 0.1026\n",
      "Epoch [70/100], Loss: 0.1004\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0988\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0858\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [88/100], Loss: 0.0806\n",
      "Test Accuracy Base Logit: 51.81%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1830\n",
      "Epoch [2/100], Loss: 2.5320\n",
      "Epoch [3/100], Loss: 1.8781\n",
      "Epoch [4/100], Loss: 1.4552\n",
      "Epoch [5/100], Loss: 1.2123\n",
      "Epoch [6/100], Loss: 1.0474\n",
      "Epoch [7/100], Loss: 0.9275\n",
      "Epoch [8/100], Loss: 0.8307\n",
      "Epoch [9/100], Loss: 0.7459\n",
      "Epoch [10/100], Loss: 0.6788\n",
      "Epoch [11/100], Loss: 0.6190\n",
      "Epoch [12/100], Loss: 0.5715\n",
      "Epoch [13/100], Loss: 0.5265\n",
      "Epoch [14/100], Loss: 0.4921\n",
      "Epoch [15/100], Loss: 0.4613\n",
      "Epoch [16/100], Loss: 0.4312\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3668\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3336\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2118\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1839\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1759\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1139\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1087\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [86/100], Loss: 0.0824\n",
      "Test Accuracy Base Logit: 51.89%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1796\n",
      "Epoch [2/100], Loss: 2.5380\n",
      "Epoch [3/100], Loss: 1.8744\n",
      "Epoch [4/100], Loss: 1.4484\n",
      "Epoch [5/100], Loss: 1.2122\n",
      "Epoch [6/100], Loss: 1.0479\n",
      "Epoch [7/100], Loss: 0.9274\n",
      "Epoch [8/100], Loss: 0.8354\n",
      "Epoch [9/100], Loss: 0.7539\n",
      "Epoch [10/100], Loss: 0.6795\n",
      "Epoch [11/100], Loss: 0.6227\n",
      "Epoch [12/100], Loss: 0.5702\n",
      "Epoch [13/100], Loss: 0.5284\n",
      "Epoch [14/100], Loss: 0.4917\n",
      "Epoch [15/100], Loss: 0.4585\n",
      "Epoch [16/100], Loss: 0.4296\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3672\n",
      "Epoch [20/100], Loss: 0.3493\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2268\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1908\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1674\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1467\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0821\n",
      "Epoch [87/100], Loss: 0.0818\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0800\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Epoch [91/100], Loss: 0.0783\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [91/100], Loss: 0.0783\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1861\n",
      "Epoch [2/100], Loss: 2.5326\n",
      "Epoch [3/100], Loss: 1.8690\n",
      "Epoch [4/100], Loss: 1.4581\n",
      "Epoch [5/100], Loss: 1.2091\n",
      "Epoch [6/100], Loss: 1.0432\n",
      "Epoch [7/100], Loss: 0.9259\n",
      "Epoch [8/100], Loss: 0.8348\n",
      "Epoch [9/100], Loss: 0.7469\n",
      "Epoch [10/100], Loss: 0.6767\n",
      "Epoch [11/100], Loss: 0.6188\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5311\n",
      "Epoch [14/100], Loss: 0.4912\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4320\n",
      "Epoch [17/100], Loss: 0.4099\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2196\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1304\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1056\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0999\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0867\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [90/100], Loss: 0.0788\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1871\n",
      "Epoch [2/100], Loss: 2.5403\n",
      "Epoch [3/100], Loss: 1.8799\n",
      "Epoch [4/100], Loss: 1.4561\n",
      "Epoch [5/100], Loss: 1.2114\n",
      "Epoch [6/100], Loss: 1.0422\n",
      "Epoch [7/100], Loss: 0.9276\n",
      "Epoch [8/100], Loss: 0.8299\n",
      "Epoch [9/100], Loss: 0.7489\n",
      "Epoch [10/100], Loss: 0.6789\n",
      "Epoch [11/100], Loss: 0.6195\n",
      "Epoch [12/100], Loss: 0.5710\n",
      "Epoch [13/100], Loss: 0.5287\n",
      "Epoch [14/100], Loss: 0.4913\n",
      "Epoch [15/100], Loss: 0.4581\n",
      "Epoch [16/100], Loss: 0.4340\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3667\n",
      "Epoch [20/100], Loss: 0.3467\n",
      "Epoch [21/100], Loss: 0.3310\n",
      "Epoch [22/100], Loss: 0.3182\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2905\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2699\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2421\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1951\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1602\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1436\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0933\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0898\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [90/100], Loss: 0.0789\n",
      "Test Accuracy Base Logit: 51.83%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1858\n",
      "Epoch [2/100], Loss: 2.5414\n",
      "Epoch [3/100], Loss: 1.8854\n",
      "Epoch [4/100], Loss: 1.4634\n",
      "Epoch [5/100], Loss: 1.2119\n",
      "Epoch [6/100], Loss: 1.0483\n",
      "Epoch [7/100], Loss: 0.9287\n",
      "Epoch [8/100], Loss: 0.8351\n",
      "Epoch [9/100], Loss: 0.7493\n",
      "Epoch [10/100], Loss: 0.6800\n",
      "Epoch [11/100], Loss: 0.6206\n",
      "Epoch [12/100], Loss: 0.5723\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4926\n",
      "Epoch [15/100], Loss: 0.4602\n",
      "Epoch [16/100], Loss: 0.4317\n",
      "Epoch [17/100], Loss: 0.4071\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3471\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3176\n",
      "Epoch [23/100], Loss: 0.3024\n",
      "Epoch [24/100], Loss: 0.2900\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2699\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1149\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0999\n",
      "Epoch [72/100], Loss: 0.0983\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [86/100], Loss: 0.0823\n",
      "Test Accuracy Base Logit: 51.92%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1879\n",
      "Epoch [2/100], Loss: 2.5242\n",
      "Epoch [3/100], Loss: 1.8770\n",
      "Epoch [4/100], Loss: 1.4602\n",
      "Epoch [5/100], Loss: 1.2138\n",
      "Epoch [6/100], Loss: 1.0487\n",
      "Epoch [7/100], Loss: 0.9267\n",
      "Epoch [8/100], Loss: 0.8309\n",
      "Epoch [9/100], Loss: 0.7466\n",
      "Epoch [10/100], Loss: 0.6771\n",
      "Epoch [11/100], Loss: 0.6208\n",
      "Epoch [12/100], Loss: 0.5694\n",
      "Epoch [13/100], Loss: 0.5288\n",
      "Epoch [14/100], Loss: 0.4930\n",
      "Epoch [15/100], Loss: 0.4605\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4081\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3660\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2922\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2699\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2207\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1958\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1532\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1330\n",
      "Epoch [54/100], Loss: 0.1304\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1250\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0976\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0945\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0912\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [92/100], Loss: 0.0771\n",
      "Test Accuracy Base Logit: 51.82%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1858\n",
      "Epoch [2/100], Loss: 2.5421\n",
      "Epoch [3/100], Loss: 1.8745\n",
      "Epoch [4/100], Loss: 1.4555\n",
      "Epoch [5/100], Loss: 1.2117\n",
      "Epoch [6/100], Loss: 1.0505\n",
      "Epoch [7/100], Loss: 0.9262\n",
      "Epoch [8/100], Loss: 0.8320\n",
      "Epoch [9/100], Loss: 0.7478\n",
      "Epoch [10/100], Loss: 0.6780\n",
      "Epoch [11/100], Loss: 0.6189\n",
      "Epoch [12/100], Loss: 0.5711\n",
      "Epoch [13/100], Loss: 0.5288\n",
      "Epoch [14/100], Loss: 0.4933\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4062\n",
      "Epoch [18/100], Loss: 0.3863\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3472\n",
      "Epoch [21/100], Loss: 0.3321\n",
      "Epoch [22/100], Loss: 0.3164\n",
      "Epoch [23/100], Loss: 0.3038\n",
      "Epoch [24/100], Loss: 0.2921\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2130\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.1997\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1807\n",
      "Epoch [40/100], Loss: 0.1767\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1260\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [87/100], Loss: 0.0816\n",
      "Test Accuracy Base Logit: 51.93%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1880\n",
      "Epoch [2/100], Loss: 2.5169\n",
      "Epoch [3/100], Loss: 1.8907\n",
      "Epoch [4/100], Loss: 1.4673\n",
      "Epoch [5/100], Loss: 1.2108\n",
      "Epoch [6/100], Loss: 1.0436\n",
      "Epoch [7/100], Loss: 0.9262\n",
      "Epoch [8/100], Loss: 0.8353\n",
      "Epoch [9/100], Loss: 0.7469\n",
      "Epoch [10/100], Loss: 0.6796\n",
      "Epoch [11/100], Loss: 0.6192\n",
      "Epoch [12/100], Loss: 0.5727\n",
      "Epoch [13/100], Loss: 0.5292\n",
      "Epoch [14/100], Loss: 0.4919\n",
      "Epoch [15/100], Loss: 0.4627\n",
      "Epoch [16/100], Loss: 0.4308\n",
      "Epoch [17/100], Loss: 0.4061\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3647\n",
      "Epoch [20/100], Loss: 0.3471\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2699\n",
      "Epoch [27/100], Loss: 0.2584\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2182\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1957\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1530\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1380\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1305\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1159\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1041\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Stopping early at epoch 84 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [84/100], Loss: 0.0845\n",
      "Test Accuracy Base Logit: 51.98%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1926\n",
      "Epoch [2/100], Loss: 2.5332\n",
      "Epoch [3/100], Loss: 1.8793\n",
      "Epoch [4/100], Loss: 1.4627\n",
      "Epoch [5/100], Loss: 1.2081\n",
      "Epoch [6/100], Loss: 1.0460\n",
      "Epoch [7/100], Loss: 0.9302\n",
      "Epoch [8/100], Loss: 0.8333\n",
      "Epoch [9/100], Loss: 0.7460\n",
      "Epoch [10/100], Loss: 0.6772\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5280\n",
      "Epoch [14/100], Loss: 0.4923\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4325\n",
      "Epoch [17/100], Loss: 0.4077\n",
      "Epoch [18/100], Loss: 0.3862\n",
      "Epoch [19/100], Loss: 0.3665\n",
      "Epoch [20/100], Loss: 0.3478\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2802\n",
      "Epoch [26/100], Loss: 0.2694\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2270\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1801\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1682\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1612\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1531\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1140\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0973\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0860\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [89/100], Loss: 0.0797\n",
      "Test Accuracy Base Logit: 51.86%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1881\n",
      "Epoch [2/100], Loss: 2.5283\n",
      "Epoch [3/100], Loss: 1.8733\n",
      "Epoch [4/100], Loss: 1.4626\n",
      "Epoch [5/100], Loss: 1.2113\n",
      "Epoch [6/100], Loss: 1.0514\n",
      "Epoch [7/100], Loss: 0.9276\n",
      "Epoch [8/100], Loss: 0.8314\n",
      "Epoch [9/100], Loss: 0.7469\n",
      "Epoch [10/100], Loss: 0.6754\n",
      "Epoch [11/100], Loss: 0.6149\n",
      "Epoch [12/100], Loss: 0.5688\n",
      "Epoch [13/100], Loss: 0.5284\n",
      "Epoch [14/100], Loss: 0.4923\n",
      "Epoch [15/100], Loss: 0.4594\n",
      "Epoch [16/100], Loss: 0.4331\n",
      "Epoch [17/100], Loss: 0.4086\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3660\n",
      "Epoch [20/100], Loss: 0.3484\n",
      "Epoch [21/100], Loss: 0.3312\n",
      "Epoch [22/100], Loss: 0.3163\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2409\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2181\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1966\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1747\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1160\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0945\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0856\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0818\n",
      "Epoch [88/100], Loss: 0.0809\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [92/100], Loss: 0.0771\n",
      "Test Accuracy Base Logit: 51.84%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1976\n",
      "Epoch [2/100], Loss: 2.5323\n",
      "Epoch [3/100], Loss: 1.8807\n",
      "Epoch [4/100], Loss: 1.4712\n",
      "Epoch [5/100], Loss: 1.2104\n",
      "Epoch [6/100], Loss: 1.0489\n",
      "Epoch [7/100], Loss: 0.9309\n",
      "Epoch [8/100], Loss: 0.8306\n",
      "Epoch [9/100], Loss: 0.7455\n",
      "Epoch [10/100], Loss: 0.6781\n",
      "Epoch [11/100], Loss: 0.6198\n",
      "Epoch [12/100], Loss: 0.5731\n",
      "Epoch [13/100], Loss: 0.5285\n",
      "Epoch [14/100], Loss: 0.4907\n",
      "Epoch [15/100], Loss: 0.4615\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4056\n",
      "Epoch [18/100], Loss: 0.3845\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3478\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2599\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2424\n",
      "Epoch [30/100], Loss: 0.2329\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2196\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1955\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1566\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [90/100], Loss: 0.0787\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1850\n",
      "Epoch [2/100], Loss: 2.5344\n",
      "Epoch [3/100], Loss: 1.8742\n",
      "Epoch [4/100], Loss: 1.4559\n",
      "Epoch [5/100], Loss: 1.2099\n",
      "Epoch [6/100], Loss: 1.0484\n",
      "Epoch [7/100], Loss: 0.9266\n",
      "Epoch [8/100], Loss: 0.8315\n",
      "Epoch [9/100], Loss: 0.7475\n",
      "Epoch [10/100], Loss: 0.6772\n",
      "Epoch [11/100], Loss: 0.6178\n",
      "Epoch [12/100], Loss: 0.5684\n",
      "Epoch [13/100], Loss: 0.5279\n",
      "Epoch [14/100], Loss: 0.4935\n",
      "Epoch [15/100], Loss: 0.4613\n",
      "Epoch [16/100], Loss: 0.4331\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3178\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2597\n",
      "Epoch [28/100], Loss: 0.2509\n",
      "Epoch [29/100], Loss: 0.2419\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2197\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1644\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1355\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1217\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1180\n",
      "Epoch [61/100], Loss: 0.1159\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0921\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [89/100], Loss: 0.0795\n",
      "Test Accuracy Base Logit: 51.91%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1825\n",
      "Epoch [2/100], Loss: 2.5336\n",
      "Epoch [3/100], Loss: 1.8864\n",
      "Epoch [4/100], Loss: 1.4673\n",
      "Epoch [5/100], Loss: 1.2159\n",
      "Epoch [6/100], Loss: 1.0458\n",
      "Epoch [7/100], Loss: 0.9302\n",
      "Epoch [8/100], Loss: 0.8319\n",
      "Epoch [9/100], Loss: 0.7470\n",
      "Epoch [10/100], Loss: 0.6778\n",
      "Epoch [11/100], Loss: 0.6205\n",
      "Epoch [12/100], Loss: 0.5727\n",
      "Epoch [13/100], Loss: 0.5292\n",
      "Epoch [14/100], Loss: 0.4956\n",
      "Epoch [15/100], Loss: 0.4620\n",
      "Epoch [16/100], Loss: 0.4329\n",
      "Epoch [17/100], Loss: 0.4063\n",
      "Epoch [18/100], Loss: 0.3871\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3464\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2329\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1599\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1532\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1329\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1025\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [88/100], Loss: 0.0805\n",
      "Test Accuracy Base Logit: 51.95%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1815\n",
      "Epoch [2/100], Loss: 2.5383\n",
      "Epoch [3/100], Loss: 1.8825\n",
      "Epoch [4/100], Loss: 1.4548\n",
      "Epoch [5/100], Loss: 1.2088\n",
      "Epoch [6/100], Loss: 1.0467\n",
      "Epoch [7/100], Loss: 0.9262\n",
      "Epoch [8/100], Loss: 0.8288\n",
      "Epoch [9/100], Loss: 0.7469\n",
      "Epoch [10/100], Loss: 0.6766\n",
      "Epoch [11/100], Loss: 0.6172\n",
      "Epoch [12/100], Loss: 0.5686\n",
      "Epoch [13/100], Loss: 0.5280\n",
      "Epoch [14/100], Loss: 0.4919\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4320\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3866\n",
      "Epoch [19/100], Loss: 0.3665\n",
      "Epoch [20/100], Loss: 0.3467\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2926\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2701\n",
      "Epoch [27/100], Loss: 0.2600\n",
      "Epoch [28/100], Loss: 0.2529\n",
      "Epoch [29/100], Loss: 0.2425\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1815\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1728\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1600\n",
      "Epoch [45/100], Loss: 0.1564\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1333\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1281\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1115\n",
      "Epoch [65/100], Loss: 0.1097\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1012\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [86/100], Loss: 0.0822\n",
      "Test Accuracy Base Logit: 51.92%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1872\n",
      "Epoch [2/100], Loss: 2.5315\n",
      "Epoch [3/100], Loss: 1.8868\n",
      "Epoch [4/100], Loss: 1.4616\n",
      "Epoch [5/100], Loss: 1.2048\n",
      "Epoch [6/100], Loss: 1.0477\n",
      "Epoch [7/100], Loss: 0.9302\n",
      "Epoch [8/100], Loss: 0.8303\n",
      "Epoch [9/100], Loss: 0.7462\n",
      "Epoch [10/100], Loss: 0.6815\n",
      "Epoch [11/100], Loss: 0.6199\n",
      "Epoch [12/100], Loss: 0.5698\n",
      "Epoch [13/100], Loss: 0.5293\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4585\n",
      "Epoch [16/100], Loss: 0.4325\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2787\n",
      "Epoch [26/100], Loss: 0.2684\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2343\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2201\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1761\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1636\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1572\n",
      "Epoch [46/100], Loss: 0.1530\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1241\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0910\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [90/100], Loss: 0.0788\n",
      "Test Accuracy Base Logit: 51.91%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1876\n",
      "Epoch [2/100], Loss: 2.5352\n",
      "Epoch [3/100], Loss: 1.8923\n",
      "Epoch [4/100], Loss: 1.4584\n",
      "Epoch [5/100], Loss: 1.2068\n",
      "Epoch [6/100], Loss: 1.0483\n",
      "Epoch [7/100], Loss: 0.9273\n",
      "Epoch [8/100], Loss: 0.8358\n",
      "Epoch [9/100], Loss: 0.7473\n",
      "Epoch [10/100], Loss: 0.6792\n",
      "Epoch [11/100], Loss: 0.6199\n",
      "Epoch [12/100], Loss: 0.5722\n",
      "Epoch [13/100], Loss: 0.5279\n",
      "Epoch [14/100], Loss: 0.4919\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4308\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3672\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0899\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0840\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [89/100], Loss: 0.0796\n",
      "Test Accuracy Base Logit: 51.86%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1866\n",
      "Epoch [2/100], Loss: 2.5286\n",
      "Epoch [3/100], Loss: 1.8867\n",
      "Epoch [4/100], Loss: 1.4616\n",
      "Epoch [5/100], Loss: 1.2119\n",
      "Epoch [6/100], Loss: 1.0469\n",
      "Epoch [7/100], Loss: 0.9289\n",
      "Epoch [8/100], Loss: 0.8372\n",
      "Epoch [9/100], Loss: 0.7488\n",
      "Epoch [10/100], Loss: 0.6789\n",
      "Epoch [11/100], Loss: 0.6217\n",
      "Epoch [12/100], Loss: 0.5706\n",
      "Epoch [13/100], Loss: 0.5266\n",
      "Epoch [14/100], Loss: 0.4916\n",
      "Epoch [15/100], Loss: 0.4590\n",
      "Epoch [16/100], Loss: 0.4333\n",
      "Epoch [17/100], Loss: 0.4089\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3488\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2812\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2600\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2407\n",
      "Epoch [30/100], Loss: 0.2326\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2132\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1436\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1273\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1157\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [87/100], Loss: 0.0815\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1779\n",
      "Epoch [2/100], Loss: 2.5311\n",
      "Epoch [3/100], Loss: 1.8778\n",
      "Epoch [4/100], Loss: 1.4588\n",
      "Epoch [5/100], Loss: 1.2138\n",
      "Epoch [6/100], Loss: 1.0430\n",
      "Epoch [7/100], Loss: 0.9247\n",
      "Epoch [8/100], Loss: 0.8332\n",
      "Epoch [9/100], Loss: 0.7537\n",
      "Epoch [10/100], Loss: 0.6803\n",
      "Epoch [11/100], Loss: 0.6205\n",
      "Epoch [12/100], Loss: 0.5697\n",
      "Epoch [13/100], Loss: 0.5304\n",
      "Epoch [14/100], Loss: 0.4900\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4082\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3664\n",
      "Epoch [20/100], Loss: 0.3471\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3023\n",
      "Epoch [24/100], Loss: 0.2919\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2683\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2516\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1106\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0974\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0936\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0910\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0890\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [91/100], Loss: 0.0780\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1889\n",
      "Epoch [2/100], Loss: 2.5372\n",
      "Epoch [3/100], Loss: 1.8781\n",
      "Epoch [4/100], Loss: 1.4618\n",
      "Epoch [5/100], Loss: 1.2103\n",
      "Epoch [6/100], Loss: 1.0468\n",
      "Epoch [7/100], Loss: 0.9301\n",
      "Epoch [8/100], Loss: 0.8321\n",
      "Epoch [9/100], Loss: 0.7478\n",
      "Epoch [10/100], Loss: 0.6773\n",
      "Epoch [11/100], Loss: 0.6191\n",
      "Epoch [12/100], Loss: 0.5683\n",
      "Epoch [13/100], Loss: 0.5278\n",
      "Epoch [14/100], Loss: 0.4898\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4315\n",
      "Epoch [17/100], Loss: 0.4113\n",
      "Epoch [18/100], Loss: 0.3862\n",
      "Epoch [19/100], Loss: 0.3686\n",
      "Epoch [20/100], Loss: 0.3488\n",
      "Epoch [21/100], Loss: 0.3309\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3037\n",
      "Epoch [24/100], Loss: 0.2919\n",
      "Epoch [25/100], Loss: 0.2808\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2598\n",
      "Epoch [28/100], Loss: 0.2508\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1853\n",
      "Epoch [39/100], Loss: 0.1791\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1639\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1566\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1380\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [91/100], Loss: 0.0779\n",
      "Test Accuracy Base Logit: 51.83%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1837\n",
      "Epoch [2/100], Loss: 2.5336\n",
      "Epoch [3/100], Loss: 1.8954\n",
      "Epoch [4/100], Loss: 1.4709\n",
      "Epoch [5/100], Loss: 1.2170\n",
      "Epoch [6/100], Loss: 1.0532\n",
      "Epoch [7/100], Loss: 0.9292\n",
      "Epoch [8/100], Loss: 0.8357\n",
      "Epoch [9/100], Loss: 0.7470\n",
      "Epoch [10/100], Loss: 0.6746\n",
      "Epoch [11/100], Loss: 0.6176\n",
      "Epoch [12/100], Loss: 0.5718\n",
      "Epoch [13/100], Loss: 0.5276\n",
      "Epoch [14/100], Loss: 0.4921\n",
      "Epoch [15/100], Loss: 0.4593\n",
      "Epoch [16/100], Loss: 0.4308\n",
      "Epoch [17/100], Loss: 0.4084\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3491\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2595\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2184\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2073\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1940\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1792\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1626\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1260\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0947\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0881\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0791\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0745\n",
      "Epoch [96/100], Loss: 0.0746\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0730\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Stopping early at epoch 100 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Base Logit: 51.74%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1840\n",
      "Epoch [2/100], Loss: 2.5375\n",
      "Epoch [3/100], Loss: 1.8805\n",
      "Epoch [4/100], Loss: 1.4670\n",
      "Epoch [5/100], Loss: 1.2116\n",
      "Epoch [6/100], Loss: 1.0422\n",
      "Epoch [7/100], Loss: 0.9265\n",
      "Epoch [8/100], Loss: 0.8331\n",
      "Epoch [9/100], Loss: 0.7493\n",
      "Epoch [10/100], Loss: 0.6791\n",
      "Epoch [11/100], Loss: 0.6194\n",
      "Epoch [12/100], Loss: 0.5709\n",
      "Epoch [13/100], Loss: 0.5313\n",
      "Epoch [14/100], Loss: 0.4914\n",
      "Epoch [15/100], Loss: 0.4608\n",
      "Epoch [16/100], Loss: 0.4337\n",
      "Epoch [17/100], Loss: 0.4065\n",
      "Epoch [18/100], Loss: 0.3865\n",
      "Epoch [19/100], Loss: 0.3663\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3327\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3043\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2597\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1634\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1356\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1087\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0977\n",
      "Epoch [73/100], Loss: 0.0970\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [88/100], Loss: 0.0805\n",
      "Test Accuracy Base Logit: 51.86%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1934\n",
      "Epoch [2/100], Loss: 2.5430\n",
      "Epoch [3/100], Loss: 1.8756\n",
      "Epoch [4/100], Loss: 1.4600\n",
      "Epoch [5/100], Loss: 1.2075\n",
      "Epoch [6/100], Loss: 1.0447\n",
      "Epoch [7/100], Loss: 0.9297\n",
      "Epoch [8/100], Loss: 0.8341\n",
      "Epoch [9/100], Loss: 0.7471\n",
      "Epoch [10/100], Loss: 0.6792\n",
      "Epoch [11/100], Loss: 0.6216\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5287\n",
      "Epoch [14/100], Loss: 0.4903\n",
      "Epoch [15/100], Loss: 0.4606\n",
      "Epoch [16/100], Loss: 0.4312\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3859\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2921\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2595\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2055\n",
      "Epoch [35/100], Loss: 0.2006\n",
      "Epoch [36/100], Loss: 0.1955\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1803\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1532\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1263\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1220\n",
      "Epoch [59/100], Loss: 0.1196\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1063\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0935\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [93/100], Loss: 0.0763\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1899\n",
      "Epoch [2/100], Loss: 2.5286\n",
      "Epoch [3/100], Loss: 1.8931\n",
      "Epoch [4/100], Loss: 1.4640\n",
      "Epoch [5/100], Loss: 1.2099\n",
      "Epoch [6/100], Loss: 1.0444\n",
      "Epoch [7/100], Loss: 0.9295\n",
      "Epoch [8/100], Loss: 0.8308\n",
      "Epoch [9/100], Loss: 0.7464\n",
      "Epoch [10/100], Loss: 0.6766\n",
      "Epoch [11/100], Loss: 0.6200\n",
      "Epoch [12/100], Loss: 0.5766\n",
      "Epoch [13/100], Loss: 0.5273\n",
      "Epoch [14/100], Loss: 0.4894\n",
      "Epoch [15/100], Loss: 0.4585\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4087\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3638\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2902\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2208\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2067\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1904\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1803\n",
      "Epoch [40/100], Loss: 0.1760\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1677\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1607\n",
      "Epoch [45/100], Loss: 0.1563\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1384\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1332\n",
      "Epoch [54/100], Loss: 0.1304\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1175\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1123\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0934\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Stopping early at epoch 84 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 75000, Epoch [84/100], Loss: 0.0844\n",
      "Test Accuracy Base Logit: 51.90%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1870\n",
      "Epoch [2/100], Loss: 2.5285\n",
      "Epoch [3/100], Loss: 1.8759\n",
      "Epoch [4/100], Loss: 1.4649\n",
      "Epoch [5/100], Loss: 1.2109\n",
      "Epoch [6/100], Loss: 1.0492\n",
      "Epoch [7/100], Loss: 0.9272\n",
      "Epoch [8/100], Loss: 0.8320\n",
      "Epoch [9/100], Loss: 0.7497\n",
      "Epoch [10/100], Loss: 0.6752\n",
      "Epoch [11/100], Loss: 0.6206\n",
      "Epoch [12/100], Loss: 0.5703\n",
      "Epoch [13/100], Loss: 0.5272\n",
      "Epoch [14/100], Loss: 0.4918\n",
      "Epoch [15/100], Loss: 0.4606\n",
      "Epoch [16/100], Loss: 0.4325\n",
      "Epoch [17/100], Loss: 0.4081\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3470\n",
      "Epoch [21/100], Loss: 0.3324\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2131\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.2025\n",
      "Epoch [36/100], Loss: 0.1955\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1637\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1329\n",
      "Epoch [54/100], Loss: 0.1310\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1237\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [87/100], Loss: 0.0814\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1885\n",
      "Epoch [2/100], Loss: 2.5370\n",
      "Epoch [3/100], Loss: 1.8838\n",
      "Epoch [4/100], Loss: 1.4649\n",
      "Epoch [5/100], Loss: 1.2042\n",
      "Epoch [6/100], Loss: 1.0471\n",
      "Epoch [7/100], Loss: 0.9319\n",
      "Epoch [8/100], Loss: 0.8326\n",
      "Epoch [9/100], Loss: 0.7510\n",
      "Epoch [10/100], Loss: 0.6799\n",
      "Epoch [11/100], Loss: 0.6213\n",
      "Epoch [12/100], Loss: 0.5696\n",
      "Epoch [13/100], Loss: 0.5269\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4330\n",
      "Epoch [17/100], Loss: 0.4113\n",
      "Epoch [18/100], Loss: 0.3843\n",
      "Epoch [19/100], Loss: 0.3643\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3302\n",
      "Epoch [22/100], Loss: 0.3164\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2341\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1941\n",
      "Epoch [37/100], Loss: 0.1904\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1803\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1720\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1499\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1415\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1215\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1048\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1026\n",
      "Epoch [70/100], Loss: 0.1013\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0898\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [91/100], Loss: 0.0779\n",
      "Test Accuracy Base Logit: 51.86%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1853\n",
      "Epoch [2/100], Loss: 2.5312\n",
      "Epoch [3/100], Loss: 1.8815\n",
      "Epoch [4/100], Loss: 1.4586\n",
      "Epoch [5/100], Loss: 1.2070\n",
      "Epoch [6/100], Loss: 1.0460\n",
      "Epoch [7/100], Loss: 0.9284\n",
      "Epoch [8/100], Loss: 0.8251\n",
      "Epoch [9/100], Loss: 0.7474\n",
      "Epoch [10/100], Loss: 0.6789\n",
      "Epoch [11/100], Loss: 0.6192\n",
      "Epoch [12/100], Loss: 0.5699\n",
      "Epoch [13/100], Loss: 0.5287\n",
      "Epoch [14/100], Loss: 0.4919\n",
      "Epoch [15/100], Loss: 0.4591\n",
      "Epoch [16/100], Loss: 0.4316\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3643\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2802\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2067\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1436\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1334\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0958\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0866\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0849\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0801\n",
      "Epoch [90/100], Loss: 0.0794\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [90/100], Loss: 0.0794\n",
      "Test Accuracy Base Logit: 51.76%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1897\n",
      "Epoch [2/100], Loss: 2.5238\n",
      "Epoch [3/100], Loss: 1.8845\n",
      "Epoch [4/100], Loss: 1.4579\n",
      "Epoch [5/100], Loss: 1.2130\n",
      "Epoch [6/100], Loss: 1.0500\n",
      "Epoch [7/100], Loss: 0.9280\n",
      "Epoch [8/100], Loss: 0.8281\n",
      "Epoch [9/100], Loss: 0.7479\n",
      "Epoch [10/100], Loss: 0.6812\n",
      "Epoch [11/100], Loss: 0.6186\n",
      "Epoch [12/100], Loss: 0.5699\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4927\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4083\n",
      "Epoch [18/100], Loss: 0.3841\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3327\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2009\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1861\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0985\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0931\n",
      "Epoch [78/100], Loss: 0.0912\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Epoch [92/100], Loss: 0.0774\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0750\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Stopping early at epoch 96 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [96/100], Loss: 0.0740\n",
      "Test Accuracy Base Logit: 51.77%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1915\n",
      "Epoch [2/100], Loss: 2.5385\n",
      "Epoch [3/100], Loss: 1.8838\n",
      "Epoch [4/100], Loss: 1.4635\n",
      "Epoch [5/100], Loss: 1.2152\n",
      "Epoch [6/100], Loss: 1.0506\n",
      "Epoch [7/100], Loss: 0.9319\n",
      "Epoch [8/100], Loss: 0.8314\n",
      "Epoch [9/100], Loss: 0.7489\n",
      "Epoch [10/100], Loss: 0.6774\n",
      "Epoch [11/100], Loss: 0.6205\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5272\n",
      "Epoch [14/100], Loss: 0.4906\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4325\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3475\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2056\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1757\n",
      "Epoch [41/100], Loss: 0.1719\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1329\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1215\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0983\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0934\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0899\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [91/100], Loss: 0.0780\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1909\n",
      "Epoch [2/100], Loss: 2.5269\n",
      "Epoch [3/100], Loss: 1.8748\n",
      "Epoch [4/100], Loss: 1.4647\n",
      "Epoch [5/100], Loss: 1.2052\n",
      "Epoch [6/100], Loss: 1.0418\n",
      "Epoch [7/100], Loss: 0.9317\n",
      "Epoch [8/100], Loss: 0.8344\n",
      "Epoch [9/100], Loss: 0.7516\n",
      "Epoch [10/100], Loss: 0.6818\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5714\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4933\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4326\n",
      "Epoch [17/100], Loss: 0.4063\n",
      "Epoch [18/100], Loss: 0.3843\n",
      "Epoch [19/100], Loss: 0.3671\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3312\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2699\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2196\n",
      "Epoch [33/100], Loss: 0.2129\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1563\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1445\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1228\n",
      "Epoch [58/100], Loss: 0.1221\n",
      "Epoch [59/100], Loss: 0.1201\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1157\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0998\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0951\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0898\n",
      "Epoch [80/100], Loss: 0.0881\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0846\n",
      "Epoch [85/100], Loss: 0.0838\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [92/100], Loss: 0.0772\n",
      "Test Accuracy Base Logit: 51.83%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1812\n",
      "Epoch [2/100], Loss: 2.5380\n",
      "Epoch [3/100], Loss: 1.8833\n",
      "Epoch [4/100], Loss: 1.4615\n",
      "Epoch [5/100], Loss: 1.2110\n",
      "Epoch [6/100], Loss: 1.0542\n",
      "Epoch [7/100], Loss: 0.9313\n",
      "Epoch [8/100], Loss: 0.8336\n",
      "Epoch [9/100], Loss: 0.7508\n",
      "Epoch [10/100], Loss: 0.6783\n",
      "Epoch [11/100], Loss: 0.6196\n",
      "Epoch [12/100], Loss: 0.5734\n",
      "Epoch [13/100], Loss: 0.5288\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4083\n",
      "Epoch [18/100], Loss: 0.3835\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3467\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2806\n",
      "Epoch [26/100], Loss: 0.2699\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2012\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1529\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1168\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0971\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0948\n",
      "Epoch [76/100], Loss: 0.0933\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0892\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [89/100], Loss: 0.0795\n",
      "Test Accuracy Base Logit: 51.89%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1853\n",
      "Epoch [2/100], Loss: 2.5422\n",
      "Epoch [3/100], Loss: 1.8796\n",
      "Epoch [4/100], Loss: 1.4650\n",
      "Epoch [5/100], Loss: 1.2132\n",
      "Epoch [6/100], Loss: 1.0523\n",
      "Epoch [7/100], Loss: 0.9270\n",
      "Epoch [8/100], Loss: 0.8313\n",
      "Epoch [9/100], Loss: 0.7477\n",
      "Epoch [10/100], Loss: 0.6801\n",
      "Epoch [11/100], Loss: 0.6212\n",
      "Epoch [12/100], Loss: 0.5702\n",
      "Epoch [13/100], Loss: 0.5276\n",
      "Epoch [14/100], Loss: 0.4913\n",
      "Epoch [15/100], Loss: 0.4623\n",
      "Epoch [16/100], Loss: 0.4317\n",
      "Epoch [17/100], Loss: 0.4077\n",
      "Epoch [18/100], Loss: 0.3866\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3025\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2787\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2492\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2325\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1889\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1767\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1676\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1535\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1141\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0997\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [90/100], Loss: 0.0786\n",
      "Test Accuracy Base Logit: 51.89%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1896\n",
      "Epoch [2/100], Loss: 2.5469\n",
      "Epoch [3/100], Loss: 1.8985\n",
      "Epoch [4/100], Loss: 1.4676\n",
      "Epoch [5/100], Loss: 1.2118\n",
      "Epoch [6/100], Loss: 1.0539\n",
      "Epoch [7/100], Loss: 0.9269\n",
      "Epoch [8/100], Loss: 0.8299\n",
      "Epoch [9/100], Loss: 0.7484\n",
      "Epoch [10/100], Loss: 0.6760\n",
      "Epoch [11/100], Loss: 0.6176\n",
      "Epoch [12/100], Loss: 0.5692\n",
      "Epoch [13/100], Loss: 0.5275\n",
      "Epoch [14/100], Loss: 0.4902\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3843\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2807\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2507\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1601\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1238\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1121\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0922\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0898\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0821\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Stopping early at epoch 96 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [96/100], Loss: 0.0739\n",
      "Test Accuracy Base Logit: 51.75%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1877\n",
      "Epoch [2/100], Loss: 2.5349\n",
      "Epoch [3/100], Loss: 1.8948\n",
      "Epoch [4/100], Loss: 1.4646\n",
      "Epoch [5/100], Loss: 1.2119\n",
      "Epoch [6/100], Loss: 1.0460\n",
      "Epoch [7/100], Loss: 0.9335\n",
      "Epoch [8/100], Loss: 0.8327\n",
      "Epoch [9/100], Loss: 0.7471\n",
      "Epoch [10/100], Loss: 0.6777\n",
      "Epoch [11/100], Loss: 0.6227\n",
      "Epoch [12/100], Loss: 0.5683\n",
      "Epoch [13/100], Loss: 0.5271\n",
      "Epoch [14/100], Loss: 0.4927\n",
      "Epoch [15/100], Loss: 0.4587\n",
      "Epoch [16/100], Loss: 0.4311\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3849\n",
      "Epoch [19/100], Loss: 0.3646\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3185\n",
      "Epoch [23/100], Loss: 0.3045\n",
      "Epoch [24/100], Loss: 0.2903\n",
      "Epoch [25/100], Loss: 0.2788\n",
      "Epoch [26/100], Loss: 0.2698\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2420\n",
      "Epoch [30/100], Loss: 0.2342\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2129\n",
      "Epoch [34/100], Loss: 0.2069\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1718\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1346\n",
      "Epoch [53/100], Loss: 0.1329\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1011\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0881\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Stopping early at epoch 83 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [83/100], Loss: 0.0853\n",
      "Test Accuracy Base Logit: 51.93%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1839\n",
      "Epoch [2/100], Loss: 2.5238\n",
      "Epoch [3/100], Loss: 1.8725\n",
      "Epoch [4/100], Loss: 1.4606\n",
      "Epoch [5/100], Loss: 1.2123\n",
      "Epoch [6/100], Loss: 1.0490\n",
      "Epoch [7/100], Loss: 0.9290\n",
      "Epoch [8/100], Loss: 0.8331\n",
      "Epoch [9/100], Loss: 0.7445\n",
      "Epoch [10/100], Loss: 0.6803\n",
      "Epoch [11/100], Loss: 0.6229\n",
      "Epoch [12/100], Loss: 0.5710\n",
      "Epoch [13/100], Loss: 0.5290\n",
      "Epoch [14/100], Loss: 0.4902\n",
      "Epoch [15/100], Loss: 0.4590\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4064\n",
      "Epoch [18/100], Loss: 0.3866\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3484\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3042\n",
      "Epoch [24/100], Loss: 0.2901\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2698\n",
      "Epoch [27/100], Loss: 0.2582\n",
      "Epoch [28/100], Loss: 0.2507\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2274\n",
      "Epoch [32/100], Loss: 0.2182\n",
      "Epoch [33/100], Loss: 0.2133\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1139\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0847\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [89/100], Loss: 0.0796\n",
      "Test Accuracy Base Logit: 51.91%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1808\n",
      "Epoch [2/100], Loss: 2.5358\n",
      "Epoch [3/100], Loss: 1.8798\n",
      "Epoch [4/100], Loss: 1.4718\n",
      "Epoch [5/100], Loss: 1.2122\n",
      "Epoch [6/100], Loss: 1.0466\n",
      "Epoch [7/100], Loss: 0.9293\n",
      "Epoch [8/100], Loss: 0.8325\n",
      "Epoch [9/100], Loss: 0.7496\n",
      "Epoch [10/100], Loss: 0.6784\n",
      "Epoch [11/100], Loss: 0.6220\n",
      "Epoch [12/100], Loss: 0.5720\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4587\n",
      "Epoch [16/100], Loss: 0.4320\n",
      "Epoch [17/100], Loss: 0.4068\n",
      "Epoch [18/100], Loss: 0.3840\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3496\n",
      "Epoch [21/100], Loss: 0.3350\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3045\n",
      "Epoch [24/100], Loss: 0.2931\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2329\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1716\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1552\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1497\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1249\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1063\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0922\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0892\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [86/100], Loss: 0.0827\n",
      "Test Accuracy Base Logit: 51.93%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1895\n",
      "Epoch [2/100], Loss: 2.5308\n",
      "Epoch [3/100], Loss: 1.8859\n",
      "Epoch [4/100], Loss: 1.4638\n",
      "Epoch [5/100], Loss: 1.2141\n",
      "Epoch [6/100], Loss: 1.0455\n",
      "Epoch [7/100], Loss: 0.9270\n",
      "Epoch [8/100], Loss: 0.8356\n",
      "Epoch [9/100], Loss: 0.7484\n",
      "Epoch [10/100], Loss: 0.6773\n",
      "Epoch [11/100], Loss: 0.6200\n",
      "Epoch [12/100], Loss: 0.5706\n",
      "Epoch [13/100], Loss: 0.5277\n",
      "Epoch [14/100], Loss: 0.4920\n",
      "Epoch [15/100], Loss: 0.4602\n",
      "Epoch [16/100], Loss: 0.4327\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3865\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3043\n",
      "Epoch [24/100], Loss: 0.2899\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2182\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2056\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1907\n",
      "Epoch [38/100], Loss: 0.1851\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1760\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1645\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1532\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1346\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1177\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0869\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [88/100], Loss: 0.0805\n",
      "Test Accuracy Base Logit: 51.93%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1985\n",
      "Epoch [2/100], Loss: 2.5317\n",
      "Epoch [3/100], Loss: 1.8791\n",
      "Epoch [4/100], Loss: 1.4602\n",
      "Epoch [5/100], Loss: 1.2132\n",
      "Epoch [6/100], Loss: 1.0426\n",
      "Epoch [7/100], Loss: 0.9232\n",
      "Epoch [8/100], Loss: 0.8321\n",
      "Epoch [9/100], Loss: 0.7500\n",
      "Epoch [10/100], Loss: 0.6758\n",
      "Epoch [11/100], Loss: 0.6207\n",
      "Epoch [12/100], Loss: 0.5714\n",
      "Epoch [13/100], Loss: 0.5274\n",
      "Epoch [14/100], Loss: 0.4897\n",
      "Epoch [15/100], Loss: 0.4594\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4081\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2902\n",
      "Epoch [25/100], Loss: 0.2788\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1805\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1356\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1239\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1104\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0910\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Stopping early at epoch 83 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [83/100], Loss: 0.0855\n",
      "Test Accuracy Base Logit: 51.95%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1881\n",
      "Epoch [2/100], Loss: 2.5388\n",
      "Epoch [3/100], Loss: 1.8712\n",
      "Epoch [4/100], Loss: 1.4565\n",
      "Epoch [5/100], Loss: 1.2124\n",
      "Epoch [6/100], Loss: 1.0472\n",
      "Epoch [7/100], Loss: 0.9320\n",
      "Epoch [8/100], Loss: 0.8353\n",
      "Epoch [9/100], Loss: 0.7497\n",
      "Epoch [10/100], Loss: 0.6798\n",
      "Epoch [11/100], Loss: 0.6242\n",
      "Epoch [12/100], Loss: 0.5711\n",
      "Epoch [13/100], Loss: 0.5269\n",
      "Epoch [14/100], Loss: 0.4913\n",
      "Epoch [15/100], Loss: 0.4623\n",
      "Epoch [16/100], Loss: 0.4341\n",
      "Epoch [17/100], Loss: 0.4080\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3478\n",
      "Epoch [21/100], Loss: 0.3330\n",
      "Epoch [22/100], Loss: 0.3176\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2681\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2407\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2118\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1940\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1838\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1747\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1131\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1003\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0983\n",
      "Epoch [73/100], Loss: 0.0981\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [88/100], Loss: 0.0805\n",
      "Test Accuracy Base Logit: 51.95%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1912\n",
      "Epoch [2/100], Loss: 2.5210\n",
      "Epoch [3/100], Loss: 1.8876\n",
      "Epoch [4/100], Loss: 1.4625\n",
      "Epoch [5/100], Loss: 1.2163\n",
      "Epoch [6/100], Loss: 1.0467\n",
      "Epoch [7/100], Loss: 0.9333\n",
      "Epoch [8/100], Loss: 0.8341\n",
      "Epoch [9/100], Loss: 0.7505\n",
      "Epoch [10/100], Loss: 0.6802\n",
      "Epoch [11/100], Loss: 0.6208\n",
      "Epoch [12/100], Loss: 0.5736\n",
      "Epoch [13/100], Loss: 0.5261\n",
      "Epoch [14/100], Loss: 0.4928\n",
      "Epoch [15/100], Loss: 0.4609\n",
      "Epoch [16/100], Loss: 0.4320\n",
      "Epoch [17/100], Loss: 0.4080\n",
      "Epoch [18/100], Loss: 0.3870\n",
      "Epoch [19/100], Loss: 0.3666\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3182\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2901\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2488\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2251\n",
      "Epoch [32/100], Loss: 0.2184\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1956\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1839\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1718\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1599\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1530\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1251\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [87/100], Loss: 0.0814\n",
      "Test Accuracy Base Logit: 51.96%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1908\n",
      "Epoch [2/100], Loss: 2.5333\n",
      "Epoch [3/100], Loss: 1.8818\n",
      "Epoch [4/100], Loss: 1.4600\n",
      "Epoch [5/100], Loss: 1.2127\n",
      "Epoch [6/100], Loss: 1.0543\n",
      "Epoch [7/100], Loss: 0.9301\n",
      "Epoch [8/100], Loss: 0.8349\n",
      "Epoch [9/100], Loss: 0.7475\n",
      "Epoch [10/100], Loss: 0.6795\n",
      "Epoch [11/100], Loss: 0.6180\n",
      "Epoch [12/100], Loss: 0.5725\n",
      "Epoch [13/100], Loss: 0.5290\n",
      "Epoch [14/100], Loss: 0.4919\n",
      "Epoch [15/100], Loss: 0.4605\n",
      "Epoch [16/100], Loss: 0.4307\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3658\n",
      "Epoch [20/100], Loss: 0.3471\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2582\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1636\n",
      "Epoch [44/100], Loss: 0.1599\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1333\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1282\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1212\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [89/100], Loss: 0.0796\n",
      "Test Accuracy Base Logit: 51.84%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1876\n",
      "Epoch [2/100], Loss: 2.5339\n",
      "Epoch [3/100], Loss: 1.8982\n",
      "Epoch [4/100], Loss: 1.4560\n",
      "Epoch [5/100], Loss: 1.2079\n",
      "Epoch [6/100], Loss: 1.0450\n",
      "Epoch [7/100], Loss: 0.9295\n",
      "Epoch [8/100], Loss: 0.8337\n",
      "Epoch [9/100], Loss: 0.7473\n",
      "Epoch [10/100], Loss: 0.6796\n",
      "Epoch [11/100], Loss: 0.6210\n",
      "Epoch [12/100], Loss: 0.5701\n",
      "Epoch [13/100], Loss: 0.5273\n",
      "Epoch [14/100], Loss: 0.4963\n",
      "Epoch [15/100], Loss: 0.4616\n",
      "Epoch [16/100], Loss: 0.4306\n",
      "Epoch [17/100], Loss: 0.4056\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3668\n",
      "Epoch [20/100], Loss: 0.3491\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3206\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2694\n",
      "Epoch [27/100], Loss: 0.2598\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2428\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2198\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1497\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1281\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1121\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [88/100], Loss: 0.0806\n",
      "Test Accuracy Base Logit: 51.90%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1914\n",
      "Epoch [2/100], Loss: 2.5341\n",
      "Epoch [3/100], Loss: 1.8778\n",
      "Epoch [4/100], Loss: 1.4575\n",
      "Epoch [5/100], Loss: 1.2156\n",
      "Epoch [6/100], Loss: 1.0488\n",
      "Epoch [7/100], Loss: 0.9265\n",
      "Epoch [8/100], Loss: 0.8352\n",
      "Epoch [9/100], Loss: 0.7481\n",
      "Epoch [10/100], Loss: 0.6758\n",
      "Epoch [11/100], Loss: 0.6174\n",
      "Epoch [12/100], Loss: 0.5680\n",
      "Epoch [13/100], Loss: 0.5269\n",
      "Epoch [14/100], Loss: 0.4911\n",
      "Epoch [15/100], Loss: 0.4608\n",
      "Epoch [16/100], Loss: 0.4329\n",
      "Epoch [17/100], Loss: 0.4088\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3478\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3160\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1759\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1487\n",
      "Epoch [48/100], Loss: 0.1471\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1356\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1056\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0946\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0776\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0757\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Stopping early at epoch 98 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [98/100], Loss: 0.0724\n",
      "Test Accuracy Base Logit: 51.82%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1876\n",
      "Epoch [2/100], Loss: 2.5346\n",
      "Epoch [3/100], Loss: 1.8755\n",
      "Epoch [4/100], Loss: 1.4714\n",
      "Epoch [5/100], Loss: 1.2110\n",
      "Epoch [6/100], Loss: 1.0460\n",
      "Epoch [7/100], Loss: 0.9321\n",
      "Epoch [8/100], Loss: 0.8354\n",
      "Epoch [9/100], Loss: 0.7472\n",
      "Epoch [10/100], Loss: 0.6839\n",
      "Epoch [11/100], Loss: 0.6201\n",
      "Epoch [12/100], Loss: 0.5707\n",
      "Epoch [13/100], Loss: 0.5307\n",
      "Epoch [14/100], Loss: 0.4923\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4316\n",
      "Epoch [17/100], Loss: 0.4087\n",
      "Epoch [18/100], Loss: 0.3863\n",
      "Epoch [19/100], Loss: 0.3666\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3163\n",
      "Epoch [23/100], Loss: 0.3045\n",
      "Epoch [24/100], Loss: 0.2905\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2609\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2407\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2118\n",
      "Epoch [34/100], Loss: 0.2053\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1838\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1438\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1382\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1305\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [89/100], Loss: 0.0795\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1856\n",
      "Epoch [2/100], Loss: 2.5281\n",
      "Epoch [3/100], Loss: 1.8822\n",
      "Epoch [4/100], Loss: 1.4672\n",
      "Epoch [5/100], Loss: 1.2113\n",
      "Epoch [6/100], Loss: 1.0491\n",
      "Epoch [7/100], Loss: 0.9297\n",
      "Epoch [8/100], Loss: 0.8319\n",
      "Epoch [9/100], Loss: 0.7494\n",
      "Epoch [10/100], Loss: 0.6799\n",
      "Epoch [11/100], Loss: 0.6196\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5289\n",
      "Epoch [14/100], Loss: 0.4908\n",
      "Epoch [15/100], Loss: 0.4589\n",
      "Epoch [16/100], Loss: 0.4338\n",
      "Epoch [17/100], Loss: 0.4068\n",
      "Epoch [18/100], Loss: 0.3878\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3493\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3185\n",
      "Epoch [23/100], Loss: 0.3050\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2817\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2608\n",
      "Epoch [28/100], Loss: 0.2509\n",
      "Epoch [29/100], Loss: 0.2422\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2183\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2054\n",
      "Epoch [35/100], Loss: 0.2010\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1852\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1497\n",
      "Epoch [48/100], Loss: 0.1468\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1158\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1033\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0901\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [92/100], Loss: 0.0772\n",
      "Test Accuracy Base Logit: 51.86%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1920\n",
      "Epoch [2/100], Loss: 2.5275\n",
      "Epoch [3/100], Loss: 1.8828\n",
      "Epoch [4/100], Loss: 1.4602\n",
      "Epoch [5/100], Loss: 1.2196\n",
      "Epoch [6/100], Loss: 1.0497\n",
      "Epoch [7/100], Loss: 0.9265\n",
      "Epoch [8/100], Loss: 0.8348\n",
      "Epoch [9/100], Loss: 0.7514\n",
      "Epoch [10/100], Loss: 0.6786\n",
      "Epoch [11/100], Loss: 0.6193\n",
      "Epoch [12/100], Loss: 0.5717\n",
      "Epoch [13/100], Loss: 0.5284\n",
      "Epoch [14/100], Loss: 0.4900\n",
      "Epoch [15/100], Loss: 0.4627\n",
      "Epoch [16/100], Loss: 0.4335\n",
      "Epoch [17/100], Loss: 0.4056\n",
      "Epoch [18/100], Loss: 0.3881\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2806\n",
      "Epoch [26/100], Loss: 0.2700\n",
      "Epoch [27/100], Loss: 0.2618\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1850\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1566\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1466\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1177\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1121\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1016\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0984\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0947\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [86/100], Loss: 0.0825\n",
      "Test Accuracy Base Logit: 51.94%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1859\n",
      "Epoch [2/100], Loss: 2.5359\n",
      "Epoch [3/100], Loss: 1.8892\n",
      "Epoch [4/100], Loss: 1.4626\n",
      "Epoch [5/100], Loss: 1.2093\n",
      "Epoch [6/100], Loss: 1.0455\n",
      "Epoch [7/100], Loss: 0.9258\n",
      "Epoch [8/100], Loss: 0.8303\n",
      "Epoch [9/100], Loss: 0.7481\n",
      "Epoch [10/100], Loss: 0.6773\n",
      "Epoch [11/100], Loss: 0.6162\n",
      "Epoch [12/100], Loss: 0.5723\n",
      "Epoch [13/100], Loss: 0.5329\n",
      "Epoch [14/100], Loss: 0.4919\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4335\n",
      "Epoch [17/100], Loss: 0.4093\n",
      "Epoch [18/100], Loss: 0.3863\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3324\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3049\n",
      "Epoch [24/100], Loss: 0.2919\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2595\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.1997\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1626\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1399\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1272\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1215\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1143\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [86/100], Loss: 0.0826\n",
      "Test Accuracy Base Logit: 51.95%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1843\n",
      "Epoch [2/100], Loss: 2.5344\n",
      "Epoch [3/100], Loss: 1.8813\n",
      "Epoch [4/100], Loss: 1.4592\n",
      "Epoch [5/100], Loss: 1.2139\n",
      "Epoch [6/100], Loss: 1.0445\n",
      "Epoch [7/100], Loss: 0.9295\n",
      "Epoch [8/100], Loss: 0.8337\n",
      "Epoch [9/100], Loss: 0.7503\n",
      "Epoch [10/100], Loss: 0.6775\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5688\n",
      "Epoch [13/100], Loss: 0.5290\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4591\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4067\n",
      "Epoch [18/100], Loss: 0.3859\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3478\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3179\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2683\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2422\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2130\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1955\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1805\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1643\n",
      "Epoch [44/100], Loss: 0.1601\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1499\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1384\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1304\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1197\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0909\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [93/100], Loss: 0.0763\n",
      "Test Accuracy Base Logit: 51.85%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1831\n",
      "Epoch [2/100], Loss: 2.5388\n",
      "Epoch [3/100], Loss: 1.8795\n",
      "Epoch [4/100], Loss: 1.4655\n",
      "Epoch [5/100], Loss: 1.2097\n",
      "Epoch [6/100], Loss: 1.0493\n",
      "Epoch [7/100], Loss: 0.9322\n",
      "Epoch [8/100], Loss: 0.8330\n",
      "Epoch [9/100], Loss: 0.7500\n",
      "Epoch [10/100], Loss: 0.6814\n",
      "Epoch [11/100], Loss: 0.6199\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5289\n",
      "Epoch [14/100], Loss: 0.4936\n",
      "Epoch [15/100], Loss: 0.4614\n",
      "Epoch [16/100], Loss: 0.4329\n",
      "Epoch [17/100], Loss: 0.4109\n",
      "Epoch [18/100], Loss: 0.3863\n",
      "Epoch [19/100], Loss: 0.3660\n",
      "Epoch [20/100], Loss: 0.3489\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3025\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2421\n",
      "Epoch [30/100], Loss: 0.2341\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1806\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1529\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1126\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1025\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0881\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [88/100], Loss: 0.0806\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1876\n",
      "Epoch [2/100], Loss: 2.5302\n",
      "Epoch [3/100], Loss: 1.8646\n",
      "Epoch [4/100], Loss: 1.4620\n",
      "Epoch [5/100], Loss: 1.2151\n",
      "Epoch [6/100], Loss: 1.0470\n",
      "Epoch [7/100], Loss: 0.9293\n",
      "Epoch [8/100], Loss: 0.8334\n",
      "Epoch [9/100], Loss: 0.7502\n",
      "Epoch [10/100], Loss: 0.6791\n",
      "Epoch [11/100], Loss: 0.6164\n",
      "Epoch [12/100], Loss: 0.5699\n",
      "Epoch [13/100], Loss: 0.5273\n",
      "Epoch [14/100], Loss: 0.4920\n",
      "Epoch [15/100], Loss: 0.4593\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4082\n",
      "Epoch [18/100], Loss: 0.3860\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3178\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2922\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2684\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2006\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1227\n",
      "Epoch [59/100], Loss: 0.1195\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1139\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0970\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0884\n",
      "Epoch [82/100], Loss: 0.0868\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [93/100], Loss: 0.0762\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1885\n",
      "Epoch [2/100], Loss: 2.5322\n",
      "Epoch [3/100], Loss: 1.8824\n",
      "Epoch [4/100], Loss: 1.4657\n",
      "Epoch [5/100], Loss: 1.2066\n",
      "Epoch [6/100], Loss: 1.0455\n",
      "Epoch [7/100], Loss: 0.9295\n",
      "Epoch [8/100], Loss: 0.8350\n",
      "Epoch [9/100], Loss: 0.7473\n",
      "Epoch [10/100], Loss: 0.6743\n",
      "Epoch [11/100], Loss: 0.6204\n",
      "Epoch [12/100], Loss: 0.5694\n",
      "Epoch [13/100], Loss: 0.5274\n",
      "Epoch [14/100], Loss: 0.4914\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3488\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2408\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2265\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1804\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1564\n",
      "Epoch [46/100], Loss: 0.1529\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1304\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1238\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1185\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1121\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1063\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0970\n",
      "Epoch [74/100], Loss: 0.0957\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0899\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [86/100], Loss: 0.0823\n",
      "Test Accuracy Base Logit: 51.89%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1910\n",
      "Epoch [2/100], Loss: 2.5246\n",
      "Epoch [3/100], Loss: 1.8768\n",
      "Epoch [4/100], Loss: 1.4584\n",
      "Epoch [5/100], Loss: 1.2104\n",
      "Epoch [6/100], Loss: 1.0490\n",
      "Epoch [7/100], Loss: 0.9293\n",
      "Epoch [8/100], Loss: 0.8317\n",
      "Epoch [9/100], Loss: 0.7450\n",
      "Epoch [10/100], Loss: 0.6813\n",
      "Epoch [11/100], Loss: 0.6176\n",
      "Epoch [12/100], Loss: 0.5719\n",
      "Epoch [13/100], Loss: 0.5305\n",
      "Epoch [14/100], Loss: 0.4939\n",
      "Epoch [15/100], Loss: 0.4591\n",
      "Epoch [16/100], Loss: 0.4333\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3840\n",
      "Epoch [19/100], Loss: 0.3675\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3329\n",
      "Epoch [22/100], Loss: 0.3180\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2805\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2584\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2131\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.1995\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1456\n",
      "Epoch [49/100], Loss: 0.1436\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1215\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [91/100], Loss: 0.0780\n",
      "Test Accuracy Base Logit: 51.86%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1857\n",
      "Epoch [2/100], Loss: 2.5312\n",
      "Epoch [3/100], Loss: 1.8791\n",
      "Epoch [4/100], Loss: 1.4520\n",
      "Epoch [5/100], Loss: 1.2131\n",
      "Epoch [6/100], Loss: 1.0460\n",
      "Epoch [7/100], Loss: 0.9280\n",
      "Epoch [8/100], Loss: 0.8356\n",
      "Epoch [9/100], Loss: 0.7500\n",
      "Epoch [10/100], Loss: 0.6804\n",
      "Epoch [11/100], Loss: 0.6224\n",
      "Epoch [12/100], Loss: 0.5704\n",
      "Epoch [13/100], Loss: 0.5259\n",
      "Epoch [14/100], Loss: 0.4912\n",
      "Epoch [15/100], Loss: 0.4611\n",
      "Epoch [16/100], Loss: 0.4335\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3870\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3487\n",
      "Epoch [21/100], Loss: 0.3310\n",
      "Epoch [22/100], Loss: 0.3176\n",
      "Epoch [23/100], Loss: 0.3050\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2603\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2420\n",
      "Epoch [30/100], Loss: 0.2342\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2076\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1763\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1308\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0925\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0866\n",
      "Stopping early at epoch 82 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [82/100], Loss: 0.0866\n",
      "Test Accuracy Base Logit: 51.85%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1921\n",
      "Epoch [2/100], Loss: 2.5348\n",
      "Epoch [3/100], Loss: 1.8746\n",
      "Epoch [4/100], Loss: 1.4647\n",
      "Epoch [5/100], Loss: 1.2148\n",
      "Epoch [6/100], Loss: 1.0473\n",
      "Epoch [7/100], Loss: 0.9278\n",
      "Epoch [8/100], Loss: 0.8312\n",
      "Epoch [9/100], Loss: 0.7513\n",
      "Epoch [10/100], Loss: 0.6796\n",
      "Epoch [11/100], Loss: 0.6206\n",
      "Epoch [12/100], Loss: 0.5690\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4913\n",
      "Epoch [15/100], Loss: 0.4587\n",
      "Epoch [16/100], Loss: 0.4310\n",
      "Epoch [17/100], Loss: 0.4067\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3177\n",
      "Epoch [23/100], Loss: 0.3037\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2407\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2067\n",
      "Epoch [35/100], Loss: 0.2009\n",
      "Epoch [36/100], Loss: 0.1959\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1456\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1012\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0977\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0957\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0911\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0887\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 50000, Epoch [90/100], Loss: 0.0789\n",
      "Test Accuracy Base Logit: 51.82%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1809\n",
      "Epoch [2/100], Loss: 2.5296\n",
      "Epoch [3/100], Loss: 1.8912\n",
      "Epoch [4/100], Loss: 1.4622\n",
      "Epoch [5/100], Loss: 1.2124\n",
      "Epoch [6/100], Loss: 1.0479\n",
      "Epoch [7/100], Loss: 0.9317\n",
      "Epoch [8/100], Loss: 0.8336\n",
      "Epoch [9/100], Loss: 0.7496\n",
      "Epoch [10/100], Loss: 0.6783\n",
      "Epoch [11/100], Loss: 0.6181\n",
      "Epoch [12/100], Loss: 0.5697\n",
      "Epoch [13/100], Loss: 0.5293\n",
      "Epoch [14/100], Loss: 0.4917\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4305\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3860\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2406\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2195\n",
      "Epoch [33/100], Loss: 0.2133\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1792\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1634\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1530\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1237\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0972\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0818\n",
      "Epoch [88/100], Loss: 0.0809\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0785\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [93/100], Loss: 0.0763\n",
      "Test Accuracy Base Logit: 51.89%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1888\n",
      "Epoch [2/100], Loss: 2.5306\n",
      "Epoch [3/100], Loss: 1.8857\n",
      "Epoch [4/100], Loss: 1.4620\n",
      "Epoch [5/100], Loss: 1.2117\n",
      "Epoch [6/100], Loss: 1.0448\n",
      "Epoch [7/100], Loss: 0.9265\n",
      "Epoch [8/100], Loss: 0.8299\n",
      "Epoch [9/100], Loss: 0.7513\n",
      "Epoch [10/100], Loss: 0.6790\n",
      "Epoch [11/100], Loss: 0.6215\n",
      "Epoch [12/100], Loss: 0.5716\n",
      "Epoch [13/100], Loss: 0.5288\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4620\n",
      "Epoch [16/100], Loss: 0.4336\n",
      "Epoch [17/100], Loss: 0.4090\n",
      "Epoch [18/100], Loss: 0.3846\n",
      "Epoch [19/100], Loss: 0.3646\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2806\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2409\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1951\n",
      "Epoch [37/100], Loss: 0.1903\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1791\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1466\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1064\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0950\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0858\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [88/100], Loss: 0.0805\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1937\n",
      "Epoch [2/100], Loss: 2.5266\n",
      "Epoch [3/100], Loss: 1.8804\n",
      "Epoch [4/100], Loss: 1.4589\n",
      "Epoch [5/100], Loss: 1.2131\n",
      "Epoch [6/100], Loss: 1.0470\n",
      "Epoch [7/100], Loss: 0.9295\n",
      "Epoch [8/100], Loss: 0.8297\n",
      "Epoch [9/100], Loss: 0.7486\n",
      "Epoch [10/100], Loss: 0.6761\n",
      "Epoch [11/100], Loss: 0.6202\n",
      "Epoch [12/100], Loss: 0.5689\n",
      "Epoch [13/100], Loss: 0.5292\n",
      "Epoch [14/100], Loss: 0.4941\n",
      "Epoch [15/100], Loss: 0.4609\n",
      "Epoch [16/100], Loss: 0.4311\n",
      "Epoch [17/100], Loss: 0.4083\n",
      "Epoch [18/100], Loss: 0.3876\n",
      "Epoch [19/100], Loss: 0.3664\n",
      "Epoch [20/100], Loss: 0.3497\n",
      "Epoch [21/100], Loss: 0.3309\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2006\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1900\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1759\n",
      "Epoch [41/100], Loss: 0.1722\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1626\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1563\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1033\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0892\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0866\n",
      "Stopping early at epoch 82 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [82/100], Loss: 0.0866\n",
      "Test Accuracy Base Logit: 51.94%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1883\n",
      "Epoch [2/100], Loss: 2.5293\n",
      "Epoch [3/100], Loss: 1.8809\n",
      "Epoch [4/100], Loss: 1.4658\n",
      "Epoch [5/100], Loss: 1.2114\n",
      "Epoch [6/100], Loss: 1.0491\n",
      "Epoch [7/100], Loss: 0.9308\n",
      "Epoch [8/100], Loss: 0.8343\n",
      "Epoch [9/100], Loss: 0.7492\n",
      "Epoch [10/100], Loss: 0.6787\n",
      "Epoch [11/100], Loss: 0.6232\n",
      "Epoch [12/100], Loss: 0.5691\n",
      "Epoch [13/100], Loss: 0.5280\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4316\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3846\n",
      "Epoch [19/100], Loss: 0.3674\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2897\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1850\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1674\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1571\n",
      "Epoch [46/100], Loss: 0.1518\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1305\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1131\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0945\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [92/100], Loss: 0.0771\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1848\n",
      "Epoch [2/100], Loss: 2.5359\n",
      "Epoch [3/100], Loss: 1.8870\n",
      "Epoch [4/100], Loss: 1.4678\n",
      "Epoch [5/100], Loss: 1.2135\n",
      "Epoch [6/100], Loss: 1.0434\n",
      "Epoch [7/100], Loss: 0.9274\n",
      "Epoch [8/100], Loss: 0.8330\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6764\n",
      "Epoch [11/100], Loss: 0.6211\n",
      "Epoch [12/100], Loss: 0.5687\n",
      "Epoch [13/100], Loss: 0.5263\n",
      "Epoch [14/100], Loss: 0.4905\n",
      "Epoch [15/100], Loss: 0.4595\n",
      "Epoch [16/100], Loss: 0.4311\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2056\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1942\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1716\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1122\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0881\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0809\n",
      "Epoch [89/100], Loss: 0.0794\n",
      "Epoch [90/100], Loss: 0.0785\n",
      "Epoch [91/100], Loss: 0.0783\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0751\n",
      "Stopping early at epoch 95 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [95/100], Loss: 0.0751\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1862\n",
      "Epoch [2/100], Loss: 2.5329\n",
      "Epoch [3/100], Loss: 1.8823\n",
      "Epoch [4/100], Loss: 1.4623\n",
      "Epoch [5/100], Loss: 1.2070\n",
      "Epoch [6/100], Loss: 1.0482\n",
      "Epoch [7/100], Loss: 0.9353\n",
      "Epoch [8/100], Loss: 0.8360\n",
      "Epoch [9/100], Loss: 0.7481\n",
      "Epoch [10/100], Loss: 0.6791\n",
      "Epoch [11/100], Loss: 0.6208\n",
      "Epoch [12/100], Loss: 0.5687\n",
      "Epoch [13/100], Loss: 0.5296\n",
      "Epoch [14/100], Loss: 0.4927\n",
      "Epoch [15/100], Loss: 0.4595\n",
      "Epoch [16/100], Loss: 0.4325\n",
      "Epoch [17/100], Loss: 0.4068\n",
      "Epoch [18/100], Loss: 0.3872\n",
      "Epoch [19/100], Loss: 0.3648\n",
      "Epoch [20/100], Loss: 0.3467\n",
      "Epoch [21/100], Loss: 0.3332\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3040\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2507\n",
      "Epoch [29/100], Loss: 0.2422\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2197\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1747\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1642\n",
      "Epoch [44/100], Loss: 0.1602\n",
      "Epoch [45/100], Loss: 0.1566\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1447\n",
      "Epoch [50/100], Loss: 0.1408\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1283\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1215\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1175\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0846\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0829\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [93/100], Loss: 0.0763\n",
      "Test Accuracy Base Logit: 51.82%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1834\n",
      "Epoch [2/100], Loss: 2.5317\n",
      "Epoch [3/100], Loss: 1.8827\n",
      "Epoch [4/100], Loss: 1.4683\n",
      "Epoch [5/100], Loss: 1.2117\n",
      "Epoch [6/100], Loss: 1.0492\n",
      "Epoch [7/100], Loss: 0.9270\n",
      "Epoch [8/100], Loss: 0.8294\n",
      "Epoch [9/100], Loss: 0.7445\n",
      "Epoch [10/100], Loss: 0.6771\n",
      "Epoch [11/100], Loss: 0.6177\n",
      "Epoch [12/100], Loss: 0.5699\n",
      "Epoch [13/100], Loss: 0.5261\n",
      "Epoch [14/100], Loss: 0.4897\n",
      "Epoch [15/100], Loss: 0.4605\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3880\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3054\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2697\n",
      "Epoch [27/100], Loss: 0.2595\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2070\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1951\n",
      "Epoch [37/100], Loss: 0.1913\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1625\n",
      "Epoch [44/100], Loss: 0.1588\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1175\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0957\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [90/100], Loss: 0.0788\n",
      "Test Accuracy Base Logit: 51.91%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1873\n",
      "Epoch [2/100], Loss: 2.5448\n",
      "Epoch [3/100], Loss: 1.8857\n",
      "Epoch [4/100], Loss: 1.4593\n",
      "Epoch [5/100], Loss: 1.2100\n",
      "Epoch [6/100], Loss: 1.0457\n",
      "Epoch [7/100], Loss: 0.9269\n",
      "Epoch [8/100], Loss: 0.8354\n",
      "Epoch [9/100], Loss: 0.7478\n",
      "Epoch [10/100], Loss: 0.6763\n",
      "Epoch [11/100], Loss: 0.6217\n",
      "Epoch [12/100], Loss: 0.5709\n",
      "Epoch [13/100], Loss: 0.5289\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4612\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4084\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3329\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2806\n",
      "Epoch [26/100], Loss: 0.2681\n",
      "Epoch [27/100], Loss: 0.2602\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1310\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [89/100], Loss: 0.0797\n",
      "Test Accuracy Base Logit: 51.91%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1879\n",
      "Epoch [2/100], Loss: 2.5303\n",
      "Epoch [3/100], Loss: 1.8768\n",
      "Epoch [4/100], Loss: 1.4670\n",
      "Epoch [5/100], Loss: 1.2054\n",
      "Epoch [6/100], Loss: 1.0474\n",
      "Epoch [7/100], Loss: 0.9286\n",
      "Epoch [8/100], Loss: 0.8319\n",
      "Epoch [9/100], Loss: 0.7475\n",
      "Epoch [10/100], Loss: 0.6794\n",
      "Epoch [11/100], Loss: 0.6220\n",
      "Epoch [12/100], Loss: 0.5683\n",
      "Epoch [13/100], Loss: 0.5303\n",
      "Epoch [14/100], Loss: 0.4907\n",
      "Epoch [15/100], Loss: 0.4601\n",
      "Epoch [16/100], Loss: 0.4308\n",
      "Epoch [17/100], Loss: 0.4066\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3470\n",
      "Epoch [21/100], Loss: 0.3328\n",
      "Epoch [22/100], Loss: 0.3161\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2424\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2118\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1758\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1626\n",
      "Epoch [44/100], Loss: 0.1601\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1438\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1389\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1305\n",
      "Epoch [55/100], Loss: 0.1284\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1089\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1004\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0957\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [88/100], Loss: 0.0804\n",
      "Test Accuracy Base Logit: 51.89%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1845\n",
      "Epoch [2/100], Loss: 2.5353\n",
      "Epoch [3/100], Loss: 1.8887\n",
      "Epoch [4/100], Loss: 1.4637\n",
      "Epoch [5/100], Loss: 1.2085\n",
      "Epoch [6/100], Loss: 1.0475\n",
      "Epoch [7/100], Loss: 0.9265\n",
      "Epoch [8/100], Loss: 0.8299\n",
      "Epoch [9/100], Loss: 0.7457\n",
      "Epoch [10/100], Loss: 0.6781\n",
      "Epoch [11/100], Loss: 0.6203\n",
      "Epoch [12/100], Loss: 0.5697\n",
      "Epoch [13/100], Loss: 0.5297\n",
      "Epoch [14/100], Loss: 0.4919\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4065\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3185\n",
      "Epoch [23/100], Loss: 0.3028\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2812\n",
      "Epoch [26/100], Loss: 0.2680\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2420\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2205\n",
      "Epoch [33/100], Loss: 0.2131\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2018\n",
      "Epoch [36/100], Loss: 0.1983\n",
      "Epoch [37/100], Loss: 0.1902\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1382\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1330\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0951\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0892\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0860\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [91/100], Loss: 0.0778\n",
      "Test Accuracy Base Logit: 51.84%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1854\n",
      "Epoch [2/100], Loss: 2.5336\n",
      "Epoch [3/100], Loss: 1.8831\n",
      "Epoch [4/100], Loss: 1.4622\n",
      "Epoch [5/100], Loss: 1.2119\n",
      "Epoch [6/100], Loss: 1.0464\n",
      "Epoch [7/100], Loss: 0.9315\n",
      "Epoch [8/100], Loss: 0.8325\n",
      "Epoch [9/100], Loss: 0.7458\n",
      "Epoch [10/100], Loss: 0.6779\n",
      "Epoch [11/100], Loss: 0.6189\n",
      "Epoch [12/100], Loss: 0.5691\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4924\n",
      "Epoch [15/100], Loss: 0.4593\n",
      "Epoch [16/100], Loss: 0.4333\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3870\n",
      "Epoch [19/100], Loss: 0.3646\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3333\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2920\n",
      "Epoch [25/100], Loss: 0.2785\n",
      "Epoch [26/100], Loss: 0.2707\n",
      "Epoch [27/100], Loss: 0.2599\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.1996\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1850\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1747\n",
      "Epoch [41/100], Loss: 0.1721\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1567\n",
      "Epoch [46/100], Loss: 0.1532\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1409\n",
      "Epoch [51/100], Loss: 0.1371\n",
      "Epoch [52/100], Loss: 0.1355\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1296\n",
      "Epoch [55/100], Loss: 0.1281\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1064\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0951\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0911\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [89/100], Loss: 0.0795\n",
      "Test Accuracy Base Logit: 51.92%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1844\n",
      "Epoch [2/100], Loss: 2.5232\n",
      "Epoch [3/100], Loss: 1.8832\n",
      "Epoch [4/100], Loss: 1.4606\n",
      "Epoch [5/100], Loss: 1.2049\n",
      "Epoch [6/100], Loss: 1.0507\n",
      "Epoch [7/100], Loss: 0.9324\n",
      "Epoch [8/100], Loss: 0.8323\n",
      "Epoch [9/100], Loss: 0.7462\n",
      "Epoch [10/100], Loss: 0.6761\n",
      "Epoch [11/100], Loss: 0.6224\n",
      "Epoch [12/100], Loss: 0.5715\n",
      "Epoch [13/100], Loss: 0.5292\n",
      "Epoch [14/100], Loss: 0.4937\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4298\n",
      "Epoch [17/100], Loss: 0.4078\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3491\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3175\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2703\n",
      "Epoch [27/100], Loss: 0.2606\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2420\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1589\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1157\n",
      "Epoch [62/100], Loss: 0.1140\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1087\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1018\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0983\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [88/100], Loss: 0.0804\n",
      "Test Accuracy Base Logit: 51.93%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1943\n",
      "Epoch [2/100], Loss: 2.5282\n",
      "Epoch [3/100], Loss: 1.8765\n",
      "Epoch [4/100], Loss: 1.4659\n",
      "Epoch [5/100], Loss: 1.2068\n",
      "Epoch [6/100], Loss: 1.0479\n",
      "Epoch [7/100], Loss: 0.9252\n",
      "Epoch [8/100], Loss: 0.8319\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6790\n",
      "Epoch [11/100], Loss: 0.6189\n",
      "Epoch [12/100], Loss: 0.5704\n",
      "Epoch [13/100], Loss: 0.5284\n",
      "Epoch [14/100], Loss: 0.4900\n",
      "Epoch [15/100], Loss: 0.4602\n",
      "Epoch [16/100], Loss: 0.4338\n",
      "Epoch [17/100], Loss: 0.4103\n",
      "Epoch [18/100], Loss: 0.3862\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2924\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2343\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1638\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1384\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0998\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0970\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0892\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0877\n",
      "Epoch [82/100], Loss: 0.0866\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [89/100], Loss: 0.0797\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1859\n",
      "Epoch [2/100], Loss: 2.5354\n",
      "Epoch [3/100], Loss: 1.8894\n",
      "Epoch [4/100], Loss: 1.4605\n",
      "Epoch [5/100], Loss: 1.2092\n",
      "Epoch [6/100], Loss: 1.0451\n",
      "Epoch [7/100], Loss: 0.9276\n",
      "Epoch [8/100], Loss: 0.8300\n",
      "Epoch [9/100], Loss: 0.7508\n",
      "Epoch [10/100], Loss: 0.6787\n",
      "Epoch [11/100], Loss: 0.6214\n",
      "Epoch [12/100], Loss: 0.5712\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4930\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4315\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3849\n",
      "Epoch [19/100], Loss: 0.3674\n",
      "Epoch [20/100], Loss: 0.3487\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2905\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2345\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1688\n",
      "Epoch [43/100], Loss: 0.1634\n",
      "Epoch [44/100], Loss: 0.1588\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1074\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1041\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0922\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [86/100], Loss: 0.0823\n",
      "Test Accuracy Base Logit: 51.96%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1946\n",
      "Epoch [2/100], Loss: 2.5350\n",
      "Epoch [3/100], Loss: 1.8763\n",
      "Epoch [4/100], Loss: 1.4584\n",
      "Epoch [5/100], Loss: 1.2103\n",
      "Epoch [6/100], Loss: 1.0406\n",
      "Epoch [7/100], Loss: 0.9272\n",
      "Epoch [8/100], Loss: 0.8313\n",
      "Epoch [9/100], Loss: 0.7454\n",
      "Epoch [10/100], Loss: 0.6772\n",
      "Epoch [11/100], Loss: 0.6206\n",
      "Epoch [12/100], Loss: 0.5710\n",
      "Epoch [13/100], Loss: 0.5303\n",
      "Epoch [14/100], Loss: 0.4906\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4085\n",
      "Epoch [18/100], Loss: 0.3872\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1716\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1530\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1124\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1018\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [88/100], Loss: 0.0805\n",
      "Test Accuracy Base Logit: 51.90%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1810\n",
      "Epoch [2/100], Loss: 2.5403\n",
      "Epoch [3/100], Loss: 1.8812\n",
      "Epoch [4/100], Loss: 1.4650\n",
      "Epoch [5/100], Loss: 1.2170\n",
      "Epoch [6/100], Loss: 1.0497\n",
      "Epoch [7/100], Loss: 0.9344\n",
      "Epoch [8/100], Loss: 0.8336\n",
      "Epoch [9/100], Loss: 0.7495\n",
      "Epoch [10/100], Loss: 0.6775\n",
      "Epoch [11/100], Loss: 0.6222\n",
      "Epoch [12/100], Loss: 0.5712\n",
      "Epoch [13/100], Loss: 0.5264\n",
      "Epoch [14/100], Loss: 0.4892\n",
      "Epoch [15/100], Loss: 0.4601\n",
      "Epoch [16/100], Loss: 0.4326\n",
      "Epoch [17/100], Loss: 0.4061\n",
      "Epoch [18/100], Loss: 0.3839\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3502\n",
      "Epoch [21/100], Loss: 0.3309\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2697\n",
      "Epoch [27/100], Loss: 0.2604\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2408\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2056\n",
      "Epoch [35/100], Loss: 0.2013\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1346\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1064\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0923\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0881\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [89/100], Loss: 0.0797\n",
      "Test Accuracy Base Logit: 51.90%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1904\n",
      "Epoch [2/100], Loss: 2.5336\n",
      "Epoch [3/100], Loss: 1.8840\n",
      "Epoch [4/100], Loss: 1.4629\n",
      "Epoch [5/100], Loss: 1.2158\n",
      "Epoch [6/100], Loss: 1.0532\n",
      "Epoch [7/100], Loss: 0.9274\n",
      "Epoch [8/100], Loss: 0.8302\n",
      "Epoch [9/100], Loss: 0.7474\n",
      "Epoch [10/100], Loss: 0.6808\n",
      "Epoch [11/100], Loss: 0.6190\n",
      "Epoch [12/100], Loss: 0.5701\n",
      "Epoch [13/100], Loss: 0.5276\n",
      "Epoch [14/100], Loss: 0.4905\n",
      "Epoch [15/100], Loss: 0.4610\n",
      "Epoch [16/100], Loss: 0.4318\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3891\n",
      "Epoch [19/100], Loss: 0.3675\n",
      "Epoch [20/100], Loss: 0.3468\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2682\n",
      "Epoch [27/100], Loss: 0.2606\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2252\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1704\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1603\n",
      "Epoch [45/100], Loss: 0.1568\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1063\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0997\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0866\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [89/100], Loss: 0.0797\n",
      "Test Accuracy Base Logit: 51.86%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1858\n",
      "Epoch [2/100], Loss: 2.5272\n",
      "Epoch [3/100], Loss: 1.8825\n",
      "Epoch [4/100], Loss: 1.4566\n",
      "Epoch [5/100], Loss: 1.2131\n",
      "Epoch [6/100], Loss: 1.0459\n",
      "Epoch [7/100], Loss: 0.9361\n",
      "Epoch [8/100], Loss: 0.8339\n",
      "Epoch [9/100], Loss: 0.7493\n",
      "Epoch [10/100], Loss: 0.6813\n",
      "Epoch [11/100], Loss: 0.6186\n",
      "Epoch [12/100], Loss: 0.5721\n",
      "Epoch [13/100], Loss: 0.5273\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4615\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3674\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2253\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1676\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1500\n",
      "Epoch [48/100], Loss: 0.1473\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1285\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1056\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.1000\n",
      "Epoch [72/100], Loss: 0.0983\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0926\n",
      "Epoch [77/100], Loss: 0.0921\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [87/100], Loss: 0.0813\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1859\n",
      "Epoch [2/100], Loss: 2.5269\n",
      "Epoch [3/100], Loss: 1.8793\n",
      "Epoch [4/100], Loss: 1.4664\n",
      "Epoch [5/100], Loss: 1.2185\n",
      "Epoch [6/100], Loss: 1.0578\n",
      "Epoch [7/100], Loss: 0.9316\n",
      "Epoch [8/100], Loss: 0.8344\n",
      "Epoch [9/100], Loss: 0.7488\n",
      "Epoch [10/100], Loss: 0.6804\n",
      "Epoch [11/100], Loss: 0.6200\n",
      "Epoch [12/100], Loss: 0.5721\n",
      "Epoch [13/100], Loss: 0.5295\n",
      "Epoch [14/100], Loss: 0.4911\n",
      "Epoch [15/100], Loss: 0.4608\n",
      "Epoch [16/100], Loss: 0.4328\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3873\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3466\n",
      "Epoch [21/100], Loss: 0.3333\n",
      "Epoch [22/100], Loss: 0.3163\n",
      "Epoch [23/100], Loss: 0.3046\n",
      "Epoch [24/100], Loss: 0.2920\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2707\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1764\n",
      "Epoch [41/100], Loss: 0.1731\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1498\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1370\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [88/100], Loss: 0.0808\n",
      "Test Accuracy Base Logit: 51.84%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1869\n",
      "Epoch [2/100], Loss: 2.5249\n",
      "Epoch [3/100], Loss: 1.8778\n",
      "Epoch [4/100], Loss: 1.4643\n",
      "Epoch [5/100], Loss: 1.2193\n",
      "Epoch [6/100], Loss: 1.0492\n",
      "Epoch [7/100], Loss: 0.9288\n",
      "Epoch [8/100], Loss: 0.8330\n",
      "Epoch [9/100], Loss: 0.7483\n",
      "Epoch [10/100], Loss: 0.6779\n",
      "Epoch [11/100], Loss: 0.6208\n",
      "Epoch [12/100], Loss: 0.5725\n",
      "Epoch [13/100], Loss: 0.5289\n",
      "Epoch [14/100], Loss: 0.4931\n",
      "Epoch [15/100], Loss: 0.4606\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4082\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3321\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2901\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2129\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1850\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1380\n",
      "Epoch [52/100], Loss: 0.1346\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1282\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1033\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0810\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [88/100], Loss: 0.0810\n",
      "Test Accuracy Base Logit: 51.76%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1848\n",
      "Epoch [2/100], Loss: 2.5313\n",
      "Epoch [3/100], Loss: 1.8847\n",
      "Epoch [4/100], Loss: 1.4642\n",
      "Epoch [5/100], Loss: 1.2127\n",
      "Epoch [6/100], Loss: 1.0548\n",
      "Epoch [7/100], Loss: 0.9302\n",
      "Epoch [8/100], Loss: 0.8289\n",
      "Epoch [9/100], Loss: 0.7500\n",
      "Epoch [10/100], Loss: 0.6785\n",
      "Epoch [11/100], Loss: 0.6244\n",
      "Epoch [12/100], Loss: 0.5706\n",
      "Epoch [13/100], Loss: 0.5295\n",
      "Epoch [14/100], Loss: 0.4899\n",
      "Epoch [15/100], Loss: 0.4588\n",
      "Epoch [16/100], Loss: 0.4315\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3863\n",
      "Epoch [19/100], Loss: 0.3648\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3175\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2904\n",
      "Epoch [25/100], Loss: 0.2806\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2582\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.1997\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1553\n",
      "Epoch [46/100], Loss: 0.1532\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1371\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1056\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [88/100], Loss: 0.0805\n",
      "Test Accuracy Base Logit: 51.86%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1916\n",
      "Epoch [2/100], Loss: 2.5371\n",
      "Epoch [3/100], Loss: 1.8792\n",
      "Epoch [4/100], Loss: 1.4644\n",
      "Epoch [5/100], Loss: 1.2081\n",
      "Epoch [6/100], Loss: 1.0549\n",
      "Epoch [7/100], Loss: 0.9269\n",
      "Epoch [8/100], Loss: 0.8354\n",
      "Epoch [9/100], Loss: 0.7483\n",
      "Epoch [10/100], Loss: 0.6809\n",
      "Epoch [11/100], Loss: 0.6204\n",
      "Epoch [12/100], Loss: 0.5729\n",
      "Epoch [13/100], Loss: 0.5276\n",
      "Epoch [14/100], Loss: 0.4905\n",
      "Epoch [15/100], Loss: 0.4593\n",
      "Epoch [16/100], Loss: 0.4308\n",
      "Epoch [17/100], Loss: 0.4060\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3164\n",
      "Epoch [23/100], Loss: 0.3038\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2701\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2509\n",
      "Epoch [29/100], Loss: 0.2424\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2265\n",
      "Epoch [32/100], Loss: 0.2196\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1889\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1664\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0821\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0804\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0766\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Stopping early at epoch 97 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [97/100], Loss: 0.0733\n",
      "Test Accuracy Base Logit: 51.82%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1866\n",
      "Epoch [2/100], Loss: 2.5401\n",
      "Epoch [3/100], Loss: 1.8765\n",
      "Epoch [4/100], Loss: 1.4642\n",
      "Epoch [5/100], Loss: 1.2079\n",
      "Epoch [6/100], Loss: 1.0472\n",
      "Epoch [7/100], Loss: 0.9286\n",
      "Epoch [8/100], Loss: 0.8292\n",
      "Epoch [9/100], Loss: 0.7506\n",
      "Epoch [10/100], Loss: 0.6822\n",
      "Epoch [11/100], Loss: 0.6211\n",
      "Epoch [12/100], Loss: 0.5727\n",
      "Epoch [13/100], Loss: 0.5268\n",
      "Epoch [14/100], Loss: 0.4906\n",
      "Epoch [15/100], Loss: 0.4615\n",
      "Epoch [16/100], Loss: 0.4334\n",
      "Epoch [17/100], Loss: 0.4071\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3164\n",
      "Epoch [23/100], Loss: 0.3027\n",
      "Epoch [24/100], Loss: 0.2919\n",
      "Epoch [25/100], Loss: 0.2817\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2507\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1804\n",
      "Epoch [40/100], Loss: 0.1758\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1679\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1529\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1044\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1004\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0866\n",
      "Epoch [83/100], Loss: 0.0856\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0828\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0790\n",
      "Epoch [92/100], Loss: 0.0774\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0757\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Stopping early at epoch 95 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [95/100], Loss: 0.0747\n",
      "Test Accuracy Base Logit: 51.81%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1862\n",
      "Epoch [2/100], Loss: 2.5190\n",
      "Epoch [3/100], Loss: 1.8797\n",
      "Epoch [4/100], Loss: 1.4632\n",
      "Epoch [5/100], Loss: 1.2152\n",
      "Epoch [6/100], Loss: 1.0449\n",
      "Epoch [7/100], Loss: 0.9268\n",
      "Epoch [8/100], Loss: 0.8302\n",
      "Epoch [9/100], Loss: 0.7487\n",
      "Epoch [10/100], Loss: 0.6778\n",
      "Epoch [11/100], Loss: 0.6204\n",
      "Epoch [12/100], Loss: 0.5722\n",
      "Epoch [13/100], Loss: 0.5282\n",
      "Epoch [14/100], Loss: 0.4938\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4330\n",
      "Epoch [17/100], Loss: 0.4060\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3484\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3046\n",
      "Epoch [24/100], Loss: 0.2929\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2684\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2515\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2198\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1717\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1436\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0818\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [87/100], Loss: 0.0818\n",
      "Test Accuracy Base Logit: 51.82%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1857\n",
      "Epoch [2/100], Loss: 2.5322\n",
      "Epoch [3/100], Loss: 1.8763\n",
      "Epoch [4/100], Loss: 1.4623\n",
      "Epoch [5/100], Loss: 1.2166\n",
      "Epoch [6/100], Loss: 1.0446\n",
      "Epoch [7/100], Loss: 0.9263\n",
      "Epoch [8/100], Loss: 0.8284\n",
      "Epoch [9/100], Loss: 0.7466\n",
      "Epoch [10/100], Loss: 0.6782\n",
      "Epoch [11/100], Loss: 0.6219\n",
      "Epoch [12/100], Loss: 0.5691\n",
      "Epoch [13/100], Loss: 0.5290\n",
      "Epoch [14/100], Loss: 0.4897\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4318\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3873\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3186\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2697\n",
      "Epoch [27/100], Loss: 0.2602\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1357\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1250\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1033\n",
      "Epoch [69/100], Loss: 0.1018\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0866\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [89/100], Loss: 0.0799\n",
      "Test Accuracy Base Logit: 51.92%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1818\n",
      "Epoch [2/100], Loss: 2.5371\n",
      "Epoch [3/100], Loss: 1.8860\n",
      "Epoch [4/100], Loss: 1.4723\n",
      "Epoch [5/100], Loss: 1.2130\n",
      "Epoch [6/100], Loss: 1.0529\n",
      "Epoch [7/100], Loss: 0.9276\n",
      "Epoch [8/100], Loss: 0.8332\n",
      "Epoch [9/100], Loss: 0.7494\n",
      "Epoch [10/100], Loss: 0.6773\n",
      "Epoch [11/100], Loss: 0.6210\n",
      "Epoch [12/100], Loss: 0.5726\n",
      "Epoch [13/100], Loss: 0.5298\n",
      "Epoch [14/100], Loss: 0.4935\n",
      "Epoch [15/100], Loss: 0.4586\n",
      "Epoch [16/100], Loss: 0.4336\n",
      "Epoch [17/100], Loss: 0.4097\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3641\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3037\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2517\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2343\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.1995\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0990\n",
      "Epoch [72/100], Loss: 0.0977\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [88/100], Loss: 0.0806\n",
      "Test Accuracy Base Logit: 51.85%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1777\n",
      "Epoch [2/100], Loss: 2.5333\n",
      "Epoch [3/100], Loss: 1.8904\n",
      "Epoch [4/100], Loss: 1.4655\n",
      "Epoch [5/100], Loss: 1.2106\n",
      "Epoch [6/100], Loss: 1.0534\n",
      "Epoch [7/100], Loss: 0.9317\n",
      "Epoch [8/100], Loss: 0.8340\n",
      "Epoch [9/100], Loss: 0.7484\n",
      "Epoch [10/100], Loss: 0.6809\n",
      "Epoch [11/100], Loss: 0.6199\n",
      "Epoch [12/100], Loss: 0.5709\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4908\n",
      "Epoch [15/100], Loss: 0.4583\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4071\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3645\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2802\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2607\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2056\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1903\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1792\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1626\n",
      "Epoch [44/100], Loss: 0.1600\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1519\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1222\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1011\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [92/100], Loss: 0.0771\n",
      "Test Accuracy Base Logit: 51.86%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1835\n",
      "Epoch [2/100], Loss: 2.5395\n",
      "Epoch [3/100], Loss: 1.8800\n",
      "Epoch [4/100], Loss: 1.4613\n",
      "Epoch [5/100], Loss: 1.2118\n",
      "Epoch [6/100], Loss: 1.0438\n",
      "Epoch [7/100], Loss: 0.9325\n",
      "Epoch [8/100], Loss: 0.8316\n",
      "Epoch [9/100], Loss: 0.7507\n",
      "Epoch [10/100], Loss: 0.6794\n",
      "Epoch [11/100], Loss: 0.6219\n",
      "Epoch [12/100], Loss: 0.5714\n",
      "Epoch [13/100], Loss: 0.5272\n",
      "Epoch [14/100], Loss: 0.4935\n",
      "Epoch [15/100], Loss: 0.4586\n",
      "Epoch [16/100], Loss: 0.4342\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3843\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3494\n",
      "Epoch [21/100], Loss: 0.3330\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3023\n",
      "Epoch [24/100], Loss: 0.2904\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2183\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1589\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1359\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0859\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [88/100], Loss: 0.0805\n",
      "Test Accuracy Base Logit: 51.89%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1858\n",
      "Epoch [2/100], Loss: 2.5377\n",
      "Epoch [3/100], Loss: 1.8774\n",
      "Epoch [4/100], Loss: 1.4666\n",
      "Epoch [5/100], Loss: 1.2227\n",
      "Epoch [6/100], Loss: 1.0500\n",
      "Epoch [7/100], Loss: 0.9293\n",
      "Epoch [8/100], Loss: 0.8357\n",
      "Epoch [9/100], Loss: 0.7484\n",
      "Epoch [10/100], Loss: 0.6804\n",
      "Epoch [11/100], Loss: 0.6194\n",
      "Epoch [12/100], Loss: 0.5675\n",
      "Epoch [13/100], Loss: 0.5276\n",
      "Epoch [14/100], Loss: 0.4942\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4310\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3312\n",
      "Epoch [22/100], Loss: 0.3175\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2183\n",
      "Epoch [33/100], Loss: 0.2132\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.1997\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1803\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1468\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1251\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1195\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1056\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0957\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0881\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Stopping early at epoch 83 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [83/100], Loss: 0.0853\n",
      "Test Accuracy Base Logit: 51.95%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1907\n",
      "Epoch [2/100], Loss: 2.5298\n",
      "Epoch [3/100], Loss: 1.8844\n",
      "Epoch [4/100], Loss: 1.4734\n",
      "Epoch [5/100], Loss: 1.2149\n",
      "Epoch [6/100], Loss: 1.0469\n",
      "Epoch [7/100], Loss: 0.9325\n",
      "Epoch [8/100], Loss: 0.8317\n",
      "Epoch [9/100], Loss: 0.7520\n",
      "Epoch [10/100], Loss: 0.6773\n",
      "Epoch [11/100], Loss: 0.6211\n",
      "Epoch [12/100], Loss: 0.5706\n",
      "Epoch [13/100], Loss: 0.5295\n",
      "Epoch [14/100], Loss: 0.4916\n",
      "Epoch [15/100], Loss: 0.4587\n",
      "Epoch [16/100], Loss: 0.4337\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3866\n",
      "Epoch [19/100], Loss: 0.3668\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3163\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2179\n",
      "Epoch [33/100], Loss: 0.2115\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1674\n",
      "Epoch [43/100], Loss: 0.1624\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1273\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1177\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1131\n",
      "Epoch [63/100], Loss: 0.1122\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1060\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0977\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0957\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 10000, Epoch [86/100], Loss: 0.0824\n",
      "Test Accuracy Base Logit: 51.98%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1932\n",
      "Epoch [2/100], Loss: 2.5303\n",
      "Epoch [3/100], Loss: 1.8804\n",
      "Epoch [4/100], Loss: 1.4577\n",
      "Epoch [5/100], Loss: 1.2113\n",
      "Epoch [6/100], Loss: 1.0441\n",
      "Epoch [7/100], Loss: 0.9286\n",
      "Epoch [8/100], Loss: 0.8364\n",
      "Epoch [9/100], Loss: 0.7499\n",
      "Epoch [10/100], Loss: 0.6796\n",
      "Epoch [11/100], Loss: 0.6212\n",
      "Epoch [12/100], Loss: 0.5696\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4935\n",
      "Epoch [15/100], Loss: 0.4590\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3662\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2682\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2201\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2015\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1760\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1684\n",
      "Epoch [43/100], Loss: 0.1636\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1563\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1072\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [89/100], Loss: 0.0797\n",
      "Test Accuracy Base Logit: 51.80%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1806\n",
      "Epoch [2/100], Loss: 2.5365\n",
      "Epoch [3/100], Loss: 1.8847\n",
      "Epoch [4/100], Loss: 1.4676\n",
      "Epoch [5/100], Loss: 1.2168\n",
      "Epoch [6/100], Loss: 1.0513\n",
      "Epoch [7/100], Loss: 0.9309\n",
      "Epoch [8/100], Loss: 0.8294\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6751\n",
      "Epoch [11/100], Loss: 0.6179\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4586\n",
      "Epoch [16/100], Loss: 0.4328\n",
      "Epoch [17/100], Loss: 0.4096\n",
      "Epoch [18/100], Loss: 0.3869\n",
      "Epoch [19/100], Loss: 0.3666\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3329\n",
      "Epoch [22/100], Loss: 0.3186\n",
      "Epoch [23/100], Loss: 0.3028\n",
      "Epoch [24/100], Loss: 0.2922\n",
      "Epoch [25/100], Loss: 0.2803\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2409\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2137\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.1997\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1601\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1259\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0990\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [88/100], Loss: 0.0807\n",
      "Test Accuracy Base Logit: 51.79%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1935\n",
      "Epoch [2/100], Loss: 2.5336\n",
      "Epoch [3/100], Loss: 1.8787\n",
      "Epoch [4/100], Loss: 1.4713\n",
      "Epoch [5/100], Loss: 1.2145\n",
      "Epoch [6/100], Loss: 1.0409\n",
      "Epoch [7/100], Loss: 0.9278\n",
      "Epoch [8/100], Loss: 0.8330\n",
      "Epoch [9/100], Loss: 0.7476\n",
      "Epoch [10/100], Loss: 0.6799\n",
      "Epoch [11/100], Loss: 0.6217\n",
      "Epoch [12/100], Loss: 0.5758\n",
      "Epoch [13/100], Loss: 0.5290\n",
      "Epoch [14/100], Loss: 0.4900\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4085\n",
      "Epoch [18/100], Loss: 0.3840\n",
      "Epoch [19/100], Loss: 0.3677\n",
      "Epoch [20/100], Loss: 0.3498\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3052\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2267\n",
      "Epoch [32/100], Loss: 0.2182\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1228\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1104\n",
      "Epoch [65/100], Loss: 0.1094\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [86/100], Loss: 0.0827\n",
      "Test Accuracy Base Logit: 51.83%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1810\n",
      "Epoch [2/100], Loss: 2.5304\n",
      "Epoch [3/100], Loss: 1.8927\n",
      "Epoch [4/100], Loss: 1.4713\n",
      "Epoch [5/100], Loss: 1.2122\n",
      "Epoch [6/100], Loss: 1.0466\n",
      "Epoch [7/100], Loss: 0.9268\n",
      "Epoch [8/100], Loss: 0.8362\n",
      "Epoch [9/100], Loss: 0.7483\n",
      "Epoch [10/100], Loss: 0.6789\n",
      "Epoch [11/100], Loss: 0.6201\n",
      "Epoch [12/100], Loss: 0.5702\n",
      "Epoch [13/100], Loss: 0.5278\n",
      "Epoch [14/100], Loss: 0.4923\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4343\n",
      "Epoch [17/100], Loss: 0.4059\n",
      "Epoch [18/100], Loss: 0.3845\n",
      "Epoch [19/100], Loss: 0.3646\n",
      "Epoch [20/100], Loss: 0.3475\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3048\n",
      "Epoch [24/100], Loss: 0.2905\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2581\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1532\n",
      "Epoch [47/100], Loss: 0.1501\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1282\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1239\n",
      "Epoch [58/100], Loss: 0.1219\n",
      "Epoch [59/100], Loss: 0.1197\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [89/100], Loss: 0.0795\n",
      "Test Accuracy Base Logit: 51.89%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1907\n",
      "Epoch [2/100], Loss: 2.5138\n",
      "Epoch [3/100], Loss: 1.8814\n",
      "Epoch [4/100], Loss: 1.4630\n",
      "Epoch [5/100], Loss: 1.2064\n",
      "Epoch [6/100], Loss: 1.0459\n",
      "Epoch [7/100], Loss: 0.9293\n",
      "Epoch [8/100], Loss: 0.8338\n",
      "Epoch [9/100], Loss: 0.7517\n",
      "Epoch [10/100], Loss: 0.6799\n",
      "Epoch [11/100], Loss: 0.6221\n",
      "Epoch [12/100], Loss: 0.5710\n",
      "Epoch [13/100], Loss: 0.5265\n",
      "Epoch [14/100], Loss: 0.4926\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4300\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3869\n",
      "Epoch [19/100], Loss: 0.3662\n",
      "Epoch [20/100], Loss: 0.3471\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3177\n",
      "Epoch [23/100], Loss: 0.3042\n",
      "Epoch [24/100], Loss: 0.2905\n",
      "Epoch [25/100], Loss: 0.2802\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2422\n",
      "Epoch [30/100], Loss: 0.2326\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1957\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1025\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Stopping early at epoch 85 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [85/100], Loss: 0.0833\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1850\n",
      "Epoch [2/100], Loss: 2.5374\n",
      "Epoch [3/100], Loss: 1.9068\n",
      "Epoch [4/100], Loss: 1.4697\n",
      "Epoch [5/100], Loss: 1.2192\n",
      "Epoch [6/100], Loss: 1.0478\n",
      "Epoch [7/100], Loss: 0.9295\n",
      "Epoch [8/100], Loss: 0.8323\n",
      "Epoch [9/100], Loss: 0.7517\n",
      "Epoch [10/100], Loss: 0.6805\n",
      "Epoch [11/100], Loss: 0.6200\n",
      "Epoch [12/100], Loss: 0.5718\n",
      "Epoch [13/100], Loss: 0.5279\n",
      "Epoch [14/100], Loss: 0.4908\n",
      "Epoch [15/100], Loss: 0.4629\n",
      "Epoch [16/100], Loss: 0.4316\n",
      "Epoch [17/100], Loss: 0.4070\n",
      "Epoch [18/100], Loss: 0.3862\n",
      "Epoch [19/100], Loss: 0.3678\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2901\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2585\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2183\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2006\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1497\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1240\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1197\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1158\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1138\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0948\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0909\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0887\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0821\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0769\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [92/100], Loss: 0.0769\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1894\n",
      "Epoch [2/100], Loss: 2.5351\n",
      "Epoch [3/100], Loss: 1.8849\n",
      "Epoch [4/100], Loss: 1.4584\n",
      "Epoch [5/100], Loss: 1.2081\n",
      "Epoch [6/100], Loss: 1.0448\n",
      "Epoch [7/100], Loss: 0.9261\n",
      "Epoch [8/100], Loss: 0.8345\n",
      "Epoch [9/100], Loss: 0.7493\n",
      "Epoch [10/100], Loss: 0.6767\n",
      "Epoch [11/100], Loss: 0.6194\n",
      "Epoch [12/100], Loss: 0.5687\n",
      "Epoch [13/100], Loss: 0.5274\n",
      "Epoch [14/100], Loss: 0.4916\n",
      "Epoch [15/100], Loss: 0.4601\n",
      "Epoch [16/100], Loss: 0.4316\n",
      "Epoch [17/100], Loss: 0.4066\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3471\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3045\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2584\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1498\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1167\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [89/100], Loss: 0.0797\n",
      "Test Accuracy Base Logit: 51.93%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1870\n",
      "Epoch [2/100], Loss: 2.5300\n",
      "Epoch [3/100], Loss: 1.8809\n",
      "Epoch [4/100], Loss: 1.4614\n",
      "Epoch [5/100], Loss: 1.2158\n",
      "Epoch [6/100], Loss: 1.0514\n",
      "Epoch [7/100], Loss: 0.9319\n",
      "Epoch [8/100], Loss: 0.8378\n",
      "Epoch [9/100], Loss: 0.7502\n",
      "Epoch [10/100], Loss: 0.6811\n",
      "Epoch [11/100], Loss: 0.6205\n",
      "Epoch [12/100], Loss: 0.5703\n",
      "Epoch [13/100], Loss: 0.5269\n",
      "Epoch [14/100], Loss: 0.4907\n",
      "Epoch [15/100], Loss: 0.4618\n",
      "Epoch [16/100], Loss: 0.4310\n",
      "Epoch [17/100], Loss: 0.4090\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3464\n",
      "Epoch [21/100], Loss: 0.3308\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3025\n",
      "Epoch [24/100], Loss: 0.2922\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2405\n",
      "Epoch [30/100], Loss: 0.2342\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2118\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1380\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1197\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0801\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [93/100], Loss: 0.0763\n",
      "Test Accuracy Base Logit: 51.84%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1846\n",
      "Epoch [2/100], Loss: 2.5412\n",
      "Epoch [3/100], Loss: 1.8887\n",
      "Epoch [4/100], Loss: 1.4556\n",
      "Epoch [5/100], Loss: 1.2116\n",
      "Epoch [6/100], Loss: 1.0504\n",
      "Epoch [7/100], Loss: 0.9296\n",
      "Epoch [8/100], Loss: 0.8318\n",
      "Epoch [9/100], Loss: 0.7505\n",
      "Epoch [10/100], Loss: 0.6812\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5719\n",
      "Epoch [13/100], Loss: 0.5272\n",
      "Epoch [14/100], Loss: 0.4914\n",
      "Epoch [15/100], Loss: 0.4609\n",
      "Epoch [16/100], Loss: 0.4306\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3658\n",
      "Epoch [20/100], Loss: 0.3466\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1852\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1599\n",
      "Epoch [45/100], Loss: 0.1567\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1399\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1092\n",
      "Epoch [66/100], Loss: 0.1073\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [90/100], Loss: 0.0790\n",
      "Test Accuracy Base Logit: 51.81%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1820\n",
      "Epoch [2/100], Loss: 2.5344\n",
      "Epoch [3/100], Loss: 1.8857\n",
      "Epoch [4/100], Loss: 1.4595\n",
      "Epoch [5/100], Loss: 1.2081\n",
      "Epoch [6/100], Loss: 1.0495\n",
      "Epoch [7/100], Loss: 0.9282\n",
      "Epoch [8/100], Loss: 0.8357\n",
      "Epoch [9/100], Loss: 0.7483\n",
      "Epoch [10/100], Loss: 0.6809\n",
      "Epoch [11/100], Loss: 0.6180\n",
      "Epoch [12/100], Loss: 0.5707\n",
      "Epoch [13/100], Loss: 0.5277\n",
      "Epoch [14/100], Loss: 0.4910\n",
      "Epoch [15/100], Loss: 0.4605\n",
      "Epoch [16/100], Loss: 0.4326\n",
      "Epoch [17/100], Loss: 0.4077\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3665\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2607\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1273\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0960\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [91/100], Loss: 0.0782\n",
      "Test Accuracy Base Logit: 51.72%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1898\n",
      "Epoch [2/100], Loss: 2.5194\n",
      "Epoch [3/100], Loss: 1.8836\n",
      "Epoch [4/100], Loss: 1.4598\n",
      "Epoch [5/100], Loss: 1.2064\n",
      "Epoch [6/100], Loss: 1.0437\n",
      "Epoch [7/100], Loss: 0.9260\n",
      "Epoch [8/100], Loss: 0.8328\n",
      "Epoch [9/100], Loss: 0.7482\n",
      "Epoch [10/100], Loss: 0.6815\n",
      "Epoch [11/100], Loss: 0.6215\n",
      "Epoch [12/100], Loss: 0.5718\n",
      "Epoch [13/100], Loss: 0.5300\n",
      "Epoch [14/100], Loss: 0.4931\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4330\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3841\n",
      "Epoch [19/100], Loss: 0.3643\n",
      "Epoch [20/100], Loss: 0.3468\n",
      "Epoch [21/100], Loss: 0.3310\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2426\n",
      "Epoch [30/100], Loss: 0.2343\n",
      "Epoch [31/100], Loss: 0.2270\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1288\n",
      "Epoch [56/100], Loss: 0.1268\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1205\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1064\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0957\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [89/100], Loss: 0.0798\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1939\n",
      "Epoch [2/100], Loss: 2.5368\n",
      "Epoch [3/100], Loss: 1.8856\n",
      "Epoch [4/100], Loss: 1.4608\n",
      "Epoch [5/100], Loss: 1.2145\n",
      "Epoch [6/100], Loss: 1.0435\n",
      "Epoch [7/100], Loss: 0.9289\n",
      "Epoch [8/100], Loss: 0.8335\n",
      "Epoch [9/100], Loss: 0.7490\n",
      "Epoch [10/100], Loss: 0.6756\n",
      "Epoch [11/100], Loss: 0.6185\n",
      "Epoch [12/100], Loss: 0.5703\n",
      "Epoch [13/100], Loss: 0.5306\n",
      "Epoch [14/100], Loss: 0.4889\n",
      "Epoch [15/100], Loss: 0.4623\n",
      "Epoch [16/100], Loss: 0.4328\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3669\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3309\n",
      "Epoch [22/100], Loss: 0.3181\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2817\n",
      "Epoch [26/100], Loss: 0.2694\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2342\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2201\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1904\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1814\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0898\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [86/100], Loss: 0.0824\n",
      "Test Accuracy Base Logit: 51.97%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1944\n",
      "Epoch [2/100], Loss: 2.5399\n",
      "Epoch [3/100], Loss: 1.8670\n",
      "Epoch [4/100], Loss: 1.4548\n",
      "Epoch [5/100], Loss: 1.2169\n",
      "Epoch [6/100], Loss: 1.0513\n",
      "Epoch [7/100], Loss: 0.9307\n",
      "Epoch [8/100], Loss: 0.8280\n",
      "Epoch [9/100], Loss: 0.7513\n",
      "Epoch [10/100], Loss: 0.6766\n",
      "Epoch [11/100], Loss: 0.6179\n",
      "Epoch [12/100], Loss: 0.5707\n",
      "Epoch [13/100], Loss: 0.5271\n",
      "Epoch [14/100], Loss: 0.4907\n",
      "Epoch [15/100], Loss: 0.4589\n",
      "Epoch [16/100], Loss: 0.4312\n",
      "Epoch [17/100], Loss: 0.4078\n",
      "Epoch [18/100], Loss: 0.3845\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3324\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3042\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2698\n",
      "Epoch [27/100], Loss: 0.2597\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2268\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1759\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1644\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1564\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1436\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1304\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1131\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0803\n",
      "Epoch [90/100], Loss: 0.0794\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0761\n",
      "Epoch [94/100], Loss: 0.0758\n",
      "Stopping early at epoch 94 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [94/100], Loss: 0.0758\n",
      "Test Accuracy Base Logit: 51.70%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1865\n",
      "Epoch [2/100], Loss: 2.5367\n",
      "Epoch [3/100], Loss: 1.8852\n",
      "Epoch [4/100], Loss: 1.4742\n",
      "Epoch [5/100], Loss: 1.2133\n",
      "Epoch [6/100], Loss: 1.0459\n",
      "Epoch [7/100], Loss: 0.9257\n",
      "Epoch [8/100], Loss: 0.8328\n",
      "Epoch [9/100], Loss: 0.7472\n",
      "Epoch [10/100], Loss: 0.6782\n",
      "Epoch [11/100], Loss: 0.6170\n",
      "Epoch [12/100], Loss: 0.5690\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4916\n",
      "Epoch [15/100], Loss: 0.4623\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3860\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3157\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2514\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2068\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1599\n",
      "Epoch [45/100], Loss: 0.1564\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1466\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1296\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1139\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0881\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0847\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0812\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [92/100], Loss: 0.0772\n",
      "Test Accuracy Base Logit: 51.90%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1893\n",
      "Epoch [2/100], Loss: 2.5308\n",
      "Epoch [3/100], Loss: 1.8763\n",
      "Epoch [4/100], Loss: 1.4651\n",
      "Epoch [5/100], Loss: 1.2097\n",
      "Epoch [6/100], Loss: 1.0490\n",
      "Epoch [7/100], Loss: 0.9304\n",
      "Epoch [8/100], Loss: 0.8322\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6811\n",
      "Epoch [11/100], Loss: 0.6207\n",
      "Epoch [12/100], Loss: 0.5699\n",
      "Epoch [13/100], Loss: 0.5263\n",
      "Epoch [14/100], Loss: 0.4928\n",
      "Epoch [15/100], Loss: 0.4607\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4086\n",
      "Epoch [18/100], Loss: 0.3873\n",
      "Epoch [19/100], Loss: 0.3664\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3159\n",
      "Epoch [23/100], Loss: 0.3040\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2584\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1940\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1792\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1675\n",
      "Epoch [43/100], Loss: 0.1636\n",
      "Epoch [44/100], Loss: 0.1589\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1273\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1140\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1064\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0933\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [89/100], Loss: 0.0798\n",
      "Test Accuracy Base Logit: 51.92%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1835\n",
      "Epoch [2/100], Loss: 2.5280\n",
      "Epoch [3/100], Loss: 1.8778\n",
      "Epoch [4/100], Loss: 1.4621\n",
      "Epoch [5/100], Loss: 1.2092\n",
      "Epoch [6/100], Loss: 1.0402\n",
      "Epoch [7/100], Loss: 0.9279\n",
      "Epoch [8/100], Loss: 0.8319\n",
      "Epoch [9/100], Loss: 0.7486\n",
      "Epoch [10/100], Loss: 0.6803\n",
      "Epoch [11/100], Loss: 0.6211\n",
      "Epoch [12/100], Loss: 0.5703\n",
      "Epoch [13/100], Loss: 0.5280\n",
      "Epoch [14/100], Loss: 0.4920\n",
      "Epoch [15/100], Loss: 0.4581\n",
      "Epoch [16/100], Loss: 0.4314\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3182\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2265\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0984\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0837\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [92/100], Loss: 0.0770\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1874\n",
      "Epoch [2/100], Loss: 2.5374\n",
      "Epoch [3/100], Loss: 1.8832\n",
      "Epoch [4/100], Loss: 1.4740\n",
      "Epoch [5/100], Loss: 1.2141\n",
      "Epoch [6/100], Loss: 1.0464\n",
      "Epoch [7/100], Loss: 0.9319\n",
      "Epoch [8/100], Loss: 0.8298\n",
      "Epoch [9/100], Loss: 0.7531\n",
      "Epoch [10/100], Loss: 0.6792\n",
      "Epoch [11/100], Loss: 0.6193\n",
      "Epoch [12/100], Loss: 0.5689\n",
      "Epoch [13/100], Loss: 0.5295\n",
      "Epoch [14/100], Loss: 0.4923\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4327\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3841\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3312\n",
      "Epoch [22/100], Loss: 0.3155\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2919\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1958\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1564\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1497\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [91/100], Loss: 0.0781\n",
      "Test Accuracy Base Logit: 51.89%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1890\n",
      "Epoch [2/100], Loss: 2.5335\n",
      "Epoch [3/100], Loss: 1.8786\n",
      "Epoch [4/100], Loss: 1.4680\n",
      "Epoch [5/100], Loss: 1.2137\n",
      "Epoch [6/100], Loss: 1.0401\n",
      "Epoch [7/100], Loss: 0.9266\n",
      "Epoch [8/100], Loss: 0.8309\n",
      "Epoch [9/100], Loss: 0.7481\n",
      "Epoch [10/100], Loss: 0.6770\n",
      "Epoch [11/100], Loss: 0.6190\n",
      "Epoch [12/100], Loss: 0.5734\n",
      "Epoch [13/100], Loss: 0.5275\n",
      "Epoch [14/100], Loss: 0.4933\n",
      "Epoch [15/100], Loss: 0.4607\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3470\n",
      "Epoch [21/100], Loss: 0.3306\n",
      "Epoch [22/100], Loss: 0.3180\n",
      "Epoch [23/100], Loss: 0.3027\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.1995\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1411\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0792\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Stopping early at epoch 94 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [94/100], Loss: 0.0755\n",
      "Test Accuracy Base Logit: 51.81%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1873\n",
      "Epoch [2/100], Loss: 2.5268\n",
      "Epoch [3/100], Loss: 1.8756\n",
      "Epoch [4/100], Loss: 1.4633\n",
      "Epoch [5/100], Loss: 1.2122\n",
      "Epoch [6/100], Loss: 1.0471\n",
      "Epoch [7/100], Loss: 0.9331\n",
      "Epoch [8/100], Loss: 0.8331\n",
      "Epoch [9/100], Loss: 0.7481\n",
      "Epoch [10/100], Loss: 0.6807\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5702\n",
      "Epoch [13/100], Loss: 0.5293\n",
      "Epoch [14/100], Loss: 0.4921\n",
      "Epoch [15/100], Loss: 0.4621\n",
      "Epoch [16/100], Loss: 0.4320\n",
      "Epoch [17/100], Loss: 0.4077\n",
      "Epoch [18/100], Loss: 0.3845\n",
      "Epoch [19/100], Loss: 0.3673\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3180\n",
      "Epoch [23/100], Loss: 0.3040\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2113\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1634\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1272\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1160\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1056\n",
      "Epoch [68/100], Loss: 0.1032\n",
      "Epoch [69/100], Loss: 0.1018\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Stopping early at epoch 84 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [84/100], Loss: 0.0844\n",
      "Test Accuracy Base Logit: 51.83%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1784\n",
      "Epoch [2/100], Loss: 2.5426\n",
      "Epoch [3/100], Loss: 1.8888\n",
      "Epoch [4/100], Loss: 1.4657\n",
      "Epoch [5/100], Loss: 1.2137\n",
      "Epoch [6/100], Loss: 1.0528\n",
      "Epoch [7/100], Loss: 0.9310\n",
      "Epoch [8/100], Loss: 0.8335\n",
      "Epoch [9/100], Loss: 0.7501\n",
      "Epoch [10/100], Loss: 0.6797\n",
      "Epoch [11/100], Loss: 0.6190\n",
      "Epoch [12/100], Loss: 0.5692\n",
      "Epoch [13/100], Loss: 0.5304\n",
      "Epoch [14/100], Loss: 0.4930\n",
      "Epoch [15/100], Loss: 0.4608\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4084\n",
      "Epoch [18/100], Loss: 0.3862\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3470\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2710\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2512\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2327\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2198\n",
      "Epoch [33/100], Loss: 0.2132\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.2011\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1888\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1675\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1487\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0997\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [93/100], Loss: 0.0762\n",
      "Test Accuracy Base Logit: 51.92%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1889\n",
      "Epoch [2/100], Loss: 2.5339\n",
      "Epoch [3/100], Loss: 1.8764\n",
      "Epoch [4/100], Loss: 1.4546\n",
      "Epoch [5/100], Loss: 1.2105\n",
      "Epoch [6/100], Loss: 1.0468\n",
      "Epoch [7/100], Loss: 0.9243\n",
      "Epoch [8/100], Loss: 0.8282\n",
      "Epoch [9/100], Loss: 0.7473\n",
      "Epoch [10/100], Loss: 0.6781\n",
      "Epoch [11/100], Loss: 0.6213\n",
      "Epoch [12/100], Loss: 0.5732\n",
      "Epoch [13/100], Loss: 0.5282\n",
      "Epoch [14/100], Loss: 0.4914\n",
      "Epoch [15/100], Loss: 0.4601\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3843\n",
      "Epoch [19/100], Loss: 0.3670\n",
      "Epoch [20/100], Loss: 0.3472\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3028\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2585\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2195\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1851\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1553\n",
      "Epoch [46/100], Loss: 0.1530\n",
      "Epoch [47/100], Loss: 0.1497\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1306\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1262\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0857\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [88/100], Loss: 0.0806\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1821\n",
      "Epoch [2/100], Loss: 2.5421\n",
      "Epoch [3/100], Loss: 1.8844\n",
      "Epoch [4/100], Loss: 1.4634\n",
      "Epoch [5/100], Loss: 1.2054\n",
      "Epoch [6/100], Loss: 1.0463\n",
      "Epoch [7/100], Loss: 0.9302\n",
      "Epoch [8/100], Loss: 0.8335\n",
      "Epoch [9/100], Loss: 0.7443\n",
      "Epoch [10/100], Loss: 0.6826\n",
      "Epoch [11/100], Loss: 0.6194\n",
      "Epoch [12/100], Loss: 0.5726\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4918\n",
      "Epoch [15/100], Loss: 0.4618\n",
      "Epoch [16/100], Loss: 0.4332\n",
      "Epoch [17/100], Loss: 0.4085\n",
      "Epoch [18/100], Loss: 0.3845\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3304\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3037\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2788\n",
      "Epoch [26/100], Loss: 0.2682\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2350\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1888\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1634\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1408\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1157\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0899\n",
      "Epoch [80/100], Loss: 0.0889\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0791\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0765\n",
      "Epoch [94/100], Loss: 0.0757\n",
      "Stopping early at epoch 94 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [94/100], Loss: 0.0757\n",
      "Test Accuracy Base Logit: 51.83%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1911\n",
      "Epoch [2/100], Loss: 2.5518\n",
      "Epoch [3/100], Loss: 1.8958\n",
      "Epoch [4/100], Loss: 1.4637\n",
      "Epoch [5/100], Loss: 1.2136\n",
      "Epoch [6/100], Loss: 1.0451\n",
      "Epoch [7/100], Loss: 0.9278\n",
      "Epoch [8/100], Loss: 0.8403\n",
      "Epoch [9/100], Loss: 0.7472\n",
      "Epoch [10/100], Loss: 0.6813\n",
      "Epoch [11/100], Loss: 0.6194\n",
      "Epoch [12/100], Loss: 0.5705\n",
      "Epoch [13/100], Loss: 0.5268\n",
      "Epoch [14/100], Loss: 0.4903\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4305\n",
      "Epoch [17/100], Loss: 0.4071\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3647\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3332\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2507\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2329\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2022\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1901\n",
      "Epoch [38/100], Loss: 0.1853\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0957\n",
      "Epoch [75/100], Loss: 0.0938\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [88/100], Loss: 0.0808\n",
      "Test Accuracy Base Logit: 51.92%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1870\n",
      "Epoch [2/100], Loss: 2.5264\n",
      "Epoch [3/100], Loss: 1.8796\n",
      "Epoch [4/100], Loss: 1.4560\n",
      "Epoch [5/100], Loss: 1.2170\n",
      "Epoch [6/100], Loss: 1.0551\n",
      "Epoch [7/100], Loss: 0.9296\n",
      "Epoch [8/100], Loss: 0.8328\n",
      "Epoch [9/100], Loss: 0.7471\n",
      "Epoch [10/100], Loss: 0.6789\n",
      "Epoch [11/100], Loss: 0.6193\n",
      "Epoch [12/100], Loss: 0.5714\n",
      "Epoch [13/100], Loss: 0.5273\n",
      "Epoch [14/100], Loss: 0.4939\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4329\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3046\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2803\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.1997\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1839\n",
      "Epoch [39/100], Loss: 0.1792\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1356\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0926\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0792\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Stopping early at epoch 94 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [94/100], Loss: 0.0755\n",
      "Test Accuracy Base Logit: 51.84%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1822\n",
      "Epoch [2/100], Loss: 2.5276\n",
      "Epoch [3/100], Loss: 1.8818\n",
      "Epoch [4/100], Loss: 1.4624\n",
      "Epoch [5/100], Loss: 1.2067\n",
      "Epoch [6/100], Loss: 1.0449\n",
      "Epoch [7/100], Loss: 0.9325\n",
      "Epoch [8/100], Loss: 0.8347\n",
      "Epoch [9/100], Loss: 0.7477\n",
      "Epoch [10/100], Loss: 0.6791\n",
      "Epoch [11/100], Loss: 0.6213\n",
      "Epoch [12/100], Loss: 0.5722\n",
      "Epoch [13/100], Loss: 0.5292\n",
      "Epoch [14/100], Loss: 0.4903\n",
      "Epoch [15/100], Loss: 0.4581\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4063\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3470\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2904\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2406\n",
      "Epoch [30/100], Loss: 0.2329\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1801\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1443\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0970\n",
      "Epoch [74/100], Loss: 0.0963\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0794\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Stopping early at epoch 94 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [94/100], Loss: 0.0755\n",
      "Test Accuracy Base Logit: 51.85%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1864\n",
      "Epoch [2/100], Loss: 2.5370\n",
      "Epoch [3/100], Loss: 1.8827\n",
      "Epoch [4/100], Loss: 1.4601\n",
      "Epoch [5/100], Loss: 1.2174\n",
      "Epoch [6/100], Loss: 1.0455\n",
      "Epoch [7/100], Loss: 0.9291\n",
      "Epoch [8/100], Loss: 0.8297\n",
      "Epoch [9/100], Loss: 0.7471\n",
      "Epoch [10/100], Loss: 0.6753\n",
      "Epoch [11/100], Loss: 0.6216\n",
      "Epoch [12/100], Loss: 0.5686\n",
      "Epoch [13/100], Loss: 0.5271\n",
      "Epoch [14/100], Loss: 0.4920\n",
      "Epoch [15/100], Loss: 0.4594\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4063\n",
      "Epoch [18/100], Loss: 0.3859\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3028\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2806\n",
      "Epoch [26/100], Loss: 0.2702\n",
      "Epoch [27/100], Loss: 0.2597\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2409\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1854\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1262\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1175\n",
      "Epoch [61/100], Loss: 0.1158\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0888\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0831\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0774\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Stopping early at epoch 96 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [96/100], Loss: 0.0740\n",
      "Test Accuracy Base Logit: 51.81%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1834\n",
      "Epoch [2/100], Loss: 2.5392\n",
      "Epoch [3/100], Loss: 1.8835\n",
      "Epoch [4/100], Loss: 1.4639\n",
      "Epoch [5/100], Loss: 1.2118\n",
      "Epoch [6/100], Loss: 1.0498\n",
      "Epoch [7/100], Loss: 0.9305\n",
      "Epoch [8/100], Loss: 0.8315\n",
      "Epoch [9/100], Loss: 0.7483\n",
      "Epoch [10/100], Loss: 0.6797\n",
      "Epoch [11/100], Loss: 0.6200\n",
      "Epoch [12/100], Loss: 0.5702\n",
      "Epoch [13/100], Loss: 0.5273\n",
      "Epoch [14/100], Loss: 0.4924\n",
      "Epoch [15/100], Loss: 0.4574\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4068\n",
      "Epoch [18/100], Loss: 0.3864\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3498\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3179\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2699\n",
      "Epoch [27/100], Loss: 0.2607\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2070\n",
      "Epoch [35/100], Loss: 0.1996\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1588\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1381\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1064\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1025\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0877\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [92/100], Loss: 0.0771\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1912\n",
      "Epoch [2/100], Loss: 2.5330\n",
      "Epoch [3/100], Loss: 1.8763\n",
      "Epoch [4/100], Loss: 1.4546\n",
      "Epoch [5/100], Loss: 1.2180\n",
      "Epoch [6/100], Loss: 1.0465\n",
      "Epoch [7/100], Loss: 0.9304\n",
      "Epoch [8/100], Loss: 0.8330\n",
      "Epoch [9/100], Loss: 0.7508\n",
      "Epoch [10/100], Loss: 0.6783\n",
      "Epoch [11/100], Loss: 0.6188\n",
      "Epoch [12/100], Loss: 0.5706\n",
      "Epoch [13/100], Loss: 0.5264\n",
      "Epoch [14/100], Loss: 0.4934\n",
      "Epoch [15/100], Loss: 0.4605\n",
      "Epoch [16/100], Loss: 0.4331\n",
      "Epoch [17/100], Loss: 0.4087\n",
      "Epoch [18/100], Loss: 0.3864\n",
      "Epoch [19/100], Loss: 0.3666\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3304\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2583\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2329\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1537\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1397\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1113\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0818\n",
      "Epoch [88/100], Loss: 0.0809\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [88/100], Loss: 0.0809\n",
      "Test Accuracy Base Logit: 51.84%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1864\n",
      "Epoch [2/100], Loss: 2.5308\n",
      "Epoch [3/100], Loss: 1.8674\n",
      "Epoch [4/100], Loss: 1.4590\n",
      "Epoch [5/100], Loss: 1.2064\n",
      "Epoch [6/100], Loss: 1.0443\n",
      "Epoch [7/100], Loss: 0.9288\n",
      "Epoch [8/100], Loss: 0.8309\n",
      "Epoch [9/100], Loss: 0.7491\n",
      "Epoch [10/100], Loss: 0.6817\n",
      "Epoch [11/100], Loss: 0.6224\n",
      "Epoch [12/100], Loss: 0.5723\n",
      "Epoch [13/100], Loss: 0.5293\n",
      "Epoch [14/100], Loss: 0.4939\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4326\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3867\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3328\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2407\n",
      "Epoch [30/100], Loss: 0.2327\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1157\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0871\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [87/100], Loss: 0.0813\n",
      "Test Accuracy Base Logit: 51.89%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1834\n",
      "Epoch [2/100], Loss: 2.5430\n",
      "Epoch [3/100], Loss: 1.8845\n",
      "Epoch [4/100], Loss: 1.4652\n",
      "Epoch [5/100], Loss: 1.2128\n",
      "Epoch [6/100], Loss: 1.0488\n",
      "Epoch [7/100], Loss: 0.9289\n",
      "Epoch [8/100], Loss: 0.8303\n",
      "Epoch [9/100], Loss: 0.7501\n",
      "Epoch [10/100], Loss: 0.6825\n",
      "Epoch [11/100], Loss: 0.6196\n",
      "Epoch [12/100], Loss: 0.5699\n",
      "Epoch [13/100], Loss: 0.5306\n",
      "Epoch [14/100], Loss: 0.4911\n",
      "Epoch [15/100], Loss: 0.4605\n",
      "Epoch [16/100], Loss: 0.4310\n",
      "Epoch [17/100], Loss: 0.4087\n",
      "Epoch [18/100], Loss: 0.3844\n",
      "Epoch [19/100], Loss: 0.3664\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3027\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2803\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2584\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2419\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1456\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1410\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1058\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0937\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 5000, Epoch [92/100], Loss: 0.0770\n",
      "Test Accuracy Base Logit: 51.83%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1898\n",
      "Epoch [2/100], Loss: 2.5295\n",
      "Epoch [3/100], Loss: 1.8889\n",
      "Epoch [4/100], Loss: 1.4531\n",
      "Epoch [5/100], Loss: 1.2145\n",
      "Epoch [6/100], Loss: 1.0526\n",
      "Epoch [7/100], Loss: 0.9293\n",
      "Epoch [8/100], Loss: 0.8349\n",
      "Epoch [9/100], Loss: 0.7512\n",
      "Epoch [10/100], Loss: 0.6775\n",
      "Epoch [11/100], Loss: 0.6202\n",
      "Epoch [12/100], Loss: 0.5694\n",
      "Epoch [13/100], Loss: 0.5271\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4606\n",
      "Epoch [16/100], Loss: 0.4336\n",
      "Epoch [17/100], Loss: 0.4056\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3469\n",
      "Epoch [21/100], Loss: 0.3302\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3037\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2598\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1747\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1499\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1442\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1330\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0977\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0841\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0812\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0794\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Stopping early at epoch 96 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [96/100], Loss: 0.0739\n",
      "Test Accuracy Base Logit: 51.85%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1939\n",
      "Epoch [2/100], Loss: 2.5372\n",
      "Epoch [3/100], Loss: 1.8811\n",
      "Epoch [4/100], Loss: 1.4633\n",
      "Epoch [5/100], Loss: 1.2148\n",
      "Epoch [6/100], Loss: 1.0518\n",
      "Epoch [7/100], Loss: 0.9293\n",
      "Epoch [8/100], Loss: 0.8295\n",
      "Epoch [9/100], Loss: 0.7510\n",
      "Epoch [10/100], Loss: 0.6803\n",
      "Epoch [11/100], Loss: 0.6183\n",
      "Epoch [12/100], Loss: 0.5693\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4928\n",
      "Epoch [15/100], Loss: 0.4593\n",
      "Epoch [16/100], Loss: 0.4341\n",
      "Epoch [17/100], Loss: 0.4090\n",
      "Epoch [18/100], Loss: 0.3858\n",
      "Epoch [19/100], Loss: 0.3663\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3024\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2597\n",
      "Epoch [28/100], Loss: 0.2490\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2340\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2130\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1637\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1472\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1381\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1238\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1004\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0970\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0869\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [87/100], Loss: 0.0815\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1872\n",
      "Epoch [2/100], Loss: 2.5409\n",
      "Epoch [3/100], Loss: 1.8815\n",
      "Epoch [4/100], Loss: 1.4579\n",
      "Epoch [5/100], Loss: 1.2046\n",
      "Epoch [6/100], Loss: 1.0459\n",
      "Epoch [7/100], Loss: 0.9247\n",
      "Epoch [8/100], Loss: 0.8296\n",
      "Epoch [9/100], Loss: 0.7479\n",
      "Epoch [10/100], Loss: 0.6809\n",
      "Epoch [11/100], Loss: 0.6218\n",
      "Epoch [12/100], Loss: 0.5681\n",
      "Epoch [13/100], Loss: 0.5287\n",
      "Epoch [14/100], Loss: 0.4953\n",
      "Epoch [15/100], Loss: 0.4607\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3836\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3037\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2805\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2600\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2422\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2129\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1951\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1388\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0997\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0961\n",
      "Epoch [75/100], Loss: 0.0948\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [87/100], Loss: 0.0814\n",
      "Test Accuracy Base Logit: 51.91%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1957\n",
      "Epoch [2/100], Loss: 2.5422\n",
      "Epoch [3/100], Loss: 1.8849\n",
      "Epoch [4/100], Loss: 1.4617\n",
      "Epoch [5/100], Loss: 1.2043\n",
      "Epoch [6/100], Loss: 1.0476\n",
      "Epoch [7/100], Loss: 0.9307\n",
      "Epoch [8/100], Loss: 0.8333\n",
      "Epoch [9/100], Loss: 0.7479\n",
      "Epoch [10/100], Loss: 0.6803\n",
      "Epoch [11/100], Loss: 0.6231\n",
      "Epoch [12/100], Loss: 0.5730\n",
      "Epoch [13/100], Loss: 0.5278\n",
      "Epoch [14/100], Loss: 0.4932\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4327\n",
      "Epoch [17/100], Loss: 0.4080\n",
      "Epoch [18/100], Loss: 0.3862\n",
      "Epoch [19/100], Loss: 0.3666\n",
      "Epoch [20/100], Loss: 0.3488\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3027\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1675\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1124\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [88/100], Loss: 0.0805\n",
      "Test Accuracy Base Logit: 51.92%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1940\n",
      "Epoch [2/100], Loss: 2.5328\n",
      "Epoch [3/100], Loss: 1.8834\n",
      "Epoch [4/100], Loss: 1.4643\n",
      "Epoch [5/100], Loss: 1.2196\n",
      "Epoch [6/100], Loss: 1.0485\n",
      "Epoch [7/100], Loss: 0.9275\n",
      "Epoch [8/100], Loss: 0.8324\n",
      "Epoch [9/100], Loss: 0.7529\n",
      "Epoch [10/100], Loss: 0.6840\n",
      "Epoch [11/100], Loss: 0.6161\n",
      "Epoch [12/100], Loss: 0.5692\n",
      "Epoch [13/100], Loss: 0.5308\n",
      "Epoch [14/100], Loss: 0.4916\n",
      "Epoch [15/100], Loss: 0.4605\n",
      "Epoch [16/100], Loss: 0.4329\n",
      "Epoch [17/100], Loss: 0.4058\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2585\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2407\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2197\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2006\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1855\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1500\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1305\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1080\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1062\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0938\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [88/100], Loss: 0.0807\n",
      "Test Accuracy Base Logit: 51.81%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1796\n",
      "Epoch [2/100], Loss: 2.5276\n",
      "Epoch [3/100], Loss: 1.8765\n",
      "Epoch [4/100], Loss: 1.4691\n",
      "Epoch [5/100], Loss: 1.2171\n",
      "Epoch [6/100], Loss: 1.0441\n",
      "Epoch [7/100], Loss: 0.9250\n",
      "Epoch [8/100], Loss: 0.8278\n",
      "Epoch [9/100], Loss: 0.7504\n",
      "Epoch [10/100], Loss: 0.6763\n",
      "Epoch [11/100], Loss: 0.6200\n",
      "Epoch [12/100], Loss: 0.5692\n",
      "Epoch [13/100], Loss: 0.5255\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4602\n",
      "Epoch [16/100], Loss: 0.4327\n",
      "Epoch [17/100], Loss: 0.4058\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3649\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3038\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2703\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2513\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2265\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1957\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1854\n",
      "Epoch [39/100], Loss: 0.1803\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1529\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1366\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1095\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [89/100], Loss: 0.0795\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1828\n",
      "Epoch [2/100], Loss: 2.5321\n",
      "Epoch [3/100], Loss: 1.8747\n",
      "Epoch [4/100], Loss: 1.4549\n",
      "Epoch [5/100], Loss: 1.2102\n",
      "Epoch [6/100], Loss: 1.0471\n",
      "Epoch [7/100], Loss: 0.9297\n",
      "Epoch [8/100], Loss: 0.8297\n",
      "Epoch [9/100], Loss: 0.7484\n",
      "Epoch [10/100], Loss: 0.6784\n",
      "Epoch [11/100], Loss: 0.6215\n",
      "Epoch [12/100], Loss: 0.5707\n",
      "Epoch [13/100], Loss: 0.5291\n",
      "Epoch [14/100], Loss: 0.4931\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4316\n",
      "Epoch [17/100], Loss: 0.4081\n",
      "Epoch [18/100], Loss: 0.3862\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2357\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2070\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1678\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1588\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1466\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1215\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1168\n",
      "Epoch [61/100], Loss: 0.1157\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1104\n",
      "Epoch [65/100], Loss: 0.1090\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0921\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [90/100], Loss: 0.0788\n",
      "Test Accuracy Base Logit: 51.91%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1892\n",
      "Epoch [2/100], Loss: 2.5276\n",
      "Epoch [3/100], Loss: 1.8726\n",
      "Epoch [4/100], Loss: 1.4644\n",
      "Epoch [5/100], Loss: 1.2144\n",
      "Epoch [6/100], Loss: 1.0436\n",
      "Epoch [7/100], Loss: 0.9264\n",
      "Epoch [8/100], Loss: 0.8320\n",
      "Epoch [9/100], Loss: 0.7481\n",
      "Epoch [10/100], Loss: 0.6765\n",
      "Epoch [11/100], Loss: 0.6185\n",
      "Epoch [12/100], Loss: 0.5698\n",
      "Epoch [13/100], Loss: 0.5264\n",
      "Epoch [14/100], Loss: 0.4907\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4315\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3331\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3038\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2694\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2492\n",
      "Epoch [29/100], Loss: 0.2420\n",
      "Epoch [30/100], Loss: 0.2340\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1501\n",
      "Epoch [48/100], Loss: 0.1466\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1025\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0926\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0813\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0791\n",
      "Epoch [91/100], Loss: 0.0783\n",
      "Epoch [92/100], Loss: 0.0778\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [92/100], Loss: 0.0778\n",
      "Test Accuracy Base Logit: 51.72%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1872\n",
      "Epoch [2/100], Loss: 2.5243\n",
      "Epoch [3/100], Loss: 1.8757\n",
      "Epoch [4/100], Loss: 1.4559\n",
      "Epoch [5/100], Loss: 1.2157\n",
      "Epoch [6/100], Loss: 1.0472\n",
      "Epoch [7/100], Loss: 0.9270\n",
      "Epoch [8/100], Loss: 0.8376\n",
      "Epoch [9/100], Loss: 0.7523\n",
      "Epoch [10/100], Loss: 0.6775\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5728\n",
      "Epoch [13/100], Loss: 0.5262\n",
      "Epoch [14/100], Loss: 0.4918\n",
      "Epoch [15/100], Loss: 0.4612\n",
      "Epoch [16/100], Loss: 0.4311\n",
      "Epoch [17/100], Loss: 0.4088\n",
      "Epoch [18/100], Loss: 0.3858\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2607\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1801\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1674\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1566\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1472\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1307\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [89/100], Loss: 0.0796\n",
      "Test Accuracy Base Logit: 51.82%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1930\n",
      "Epoch [2/100], Loss: 2.5392\n",
      "Epoch [3/100], Loss: 1.8745\n",
      "Epoch [4/100], Loss: 1.4631\n",
      "Epoch [5/100], Loss: 1.2073\n",
      "Epoch [6/100], Loss: 1.0450\n",
      "Epoch [7/100], Loss: 0.9276\n",
      "Epoch [8/100], Loss: 0.8319\n",
      "Epoch [9/100], Loss: 0.7504\n",
      "Epoch [10/100], Loss: 0.6810\n",
      "Epoch [11/100], Loss: 0.6183\n",
      "Epoch [12/100], Loss: 0.5699\n",
      "Epoch [13/100], Loss: 0.5312\n",
      "Epoch [14/100], Loss: 0.4920\n",
      "Epoch [15/100], Loss: 0.4606\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4082\n",
      "Epoch [18/100], Loss: 0.3844\n",
      "Epoch [19/100], Loss: 0.3668\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3309\n",
      "Epoch [22/100], Loss: 0.3181\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2803\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2585\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2131\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1064\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0922\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0898\n",
      "Epoch [80/100], Loss: 0.0888\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Stopping early at epoch 87 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [87/100], Loss: 0.0816\n",
      "Test Accuracy Base Logit: 51.95%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1848\n",
      "Epoch [2/100], Loss: 2.5344\n",
      "Epoch [3/100], Loss: 1.8739\n",
      "Epoch [4/100], Loss: 1.4685\n",
      "Epoch [5/100], Loss: 1.2155\n",
      "Epoch [6/100], Loss: 1.0559\n",
      "Epoch [7/100], Loss: 0.9346\n",
      "Epoch [8/100], Loss: 0.8322\n",
      "Epoch [9/100], Loss: 0.7524\n",
      "Epoch [10/100], Loss: 0.6785\n",
      "Epoch [11/100], Loss: 0.6203\n",
      "Epoch [12/100], Loss: 0.5727\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4898\n",
      "Epoch [15/100], Loss: 0.4613\n",
      "Epoch [16/100], Loss: 0.4308\n",
      "Epoch [17/100], Loss: 0.4058\n",
      "Epoch [18/100], Loss: 0.3881\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3164\n",
      "Epoch [23/100], Loss: 0.3045\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2789\n",
      "Epoch [26/100], Loss: 0.2698\n",
      "Epoch [27/100], Loss: 0.2584\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2327\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2067\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1589\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1356\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1080\n",
      "Epoch [66/100], Loss: 0.1072\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0951\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0916\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [88/100], Loss: 0.0807\n",
      "Test Accuracy Base Logit: 51.91%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1852\n",
      "Epoch [2/100], Loss: 2.5365\n",
      "Epoch [3/100], Loss: 1.8775\n",
      "Epoch [4/100], Loss: 1.4524\n",
      "Epoch [5/100], Loss: 1.2130\n",
      "Epoch [6/100], Loss: 1.0447\n",
      "Epoch [7/100], Loss: 0.9269\n",
      "Epoch [8/100], Loss: 0.8312\n",
      "Epoch [9/100], Loss: 0.7461\n",
      "Epoch [10/100], Loss: 0.6757\n",
      "Epoch [11/100], Loss: 0.6210\n",
      "Epoch [12/100], Loss: 0.5717\n",
      "Epoch [13/100], Loss: 0.5279\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3475\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3180\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2419\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1759\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1600\n",
      "Epoch [45/100], Loss: 0.1564\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1436\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1283\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1113\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [91/100], Loss: 0.0781\n",
      "Test Accuracy Base Logit: 51.82%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1850\n",
      "Epoch [2/100], Loss: 2.5307\n",
      "Epoch [3/100], Loss: 1.8716\n",
      "Epoch [4/100], Loss: 1.4601\n",
      "Epoch [5/100], Loss: 1.2153\n",
      "Epoch [6/100], Loss: 1.0513\n",
      "Epoch [7/100], Loss: 0.9311\n",
      "Epoch [8/100], Loss: 0.8298\n",
      "Epoch [9/100], Loss: 0.7520\n",
      "Epoch [10/100], Loss: 0.6785\n",
      "Epoch [11/100], Loss: 0.6183\n",
      "Epoch [12/100], Loss: 0.5677\n",
      "Epoch [13/100], Loss: 0.5280\n",
      "Epoch [14/100], Loss: 0.4930\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4336\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3864\n",
      "Epoch [19/100], Loss: 0.3648\n",
      "Epoch [20/100], Loss: 0.3501\n",
      "Epoch [21/100], Loss: 0.3336\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2789\n",
      "Epoch [26/100], Loss: 0.2684\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2406\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2131\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1851\n",
      "Epoch [39/100], Loss: 0.1801\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1281\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1107\n",
      "Epoch [65/100], Loss: 0.1090\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1056\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0898\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [86/100], Loss: 0.0824\n",
      "Test Accuracy Base Logit: 51.95%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1800\n",
      "Epoch [2/100], Loss: 2.5515\n",
      "Epoch [3/100], Loss: 1.8919\n",
      "Epoch [4/100], Loss: 1.4607\n",
      "Epoch [5/100], Loss: 1.2235\n",
      "Epoch [6/100], Loss: 1.0510\n",
      "Epoch [7/100], Loss: 0.9338\n",
      "Epoch [8/100], Loss: 0.8324\n",
      "Epoch [9/100], Loss: 0.7496\n",
      "Epoch [10/100], Loss: 0.6801\n",
      "Epoch [11/100], Loss: 0.6194\n",
      "Epoch [12/100], Loss: 0.5693\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4912\n",
      "Epoch [15/100], Loss: 0.4602\n",
      "Epoch [16/100], Loss: 0.4330\n",
      "Epoch [17/100], Loss: 0.4058\n",
      "Epoch [18/100], Loss: 0.3846\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3494\n",
      "Epoch [21/100], Loss: 0.3321\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3040\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2054\n",
      "Epoch [35/100], Loss: 0.2011\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1854\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1716\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1626\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1320\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Stopping early at epoch 85 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [85/100], Loss: 0.0833\n",
      "Test Accuracy Base Logit: 51.91%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1887\n",
      "Epoch [2/100], Loss: 2.5471\n",
      "Epoch [3/100], Loss: 1.8769\n",
      "Epoch [4/100], Loss: 1.4629\n",
      "Epoch [5/100], Loss: 1.2194\n",
      "Epoch [6/100], Loss: 1.0465\n",
      "Epoch [7/100], Loss: 0.9279\n",
      "Epoch [8/100], Loss: 0.8321\n",
      "Epoch [9/100], Loss: 0.7473\n",
      "Epoch [10/100], Loss: 0.6782\n",
      "Epoch [11/100], Loss: 0.6206\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5275\n",
      "Epoch [14/100], Loss: 0.4921\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4094\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3324\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2409\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1853\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1760\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1564\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1385\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1310\n",
      "Epoch [55/100], Loss: 0.1286\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1238\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1089\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1031\n",
      "Epoch [70/100], Loss: 0.1013\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0989\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0949\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0870\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Stopping early at epoch 84 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [84/100], Loss: 0.0844\n",
      "Test Accuracy Base Logit: 51.96%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1815\n",
      "Epoch [2/100], Loss: 2.5228\n",
      "Epoch [3/100], Loss: 1.8801\n",
      "Epoch [4/100], Loss: 1.4673\n",
      "Epoch [5/100], Loss: 1.2130\n",
      "Epoch [6/100], Loss: 1.0575\n",
      "Epoch [7/100], Loss: 0.9281\n",
      "Epoch [8/100], Loss: 0.8317\n",
      "Epoch [9/100], Loss: 0.7516\n",
      "Epoch [10/100], Loss: 0.6836\n",
      "Epoch [11/100], Loss: 0.6227\n",
      "Epoch [12/100], Loss: 0.5717\n",
      "Epoch [13/100], Loss: 0.5287\n",
      "Epoch [14/100], Loss: 0.4935\n",
      "Epoch [15/100], Loss: 0.4612\n",
      "Epoch [16/100], Loss: 0.4318\n",
      "Epoch [17/100], Loss: 0.4068\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2802\n",
      "Epoch [26/100], Loss: 0.2682\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.1995\n",
      "Epoch [36/100], Loss: 0.1940\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1563\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1413\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1124\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1079\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1061\n",
      "Epoch [68/100], Loss: 0.1041\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0917\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0881\n",
      "Epoch [82/100], Loss: 0.0870\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [92/100], Loss: 0.0772\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1835\n",
      "Epoch [2/100], Loss: 2.5419\n",
      "Epoch [3/100], Loss: 1.8714\n",
      "Epoch [4/100], Loss: 1.4661\n",
      "Epoch [5/100], Loss: 1.2108\n",
      "Epoch [6/100], Loss: 1.0430\n",
      "Epoch [7/100], Loss: 0.9284\n",
      "Epoch [8/100], Loss: 0.8357\n",
      "Epoch [9/100], Loss: 0.7469\n",
      "Epoch [10/100], Loss: 0.6773\n",
      "Epoch [11/100], Loss: 0.6223\n",
      "Epoch [12/100], Loss: 0.5707\n",
      "Epoch [13/100], Loss: 0.5299\n",
      "Epoch [14/100], Loss: 0.4910\n",
      "Epoch [15/100], Loss: 0.4607\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2789\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2342\n",
      "Epoch [31/100], Loss: 0.2269\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2068\n",
      "Epoch [35/100], Loss: 0.2016\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1532\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1281\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1215\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1158\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0883\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0761\n",
      "Epoch [94/100], Loss: 0.0757\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0743\n",
      "Stopping early at epoch 96 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [96/100], Loss: 0.0743\n",
      "Test Accuracy Base Logit: 51.73%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1859\n",
      "Epoch [2/100], Loss: 2.5344\n",
      "Epoch [3/100], Loss: 1.8819\n",
      "Epoch [4/100], Loss: 1.4670\n",
      "Epoch [5/100], Loss: 1.2130\n",
      "Epoch [6/100], Loss: 1.0443\n",
      "Epoch [7/100], Loss: 0.9311\n",
      "Epoch [8/100], Loss: 0.8276\n",
      "Epoch [9/100], Loss: 0.7456\n",
      "Epoch [10/100], Loss: 0.6771\n",
      "Epoch [11/100], Loss: 0.6186\n",
      "Epoch [12/100], Loss: 0.5697\n",
      "Epoch [13/100], Loss: 0.5280\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4596\n",
      "Epoch [16/100], Loss: 0.4306\n",
      "Epoch [17/100], Loss: 0.4104\n",
      "Epoch [18/100], Loss: 0.3859\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3340\n",
      "Epoch [22/100], Loss: 0.3184\n",
      "Epoch [23/100], Loss: 0.3045\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1502\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1444\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1370\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1334\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0999\n",
      "Epoch [72/100], Loss: 0.0987\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0926\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0860\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [93/100], Loss: 0.0762\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1796\n",
      "Epoch [2/100], Loss: 2.5322\n",
      "Epoch [3/100], Loss: 1.8833\n",
      "Epoch [4/100], Loss: 1.4670\n",
      "Epoch [5/100], Loss: 1.2069\n",
      "Epoch [6/100], Loss: 1.0481\n",
      "Epoch [7/100], Loss: 0.9320\n",
      "Epoch [8/100], Loss: 0.8365\n",
      "Epoch [9/100], Loss: 0.7505\n",
      "Epoch [10/100], Loss: 0.6772\n",
      "Epoch [11/100], Loss: 0.6224\n",
      "Epoch [12/100], Loss: 0.5712\n",
      "Epoch [13/100], Loss: 0.5277\n",
      "Epoch [14/100], Loss: 0.4919\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4305\n",
      "Epoch [17/100], Loss: 0.4067\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3021\n",
      "Epoch [24/100], Loss: 0.2904\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2680\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2409\n",
      "Epoch [30/100], Loss: 0.2326\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1951\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1073\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0970\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0856\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Stopping early at epoch 93 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [93/100], Loss: 0.0764\n",
      "Test Accuracy Base Logit: 51.81%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1889\n",
      "Epoch [2/100], Loss: 2.5270\n",
      "Epoch [3/100], Loss: 1.8752\n",
      "Epoch [4/100], Loss: 1.4619\n",
      "Epoch [5/100], Loss: 1.2103\n",
      "Epoch [6/100], Loss: 1.0505\n",
      "Epoch [7/100], Loss: 0.9313\n",
      "Epoch [8/100], Loss: 0.8319\n",
      "Epoch [9/100], Loss: 0.7477\n",
      "Epoch [10/100], Loss: 0.6759\n",
      "Epoch [11/100], Loss: 0.6176\n",
      "Epoch [12/100], Loss: 0.5701\n",
      "Epoch [13/100], Loss: 0.5284\n",
      "Epoch [14/100], Loss: 0.4912\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3862\n",
      "Epoch [19/100], Loss: 0.3647\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3163\n",
      "Epoch [23/100], Loss: 0.3027\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2603\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1803\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1380\n",
      "Epoch [52/100], Loss: 0.1346\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1305\n",
      "Epoch [55/100], Loss: 0.1281\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1144\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [86/100], Loss: 0.0823\n",
      "Test Accuracy Base Logit: 51.89%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1838\n",
      "Epoch [2/100], Loss: 2.5391\n",
      "Epoch [3/100], Loss: 1.8832\n",
      "Epoch [4/100], Loss: 1.4709\n",
      "Epoch [5/100], Loss: 1.2123\n",
      "Epoch [6/100], Loss: 1.0510\n",
      "Epoch [7/100], Loss: 0.9269\n",
      "Epoch [8/100], Loss: 0.8311\n",
      "Epoch [9/100], Loss: 0.7496\n",
      "Epoch [10/100], Loss: 0.6763\n",
      "Epoch [11/100], Loss: 0.6199\n",
      "Epoch [12/100], Loss: 0.5698\n",
      "Epoch [13/100], Loss: 0.5290\n",
      "Epoch [14/100], Loss: 0.4921\n",
      "Epoch [15/100], Loss: 0.4615\n",
      "Epoch [16/100], Loss: 0.4314\n",
      "Epoch [17/100], Loss: 0.4083\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3639\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3158\n",
      "Epoch [23/100], Loss: 0.3040\n",
      "Epoch [24/100], Loss: 0.2903\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2419\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1411\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1330\n",
      "Epoch [54/100], Loss: 0.1305\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1240\n",
      "Epoch [58/100], Loss: 0.1208\n",
      "Epoch [59/100], Loss: 0.1197\n",
      "Epoch [60/100], Loss: 0.1175\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.1005\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0820\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [91/100], Loss: 0.0782\n",
      "Test Accuracy Base Logit: 51.83%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1865\n",
      "Epoch [2/100], Loss: 2.5271\n",
      "Epoch [3/100], Loss: 1.8763\n",
      "Epoch [4/100], Loss: 1.4611\n",
      "Epoch [5/100], Loss: 1.2094\n",
      "Epoch [6/100], Loss: 1.0529\n",
      "Epoch [7/100], Loss: 0.9315\n",
      "Epoch [8/100], Loss: 0.8335\n",
      "Epoch [9/100], Loss: 0.7478\n",
      "Epoch [10/100], Loss: 0.6800\n",
      "Epoch [11/100], Loss: 0.6194\n",
      "Epoch [12/100], Loss: 0.5713\n",
      "Epoch [13/100], Loss: 0.5287\n",
      "Epoch [14/100], Loss: 0.4939\n",
      "Epoch [15/100], Loss: 0.4608\n",
      "Epoch [16/100], Loss: 0.4338\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3490\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2805\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2582\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2419\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1475\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1330\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1157\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1121\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1033\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [90/100], Loss: 0.0789\n",
      "Test Accuracy Base Logit: 51.97%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1876\n",
      "Epoch [2/100], Loss: 2.5249\n",
      "Epoch [3/100], Loss: 1.8803\n",
      "Epoch [4/100], Loss: 1.4602\n",
      "Epoch [5/100], Loss: 1.2143\n",
      "Epoch [6/100], Loss: 1.0465\n",
      "Epoch [7/100], Loss: 0.9254\n",
      "Epoch [8/100], Loss: 0.8327\n",
      "Epoch [9/100], Loss: 0.7447\n",
      "Epoch [10/100], Loss: 0.6784\n",
      "Epoch [11/100], Loss: 0.6195\n",
      "Epoch [12/100], Loss: 0.5682\n",
      "Epoch [13/100], Loss: 0.5274\n",
      "Epoch [14/100], Loss: 0.4930\n",
      "Epoch [15/100], Loss: 0.4606\n",
      "Epoch [16/100], Loss: 0.4332\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3037\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2130\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1958\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1762\n",
      "Epoch [41/100], Loss: 0.1718\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1637\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0898\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0866\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Stopping early at epoch 86 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [86/100], Loss: 0.0823\n",
      "Test Accuracy Base Logit: 51.87%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1865\n",
      "Epoch [2/100], Loss: 2.5371\n",
      "Epoch [3/100], Loss: 1.8833\n",
      "Epoch [4/100], Loss: 1.4567\n",
      "Epoch [5/100], Loss: 1.2159\n",
      "Epoch [6/100], Loss: 1.0488\n",
      "Epoch [7/100], Loss: 0.9283\n",
      "Epoch [8/100], Loss: 0.8338\n",
      "Epoch [9/100], Loss: 0.7477\n",
      "Epoch [10/100], Loss: 0.6789\n",
      "Epoch [11/100], Loss: 0.6234\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5281\n",
      "Epoch [14/100], Loss: 0.4910\n",
      "Epoch [15/100], Loss: 0.4588\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3643\n",
      "Epoch [20/100], Loss: 0.3484\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3037\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2598\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2197\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.1997\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1563\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Stopping early at epoch 92 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [92/100], Loss: 0.0772\n",
      "Test Accuracy Base Logit: 51.94%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1980\n",
      "Epoch [2/100], Loss: 2.5360\n",
      "Epoch [3/100], Loss: 1.8894\n",
      "Epoch [4/100], Loss: 1.4660\n",
      "Epoch [5/100], Loss: 1.2076\n",
      "Epoch [6/100], Loss: 1.0406\n",
      "Epoch [7/100], Loss: 0.9270\n",
      "Epoch [8/100], Loss: 0.8310\n",
      "Epoch [9/100], Loss: 0.7516\n",
      "Epoch [10/100], Loss: 0.6776\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5256\n",
      "Epoch [14/100], Loss: 0.4905\n",
      "Epoch [15/100], Loss: 0.4584\n",
      "Epoch [16/100], Loss: 0.4329\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3662\n",
      "Epoch [20/100], Loss: 0.3484\n",
      "Epoch [21/100], Loss: 0.3321\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3046\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2684\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2117\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2010\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1588\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1411\n",
      "Epoch [51/100], Loss: 0.1380\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1329\n",
      "Epoch [54/100], Loss: 0.1304\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1238\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0921\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Stopping early at epoch 85 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [85/100], Loss: 0.0834\n",
      "Test Accuracy Base Logit: 51.95%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1877\n",
      "Epoch [2/100], Loss: 2.5302\n",
      "Epoch [3/100], Loss: 1.8872\n",
      "Epoch [4/100], Loss: 1.4671\n",
      "Epoch [5/100], Loss: 1.2155\n",
      "Epoch [6/100], Loss: 1.0435\n",
      "Epoch [7/100], Loss: 0.9277\n",
      "Epoch [8/100], Loss: 0.8347\n",
      "Epoch [9/100], Loss: 0.7475\n",
      "Epoch [10/100], Loss: 0.6749\n",
      "Epoch [11/100], Loss: 0.6181\n",
      "Epoch [12/100], Loss: 0.5677\n",
      "Epoch [13/100], Loss: 0.5266\n",
      "Epoch [14/100], Loss: 0.4904\n",
      "Epoch [15/100], Loss: 0.4595\n",
      "Epoch [16/100], Loss: 0.4340\n",
      "Epoch [17/100], Loss: 0.4081\n",
      "Epoch [18/100], Loss: 0.3860\n",
      "Epoch [19/100], Loss: 0.3666\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3321\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2903\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2492\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1850\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1088\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0913\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Stopping early at epoch 88 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [88/100], Loss: 0.0807\n",
      "Test Accuracy Base Logit: 51.90%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1899\n",
      "Epoch [2/100], Loss: 2.5369\n",
      "Epoch [3/100], Loss: 1.8704\n",
      "Epoch [4/100], Loss: 1.4612\n",
      "Epoch [5/100], Loss: 1.2222\n",
      "Epoch [6/100], Loss: 1.0414\n",
      "Epoch [7/100], Loss: 0.9253\n",
      "Epoch [8/100], Loss: 0.8348\n",
      "Epoch [9/100], Loss: 0.7487\n",
      "Epoch [10/100], Loss: 0.6748\n",
      "Epoch [11/100], Loss: 0.6217\n",
      "Epoch [12/100], Loss: 0.5726\n",
      "Epoch [13/100], Loss: 0.5302\n",
      "Epoch [14/100], Loss: 0.4913\n",
      "Epoch [15/100], Loss: 0.4638\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3662\n",
      "Epoch [20/100], Loss: 0.3475\n",
      "Epoch [21/100], Loss: 0.3333\n",
      "Epoch [22/100], Loss: 0.3179\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2708\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2405\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2056\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1900\n",
      "Epoch [38/100], Loss: 0.1849\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0933\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0878\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0825\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Stopping early at epoch 91 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [91/100], Loss: 0.0782\n",
      "Test Accuracy Base Logit: 51.88%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1856\n",
      "Epoch [2/100], Loss: 2.5328\n",
      "Epoch [3/100], Loss: 1.8812\n",
      "Epoch [4/100], Loss: 1.4628\n",
      "Epoch [5/100], Loss: 1.2105\n",
      "Epoch [6/100], Loss: 1.0459\n",
      "Epoch [7/100], Loss: 0.9337\n",
      "Epoch [8/100], Loss: 0.8310\n",
      "Epoch [9/100], Loss: 0.7464\n",
      "Epoch [10/100], Loss: 0.6777\n",
      "Epoch [11/100], Loss: 0.6221\n",
      "Epoch [12/100], Loss: 0.5691\n",
      "Epoch [13/100], Loss: 0.5298\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4595\n",
      "Epoch [16/100], Loss: 0.4315\n",
      "Epoch [17/100], Loss: 0.4086\n",
      "Epoch [18/100], Loss: 0.3862\n",
      "Epoch [19/100], Loss: 0.3666\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3321\n",
      "Epoch [22/100], Loss: 0.3163\n",
      "Epoch [23/100], Loss: 0.3037\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2595\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2421\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2268\n",
      "Epoch [32/100], Loss: 0.2201\n",
      "Epoch [33/100], Loss: 0.2143\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1954\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1854\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1717\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1467\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0971\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0878\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Stopping early at epoch 90 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [90/100], Loss: 0.0789\n",
      "Test Accuracy Base Logit: 51.91%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1914\n",
      "Epoch [2/100], Loss: 2.5448\n",
      "Epoch [3/100], Loss: 1.8726\n",
      "Epoch [4/100], Loss: 1.4616\n",
      "Epoch [5/100], Loss: 1.2124\n",
      "Epoch [6/100], Loss: 1.0483\n",
      "Epoch [7/100], Loss: 0.9305\n",
      "Epoch [8/100], Loss: 0.8354\n",
      "Epoch [9/100], Loss: 0.7465\n",
      "Epoch [10/100], Loss: 0.6814\n",
      "Epoch [11/100], Loss: 0.6185\n",
      "Epoch [12/100], Loss: 0.5684\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4935\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4334\n",
      "Epoch [17/100], Loss: 0.4080\n",
      "Epoch [18/100], Loss: 0.3860\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3487\n",
      "Epoch [21/100], Loss: 0.3312\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3038\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2684\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2184\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1487\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1026\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0957\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0923\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0838\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Stopping early at epoch 89 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [89/100], Loss: 0.0796\n",
      "Test Accuracy Base Logit: 51.90%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1822\n",
      "Epoch [2/100], Loss: 2.5282\n",
      "Epoch [3/100], Loss: 1.8723\n",
      "Epoch [4/100], Loss: 1.4676\n",
      "Epoch [5/100], Loss: 1.2164\n",
      "Epoch [6/100], Loss: 1.0469\n",
      "Epoch [7/100], Loss: 0.9283\n",
      "Epoch [8/100], Loss: 0.8313\n",
      "Epoch [9/100], Loss: 0.7472\n",
      "Epoch [10/100], Loss: 0.6790\n",
      "Epoch [11/100], Loss: 0.6234\n",
      "Epoch [12/100], Loss: 0.5723\n",
      "Epoch [13/100], Loss: 0.5295\n",
      "Epoch [14/100], Loss: 0.4927\n",
      "Epoch [15/100], Loss: 0.4595\n",
      "Epoch [16/100], Loss: 0.4331\n",
      "Epoch [17/100], Loss: 0.4067\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3658\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3309\n",
      "Epoch [22/100], Loss: 0.3164\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2612\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2403\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2022\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1307\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0830\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Stopping early at epoch 95 (Loss improvement < 0.001 for 3 epochs)\n",
      "Subset 1000, Epoch [95/100], Loss: 0.0749\n",
      "Test Accuracy Base Logit: 51.87%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "logit_accuracy = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "\n",
    "    for _ in range(30):\n",
    "        logit_model = LogisticRegression(input_dim, num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(logit_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "        print(f\"Training with subset size: {subset_size}\")\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = Subset(train_set, subset_indices)\n",
    "        train_loader = DataLoader(subset, batch_size=128, shuffle=True)\n",
    "\n",
    "        previous_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            logit_model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = logit_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "            # Check for early stopping\n",
    "            if previous_loss - current_loss < tolerance:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                    break\n",
    "            else:\n",
    "                epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "            previous_loss = current_loss\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        logit_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28*28)\n",
    "                outputs = logit_model(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy Base Logit: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000    51.869071\n",
       " 50000    51.875481\n",
       " 10000    51.880128\n",
       " 5000     51.853846\n",
       " 1000     51.879968\n",
       " dtype: float64,\n",
       " 75000    0.058252\n",
       " 50000    0.056129\n",
       " 10000    0.048109\n",
       " 5000     0.058727\n",
       " 1000     0.062283\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_df = pd.DataFrame(logit_accuracy)\n",
    "logit_df.mean(), logit_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 75000\n",
      "Epoch [1/100], Loss: 3.1913\n",
      "Epoch [2/100], Loss: 2.5346\n",
      "Epoch [3/100], Loss: 1.8812\n",
      "Epoch [4/100], Loss: 1.4624\n",
      "Epoch [5/100], Loss: 1.2147\n",
      "Epoch [6/100], Loss: 1.0494\n",
      "Epoch [7/100], Loss: 0.9298\n",
      "Epoch [8/100], Loss: 0.8361\n",
      "Epoch [9/100], Loss: 0.7509\n",
      "Epoch [10/100], Loss: 0.6824\n",
      "Epoch [11/100], Loss: 0.6215\n",
      "Epoch [12/100], Loss: 0.5703\n",
      "Epoch [13/100], Loss: 0.5277\n",
      "Epoch [14/100], Loss: 0.4917\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4311\n",
      "Epoch [17/100], Loss: 0.4057\n",
      "Epoch [18/100], Loss: 0.3873\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3469\n",
      "Epoch [21/100], Loss: 0.3312\n",
      "Epoch [22/100], Loss: 0.3175\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2784\n",
      "Epoch [26/100], Loss: 0.2697\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2419\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2200\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2072\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1757\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1304\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1251\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0903\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1796\n",
      "Epoch [2/100], Loss: 2.5232\n",
      "Epoch [3/100], Loss: 1.8832\n",
      "Epoch [4/100], Loss: 1.4582\n",
      "Epoch [5/100], Loss: 1.2078\n",
      "Epoch [6/100], Loss: 1.0466\n",
      "Epoch [7/100], Loss: 0.9284\n",
      "Epoch [8/100], Loss: 0.8330\n",
      "Epoch [9/100], Loss: 0.7467\n",
      "Epoch [10/100], Loss: 0.6788\n",
      "Epoch [11/100], Loss: 0.6206\n",
      "Epoch [12/100], Loss: 0.5720\n",
      "Epoch [13/100], Loss: 0.5288\n",
      "Epoch [14/100], Loss: 0.4927\n",
      "Epoch [15/100], Loss: 0.4596\n",
      "Epoch [16/100], Loss: 0.4332\n",
      "Epoch [17/100], Loss: 0.4088\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3647\n",
      "Epoch [20/100], Loss: 0.3487\n",
      "Epoch [21/100], Loss: 0.3330\n",
      "Epoch [22/100], Loss: 0.3156\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2344\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2012\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1851\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0970\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0945\n",
      "Epoch [76/100], Loss: 0.0935\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0909\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0867\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0846\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0753\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1904\n",
      "Epoch [2/100], Loss: 2.5362\n",
      "Epoch [3/100], Loss: 1.8800\n",
      "Epoch [4/100], Loss: 1.4592\n",
      "Epoch [5/100], Loss: 1.2175\n",
      "Epoch [6/100], Loss: 1.0435\n",
      "Epoch [7/100], Loss: 0.9254\n",
      "Epoch [8/100], Loss: 0.8297\n",
      "Epoch [9/100], Loss: 0.7504\n",
      "Epoch [10/100], Loss: 0.6790\n",
      "Epoch [11/100], Loss: 0.6188\n",
      "Epoch [12/100], Loss: 0.5723\n",
      "Epoch [13/100], Loss: 0.5299\n",
      "Epoch [14/100], Loss: 0.4910\n",
      "Epoch [15/100], Loss: 0.4609\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4093\n",
      "Epoch [18/100], Loss: 0.3872\n",
      "Epoch [19/100], Loss: 0.3665\n",
      "Epoch [20/100], Loss: 0.3484\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2901\n",
      "Epoch [25/100], Loss: 0.2789\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2602\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2326\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2184\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1888\n",
      "Epoch [38/100], Loss: 0.1851\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1410\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1331\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0933\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0709\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0709\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1909\n",
      "Epoch [2/100], Loss: 2.5405\n",
      "Epoch [3/100], Loss: 1.8780\n",
      "Epoch [4/100], Loss: 1.4619\n",
      "Epoch [5/100], Loss: 1.2120\n",
      "Epoch [6/100], Loss: 1.0418\n",
      "Epoch [7/100], Loss: 0.9253\n",
      "Epoch [8/100], Loss: 0.8357\n",
      "Epoch [9/100], Loss: 0.7501\n",
      "Epoch [10/100], Loss: 0.6812\n",
      "Epoch [11/100], Loss: 0.6224\n",
      "Epoch [12/100], Loss: 0.5704\n",
      "Epoch [13/100], Loss: 0.5266\n",
      "Epoch [14/100], Loss: 0.4928\n",
      "Epoch [15/100], Loss: 0.4626\n",
      "Epoch [16/100], Loss: 0.4338\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3666\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2423\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2118\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2015\n",
      "Epoch [36/100], Loss: 0.1954\n",
      "Epoch [37/100], Loss: 0.1905\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1801\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1704\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1634\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1466\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1251\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1207\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1113\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0809\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.83%\n",
      "Epoch [1/100], Loss: 3.1903\n",
      "Epoch [2/100], Loss: 2.5439\n",
      "Epoch [3/100], Loss: 1.8832\n",
      "Epoch [4/100], Loss: 1.4614\n",
      "Epoch [5/100], Loss: 1.2123\n",
      "Epoch [6/100], Loss: 1.0479\n",
      "Epoch [7/100], Loss: 0.9282\n",
      "Epoch [8/100], Loss: 0.8324\n",
      "Epoch [9/100], Loss: 0.7519\n",
      "Epoch [10/100], Loss: 0.6776\n",
      "Epoch [11/100], Loss: 0.6239\n",
      "Epoch [12/100], Loss: 0.5713\n",
      "Epoch [13/100], Loss: 0.5265\n",
      "Epoch [14/100], Loss: 0.4917\n",
      "Epoch [15/100], Loss: 0.4617\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4067\n",
      "Epoch [18/100], Loss: 0.3859\n",
      "Epoch [19/100], Loss: 0.3671\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3324\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3049\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2817\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2608\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.1996\n",
      "Epoch [36/100], Loss: 0.1955\n",
      "Epoch [37/100], Loss: 0.1901\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1808\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1331\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1096\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0970\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0914\n",
      "Epoch [78/100], Loss: 0.0917\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0857\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0728\n",
      "Epoch [99/100], Loss: 0.0716\n",
      "Epoch [100/100], Loss: 0.0715\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0715\n",
      "Test Accuracy Logit Lipschitz: 51.87%\n",
      "Epoch [1/100], Loss: 3.1818\n",
      "Epoch [2/100], Loss: 2.5312\n",
      "Epoch [3/100], Loss: 1.8805\n",
      "Epoch [4/100], Loss: 1.4564\n",
      "Epoch [5/100], Loss: 1.2102\n",
      "Epoch [6/100], Loss: 1.0411\n",
      "Epoch [7/100], Loss: 0.9293\n",
      "Epoch [8/100], Loss: 0.8319\n",
      "Epoch [9/100], Loss: 0.7450\n",
      "Epoch [10/100], Loss: 0.6791\n",
      "Epoch [11/100], Loss: 0.6165\n",
      "Epoch [12/100], Loss: 0.5688\n",
      "Epoch [13/100], Loss: 0.5281\n",
      "Epoch [14/100], Loss: 0.4916\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4071\n",
      "Epoch [18/100], Loss: 0.3886\n",
      "Epoch [19/100], Loss: 0.3649\n",
      "Epoch [20/100], Loss: 0.3469\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2595\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2079\n",
      "Epoch [35/100], Loss: 0.2006\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1330\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1149\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0812\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0735\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1932\n",
      "Epoch [2/100], Loss: 2.5279\n",
      "Epoch [3/100], Loss: 1.8767\n",
      "Epoch [4/100], Loss: 1.4597\n",
      "Epoch [5/100], Loss: 1.2157\n",
      "Epoch [6/100], Loss: 1.0448\n",
      "Epoch [7/100], Loss: 0.9301\n",
      "Epoch [8/100], Loss: 0.8340\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6802\n",
      "Epoch [11/100], Loss: 0.6200\n",
      "Epoch [12/100], Loss: 0.5702\n",
      "Epoch [13/100], Loss: 0.5263\n",
      "Epoch [14/100], Loss: 0.4923\n",
      "Epoch [15/100], Loss: 0.4612\n",
      "Epoch [16/100], Loss: 0.4316\n",
      "Epoch [17/100], Loss: 0.4071\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3302\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2426\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2056\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1381\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1251\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0891\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0857\n",
      "Epoch [84/100], Loss: 0.0847\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0820\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0725\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1878\n",
      "Epoch [2/100], Loss: 2.5396\n",
      "Epoch [3/100], Loss: 1.8847\n",
      "Epoch [4/100], Loss: 1.4676\n",
      "Epoch [5/100], Loss: 1.2185\n",
      "Epoch [6/100], Loss: 1.0499\n",
      "Epoch [7/100], Loss: 0.9290\n",
      "Epoch [8/100], Loss: 0.8333\n",
      "Epoch [9/100], Loss: 0.7481\n",
      "Epoch [10/100], Loss: 0.6785\n",
      "Epoch [11/100], Loss: 0.6204\n",
      "Epoch [12/100], Loss: 0.5709\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4606\n",
      "Epoch [16/100], Loss: 0.4332\n",
      "Epoch [17/100], Loss: 0.4090\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3158\n",
      "Epoch [23/100], Loss: 0.3045\n",
      "Epoch [24/100], Loss: 0.2901\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2182\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1904\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1759\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1498\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1820\n",
      "Epoch [2/100], Loss: 2.5315\n",
      "Epoch [3/100], Loss: 1.8793\n",
      "Epoch [4/100], Loss: 1.4542\n",
      "Epoch [5/100], Loss: 1.2135\n",
      "Epoch [6/100], Loss: 1.0462\n",
      "Epoch [7/100], Loss: 0.9268\n",
      "Epoch [8/100], Loss: 0.8319\n",
      "Epoch [9/100], Loss: 0.7476\n",
      "Epoch [10/100], Loss: 0.6777\n",
      "Epoch [11/100], Loss: 0.6174\n",
      "Epoch [12/100], Loss: 0.5706\n",
      "Epoch [13/100], Loss: 0.5271\n",
      "Epoch [14/100], Loss: 0.4942\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4318\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3870\n",
      "Epoch [19/100], Loss: 0.3677\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3324\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3038\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2595\n",
      "Epoch [28/100], Loss: 0.2508\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2343\n",
      "Epoch [31/100], Loss: 0.2253\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1888\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1761\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1589\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1534\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1296\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1261\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1834\n",
      "Epoch [2/100], Loss: 2.5313\n",
      "Epoch [3/100], Loss: 1.8816\n",
      "Epoch [4/100], Loss: 1.4665\n",
      "Epoch [5/100], Loss: 1.2111\n",
      "Epoch [6/100], Loss: 1.0469\n",
      "Epoch [7/100], Loss: 0.9262\n",
      "Epoch [8/100], Loss: 0.8351\n",
      "Epoch [9/100], Loss: 0.7494\n",
      "Epoch [10/100], Loss: 0.6763\n",
      "Epoch [11/100], Loss: 0.6191\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5294\n",
      "Epoch [14/100], Loss: 0.4922\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4305\n",
      "Epoch [17/100], Loss: 0.4087\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3660\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3312\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3049\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2581\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2324\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2055\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1757\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1273\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1181\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1139\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0850\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.74%\n",
      "Epoch [1/100], Loss: 3.1847\n",
      "Epoch [2/100], Loss: 2.5400\n",
      "Epoch [3/100], Loss: 1.8753\n",
      "Epoch [4/100], Loss: 1.4700\n",
      "Epoch [5/100], Loss: 1.2101\n",
      "Epoch [6/100], Loss: 1.0472\n",
      "Epoch [7/100], Loss: 0.9319\n",
      "Epoch [8/100], Loss: 0.8331\n",
      "Epoch [9/100], Loss: 0.7475\n",
      "Epoch [10/100], Loss: 0.6753\n",
      "Epoch [11/100], Loss: 0.6192\n",
      "Epoch [12/100], Loss: 0.5701\n",
      "Epoch [13/100], Loss: 0.5276\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4589\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4083\n",
      "Epoch [18/100], Loss: 0.3849\n",
      "Epoch [19/100], Loss: 0.3649\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3306\n",
      "Epoch [22/100], Loss: 0.3164\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2802\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2584\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2129\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1759\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1062\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1018\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0856\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0761\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0752\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0721\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.74%\n",
      "Epoch [1/100], Loss: 3.1838\n",
      "Epoch [2/100], Loss: 2.5311\n",
      "Epoch [3/100], Loss: 1.8801\n",
      "Epoch [4/100], Loss: 1.4646\n",
      "Epoch [5/100], Loss: 1.2122\n",
      "Epoch [6/100], Loss: 1.0439\n",
      "Epoch [7/100], Loss: 0.9244\n",
      "Epoch [8/100], Loss: 0.8321\n",
      "Epoch [9/100], Loss: 0.7502\n",
      "Epoch [10/100], Loss: 0.6815\n",
      "Epoch [11/100], Loss: 0.6199\n",
      "Epoch [12/100], Loss: 0.5728\n",
      "Epoch [13/100], Loss: 0.5279\n",
      "Epoch [14/100], Loss: 0.4927\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4341\n",
      "Epoch [17/100], Loss: 0.4088\n",
      "Epoch [18/100], Loss: 0.3869\n",
      "Epoch [19/100], Loss: 0.3646\n",
      "Epoch [20/100], Loss: 0.3492\n",
      "Epoch [21/100], Loss: 0.3328\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1914\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1804\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1565\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1498\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1259\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1076\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0856\n",
      "Epoch [84/100], Loss: 0.0846\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0812\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0709\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0709\n",
      "Test Accuracy Logit Lipschitz: 51.73%\n",
      "Epoch [1/100], Loss: 3.1848\n",
      "Epoch [2/100], Loss: 2.5350\n",
      "Epoch [3/100], Loss: 1.8889\n",
      "Epoch [4/100], Loss: 1.4569\n",
      "Epoch [5/100], Loss: 1.2107\n",
      "Epoch [6/100], Loss: 1.0498\n",
      "Epoch [7/100], Loss: 0.9294\n",
      "Epoch [8/100], Loss: 0.8345\n",
      "Epoch [9/100], Loss: 0.7474\n",
      "Epoch [10/100], Loss: 0.6787\n",
      "Epoch [11/100], Loss: 0.6197\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5294\n",
      "Epoch [14/100], Loss: 0.4916\n",
      "Epoch [15/100], Loss: 0.4594\n",
      "Epoch [16/100], Loss: 0.4341\n",
      "Epoch [17/100], Loss: 0.4062\n",
      "Epoch [18/100], Loss: 0.3844\n",
      "Epoch [19/100], Loss: 0.3668\n",
      "Epoch [20/100], Loss: 0.3466\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2408\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2267\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1411\n",
      "Epoch [51/100], Loss: 0.1380\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1284\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1063\n",
      "Epoch [67/100], Loss: 0.1059\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1018\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0991\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0783\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0762\n",
      "Epoch [95/100], Loss: 0.0752\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.77%\n",
      "Epoch [1/100], Loss: 3.1918\n",
      "Epoch [2/100], Loss: 2.5432\n",
      "Epoch [3/100], Loss: 1.9006\n",
      "Epoch [4/100], Loss: 1.4681\n",
      "Epoch [5/100], Loss: 1.2118\n",
      "Epoch [6/100], Loss: 1.0477\n",
      "Epoch [7/100], Loss: 0.9273\n",
      "Epoch [8/100], Loss: 0.8367\n",
      "Epoch [9/100], Loss: 0.7508\n",
      "Epoch [10/100], Loss: 0.6792\n",
      "Epoch [11/100], Loss: 0.6184\n",
      "Epoch [12/100], Loss: 0.5725\n",
      "Epoch [13/100], Loss: 0.5284\n",
      "Epoch [14/100], Loss: 0.4910\n",
      "Epoch [15/100], Loss: 0.4591\n",
      "Epoch [16/100], Loss: 0.4312\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3860\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3471\n",
      "Epoch [21/100], Loss: 0.3310\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2340\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2130\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2026\n",
      "Epoch [36/100], Loss: 0.1959\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1716\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1355\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1840\n",
      "Epoch [2/100], Loss: 2.5326\n",
      "Epoch [3/100], Loss: 1.8706\n",
      "Epoch [4/100], Loss: 1.4630\n",
      "Epoch [5/100], Loss: 1.2139\n",
      "Epoch [6/100], Loss: 1.0481\n",
      "Epoch [7/100], Loss: 0.9282\n",
      "Epoch [8/100], Loss: 0.8426\n",
      "Epoch [9/100], Loss: 0.7451\n",
      "Epoch [10/100], Loss: 0.6788\n",
      "Epoch [11/100], Loss: 0.6218\n",
      "Epoch [12/100], Loss: 0.5682\n",
      "Epoch [13/100], Loss: 0.5285\n",
      "Epoch [14/100], Loss: 0.4937\n",
      "Epoch [15/100], Loss: 0.4595\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4092\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3649\n",
      "Epoch [20/100], Loss: 0.3471\n",
      "Epoch [21/100], Loss: 0.3327\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3051\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2252\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1027\n",
      "Epoch [70/100], Loss: 0.1013\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0937\n",
      "Epoch [77/100], Loss: 0.0921\n",
      "Epoch [78/100], Loss: 0.0909\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0812\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1823\n",
      "Epoch [2/100], Loss: 2.5321\n",
      "Epoch [3/100], Loss: 1.8932\n",
      "Epoch [4/100], Loss: 1.4617\n",
      "Epoch [5/100], Loss: 1.2178\n",
      "Epoch [6/100], Loss: 1.0450\n",
      "Epoch [7/100], Loss: 0.9318\n",
      "Epoch [8/100], Loss: 0.8383\n",
      "Epoch [9/100], Loss: 0.7499\n",
      "Epoch [10/100], Loss: 0.6807\n",
      "Epoch [11/100], Loss: 0.6191\n",
      "Epoch [12/100], Loss: 0.5705\n",
      "Epoch [13/100], Loss: 0.5254\n",
      "Epoch [14/100], Loss: 0.4928\n",
      "Epoch [15/100], Loss: 0.4594\n",
      "Epoch [16/100], Loss: 0.4310\n",
      "Epoch [17/100], Loss: 0.4078\n",
      "Epoch [18/100], Loss: 0.3844\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2948\n",
      "Epoch [25/100], Loss: 0.2810\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2598\n",
      "Epoch [28/100], Loss: 0.2511\n",
      "Epoch [29/100], Loss: 0.2408\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2183\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2078\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1889\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1804\n",
      "Epoch [40/100], Loss: 0.1745\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1087\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1048\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0923\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0888\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0858\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0812\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0765\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1841\n",
      "Epoch [2/100], Loss: 2.5351\n",
      "Epoch [3/100], Loss: 1.8820\n",
      "Epoch [4/100], Loss: 1.4636\n",
      "Epoch [5/100], Loss: 1.2055\n",
      "Epoch [6/100], Loss: 1.0436\n",
      "Epoch [7/100], Loss: 0.9285\n",
      "Epoch [8/100], Loss: 0.8309\n",
      "Epoch [9/100], Loss: 0.7464\n",
      "Epoch [10/100], Loss: 0.6805\n",
      "Epoch [11/100], Loss: 0.6180\n",
      "Epoch [12/100], Loss: 0.5713\n",
      "Epoch [13/100], Loss: 0.5285\n",
      "Epoch [14/100], Loss: 0.4897\n",
      "Epoch [15/100], Loss: 0.4593\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4064\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3332\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2696\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2009\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1889\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1714\n",
      "Epoch [42/100], Loss: 0.1678\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1531\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1332\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1162\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0988\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0888\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0857\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0729\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1877\n",
      "Epoch [2/100], Loss: 2.5497\n",
      "Epoch [3/100], Loss: 1.8984\n",
      "Epoch [4/100], Loss: 1.4691\n",
      "Epoch [5/100], Loss: 1.2029\n",
      "Epoch [6/100], Loss: 1.0425\n",
      "Epoch [7/100], Loss: 0.9295\n",
      "Epoch [8/100], Loss: 0.8340\n",
      "Epoch [9/100], Loss: 0.7433\n",
      "Epoch [10/100], Loss: 0.6794\n",
      "Epoch [11/100], Loss: 0.6208\n",
      "Epoch [12/100], Loss: 0.5711\n",
      "Epoch [13/100], Loss: 0.5290\n",
      "Epoch [14/100], Loss: 0.4924\n",
      "Epoch [15/100], Loss: 0.4609\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4084\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3490\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2610\n",
      "Epoch [28/100], Loss: 0.2508\n",
      "Epoch [29/100], Loss: 0.2419\n",
      "Epoch [30/100], Loss: 0.2340\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2196\n",
      "Epoch [33/100], Loss: 0.2136\n",
      "Epoch [34/100], Loss: 0.2074\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1939\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1855\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1704\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1498\n",
      "Epoch [48/100], Loss: 0.1455\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1273\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1143\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1096\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1072\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0777\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1884\n",
      "Epoch [2/100], Loss: 2.5440\n",
      "Epoch [3/100], Loss: 1.8847\n",
      "Epoch [4/100], Loss: 1.4619\n",
      "Epoch [5/100], Loss: 1.2116\n",
      "Epoch [6/100], Loss: 1.0505\n",
      "Epoch [7/100], Loss: 0.9283\n",
      "Epoch [8/100], Loss: 0.8335\n",
      "Epoch [9/100], Loss: 0.7479\n",
      "Epoch [10/100], Loss: 0.6790\n",
      "Epoch [11/100], Loss: 0.6232\n",
      "Epoch [12/100], Loss: 0.5712\n",
      "Epoch [13/100], Loss: 0.5258\n",
      "Epoch [14/100], Loss: 0.4924\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4327\n",
      "Epoch [17/100], Loss: 0.4084\n",
      "Epoch [18/100], Loss: 0.3864\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3475\n",
      "Epoch [21/100], Loss: 0.3324\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3066\n",
      "Epoch [24/100], Loss: 0.2933\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2011\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1762\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1674\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1410\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1259\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1157\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0887\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0830\n",
      "Epoch [86/100], Loss: 0.0829\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0743\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0723\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1891\n",
      "Epoch [2/100], Loss: 2.5377\n",
      "Epoch [3/100], Loss: 1.8717\n",
      "Epoch [4/100], Loss: 1.4666\n",
      "Epoch [5/100], Loss: 1.2147\n",
      "Epoch [6/100], Loss: 1.0511\n",
      "Epoch [7/100], Loss: 0.9304\n",
      "Epoch [8/100], Loss: 0.8329\n",
      "Epoch [9/100], Loss: 0.7530\n",
      "Epoch [10/100], Loss: 0.6780\n",
      "Epoch [11/100], Loss: 0.6210\n",
      "Epoch [12/100], Loss: 0.5714\n",
      "Epoch [13/100], Loss: 0.5272\n",
      "Epoch [14/100], Loss: 0.4939\n",
      "Epoch [15/100], Loss: 0.4624\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3859\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3472\n",
      "Epoch [21/100], Loss: 0.3324\n",
      "Epoch [22/100], Loss: 0.3176\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2904\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2585\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2196\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1851\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1259\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1179\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1123\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0792\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0715\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0715\n",
      "Test Accuracy Logit Lipschitz: 51.65%\n",
      "Epoch [1/100], Loss: 3.1885\n",
      "Epoch [2/100], Loss: 2.5296\n",
      "Epoch [3/100], Loss: 1.8743\n",
      "Epoch [4/100], Loss: 1.4603\n",
      "Epoch [5/100], Loss: 1.2118\n",
      "Epoch [6/100], Loss: 1.0455\n",
      "Epoch [7/100], Loss: 0.9228\n",
      "Epoch [8/100], Loss: 0.8302\n",
      "Epoch [9/100], Loss: 0.7480\n",
      "Epoch [10/100], Loss: 0.6745\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5670\n",
      "Epoch [13/100], Loss: 0.5258\n",
      "Epoch [14/100], Loss: 0.4908\n",
      "Epoch [15/100], Loss: 0.4613\n",
      "Epoch [16/100], Loss: 0.4308\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3859\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3487\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3042\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2713\n",
      "Epoch [27/100], Loss: 0.2604\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2070\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1951\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1682\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1589\n",
      "Epoch [45/100], Loss: 0.1565\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1466\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1409\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1333\n",
      "Epoch [54/100], Loss: 0.1304\n",
      "Epoch [55/100], Loss: 0.1281\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.1000\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0961\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1873\n",
      "Epoch [2/100], Loss: 2.5276\n",
      "Epoch [3/100], Loss: 1.8792\n",
      "Epoch [4/100], Loss: 1.4572\n",
      "Epoch [5/100], Loss: 1.2098\n",
      "Epoch [6/100], Loss: 1.0484\n",
      "Epoch [7/100], Loss: 0.9307\n",
      "Epoch [8/100], Loss: 0.8354\n",
      "Epoch [9/100], Loss: 0.7517\n",
      "Epoch [10/100], Loss: 0.6815\n",
      "Epoch [11/100], Loss: 0.6219\n",
      "Epoch [12/100], Loss: 0.5722\n",
      "Epoch [13/100], Loss: 0.5311\n",
      "Epoch [14/100], Loss: 0.4906\n",
      "Epoch [15/100], Loss: 0.4591\n",
      "Epoch [16/100], Loss: 0.4327\n",
      "Epoch [17/100], Loss: 0.4067\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3328\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2902\n",
      "Epoch [25/100], Loss: 0.2805\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2601\n",
      "Epoch [28/100], Loss: 0.2511\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1911\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1758\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1678\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1605\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1408\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1158\n",
      "Epoch [62/100], Loss: 0.1140\n",
      "Epoch [63/100], Loss: 0.1113\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1089\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1026\n",
      "Epoch [70/100], Loss: 0.1015\n",
      "Epoch [71/100], Loss: 0.0999\n",
      "Epoch [72/100], Loss: 0.0985\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0751\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.71%\n",
      "Epoch [1/100], Loss: 3.1929\n",
      "Epoch [2/100], Loss: 2.5335\n",
      "Epoch [3/100], Loss: 1.8840\n",
      "Epoch [4/100], Loss: 1.4701\n",
      "Epoch [5/100], Loss: 1.2156\n",
      "Epoch [6/100], Loss: 1.0476\n",
      "Epoch [7/100], Loss: 0.9241\n",
      "Epoch [8/100], Loss: 0.8324\n",
      "Epoch [9/100], Loss: 0.7489\n",
      "Epoch [10/100], Loss: 0.6788\n",
      "Epoch [11/100], Loss: 0.6188\n",
      "Epoch [12/100], Loss: 0.5715\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4912\n",
      "Epoch [15/100], Loss: 0.4602\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4083\n",
      "Epoch [18/100], Loss: 0.3866\n",
      "Epoch [19/100], Loss: 0.3664\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3027\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2802\n",
      "Epoch [26/100], Loss: 0.2694\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2184\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1718\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1529\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1408\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1107\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0997\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0898\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0837\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0824\n",
      "Epoch [88/100], Loss: 0.0813\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0791\n",
      "Epoch [91/100], Loss: 0.0784\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0766\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0737\n",
      "Epoch [98/100], Loss: 0.0723\n",
      "Epoch [99/100], Loss: 0.0723\n",
      "Epoch [100/100], Loss: 0.0714\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0714\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1843\n",
      "Epoch [2/100], Loss: 2.5277\n",
      "Epoch [3/100], Loss: 1.8877\n",
      "Epoch [4/100], Loss: 1.4561\n",
      "Epoch [5/100], Loss: 1.2152\n",
      "Epoch [6/100], Loss: 1.0449\n",
      "Epoch [7/100], Loss: 0.9280\n",
      "Epoch [8/100], Loss: 0.8370\n",
      "Epoch [9/100], Loss: 0.7499\n",
      "Epoch [10/100], Loss: 0.6798\n",
      "Epoch [11/100], Loss: 0.6208\n",
      "Epoch [12/100], Loss: 0.5678\n",
      "Epoch [13/100], Loss: 0.5277\n",
      "Epoch [14/100], Loss: 0.4905\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4084\n",
      "Epoch [18/100], Loss: 0.3858\n",
      "Epoch [19/100], Loss: 0.3664\n",
      "Epoch [20/100], Loss: 0.3470\n",
      "Epoch [21/100], Loss: 0.3331\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2599\n",
      "Epoch [28/100], Loss: 0.2516\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2269\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1854\n",
      "Epoch [39/100], Loss: 0.1813\n",
      "Epoch [40/100], Loss: 0.1758\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1261\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1016\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0914\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0856\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Epoch [92/100], Loss: 0.0782\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0760\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.69%\n",
      "Epoch [1/100], Loss: 3.1879\n",
      "Epoch [2/100], Loss: 2.5465\n",
      "Epoch [3/100], Loss: 1.8724\n",
      "Epoch [4/100], Loss: 1.4628\n",
      "Epoch [5/100], Loss: 1.2056\n",
      "Epoch [6/100], Loss: 1.0479\n",
      "Epoch [7/100], Loss: 0.9277\n",
      "Epoch [8/100], Loss: 0.8311\n",
      "Epoch [9/100], Loss: 0.7499\n",
      "Epoch [10/100], Loss: 0.6830\n",
      "Epoch [11/100], Loss: 0.6206\n",
      "Epoch [12/100], Loss: 0.5712\n",
      "Epoch [13/100], Loss: 0.5278\n",
      "Epoch [14/100], Loss: 0.4923\n",
      "Epoch [15/100], Loss: 0.4590\n",
      "Epoch [16/100], Loss: 0.4340\n",
      "Epoch [17/100], Loss: 0.4087\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3045\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2269\n",
      "Epoch [32/100], Loss: 0.2196\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2010\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1329\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0984\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0888\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0757\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0744\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1814\n",
      "Epoch [2/100], Loss: 2.5229\n",
      "Epoch [3/100], Loss: 1.8787\n",
      "Epoch [4/100], Loss: 1.4604\n",
      "Epoch [5/100], Loss: 1.2179\n",
      "Epoch [6/100], Loss: 1.0509\n",
      "Epoch [7/100], Loss: 0.9294\n",
      "Epoch [8/100], Loss: 0.8306\n",
      "Epoch [9/100], Loss: 0.7536\n",
      "Epoch [10/100], Loss: 0.6808\n",
      "Epoch [11/100], Loss: 0.6190\n",
      "Epoch [12/100], Loss: 0.5689\n",
      "Epoch [13/100], Loss: 0.5290\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4614\n",
      "Epoch [16/100], Loss: 0.4328\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3468\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3043\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2806\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2601\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2421\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1810\n",
      "Epoch [40/100], Loss: 0.1758\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1104\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1072\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1012\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0887\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0858\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.87%\n",
      "Epoch [1/100], Loss: 3.1825\n",
      "Epoch [2/100], Loss: 2.5336\n",
      "Epoch [3/100], Loss: 1.8821\n",
      "Epoch [4/100], Loss: 1.4698\n",
      "Epoch [5/100], Loss: 1.2130\n",
      "Epoch [6/100], Loss: 1.0481\n",
      "Epoch [7/100], Loss: 0.9288\n",
      "Epoch [8/100], Loss: 0.8299\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6789\n",
      "Epoch [11/100], Loss: 0.6193\n",
      "Epoch [12/100], Loss: 0.5718\n",
      "Epoch [13/100], Loss: 0.5309\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4606\n",
      "Epoch [16/100], Loss: 0.4316\n",
      "Epoch [17/100], Loss: 0.4089\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3649\n",
      "Epoch [20/100], Loss: 0.3470\n",
      "Epoch [21/100], Loss: 0.3321\n",
      "Epoch [22/100], Loss: 0.3179\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2684\n",
      "Epoch [27/100], Loss: 0.2601\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2404\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1849\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1725\n",
      "Epoch [42/100], Loss: 0.1684\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1606\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1229\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1168\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0791\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0753\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1898\n",
      "Epoch [2/100], Loss: 2.5337\n",
      "Epoch [3/100], Loss: 1.8886\n",
      "Epoch [4/100], Loss: 1.4641\n",
      "Epoch [5/100], Loss: 1.2100\n",
      "Epoch [6/100], Loss: 1.0504\n",
      "Epoch [7/100], Loss: 0.9264\n",
      "Epoch [8/100], Loss: 0.8316\n",
      "Epoch [9/100], Loss: 0.7478\n",
      "Epoch [10/100], Loss: 0.6756\n",
      "Epoch [11/100], Loss: 0.6197\n",
      "Epoch [12/100], Loss: 0.5739\n",
      "Epoch [13/100], Loss: 0.5325\n",
      "Epoch [14/100], Loss: 0.4916\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4327\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3475\n",
      "Epoch [21/100], Loss: 0.3324\n",
      "Epoch [22/100], Loss: 0.3163\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2803\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2073\n",
      "Epoch [35/100], Loss: 0.2025\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1852\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1064\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1004\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0977\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1915\n",
      "Epoch [2/100], Loss: 2.5348\n",
      "Epoch [3/100], Loss: 1.8823\n",
      "Epoch [4/100], Loss: 1.4642\n",
      "Epoch [5/100], Loss: 1.2151\n",
      "Epoch [6/100], Loss: 1.0490\n",
      "Epoch [7/100], Loss: 0.9342\n",
      "Epoch [8/100], Loss: 0.8328\n",
      "Epoch [9/100], Loss: 0.7451\n",
      "Epoch [10/100], Loss: 0.6786\n",
      "Epoch [11/100], Loss: 0.6183\n",
      "Epoch [12/100], Loss: 0.5711\n",
      "Epoch [13/100], Loss: 0.5260\n",
      "Epoch [14/100], Loss: 0.4932\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4304\n",
      "Epoch [17/100], Loss: 0.4089\n",
      "Epoch [18/100], Loss: 0.3860\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3028\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2340\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2135\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1589\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0960\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0784\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1806\n",
      "Epoch [2/100], Loss: 2.5385\n",
      "Epoch [3/100], Loss: 1.8766\n",
      "Epoch [4/100], Loss: 1.4636\n",
      "Epoch [5/100], Loss: 1.2155\n",
      "Epoch [6/100], Loss: 1.0523\n",
      "Epoch [7/100], Loss: 0.9286\n",
      "Epoch [8/100], Loss: 0.8322\n",
      "Epoch [9/100], Loss: 0.7496\n",
      "Epoch [10/100], Loss: 0.6767\n",
      "Epoch [11/100], Loss: 0.6203\n",
      "Epoch [12/100], Loss: 0.5710\n",
      "Epoch [13/100], Loss: 0.5271\n",
      "Epoch [14/100], Loss: 0.4917\n",
      "Epoch [15/100], Loss: 0.4596\n",
      "Epoch [16/100], Loss: 0.4312\n",
      "Epoch [17/100], Loss: 0.4059\n",
      "Epoch [18/100], Loss: 0.3870\n",
      "Epoch [19/100], Loss: 0.3649\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3027\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2697\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2509\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2014\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1091\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0951\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0767\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 75000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/100], Loss: 3.1852\n",
      "Epoch [2/100], Loss: 2.5508\n",
      "Epoch [3/100], Loss: 1.8837\n",
      "Epoch [4/100], Loss: 1.4662\n",
      "Epoch [5/100], Loss: 1.2107\n",
      "Epoch [6/100], Loss: 1.0452\n",
      "Epoch [7/100], Loss: 0.9334\n",
      "Epoch [8/100], Loss: 0.8323\n",
      "Epoch [9/100], Loss: 0.7451\n",
      "Epoch [10/100], Loss: 0.6794\n",
      "Epoch [11/100], Loss: 0.6229\n",
      "Epoch [12/100], Loss: 0.5691\n",
      "Epoch [13/100], Loss: 0.5273\n",
      "Epoch [14/100], Loss: 0.4893\n",
      "Epoch [15/100], Loss: 0.4622\n",
      "Epoch [16/100], Loss: 0.4341\n",
      "Epoch [17/100], Loss: 0.4092\n",
      "Epoch [18/100], Loss: 0.3869\n",
      "Epoch [19/100], Loss: 0.3668\n",
      "Epoch [20/100], Loss: 0.3471\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2492\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2117\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.1997\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1497\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1304\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1087\n",
      "Epoch [66/100], Loss: 0.1072\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1011\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0887\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0721\n",
      "Epoch [100/100], Loss: 0.0709\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0709\n",
      "Test Accuracy Logit Lipschitz: 51.83%\n",
      "Epoch [1/100], Loss: 3.1804\n",
      "Epoch [2/100], Loss: 2.5383\n",
      "Epoch [3/100], Loss: 1.8854\n",
      "Epoch [4/100], Loss: 1.4558\n",
      "Epoch [5/100], Loss: 1.2161\n",
      "Epoch [6/100], Loss: 1.0471\n",
      "Epoch [7/100], Loss: 0.9324\n",
      "Epoch [8/100], Loss: 0.8323\n",
      "Epoch [9/100], Loss: 0.7507\n",
      "Epoch [10/100], Loss: 0.6795\n",
      "Epoch [11/100], Loss: 0.6221\n",
      "Epoch [12/100], Loss: 0.5704\n",
      "Epoch [13/100], Loss: 0.5276\n",
      "Epoch [14/100], Loss: 0.4921\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4310\n",
      "Epoch [17/100], Loss: 0.4054\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3668\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2277\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2019\n",
      "Epoch [36/100], Loss: 0.1951\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1664\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1175\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1015\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0846\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0766\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0728\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1880\n",
      "Epoch [2/100], Loss: 2.5381\n",
      "Epoch [3/100], Loss: 1.8762\n",
      "Epoch [4/100], Loss: 1.4635\n",
      "Epoch [5/100], Loss: 1.2100\n",
      "Epoch [6/100], Loss: 1.0441\n",
      "Epoch [7/100], Loss: 0.9295\n",
      "Epoch [8/100], Loss: 0.8380\n",
      "Epoch [9/100], Loss: 0.7488\n",
      "Epoch [10/100], Loss: 0.6799\n",
      "Epoch [11/100], Loss: 0.6208\n",
      "Epoch [12/100], Loss: 0.5708\n",
      "Epoch [13/100], Loss: 0.5287\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4616\n",
      "Epoch [16/100], Loss: 0.4318\n",
      "Epoch [17/100], Loss: 0.4064\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3672\n",
      "Epoch [20/100], Loss: 0.3466\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2603\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2422\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2073\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1904\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1309\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1267\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1178\n",
      "Epoch [61/100], Loss: 0.1157\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0800\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0709\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0709\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1866\n",
      "Epoch [2/100], Loss: 2.5439\n",
      "Epoch [3/100], Loss: 1.8836\n",
      "Epoch [4/100], Loss: 1.4565\n",
      "Epoch [5/100], Loss: 1.2072\n",
      "Epoch [6/100], Loss: 1.0411\n",
      "Epoch [7/100], Loss: 0.9279\n",
      "Epoch [8/100], Loss: 0.8329\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6781\n",
      "Epoch [11/100], Loss: 0.6223\n",
      "Epoch [12/100], Loss: 0.5714\n",
      "Epoch [13/100], Loss: 0.5264\n",
      "Epoch [14/100], Loss: 0.4931\n",
      "Epoch [15/100], Loss: 0.4583\n",
      "Epoch [16/100], Loss: 0.4315\n",
      "Epoch [17/100], Loss: 0.4066\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3662\n",
      "Epoch [20/100], Loss: 0.3489\n",
      "Epoch [21/100], Loss: 0.3324\n",
      "Epoch [22/100], Loss: 0.3178\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2903\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1634\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1553\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1203\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1089\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1035\n",
      "Epoch [70/100], Loss: 0.1011\n",
      "Epoch [71/100], Loss: 0.1001\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0963\n",
      "Epoch [74/100], Loss: 0.0957\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0926\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0910\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1851\n",
      "Epoch [2/100], Loss: 2.5250\n",
      "Epoch [3/100], Loss: 1.8801\n",
      "Epoch [4/100], Loss: 1.4580\n",
      "Epoch [5/100], Loss: 1.2185\n",
      "Epoch [6/100], Loss: 1.0522\n",
      "Epoch [7/100], Loss: 0.9294\n",
      "Epoch [8/100], Loss: 0.8325\n",
      "Epoch [9/100], Loss: 0.7524\n",
      "Epoch [10/100], Loss: 0.6779\n",
      "Epoch [11/100], Loss: 0.6186\n",
      "Epoch [12/100], Loss: 0.5696\n",
      "Epoch [13/100], Loss: 0.5272\n",
      "Epoch [14/100], Loss: 0.4919\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4303\n",
      "Epoch [17/100], Loss: 0.4077\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3178\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2706\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2547\n",
      "Epoch [29/100], Loss: 0.2429\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2265\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1809\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1719\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1569\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1444\n",
      "Epoch [50/100], Loss: 0.1408\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1221\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1158\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1079\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0802\n",
      "Epoch [90/100], Loss: 0.0791\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0758\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0737\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1816\n",
      "Epoch [2/100], Loss: 2.5258\n",
      "Epoch [3/100], Loss: 1.8809\n",
      "Epoch [4/100], Loss: 1.4532\n",
      "Epoch [5/100], Loss: 1.2114\n",
      "Epoch [6/100], Loss: 1.0463\n",
      "Epoch [7/100], Loss: 0.9326\n",
      "Epoch [8/100], Loss: 0.8358\n",
      "Epoch [9/100], Loss: 0.7469\n",
      "Epoch [10/100], Loss: 0.6821\n",
      "Epoch [11/100], Loss: 0.6235\n",
      "Epoch [12/100], Loss: 0.5691\n",
      "Epoch [13/100], Loss: 0.5263\n",
      "Epoch [14/100], Loss: 0.4914\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4320\n",
      "Epoch [17/100], Loss: 0.4070\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3668\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2903\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2507\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2252\n",
      "Epoch [32/100], Loss: 0.2205\n",
      "Epoch [33/100], Loss: 0.2134\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1358\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1295\n",
      "Epoch [56/100], Loss: 0.1262\n",
      "Epoch [57/100], Loss: 0.1240\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1175\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1854\n",
      "Epoch [2/100], Loss: 2.5290\n",
      "Epoch [3/100], Loss: 1.8817\n",
      "Epoch [4/100], Loss: 1.4580\n",
      "Epoch [5/100], Loss: 1.2168\n",
      "Epoch [6/100], Loss: 1.0463\n",
      "Epoch [7/100], Loss: 0.9302\n",
      "Epoch [8/100], Loss: 0.8317\n",
      "Epoch [9/100], Loss: 0.7471\n",
      "Epoch [10/100], Loss: 0.6803\n",
      "Epoch [11/100], Loss: 0.6210\n",
      "Epoch [12/100], Loss: 0.5696\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4914\n",
      "Epoch [15/100], Loss: 0.4614\n",
      "Epoch [16/100], Loss: 0.4316\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3328\n",
      "Epoch [22/100], Loss: 0.3186\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2510\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2009\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1142\n",
      "Epoch [63/100], Loss: 0.1120\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.81%\n",
      "Epoch [1/100], Loss: 3.1977\n",
      "Epoch [2/100], Loss: 2.5451\n",
      "Epoch [3/100], Loss: 1.8881\n",
      "Epoch [4/100], Loss: 1.4676\n",
      "Epoch [5/100], Loss: 1.2143\n",
      "Epoch [6/100], Loss: 1.0482\n",
      "Epoch [7/100], Loss: 0.9294\n",
      "Epoch [8/100], Loss: 0.8335\n",
      "Epoch [9/100], Loss: 0.7471\n",
      "Epoch [10/100], Loss: 0.6800\n",
      "Epoch [11/100], Loss: 0.6205\n",
      "Epoch [12/100], Loss: 0.5715\n",
      "Epoch [13/100], Loss: 0.5275\n",
      "Epoch [14/100], Loss: 0.4914\n",
      "Epoch [15/100], Loss: 0.4609\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2803\n",
      "Epoch [26/100], Loss: 0.2681\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2347\n",
      "Epoch [31/100], Loss: 0.2253\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1902\n",
      "Epoch [38/100], Loss: 0.1852\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1767\n",
      "Epoch [41/100], Loss: 0.1725\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1589\n",
      "Epoch [45/100], Loss: 0.1567\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1386\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1310\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1011\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0723\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1865\n",
      "Epoch [2/100], Loss: 2.5253\n",
      "Epoch [3/100], Loss: 1.8772\n",
      "Epoch [4/100], Loss: 1.4577\n",
      "Epoch [5/100], Loss: 1.2173\n",
      "Epoch [6/100], Loss: 1.0467\n",
      "Epoch [7/100], Loss: 0.9286\n",
      "Epoch [8/100], Loss: 0.8375\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6804\n",
      "Epoch [11/100], Loss: 0.6185\n",
      "Epoch [12/100], Loss: 0.5717\n",
      "Epoch [13/100], Loss: 0.5295\n",
      "Epoch [14/100], Loss: 0.4907\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4317\n",
      "Epoch [17/100], Loss: 0.4063\n",
      "Epoch [18/100], Loss: 0.3864\n",
      "Epoch [19/100], Loss: 0.3662\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3324\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3027\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2816\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2599\n",
      "Epoch [28/100], Loss: 0.2515\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2329\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2186\n",
      "Epoch [33/100], Loss: 0.2118\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1889\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1475\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1409\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1207\n",
      "Epoch [59/100], Loss: 0.1196\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1139\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1113\n",
      "Epoch [65/100], Loss: 0.1093\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0984\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1885\n",
      "Epoch [2/100], Loss: 2.5350\n",
      "Epoch [3/100], Loss: 1.8784\n",
      "Epoch [4/100], Loss: 1.4698\n",
      "Epoch [5/100], Loss: 1.2093\n",
      "Epoch [6/100], Loss: 1.0485\n",
      "Epoch [7/100], Loss: 0.9266\n",
      "Epoch [8/100], Loss: 0.8308\n",
      "Epoch [9/100], Loss: 0.7532\n",
      "Epoch [10/100], Loss: 0.6770\n",
      "Epoch [11/100], Loss: 0.6215\n",
      "Epoch [12/100], Loss: 0.5692\n",
      "Epoch [13/100], Loss: 0.5275\n",
      "Epoch [14/100], Loss: 0.4921\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4315\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3863\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2928\n",
      "Epoch [25/100], Loss: 0.2806\n",
      "Epoch [26/100], Loss: 0.2694\n",
      "Epoch [27/100], Loss: 0.2597\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2409\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1951\n",
      "Epoch [37/100], Loss: 0.1903\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1804\n",
      "Epoch [40/100], Loss: 0.1761\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1149\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1087\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1056\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0997\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0755\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.0737\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1879\n",
      "Epoch [2/100], Loss: 2.5309\n",
      "Epoch [3/100], Loss: 1.8720\n",
      "Epoch [4/100], Loss: 1.4633\n",
      "Epoch [5/100], Loss: 1.2152\n",
      "Epoch [6/100], Loss: 1.0474\n",
      "Epoch [7/100], Loss: 0.9322\n",
      "Epoch [8/100], Loss: 0.8335\n",
      "Epoch [9/100], Loss: 0.7489\n",
      "Epoch [10/100], Loss: 0.6822\n",
      "Epoch [11/100], Loss: 0.6203\n",
      "Epoch [12/100], Loss: 0.5729\n",
      "Epoch [13/100], Loss: 0.5293\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4583\n",
      "Epoch [16/100], Loss: 0.4333\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3863\n",
      "Epoch [19/100], Loss: 0.3649\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3184\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2789\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2599\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2184\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1439\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1394\n",
      "Epoch [52/100], Loss: 0.1364\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1286\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1139\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1096\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0983\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0951\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0910\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0877\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1872\n",
      "Epoch [2/100], Loss: 2.5241\n",
      "Epoch [3/100], Loss: 1.8722\n",
      "Epoch [4/100], Loss: 1.4709\n",
      "Epoch [5/100], Loss: 1.2141\n",
      "Epoch [6/100], Loss: 1.0449\n",
      "Epoch [7/100], Loss: 0.9300\n",
      "Epoch [8/100], Loss: 0.8311\n",
      "Epoch [9/100], Loss: 0.7491\n",
      "Epoch [10/100], Loss: 0.6793\n",
      "Epoch [11/100], Loss: 0.6190\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5275\n",
      "Epoch [14/100], Loss: 0.4933\n",
      "Epoch [15/100], Loss: 0.4614\n",
      "Epoch [16/100], Loss: 0.4337\n",
      "Epoch [17/100], Loss: 0.4080\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3663\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2615\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2420\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2129\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1909\n",
      "Epoch [38/100], Loss: 0.1849\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1563\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1469\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1409\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1331\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1241\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1043\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1004\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0914\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0821\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0769\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0736\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.71%\n",
      "Epoch [1/100], Loss: 3.1870\n",
      "Epoch [2/100], Loss: 2.5314\n",
      "Epoch [3/100], Loss: 1.8863\n",
      "Epoch [4/100], Loss: 1.4616\n",
      "Epoch [5/100], Loss: 1.2085\n",
      "Epoch [6/100], Loss: 1.0457\n",
      "Epoch [7/100], Loss: 0.9292\n",
      "Epoch [8/100], Loss: 0.8327\n",
      "Epoch [9/100], Loss: 0.7484\n",
      "Epoch [10/100], Loss: 0.6790\n",
      "Epoch [11/100], Loss: 0.6217\n",
      "Epoch [12/100], Loss: 0.5705\n",
      "Epoch [13/100], Loss: 0.5281\n",
      "Epoch [14/100], Loss: 0.4923\n",
      "Epoch [15/100], Loss: 0.4623\n",
      "Epoch [16/100], Loss: 0.4328\n",
      "Epoch [17/100], Loss: 0.4081\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1801\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1438\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1355\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0784\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.72%\n",
      "Epoch [1/100], Loss: 3.1942\n",
      "Epoch [2/100], Loss: 2.5264\n",
      "Epoch [3/100], Loss: 1.8771\n",
      "Epoch [4/100], Loss: 1.4622\n",
      "Epoch [5/100], Loss: 1.2120\n",
      "Epoch [6/100], Loss: 1.0440\n",
      "Epoch [7/100], Loss: 0.9258\n",
      "Epoch [8/100], Loss: 0.8339\n",
      "Epoch [9/100], Loss: 0.7530\n",
      "Epoch [10/100], Loss: 0.6812\n",
      "Epoch [11/100], Loss: 0.6217\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5292\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4328\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3484\n",
      "Epoch [21/100], Loss: 0.3329\n",
      "Epoch [22/100], Loss: 0.3185\n",
      "Epoch [23/100], Loss: 0.3043\n",
      "Epoch [24/100], Loss: 0.2928\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2708\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2516\n",
      "Epoch [29/100], Loss: 0.2425\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2182\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1850\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1188\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1042\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1004\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0821\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0769\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0748\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0716\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0716\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1854\n",
      "Epoch [2/100], Loss: 2.5306\n",
      "Epoch [3/100], Loss: 1.8827\n",
      "Epoch [4/100], Loss: 1.4656\n",
      "Epoch [5/100], Loss: 1.2142\n",
      "Epoch [6/100], Loss: 1.0502\n",
      "Epoch [7/100], Loss: 0.9261\n",
      "Epoch [8/100], Loss: 0.8292\n",
      "Epoch [9/100], Loss: 0.7467\n",
      "Epoch [10/100], Loss: 0.6791\n",
      "Epoch [11/100], Loss: 0.6197\n",
      "Epoch [12/100], Loss: 0.5701\n",
      "Epoch [13/100], Loss: 0.5285\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4326\n",
      "Epoch [17/100], Loss: 0.4070\n",
      "Epoch [18/100], Loss: 0.3859\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3523\n",
      "Epoch [21/100], Loss: 0.3321\n",
      "Epoch [22/100], Loss: 0.3182\n",
      "Epoch [23/100], Loss: 0.3045\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2684\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2356\n",
      "Epoch [31/100], Loss: 0.2267\n",
      "Epoch [32/100], Loss: 0.2198\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1902\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1534\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1408\n",
      "Epoch [51/100], Loss: 0.1380\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0892\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1934\n",
      "Epoch [2/100], Loss: 2.5394\n",
      "Epoch [3/100], Loss: 1.8823\n",
      "Epoch [4/100], Loss: 1.4571\n",
      "Epoch [5/100], Loss: 1.2027\n",
      "Epoch [6/100], Loss: 1.0492\n",
      "Epoch [7/100], Loss: 0.9290\n",
      "Epoch [8/100], Loss: 0.8331\n",
      "Epoch [9/100], Loss: 0.7480\n",
      "Epoch [10/100], Loss: 0.6823\n",
      "Epoch [11/100], Loss: 0.6199\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5298\n",
      "Epoch [14/100], Loss: 0.4932\n",
      "Epoch [15/100], Loss: 0.4613\n",
      "Epoch [16/100], Loss: 0.4320\n",
      "Epoch [17/100], Loss: 0.4084\n",
      "Epoch [18/100], Loss: 0.3844\n",
      "Epoch [19/100], Loss: 0.3669\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3336\n",
      "Epoch [22/100], Loss: 0.3177\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2697\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2426\n",
      "Epoch [30/100], Loss: 0.2347\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2197\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1902\n",
      "Epoch [38/100], Loss: 0.1838\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1409\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0794\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0761\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1862\n",
      "Epoch [2/100], Loss: 2.5306\n",
      "Epoch [3/100], Loss: 1.8769\n",
      "Epoch [4/100], Loss: 1.4650\n",
      "Epoch [5/100], Loss: 1.2122\n",
      "Epoch [6/100], Loss: 1.0514\n",
      "Epoch [7/100], Loss: 0.9317\n",
      "Epoch [8/100], Loss: 0.8357\n",
      "Epoch [9/100], Loss: 0.7493\n",
      "Epoch [10/100], Loss: 0.6800\n",
      "Epoch [11/100], Loss: 0.6202\n",
      "Epoch [12/100], Loss: 0.5686\n",
      "Epoch [13/100], Loss: 0.5295\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4589\n",
      "Epoch [16/100], Loss: 0.4305\n",
      "Epoch [17/100], Loss: 0.4060\n",
      "Epoch [18/100], Loss: 0.3842\n",
      "Epoch [19/100], Loss: 0.3664\n",
      "Epoch [20/100], Loss: 0.3478\n",
      "Epoch [21/100], Loss: 0.3312\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2682\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2341\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2195\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1723\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1506\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1416\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1149\n",
      "Epoch [62/100], Loss: 0.1131\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1026\n",
      "Epoch [70/100], Loss: 0.1014\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0958\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0903\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0837\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0797\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0765\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1797\n",
      "Epoch [2/100], Loss: 2.5274\n",
      "Epoch [3/100], Loss: 1.8835\n",
      "Epoch [4/100], Loss: 1.4597\n",
      "Epoch [5/100], Loss: 1.2108\n",
      "Epoch [6/100], Loss: 1.0496\n",
      "Epoch [7/100], Loss: 0.9306\n",
      "Epoch [8/100], Loss: 0.8306\n",
      "Epoch [9/100], Loss: 0.7490\n",
      "Epoch [10/100], Loss: 0.6808\n",
      "Epoch [11/100], Loss: 0.6221\n",
      "Epoch [12/100], Loss: 0.5705\n",
      "Epoch [13/100], Loss: 0.5267\n",
      "Epoch [14/100], Loss: 0.4908\n",
      "Epoch [15/100], Loss: 0.4589\n",
      "Epoch [16/100], Loss: 0.4311\n",
      "Epoch [17/100], Loss: 0.4085\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3028\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2805\n",
      "Epoch [26/100], Loss: 0.2696\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1714\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1825\n",
      "Epoch [2/100], Loss: 2.5483\n",
      "Epoch [3/100], Loss: 1.8945\n",
      "Epoch [4/100], Loss: 1.4707\n",
      "Epoch [5/100], Loss: 1.2124\n",
      "Epoch [6/100], Loss: 1.0455\n",
      "Epoch [7/100], Loss: 0.9291\n",
      "Epoch [8/100], Loss: 0.8338\n",
      "Epoch [9/100], Loss: 0.7493\n",
      "Epoch [10/100], Loss: 0.6772\n",
      "Epoch [11/100], Loss: 0.6238\n",
      "Epoch [12/100], Loss: 0.5753\n",
      "Epoch [13/100], Loss: 0.5280\n",
      "Epoch [14/100], Loss: 0.4911\n",
      "Epoch [15/100], Loss: 0.4589\n",
      "Epoch [16/100], Loss: 0.4331\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3858\n",
      "Epoch [19/100], Loss: 0.3667\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3305\n",
      "Epoch [22/100], Loss: 0.3175\n",
      "Epoch [23/100], Loss: 0.3048\n",
      "Epoch [24/100], Loss: 0.2926\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2267\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1409\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1320\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1215\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1105\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1028\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0997\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0860\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0792\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0723\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.83%\n",
      "Epoch [1/100], Loss: 3.1808\n",
      "Epoch [2/100], Loss: 2.5281\n",
      "Epoch [3/100], Loss: 1.8877\n",
      "Epoch [4/100], Loss: 1.4620\n",
      "Epoch [5/100], Loss: 1.2127\n",
      "Epoch [6/100], Loss: 1.0450\n",
      "Epoch [7/100], Loss: 0.9287\n",
      "Epoch [8/100], Loss: 0.8284\n",
      "Epoch [9/100], Loss: 0.7505\n",
      "Epoch [10/100], Loss: 0.6759\n",
      "Epoch [11/100], Loss: 0.6190\n",
      "Epoch [12/100], Loss: 0.5723\n",
      "Epoch [13/100], Loss: 0.5268\n",
      "Epoch [14/100], Loss: 0.4912\n",
      "Epoch [15/100], Loss: 0.4589\n",
      "Epoch [16/100], Loss: 0.4345\n",
      "Epoch [17/100], Loss: 0.4057\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3662\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3185\n",
      "Epoch [23/100], Loss: 0.3046\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2806\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2265\n",
      "Epoch [32/100], Loss: 0.2184\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1677\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1219\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1140\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0990\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0821\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0738\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0721\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.66%\n",
      "Epoch [1/100], Loss: 3.1862\n",
      "Epoch [2/100], Loss: 2.5179\n",
      "Epoch [3/100], Loss: 1.8810\n",
      "Epoch [4/100], Loss: 1.4641\n",
      "Epoch [5/100], Loss: 1.2051\n",
      "Epoch [6/100], Loss: 1.0470\n",
      "Epoch [7/100], Loss: 0.9247\n",
      "Epoch [8/100], Loss: 0.8348\n",
      "Epoch [9/100], Loss: 0.7479\n",
      "Epoch [10/100], Loss: 0.6770\n",
      "Epoch [11/100], Loss: 0.6189\n",
      "Epoch [12/100], Loss: 0.5719\n",
      "Epoch [13/100], Loss: 0.5290\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4610\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4089\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3649\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3040\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2694\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2340\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1046\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1014\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0927\n",
      "Epoch [78/100], Loss: 0.0909\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0842\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1833\n",
      "Epoch [2/100], Loss: 2.5093\n",
      "Epoch [3/100], Loss: 1.8863\n",
      "Epoch [4/100], Loss: 1.4596\n",
      "Epoch [5/100], Loss: 1.2084\n",
      "Epoch [6/100], Loss: 1.0527\n",
      "Epoch [7/100], Loss: 0.9280\n",
      "Epoch [8/100], Loss: 0.8329\n",
      "Epoch [9/100], Loss: 0.7477\n",
      "Epoch [10/100], Loss: 0.6820\n",
      "Epoch [11/100], Loss: 0.6212\n",
      "Epoch [12/100], Loss: 0.5704\n",
      "Epoch [13/100], Loss: 0.5293\n",
      "Epoch [14/100], Loss: 0.4930\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4344\n",
      "Epoch [17/100], Loss: 0.4086\n",
      "Epoch [18/100], Loss: 0.3844\n",
      "Epoch [19/100], Loss: 0.3677\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3026\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2802\n",
      "Epoch [26/100], Loss: 0.2698\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2183\n",
      "Epoch [33/100], Loss: 0.2132\n",
      "Epoch [34/100], Loss: 0.2053\n",
      "Epoch [35/100], Loss: 0.2015\n",
      "Epoch [36/100], Loss: 0.1954\n",
      "Epoch [37/100], Loss: 0.1904\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1360\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1308\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1238\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1196\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1088\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0898\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0868\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0785\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0774\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1835\n",
      "Epoch [2/100], Loss: 2.5362\n",
      "Epoch [3/100], Loss: 1.8885\n",
      "Epoch [4/100], Loss: 1.4576\n",
      "Epoch [5/100], Loss: 1.2065\n",
      "Epoch [6/100], Loss: 1.0497\n",
      "Epoch [7/100], Loss: 0.9294\n",
      "Epoch [8/100], Loss: 0.8343\n",
      "Epoch [9/100], Loss: 0.7515\n",
      "Epoch [10/100], Loss: 0.6793\n",
      "Epoch [11/100], Loss: 0.6182\n",
      "Epoch [12/100], Loss: 0.5719\n",
      "Epoch [13/100], Loss: 0.5270\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4071\n",
      "Epoch [18/100], Loss: 0.3870\n",
      "Epoch [19/100], Loss: 0.3670\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3329\n",
      "Epoch [22/100], Loss: 0.3178\n",
      "Epoch [23/100], Loss: 0.3028\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1909\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1680\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1229\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0839\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0812\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0757\n",
      "Epoch [95/100], Loss: 0.0752\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.70%\n",
      "Epoch [1/100], Loss: 3.1852\n",
      "Epoch [2/100], Loss: 2.5223\n",
      "Epoch [3/100], Loss: 1.8893\n",
      "Epoch [4/100], Loss: 1.4590\n",
      "Epoch [5/100], Loss: 1.2087\n",
      "Epoch [6/100], Loss: 1.0448\n",
      "Epoch [7/100], Loss: 0.9265\n",
      "Epoch [8/100], Loss: 0.8311\n",
      "Epoch [9/100], Loss: 0.7468\n",
      "Epoch [10/100], Loss: 0.6789\n",
      "Epoch [11/100], Loss: 0.6216\n",
      "Epoch [12/100], Loss: 0.5727\n",
      "Epoch [13/100], Loss: 0.5273\n",
      "Epoch [14/100], Loss: 0.4910\n",
      "Epoch [15/100], Loss: 0.4621\n",
      "Epoch [16/100], Loss: 0.4305\n",
      "Epoch [17/100], Loss: 0.4070\n",
      "Epoch [18/100], Loss: 0.3858\n",
      "Epoch [19/100], Loss: 0.3658\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3185\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2696\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2252\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2069\n",
      "Epoch [35/100], Loss: 0.2012\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1803\n",
      "Epoch [40/100], Loss: 0.1758\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1238\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1157\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0998\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0714\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0714\n",
      "Test Accuracy Logit Lipschitz: 51.72%\n",
      "Epoch [1/100], Loss: 3.1901\n",
      "Epoch [2/100], Loss: 2.5232\n",
      "Epoch [3/100], Loss: 1.8741\n",
      "Epoch [4/100], Loss: 1.4651\n",
      "Epoch [5/100], Loss: 1.2072\n",
      "Epoch [6/100], Loss: 1.0454\n",
      "Epoch [7/100], Loss: 0.9268\n",
      "Epoch [8/100], Loss: 0.8318\n",
      "Epoch [9/100], Loss: 0.7465\n",
      "Epoch [10/100], Loss: 0.6778\n",
      "Epoch [11/100], Loss: 0.6196\n",
      "Epoch [12/100], Loss: 0.5702\n",
      "Epoch [13/100], Loss: 0.5298\n",
      "Epoch [14/100], Loss: 0.4911\n",
      "Epoch [15/100], Loss: 0.4619\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4084\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3312\n",
      "Epoch [22/100], Loss: 0.3180\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2802\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2598\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2343\n",
      "Epoch [31/100], Loss: 0.2265\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2009\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1851\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1563\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0887\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0802\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0758\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0738\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0715\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0715\n",
      "Test Accuracy Logit Lipschitz: 51.68%\n",
      "Epoch [1/100], Loss: 3.1886\n",
      "Epoch [2/100], Loss: 2.5311\n",
      "Epoch [3/100], Loss: 1.8788\n",
      "Epoch [4/100], Loss: 1.4586\n",
      "Epoch [5/100], Loss: 1.2072\n",
      "Epoch [6/100], Loss: 1.0497\n",
      "Epoch [7/100], Loss: 0.9334\n",
      "Epoch [8/100], Loss: 0.8343\n",
      "Epoch [9/100], Loss: 0.7473\n",
      "Epoch [10/100], Loss: 0.6787\n",
      "Epoch [11/100], Loss: 0.6220\n",
      "Epoch [12/100], Loss: 0.5734\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4917\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4318\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3858\n",
      "Epoch [19/100], Loss: 0.3668\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3160\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2253\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2130\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1284\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1229\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1175\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1140\n",
      "Epoch [63/100], Loss: 0.1129\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0985\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0791\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.81%\n",
      "Epoch [1/100], Loss: 3.1876\n",
      "Epoch [2/100], Loss: 2.5260\n",
      "Epoch [3/100], Loss: 1.8897\n",
      "Epoch [4/100], Loss: 1.4677\n",
      "Epoch [5/100], Loss: 1.2060\n",
      "Epoch [6/100], Loss: 1.0479\n",
      "Epoch [7/100], Loss: 0.9285\n",
      "Epoch [8/100], Loss: 0.8285\n",
      "Epoch [9/100], Loss: 0.7507\n",
      "Epoch [10/100], Loss: 0.6787\n",
      "Epoch [11/100], Loss: 0.6207\n",
      "Epoch [12/100], Loss: 0.5717\n",
      "Epoch [13/100], Loss: 0.5271\n",
      "Epoch [14/100], Loss: 0.4922\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4338\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3868\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3308\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2508\n",
      "Epoch [29/100], Loss: 0.2422\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2130\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1912\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1320\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1273\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1123\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1048\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1014\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0984\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0785\n",
      "Epoch [92/100], Loss: 0.0774\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.73%\n",
      "Epoch [1/100], Loss: 3.1851\n",
      "Epoch [2/100], Loss: 2.5212\n",
      "Epoch [3/100], Loss: 1.8719\n",
      "Epoch [4/100], Loss: 1.4491\n",
      "Epoch [5/100], Loss: 1.2117\n",
      "Epoch [6/100], Loss: 1.0457\n",
      "Epoch [7/100], Loss: 0.9281\n",
      "Epoch [8/100], Loss: 0.8335\n",
      "Epoch [9/100], Loss: 0.7478\n",
      "Epoch [10/100], Loss: 0.6772\n",
      "Epoch [11/100], Loss: 0.6202\n",
      "Epoch [12/100], Loss: 0.5721\n",
      "Epoch [13/100], Loss: 0.5301\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4616\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4082\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3176\n",
      "Epoch [23/100], Loss: 0.3048\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2806\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2329\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1849\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1717\n",
      "Epoch [42/100], Loss: 0.1675\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1535\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1251\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1195\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1012\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0971\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0868\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0847\n",
      "Epoch [85/100], Loss: 0.0837\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0800\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0714\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0714\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1871\n",
      "Epoch [2/100], Loss: 2.5316\n",
      "Epoch [3/100], Loss: 1.8803\n",
      "Epoch [4/100], Loss: 1.4593\n",
      "Epoch [5/100], Loss: 1.2066\n",
      "Epoch [6/100], Loss: 1.0427\n",
      "Epoch [7/100], Loss: 0.9282\n",
      "Epoch [8/100], Loss: 0.8312\n",
      "Epoch [9/100], Loss: 0.7510\n",
      "Epoch [10/100], Loss: 0.6767\n",
      "Epoch [11/100], Loss: 0.6188\n",
      "Epoch [12/100], Loss: 0.5690\n",
      "Epoch [13/100], Loss: 0.5274\n",
      "Epoch [14/100], Loss: 0.4926\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3658\n",
      "Epoch [20/100], Loss: 0.3495\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3042\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2706\n",
      "Epoch [27/100], Loss: 0.2601\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2408\n",
      "Epoch [30/100], Loss: 0.2341\n",
      "Epoch [31/100], Loss: 0.2267\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2131\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1436\n",
      "Epoch [50/100], Loss: 0.1409\n",
      "Epoch [51/100], Loss: 0.1384\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1165\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1025\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0810\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0768\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1875\n",
      "Epoch [2/100], Loss: 2.5288\n",
      "Epoch [3/100], Loss: 1.8829\n",
      "Epoch [4/100], Loss: 1.4648\n",
      "Epoch [5/100], Loss: 1.2071\n",
      "Epoch [6/100], Loss: 1.0441\n",
      "Epoch [7/100], Loss: 0.9262\n",
      "Epoch [8/100], Loss: 0.8317\n",
      "Epoch [9/100], Loss: 0.7482\n",
      "Epoch [10/100], Loss: 0.6790\n",
      "Epoch [11/100], Loss: 0.6175\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5303\n",
      "Epoch [14/100], Loss: 0.4913\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4342\n",
      "Epoch [17/100], Loss: 0.4078\n",
      "Epoch [18/100], Loss: 0.3858\n",
      "Epoch [19/100], Loss: 0.3669\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2054\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1675\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1273\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0892\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0812\n",
      "Epoch [88/100], Loss: 0.0810\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0785\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0766\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0735\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0721\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 50000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/100], Loss: 3.1804\n",
      "Epoch [2/100], Loss: 2.5348\n",
      "Epoch [3/100], Loss: 1.8921\n",
      "Epoch [4/100], Loss: 1.4609\n",
      "Epoch [5/100], Loss: 1.2177\n",
      "Epoch [6/100], Loss: 1.0446\n",
      "Epoch [7/100], Loss: 0.9301\n",
      "Epoch [8/100], Loss: 0.8356\n",
      "Epoch [9/100], Loss: 0.7484\n",
      "Epoch [10/100], Loss: 0.6787\n",
      "Epoch [11/100], Loss: 0.6201\n",
      "Epoch [12/100], Loss: 0.5711\n",
      "Epoch [13/100], Loss: 0.5281\n",
      "Epoch [14/100], Loss: 0.4905\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4307\n",
      "Epoch [17/100], Loss: 0.4092\n",
      "Epoch [18/100], Loss: 0.3852\n",
      "Epoch [19/100], Loss: 0.3646\n",
      "Epoch [20/100], Loss: 0.3470\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3053\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2701\n",
      "Epoch [27/100], Loss: 0.2597\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0738\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0730\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1837\n",
      "Epoch [2/100], Loss: 2.5241\n",
      "Epoch [3/100], Loss: 1.8746\n",
      "Epoch [4/100], Loss: 1.4548\n",
      "Epoch [5/100], Loss: 1.2095\n",
      "Epoch [6/100], Loss: 1.0446\n",
      "Epoch [7/100], Loss: 0.9287\n",
      "Epoch [8/100], Loss: 0.8286\n",
      "Epoch [9/100], Loss: 0.7478\n",
      "Epoch [10/100], Loss: 0.6755\n",
      "Epoch [11/100], Loss: 0.6179\n",
      "Epoch [12/100], Loss: 0.5702\n",
      "Epoch [13/100], Loss: 0.5257\n",
      "Epoch [14/100], Loss: 0.4917\n",
      "Epoch [15/100], Loss: 0.4605\n",
      "Epoch [16/100], Loss: 0.4331\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3845\n",
      "Epoch [19/100], Loss: 0.3662\n",
      "Epoch [20/100], Loss: 0.3493\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2605\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2407\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2268\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1714\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1041\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0934\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0901\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0838\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0769\n",
      "Epoch [93/100], Loss: 0.0765\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1818\n",
      "Epoch [2/100], Loss: 2.5320\n",
      "Epoch [3/100], Loss: 1.8828\n",
      "Epoch [4/100], Loss: 1.4652\n",
      "Epoch [5/100], Loss: 1.2134\n",
      "Epoch [6/100], Loss: 1.0523\n",
      "Epoch [7/100], Loss: 0.9309\n",
      "Epoch [8/100], Loss: 0.8343\n",
      "Epoch [9/100], Loss: 0.7476\n",
      "Epoch [10/100], Loss: 0.6835\n",
      "Epoch [11/100], Loss: 0.6217\n",
      "Epoch [12/100], Loss: 0.5688\n",
      "Epoch [13/100], Loss: 0.5294\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4320\n",
      "Epoch [17/100], Loss: 0.4093\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3649\n",
      "Epoch [20/100], Loss: 0.3468\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3026\n",
      "Epoch [24/100], Loss: 0.2905\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1887\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1704\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1196\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1122\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1091\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1033\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0736\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1949\n",
      "Epoch [2/100], Loss: 2.5361\n",
      "Epoch [3/100], Loss: 1.8771\n",
      "Epoch [4/100], Loss: 1.4586\n",
      "Epoch [5/100], Loss: 1.2051\n",
      "Epoch [6/100], Loss: 1.0501\n",
      "Epoch [7/100], Loss: 0.9297\n",
      "Epoch [8/100], Loss: 0.8296\n",
      "Epoch [9/100], Loss: 0.7478\n",
      "Epoch [10/100], Loss: 0.6771\n",
      "Epoch [11/100], Loss: 0.6208\n",
      "Epoch [12/100], Loss: 0.5701\n",
      "Epoch [13/100], Loss: 0.5266\n",
      "Epoch [14/100], Loss: 0.4904\n",
      "Epoch [15/100], Loss: 0.4591\n",
      "Epoch [16/100], Loss: 0.4341\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3846\n",
      "Epoch [19/100], Loss: 0.3660\n",
      "Epoch [20/100], Loss: 0.3478\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3040\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2683\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2070\n",
      "Epoch [35/100], Loss: 0.2011\n",
      "Epoch [36/100], Loss: 0.1942\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1854\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1719\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0809\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0722\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1858\n",
      "Epoch [2/100], Loss: 2.5177\n",
      "Epoch [3/100], Loss: 1.8757\n",
      "Epoch [4/100], Loss: 1.4701\n",
      "Epoch [5/100], Loss: 1.2068\n",
      "Epoch [6/100], Loss: 1.0445\n",
      "Epoch [7/100], Loss: 0.9305\n",
      "Epoch [8/100], Loss: 0.8331\n",
      "Epoch [9/100], Loss: 0.7464\n",
      "Epoch [10/100], Loss: 0.6772\n",
      "Epoch [11/100], Loss: 0.6237\n",
      "Epoch [12/100], Loss: 0.5705\n",
      "Epoch [13/100], Loss: 0.5282\n",
      "Epoch [14/100], Loss: 0.4904\n",
      "Epoch [15/100], Loss: 0.4587\n",
      "Epoch [16/100], Loss: 0.4327\n",
      "Epoch [17/100], Loss: 0.4065\n",
      "Epoch [18/100], Loss: 0.3862\n",
      "Epoch [19/100], Loss: 0.3663\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3329\n",
      "Epoch [22/100], Loss: 0.3187\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2812\n",
      "Epoch [26/100], Loss: 0.2698\n",
      "Epoch [27/100], Loss: 0.2600\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2355\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1850\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1678\n",
      "Epoch [43/100], Loss: 0.1639\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0951\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0881\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0785\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1847\n",
      "Epoch [2/100], Loss: 2.5440\n",
      "Epoch [3/100], Loss: 1.8815\n",
      "Epoch [4/100], Loss: 1.4682\n",
      "Epoch [5/100], Loss: 1.2161\n",
      "Epoch [6/100], Loss: 1.0488\n",
      "Epoch [7/100], Loss: 0.9287\n",
      "Epoch [8/100], Loss: 0.8331\n",
      "Epoch [9/100], Loss: 0.7502\n",
      "Epoch [10/100], Loss: 0.6823\n",
      "Epoch [11/100], Loss: 0.6229\n",
      "Epoch [12/100], Loss: 0.5688\n",
      "Epoch [13/100], Loss: 0.5270\n",
      "Epoch [14/100], Loss: 0.4911\n",
      "Epoch [15/100], Loss: 0.4602\n",
      "Epoch [16/100], Loss: 0.4303\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3866\n",
      "Epoch [19/100], Loss: 0.3644\n",
      "Epoch [20/100], Loss: 0.3510\n",
      "Epoch [21/100], Loss: 0.3342\n",
      "Epoch [22/100], Loss: 0.3180\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2681\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2406\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1903\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1757\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1381\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1320\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0878\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0848\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0821\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0757\n",
      "Epoch [95/100], Loss: 0.0750\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.77%\n",
      "Epoch [1/100], Loss: 3.1892\n",
      "Epoch [2/100], Loss: 2.5176\n",
      "Epoch [3/100], Loss: 1.8774\n",
      "Epoch [4/100], Loss: 1.4570\n",
      "Epoch [5/100], Loss: 1.2039\n",
      "Epoch [6/100], Loss: 1.0430\n",
      "Epoch [7/100], Loss: 0.9306\n",
      "Epoch [8/100], Loss: 0.8338\n",
      "Epoch [9/100], Loss: 0.7458\n",
      "Epoch [10/100], Loss: 0.6783\n",
      "Epoch [11/100], Loss: 0.6194\n",
      "Epoch [12/100], Loss: 0.5725\n",
      "Epoch [13/100], Loss: 0.5281\n",
      "Epoch [14/100], Loss: 0.4894\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4066\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3665\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3048\n",
      "Epoch [24/100], Loss: 0.2923\n",
      "Epoch [25/100], Loss: 0.2803\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1760\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1634\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0849\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.77%\n",
      "Epoch [1/100], Loss: 3.1909\n",
      "Epoch [2/100], Loss: 2.5364\n",
      "Epoch [3/100], Loss: 1.8797\n",
      "Epoch [4/100], Loss: 1.4615\n",
      "Epoch [5/100], Loss: 1.2107\n",
      "Epoch [6/100], Loss: 1.0477\n",
      "Epoch [7/100], Loss: 0.9306\n",
      "Epoch [8/100], Loss: 0.8309\n",
      "Epoch [9/100], Loss: 0.7512\n",
      "Epoch [10/100], Loss: 0.6792\n",
      "Epoch [11/100], Loss: 0.6237\n",
      "Epoch [12/100], Loss: 0.5701\n",
      "Epoch [13/100], Loss: 0.5257\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4602\n",
      "Epoch [16/100], Loss: 0.4312\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3859\n",
      "Epoch [19/100], Loss: 0.3662\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2787\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1849\n",
      "Epoch [39/100], Loss: 0.1791\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1331\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1229\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1197\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0769\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1869\n",
      "Epoch [2/100], Loss: 2.5421\n",
      "Epoch [3/100], Loss: 1.8804\n",
      "Epoch [4/100], Loss: 1.4657\n",
      "Epoch [5/100], Loss: 1.2136\n",
      "Epoch [6/100], Loss: 1.0450\n",
      "Epoch [7/100], Loss: 0.9263\n",
      "Epoch [8/100], Loss: 0.8332\n",
      "Epoch [9/100], Loss: 0.7511\n",
      "Epoch [10/100], Loss: 0.6770\n",
      "Epoch [11/100], Loss: 0.6200\n",
      "Epoch [12/100], Loss: 0.5697\n",
      "Epoch [13/100], Loss: 0.5280\n",
      "Epoch [14/100], Loss: 0.4935\n",
      "Epoch [15/100], Loss: 0.4608\n",
      "Epoch [16/100], Loss: 0.4306\n",
      "Epoch [17/100], Loss: 0.4077\n",
      "Epoch [18/100], Loss: 0.3849\n",
      "Epoch [19/100], Loss: 0.3648\n",
      "Epoch [20/100], Loss: 0.3487\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3179\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2905\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2343\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1852\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1636\n",
      "Epoch [44/100], Loss: 0.1600\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1346\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1283\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1139\n",
      "Epoch [63/100], Loss: 0.1112\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0888\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0791\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.0730\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0715\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0715\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1870\n",
      "Epoch [2/100], Loss: 2.5427\n",
      "Epoch [3/100], Loss: 1.8905\n",
      "Epoch [4/100], Loss: 1.4610\n",
      "Epoch [5/100], Loss: 1.2141\n",
      "Epoch [6/100], Loss: 1.0497\n",
      "Epoch [7/100], Loss: 0.9336\n",
      "Epoch [8/100], Loss: 0.8350\n",
      "Epoch [9/100], Loss: 0.7465\n",
      "Epoch [10/100], Loss: 0.6794\n",
      "Epoch [11/100], Loss: 0.6180\n",
      "Epoch [12/100], Loss: 0.5719\n",
      "Epoch [13/100], Loss: 0.5278\n",
      "Epoch [14/100], Loss: 0.4890\n",
      "Epoch [15/100], Loss: 0.4591\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4070\n",
      "Epoch [18/100], Loss: 0.3865\n",
      "Epoch [19/100], Loss: 0.3662\n",
      "Epoch [20/100], Loss: 0.3468\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3157\n",
      "Epoch [23/100], Loss: 0.3042\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1807\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1704\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1599\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1409\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1334\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1251\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0903\n",
      "Epoch [79/100], Loss: 0.0900\n",
      "Epoch [80/100], Loss: 0.0887\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0841\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0822\n",
      "Epoch [88/100], Loss: 0.0810\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0769\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0728\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.77%\n",
      "Epoch [1/100], Loss: 3.1875\n",
      "Epoch [2/100], Loss: 2.5441\n",
      "Epoch [3/100], Loss: 1.8864\n",
      "Epoch [4/100], Loss: 1.4695\n",
      "Epoch [5/100], Loss: 1.2142\n",
      "Epoch [6/100], Loss: 1.0471\n",
      "Epoch [7/100], Loss: 0.9351\n",
      "Epoch [8/100], Loss: 0.8271\n",
      "Epoch [9/100], Loss: 0.7475\n",
      "Epoch [10/100], Loss: 0.6788\n",
      "Epoch [11/100], Loss: 0.6200\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5270\n",
      "Epoch [14/100], Loss: 0.4924\n",
      "Epoch [15/100], Loss: 0.4588\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4055\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3641\n",
      "Epoch [20/100], Loss: 0.3484\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3047\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2694\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2184\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2006\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1805\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0957\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0935\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0758\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.83%\n",
      "Epoch [1/100], Loss: 3.1903\n",
      "Epoch [2/100], Loss: 2.5320\n",
      "Epoch [3/100], Loss: 1.8794\n",
      "Epoch [4/100], Loss: 1.4628\n",
      "Epoch [5/100], Loss: 1.2099\n",
      "Epoch [6/100], Loss: 1.0426\n",
      "Epoch [7/100], Loss: 0.9287\n",
      "Epoch [8/100], Loss: 0.8352\n",
      "Epoch [9/100], Loss: 0.7503\n",
      "Epoch [10/100], Loss: 0.6768\n",
      "Epoch [11/100], Loss: 0.6218\n",
      "Epoch [12/100], Loss: 0.5694\n",
      "Epoch [13/100], Loss: 0.5270\n",
      "Epoch [14/100], Loss: 0.4911\n",
      "Epoch [15/100], Loss: 0.4607\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3310\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3048\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2419\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2200\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1320\n",
      "Epoch [54/100], Loss: 0.1306\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1217\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1042\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0984\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1844\n",
      "Epoch [2/100], Loss: 2.5384\n",
      "Epoch [3/100], Loss: 1.8785\n",
      "Epoch [4/100], Loss: 1.4552\n",
      "Epoch [5/100], Loss: 1.2147\n",
      "Epoch [6/100], Loss: 1.0534\n",
      "Epoch [7/100], Loss: 0.9306\n",
      "Epoch [8/100], Loss: 0.8321\n",
      "Epoch [9/100], Loss: 0.7508\n",
      "Epoch [10/100], Loss: 0.6760\n",
      "Epoch [11/100], Loss: 0.6193\n",
      "Epoch [12/100], Loss: 0.5709\n",
      "Epoch [13/100], Loss: 0.5276\n",
      "Epoch [14/100], Loss: 0.4922\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4083\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3038\n",
      "Epoch [24/100], Loss: 0.2920\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2118\n",
      "Epoch [34/100], Loss: 0.2083\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1941\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1807\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1636\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1458\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0887\n",
      "Epoch [81/100], Loss: 0.0880\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0761\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1863\n",
      "Epoch [2/100], Loss: 2.5350\n",
      "Epoch [3/100], Loss: 1.8817\n",
      "Epoch [4/100], Loss: 1.4568\n",
      "Epoch [5/100], Loss: 1.2145\n",
      "Epoch [6/100], Loss: 1.0488\n",
      "Epoch [7/100], Loss: 0.9244\n",
      "Epoch [8/100], Loss: 0.8337\n",
      "Epoch [9/100], Loss: 0.7517\n",
      "Epoch [10/100], Loss: 0.6793\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5706\n",
      "Epoch [13/100], Loss: 0.5288\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4608\n",
      "Epoch [16/100], Loss: 0.4326\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3849\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3472\n",
      "Epoch [21/100], Loss: 0.3306\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3040\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2808\n",
      "Epoch [26/100], Loss: 0.2680\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2508\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1747\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1273\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0848\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.81%\n",
      "Epoch [1/100], Loss: 3.1821\n",
      "Epoch [2/100], Loss: 2.5266\n",
      "Epoch [3/100], Loss: 1.8870\n",
      "Epoch [4/100], Loss: 1.4677\n",
      "Epoch [5/100], Loss: 1.2165\n",
      "Epoch [6/100], Loss: 1.0486\n",
      "Epoch [7/100], Loss: 0.9261\n",
      "Epoch [8/100], Loss: 0.8321\n",
      "Epoch [9/100], Loss: 0.7503\n",
      "Epoch [10/100], Loss: 0.6807\n",
      "Epoch [11/100], Loss: 0.6242\n",
      "Epoch [12/100], Loss: 0.5704\n",
      "Epoch [13/100], Loss: 0.5310\n",
      "Epoch [14/100], Loss: 0.4928\n",
      "Epoch [15/100], Loss: 0.4607\n",
      "Epoch [16/100], Loss: 0.4331\n",
      "Epoch [17/100], Loss: 0.4080\n",
      "Epoch [18/100], Loss: 0.3864\n",
      "Epoch [19/100], Loss: 0.3670\n",
      "Epoch [20/100], Loss: 0.3491\n",
      "Epoch [21/100], Loss: 0.3329\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2268\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2117\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1676\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1600\n",
      "Epoch [45/100], Loss: 0.1563\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1497\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1418\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1056\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1026\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0877\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0857\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0738\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1847\n",
      "Epoch [2/100], Loss: 2.5350\n",
      "Epoch [3/100], Loss: 1.8854\n",
      "Epoch [4/100], Loss: 1.4582\n",
      "Epoch [5/100], Loss: 1.2078\n",
      "Epoch [6/100], Loss: 1.0441\n",
      "Epoch [7/100], Loss: 0.9310\n",
      "Epoch [8/100], Loss: 0.8364\n",
      "Epoch [9/100], Loss: 0.7513\n",
      "Epoch [10/100], Loss: 0.6815\n",
      "Epoch [11/100], Loss: 0.6184\n",
      "Epoch [12/100], Loss: 0.5701\n",
      "Epoch [13/100], Loss: 0.5288\n",
      "Epoch [14/100], Loss: 0.4914\n",
      "Epoch [15/100], Loss: 0.4589\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4077\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3489\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2697\n",
      "Epoch [27/100], Loss: 0.2597\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2424\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2132\n",
      "Epoch [34/100], Loss: 0.2055\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1626\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1564\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1381\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0977\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0716\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1924\n",
      "Epoch [2/100], Loss: 2.5362\n",
      "Epoch [3/100], Loss: 1.8790\n",
      "Epoch [4/100], Loss: 1.4568\n",
      "Epoch [5/100], Loss: 1.2122\n",
      "Epoch [6/100], Loss: 1.0481\n",
      "Epoch [7/100], Loss: 0.9311\n",
      "Epoch [8/100], Loss: 0.8304\n",
      "Epoch [9/100], Loss: 0.7480\n",
      "Epoch [10/100], Loss: 0.6787\n",
      "Epoch [11/100], Loss: 0.6164\n",
      "Epoch [12/100], Loss: 0.5694\n",
      "Epoch [13/100], Loss: 0.5280\n",
      "Epoch [14/100], Loss: 0.4927\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4325\n",
      "Epoch [17/100], Loss: 0.4089\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3475\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3028\n",
      "Epoch [24/100], Loss: 0.2921\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2602\n",
      "Epoch [28/100], Loss: 0.2493\n",
      "Epoch [29/100], Loss: 0.2422\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2269\n",
      "Epoch [32/100], Loss: 0.2196\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2076\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1812\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1681\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1074\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1026\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0820\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0774\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1853\n",
      "Epoch [2/100], Loss: 2.5362\n",
      "Epoch [3/100], Loss: 1.8924\n",
      "Epoch [4/100], Loss: 1.4641\n",
      "Epoch [5/100], Loss: 1.2143\n",
      "Epoch [6/100], Loss: 1.0441\n",
      "Epoch [7/100], Loss: 0.9268\n",
      "Epoch [8/100], Loss: 0.8305\n",
      "Epoch [9/100], Loss: 0.7463\n",
      "Epoch [10/100], Loss: 0.6777\n",
      "Epoch [11/100], Loss: 0.6204\n",
      "Epoch [12/100], Loss: 0.5735\n",
      "Epoch [13/100], Loss: 0.5253\n",
      "Epoch [14/100], Loss: 0.4913\n",
      "Epoch [15/100], Loss: 0.4593\n",
      "Epoch [16/100], Loss: 0.4337\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3837\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3306\n",
      "Epoch [22/100], Loss: 0.3160\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2584\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2409\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2068\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1904\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1677\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1261\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1196\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1113\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1080\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0984\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0961\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.74%\n",
      "Epoch [1/100], Loss: 3.1897\n",
      "Epoch [2/100], Loss: 2.5293\n",
      "Epoch [3/100], Loss: 1.8870\n",
      "Epoch [4/100], Loss: 1.4611\n",
      "Epoch [5/100], Loss: 1.2108\n",
      "Epoch [6/100], Loss: 1.0579\n",
      "Epoch [7/100], Loss: 0.9294\n",
      "Epoch [8/100], Loss: 0.8284\n",
      "Epoch [9/100], Loss: 0.7434\n",
      "Epoch [10/100], Loss: 0.6768\n",
      "Epoch [11/100], Loss: 0.6203\n",
      "Epoch [12/100], Loss: 0.5717\n",
      "Epoch [13/100], Loss: 0.5262\n",
      "Epoch [14/100], Loss: 0.4914\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4314\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3859\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3177\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2697\n",
      "Epoch [27/100], Loss: 0.2598\n",
      "Epoch [28/100], Loss: 0.2507\n",
      "Epoch [29/100], Loss: 0.2421\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1499\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1215\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1168\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0977\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0933\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0857\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0759\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1805\n",
      "Epoch [2/100], Loss: 2.5456\n",
      "Epoch [3/100], Loss: 1.8861\n",
      "Epoch [4/100], Loss: 1.4692\n",
      "Epoch [5/100], Loss: 1.2150\n",
      "Epoch [6/100], Loss: 1.0469\n",
      "Epoch [7/100], Loss: 0.9291\n",
      "Epoch [8/100], Loss: 0.8345\n",
      "Epoch [9/100], Loss: 0.7475\n",
      "Epoch [10/100], Loss: 0.6813\n",
      "Epoch [11/100], Loss: 0.6195\n",
      "Epoch [12/100], Loss: 0.5684\n",
      "Epoch [13/100], Loss: 0.5273\n",
      "Epoch [14/100], Loss: 0.4935\n",
      "Epoch [15/100], Loss: 0.4584\n",
      "Epoch [16/100], Loss: 0.4343\n",
      "Epoch [17/100], Loss: 0.4090\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3649\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3310\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3037\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2791\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2584\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2069\n",
      "Epoch [35/100], Loss: 0.2006\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1746\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1334\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1003\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0971\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0909\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1860\n",
      "Epoch [2/100], Loss: 2.5367\n",
      "Epoch [3/100], Loss: 1.8804\n",
      "Epoch [4/100], Loss: 1.4659\n",
      "Epoch [5/100], Loss: 1.2163\n",
      "Epoch [6/100], Loss: 1.0408\n",
      "Epoch [7/100], Loss: 0.9258\n",
      "Epoch [8/100], Loss: 0.8325\n",
      "Epoch [9/100], Loss: 0.7466\n",
      "Epoch [10/100], Loss: 0.6789\n",
      "Epoch [11/100], Loss: 0.6194\n",
      "Epoch [12/100], Loss: 0.5710\n",
      "Epoch [13/100], Loss: 0.5291\n",
      "Epoch [14/100], Loss: 0.4914\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3658\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2900\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1849\n",
      "Epoch [39/100], Loss: 0.1804\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1589\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1087\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1017\n",
      "Epoch [71/100], Loss: 0.0999\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0946\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0926\n",
      "Epoch [78/100], Loss: 0.0911\n",
      "Epoch [79/100], Loss: 0.0892\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0878\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0847\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0794\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0769\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0758\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0743\n",
      "Epoch [97/100], Loss: 0.0736\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1843\n",
      "Epoch [2/100], Loss: 2.5446\n",
      "Epoch [3/100], Loss: 1.8938\n",
      "Epoch [4/100], Loss: 1.4644\n",
      "Epoch [5/100], Loss: 1.2082\n",
      "Epoch [6/100], Loss: 1.0537\n",
      "Epoch [7/100], Loss: 0.9263\n",
      "Epoch [8/100], Loss: 0.8314\n",
      "Epoch [9/100], Loss: 0.7469\n",
      "Epoch [10/100], Loss: 0.6774\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5706\n",
      "Epoch [13/100], Loss: 0.5288\n",
      "Epoch [14/100], Loss: 0.4904\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4341\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3470\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3162\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2599\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2133\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2009\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1408\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1283\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1178\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1143\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1087\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1013\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0888\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0856\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0750\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1871\n",
      "Epoch [2/100], Loss: 2.5334\n",
      "Epoch [3/100], Loss: 1.8756\n",
      "Epoch [4/100], Loss: 1.4618\n",
      "Epoch [5/100], Loss: 1.2099\n",
      "Epoch [6/100], Loss: 1.0493\n",
      "Epoch [7/100], Loss: 0.9286\n",
      "Epoch [8/100], Loss: 0.8341\n",
      "Epoch [9/100], Loss: 0.7502\n",
      "Epoch [10/100], Loss: 0.6815\n",
      "Epoch [11/100], Loss: 0.6197\n",
      "Epoch [12/100], Loss: 0.5718\n",
      "Epoch [13/100], Loss: 0.5291\n",
      "Epoch [14/100], Loss: 0.4903\n",
      "Epoch [15/100], Loss: 0.4625\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3042\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2788\n",
      "Epoch [26/100], Loss: 0.2682\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2055\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1680\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1043\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0784\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0759\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1924\n",
      "Epoch [2/100], Loss: 2.5349\n",
      "Epoch [3/100], Loss: 1.8764\n",
      "Epoch [4/100], Loss: 1.4583\n",
      "Epoch [5/100], Loss: 1.2099\n",
      "Epoch [6/100], Loss: 1.0434\n",
      "Epoch [7/100], Loss: 0.9276\n",
      "Epoch [8/100], Loss: 0.8313\n",
      "Epoch [9/100], Loss: 0.7482\n",
      "Epoch [10/100], Loss: 0.6767\n",
      "Epoch [11/100], Loss: 0.6199\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5292\n",
      "Epoch [14/100], Loss: 0.4938\n",
      "Epoch [15/100], Loss: 0.4601\n",
      "Epoch [16/100], Loss: 0.4330\n",
      "Epoch [17/100], Loss: 0.4067\n",
      "Epoch [18/100], Loss: 0.3864\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3182\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2926\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2683\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2347\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2196\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1107\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0899\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0792\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1875\n",
      "Epoch [2/100], Loss: 2.5272\n",
      "Epoch [3/100], Loss: 1.8859\n",
      "Epoch [4/100], Loss: 1.4623\n",
      "Epoch [5/100], Loss: 1.2091\n",
      "Epoch [6/100], Loss: 1.0469\n",
      "Epoch [7/100], Loss: 0.9305\n",
      "Epoch [8/100], Loss: 0.8321\n",
      "Epoch [9/100], Loss: 0.7482\n",
      "Epoch [10/100], Loss: 0.6780\n",
      "Epoch [11/100], Loss: 0.6189\n",
      "Epoch [12/100], Loss: 0.5709\n",
      "Epoch [13/100], Loss: 0.5300\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4588\n",
      "Epoch [16/100], Loss: 0.4320\n",
      "Epoch [17/100], Loss: 0.4077\n",
      "Epoch [18/100], Loss: 0.3844\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3182\n",
      "Epoch [23/100], Loss: 0.3045\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2597\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2421\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2198\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2014\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1714\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1600\n",
      "Epoch [45/100], Loss: 0.1568\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1497\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1215\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1140\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0765\n",
      "Epoch [94/100], Loss: 0.0758\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1904\n",
      "Epoch [2/100], Loss: 2.5376\n",
      "Epoch [3/100], Loss: 1.8839\n",
      "Epoch [4/100], Loss: 1.4597\n",
      "Epoch [5/100], Loss: 1.2133\n",
      "Epoch [6/100], Loss: 1.0531\n",
      "Epoch [7/100], Loss: 0.9264\n",
      "Epoch [8/100], Loss: 0.8294\n",
      "Epoch [9/100], Loss: 0.7453\n",
      "Epoch [10/100], Loss: 0.6768\n",
      "Epoch [11/100], Loss: 0.6188\n",
      "Epoch [12/100], Loss: 0.5713\n",
      "Epoch [13/100], Loss: 0.5261\n",
      "Epoch [14/100], Loss: 0.4922\n",
      "Epoch [15/100], Loss: 0.4601\n",
      "Epoch [16/100], Loss: 0.4332\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3849\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3326\n",
      "Epoch [22/100], Loss: 0.3176\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2683\n",
      "Epoch [27/100], Loss: 0.2585\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2183\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2067\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1951\n",
      "Epoch [37/100], Loss: 0.1905\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1499\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1041\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0809\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Epoch [92/100], Loss: 0.0774\n",
      "Epoch [93/100], Loss: 0.0765\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1824\n",
      "Epoch [2/100], Loss: 2.5271\n",
      "Epoch [3/100], Loss: 1.8843\n",
      "Epoch [4/100], Loss: 1.4594\n",
      "Epoch [5/100], Loss: 1.2112\n",
      "Epoch [6/100], Loss: 1.0505\n",
      "Epoch [7/100], Loss: 0.9302\n",
      "Epoch [8/100], Loss: 0.8337\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6777\n",
      "Epoch [11/100], Loss: 0.6215\n",
      "Epoch [12/100], Loss: 0.5693\n",
      "Epoch [13/100], Loss: 0.5291\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4618\n",
      "Epoch [16/100], Loss: 0.4320\n",
      "Epoch [17/100], Loss: 0.4099\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3647\n",
      "Epoch [20/100], Loss: 0.3487\n",
      "Epoch [21/100], Loss: 0.3335\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3038\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2788\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2419\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2200\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1760\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1399\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1140\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0983\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0901\n",
      "Epoch [80/100], Loss: 0.0887\n",
      "Epoch [81/100], Loss: 0.0877\n",
      "Epoch [82/100], Loss: 0.0868\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0840\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0800\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0766\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0750\n",
      "Epoch [96/100], Loss: 0.0744\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0731\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1856\n",
      "Epoch [2/100], Loss: 2.5258\n",
      "Epoch [3/100], Loss: 1.8714\n",
      "Epoch [4/100], Loss: 1.4575\n",
      "Epoch [5/100], Loss: 1.2124\n",
      "Epoch [6/100], Loss: 1.0521\n",
      "Epoch [7/100], Loss: 0.9264\n",
      "Epoch [8/100], Loss: 0.8328\n",
      "Epoch [9/100], Loss: 0.7500\n",
      "Epoch [10/100], Loss: 0.6762\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5710\n",
      "Epoch [13/100], Loss: 0.5275\n",
      "Epoch [14/100], Loss: 0.4932\n",
      "Epoch [15/100], Loss: 0.4632\n",
      "Epoch [16/100], Loss: 0.4325\n",
      "Epoch [17/100], Loss: 0.4077\n",
      "Epoch [18/100], Loss: 0.3872\n",
      "Epoch [19/100], Loss: 0.3660\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2787\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2426\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1951\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1636\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1568\n",
      "Epoch [46/100], Loss: 0.1532\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1438\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1280\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1207\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1124\n",
      "Epoch [64/100], Loss: 0.1108\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0777\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1844\n",
      "Epoch [2/100], Loss: 2.5334\n",
      "Epoch [3/100], Loss: 1.8807\n",
      "Epoch [4/100], Loss: 1.4628\n",
      "Epoch [5/100], Loss: 1.2085\n",
      "Epoch [6/100], Loss: 1.0472\n",
      "Epoch [7/100], Loss: 0.9294\n",
      "Epoch [8/100], Loss: 0.8292\n",
      "Epoch [9/100], Loss: 0.7479\n",
      "Epoch [10/100], Loss: 0.6749\n",
      "Epoch [11/100], Loss: 0.6218\n",
      "Epoch [12/100], Loss: 0.5714\n",
      "Epoch [13/100], Loss: 0.5297\n",
      "Epoch [14/100], Loss: 0.4899\n",
      "Epoch [15/100], Loss: 0.4591\n",
      "Epoch [16/100], Loss: 0.4329\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3317\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2694\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1383\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1158\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0792\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0767\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0752\n",
      "Epoch [96/100], Loss: 0.0743\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1843\n",
      "Epoch [2/100], Loss: 2.5311\n",
      "Epoch [3/100], Loss: 1.8816\n",
      "Epoch [4/100], Loss: 1.4610\n",
      "Epoch [5/100], Loss: 1.2139\n",
      "Epoch [6/100], Loss: 1.0419\n",
      "Epoch [7/100], Loss: 0.9291\n",
      "Epoch [8/100], Loss: 0.8316\n",
      "Epoch [9/100], Loss: 0.7571\n",
      "Epoch [10/100], Loss: 0.6804\n",
      "Epoch [11/100], Loss: 0.6208\n",
      "Epoch [12/100], Loss: 0.5706\n",
      "Epoch [13/100], Loss: 0.5291\n",
      "Epoch [14/100], Loss: 0.4909\n",
      "Epoch [15/100], Loss: 0.4594\n",
      "Epoch [16/100], Loss: 0.4309\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3667\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1849\n",
      "Epoch [39/100], Loss: 0.1801\n",
      "Epoch [40/100], Loss: 0.1764\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0933\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0866\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0840\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 10000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.84%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/100], Loss: 3.1869\n",
      "Epoch [2/100], Loss: 2.5296\n",
      "Epoch [3/100], Loss: 1.8853\n",
      "Epoch [4/100], Loss: 1.4677\n",
      "Epoch [5/100], Loss: 1.2185\n",
      "Epoch [6/100], Loss: 1.0486\n",
      "Epoch [7/100], Loss: 0.9291\n",
      "Epoch [8/100], Loss: 0.8391\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6752\n",
      "Epoch [11/100], Loss: 0.6193\n",
      "Epoch [12/100], Loss: 0.5683\n",
      "Epoch [13/100], Loss: 0.5285\n",
      "Epoch [14/100], Loss: 0.4903\n",
      "Epoch [15/100], Loss: 0.4593\n",
      "Epoch [16/100], Loss: 0.4323\n",
      "Epoch [17/100], Loss: 0.4063\n",
      "Epoch [18/100], Loss: 0.3844\n",
      "Epoch [19/100], Loss: 0.3660\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2905\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2427\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2130\n",
      "Epoch [34/100], Loss: 0.2056\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1758\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1408\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1258\n",
      "Epoch [57/100], Loss: 0.1237\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.1000\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0840\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0761\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0745\n",
      "Epoch [97/100], Loss: 0.0739\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0721\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.67%\n",
      "Epoch [1/100], Loss: 3.1868\n",
      "Epoch [2/100], Loss: 2.5387\n",
      "Epoch [3/100], Loss: 1.8736\n",
      "Epoch [4/100], Loss: 1.4662\n",
      "Epoch [5/100], Loss: 1.2149\n",
      "Epoch [6/100], Loss: 1.0445\n",
      "Epoch [7/100], Loss: 0.9306\n",
      "Epoch [8/100], Loss: 0.8330\n",
      "Epoch [9/100], Loss: 0.7455\n",
      "Epoch [10/100], Loss: 0.6753\n",
      "Epoch [11/100], Loss: 0.6187\n",
      "Epoch [12/100], Loss: 0.5717\n",
      "Epoch [13/100], Loss: 0.5281\n",
      "Epoch [14/100], Loss: 0.4916\n",
      "Epoch [15/100], Loss: 0.4584\n",
      "Epoch [16/100], Loss: 0.4311\n",
      "Epoch [17/100], Loss: 0.4094\n",
      "Epoch [18/100], Loss: 0.3874\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3022\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2421\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1955\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1807\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1198\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1064\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0923\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0758\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0750\n",
      "Epoch [97/100], Loss: 0.0737\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1909\n",
      "Epoch [2/100], Loss: 2.5258\n",
      "Epoch [3/100], Loss: 1.8808\n",
      "Epoch [4/100], Loss: 1.4600\n",
      "Epoch [5/100], Loss: 1.2077\n",
      "Epoch [6/100], Loss: 1.0434\n",
      "Epoch [7/100], Loss: 0.9305\n",
      "Epoch [8/100], Loss: 0.8313\n",
      "Epoch [9/100], Loss: 0.7456\n",
      "Epoch [10/100], Loss: 0.6785\n",
      "Epoch [11/100], Loss: 0.6195\n",
      "Epoch [12/100], Loss: 0.5702\n",
      "Epoch [13/100], Loss: 0.5292\n",
      "Epoch [14/100], Loss: 0.4956\n",
      "Epoch [15/100], Loss: 0.4613\n",
      "Epoch [16/100], Loss: 0.4325\n",
      "Epoch [17/100], Loss: 0.4082\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3310\n",
      "Epoch [22/100], Loss: 0.3175\n",
      "Epoch [23/100], Loss: 0.3021\n",
      "Epoch [24/100], Loss: 0.2921\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2703\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2510\n",
      "Epoch [29/100], Loss: 0.2419\n",
      "Epoch [30/100], Loss: 0.2329\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2055\n",
      "Epoch [35/100], Loss: 0.1996\n",
      "Epoch [36/100], Loss: 0.1941\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1568\n",
      "Epoch [46/100], Loss: 0.1532\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0914\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0800\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0750\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1941\n",
      "Epoch [2/100], Loss: 2.5390\n",
      "Epoch [3/100], Loss: 1.8794\n",
      "Epoch [4/100], Loss: 1.4653\n",
      "Epoch [5/100], Loss: 1.2106\n",
      "Epoch [6/100], Loss: 1.0459\n",
      "Epoch [7/100], Loss: 0.9321\n",
      "Epoch [8/100], Loss: 0.8364\n",
      "Epoch [9/100], Loss: 0.7490\n",
      "Epoch [10/100], Loss: 0.6800\n",
      "Epoch [11/100], Loss: 0.6232\n",
      "Epoch [12/100], Loss: 0.5689\n",
      "Epoch [13/100], Loss: 0.5270\n",
      "Epoch [14/100], Loss: 0.4939\n",
      "Epoch [15/100], Loss: 0.4600\n",
      "Epoch [16/100], Loss: 0.4326\n",
      "Epoch [17/100], Loss: 0.4085\n",
      "Epoch [18/100], Loss: 0.3866\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3066\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2789\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2509\n",
      "Epoch [29/100], Loss: 0.2409\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2130\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1942\n",
      "Epoch [37/100], Loss: 0.1902\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1804\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1229\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1108\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1025\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0903\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0709\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0709\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1765\n",
      "Epoch [2/100], Loss: 2.5344\n",
      "Epoch [3/100], Loss: 1.8842\n",
      "Epoch [4/100], Loss: 1.4551\n",
      "Epoch [5/100], Loss: 1.2118\n",
      "Epoch [6/100], Loss: 1.0457\n",
      "Epoch [7/100], Loss: 0.9317\n",
      "Epoch [8/100], Loss: 0.8332\n",
      "Epoch [9/100], Loss: 0.7507\n",
      "Epoch [10/100], Loss: 0.6772\n",
      "Epoch [11/100], Loss: 0.6183\n",
      "Epoch [12/100], Loss: 0.5693\n",
      "Epoch [13/100], Loss: 0.5273\n",
      "Epoch [14/100], Loss: 0.4903\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4343\n",
      "Epoch [17/100], Loss: 0.4065\n",
      "Epoch [18/100], Loss: 0.3849\n",
      "Epoch [19/100], Loss: 0.3669\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3177\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2937\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2595\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2271\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2114\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1748\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1637\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1399\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1159\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1096\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1004\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0986\n",
      "Epoch [73/100], Loss: 0.0973\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0826\n",
      "Epoch [87/100], Loss: 0.0812\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0800\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0738\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0723\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.73%\n",
      "Epoch [1/100], Loss: 3.1827\n",
      "Epoch [2/100], Loss: 2.5419\n",
      "Epoch [3/100], Loss: 1.8759\n",
      "Epoch [4/100], Loss: 1.4643\n",
      "Epoch [5/100], Loss: 1.2094\n",
      "Epoch [6/100], Loss: 1.0487\n",
      "Epoch [7/100], Loss: 0.9295\n",
      "Epoch [8/100], Loss: 0.8343\n",
      "Epoch [9/100], Loss: 0.7464\n",
      "Epoch [10/100], Loss: 0.6806\n",
      "Epoch [11/100], Loss: 0.6190\n",
      "Epoch [12/100], Loss: 0.5759\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4922\n",
      "Epoch [15/100], Loss: 0.4611\n",
      "Epoch [16/100], Loss: 0.4334\n",
      "Epoch [17/100], Loss: 0.4084\n",
      "Epoch [18/100], Loss: 0.3836\n",
      "Epoch [19/100], Loss: 0.3659\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3308\n",
      "Epoch [22/100], Loss: 0.3160\n",
      "Epoch [23/100], Loss: 0.3038\n",
      "Epoch [24/100], Loss: 0.2901\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2698\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2511\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2201\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2056\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1839\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1175\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0977\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1875\n",
      "Epoch [2/100], Loss: 2.5394\n",
      "Epoch [3/100], Loss: 1.8860\n",
      "Epoch [4/100], Loss: 1.4583\n",
      "Epoch [5/100], Loss: 1.2091\n",
      "Epoch [6/100], Loss: 1.0476\n",
      "Epoch [7/100], Loss: 0.9280\n",
      "Epoch [8/100], Loss: 0.8315\n",
      "Epoch [9/100], Loss: 0.7506\n",
      "Epoch [10/100], Loss: 0.6772\n",
      "Epoch [11/100], Loss: 0.6192\n",
      "Epoch [12/100], Loss: 0.5737\n",
      "Epoch [13/100], Loss: 0.5278\n",
      "Epoch [14/100], Loss: 0.4906\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4346\n",
      "Epoch [17/100], Loss: 0.4090\n",
      "Epoch [18/100], Loss: 0.3866\n",
      "Epoch [19/100], Loss: 0.3665\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3310\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1158\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0909\n",
      "Epoch [79/100], Loss: 0.0901\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0818\n",
      "Epoch [88/100], Loss: 0.0809\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0766\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.73%\n",
      "Epoch [1/100], Loss: 3.1853\n",
      "Epoch [2/100], Loss: 2.5499\n",
      "Epoch [3/100], Loss: 1.8835\n",
      "Epoch [4/100], Loss: 1.4662\n",
      "Epoch [5/100], Loss: 1.2152\n",
      "Epoch [6/100], Loss: 1.0395\n",
      "Epoch [7/100], Loss: 0.9244\n",
      "Epoch [8/100], Loss: 0.8322\n",
      "Epoch [9/100], Loss: 0.7479\n",
      "Epoch [10/100], Loss: 0.6829\n",
      "Epoch [11/100], Loss: 0.6170\n",
      "Epoch [12/100], Loss: 0.5689\n",
      "Epoch [13/100], Loss: 0.5294\n",
      "Epoch [14/100], Loss: 0.4914\n",
      "Epoch [15/100], Loss: 0.4602\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4081\n",
      "Epoch [18/100], Loss: 0.3849\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2919\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2696\n",
      "Epoch [27/100], Loss: 0.2599\n",
      "Epoch [28/100], Loss: 0.2508\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1409\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1195\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0983\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0769\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.0736\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.73%\n",
      "Epoch [1/100], Loss: 3.1854\n",
      "Epoch [2/100], Loss: 2.5273\n",
      "Epoch [3/100], Loss: 1.8825\n",
      "Epoch [4/100], Loss: 1.4628\n",
      "Epoch [5/100], Loss: 1.2019\n",
      "Epoch [6/100], Loss: 1.0436\n",
      "Epoch [7/100], Loss: 0.9221\n",
      "Epoch [8/100], Loss: 0.8294\n",
      "Epoch [9/100], Loss: 0.7507\n",
      "Epoch [10/100], Loss: 0.6788\n",
      "Epoch [11/100], Loss: 0.6204\n",
      "Epoch [12/100], Loss: 0.5753\n",
      "Epoch [13/100], Loss: 0.5271\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4596\n",
      "Epoch [16/100], Loss: 0.4340\n",
      "Epoch [17/100], Loss: 0.4086\n",
      "Epoch [18/100], Loss: 0.3845\n",
      "Epoch [19/100], Loss: 0.3687\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3189\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2919\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2601\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2131\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1851\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1757\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1217\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1025\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0970\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0880\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0801\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0777\n",
      "Epoch [92/100], Loss: 0.0774\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0753\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0721\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1865\n",
      "Epoch [2/100], Loss: 2.5322\n",
      "Epoch [3/100], Loss: 1.8900\n",
      "Epoch [4/100], Loss: 1.4589\n",
      "Epoch [5/100], Loss: 1.2094\n",
      "Epoch [6/100], Loss: 1.0451\n",
      "Epoch [7/100], Loss: 0.9292\n",
      "Epoch [8/100], Loss: 0.8323\n",
      "Epoch [9/100], Loss: 0.7487\n",
      "Epoch [10/100], Loss: 0.6838\n",
      "Epoch [11/100], Loss: 0.6230\n",
      "Epoch [12/100], Loss: 0.5713\n",
      "Epoch [13/100], Loss: 0.5276\n",
      "Epoch [14/100], Loss: 0.4924\n",
      "Epoch [15/100], Loss: 0.4608\n",
      "Epoch [16/100], Loss: 0.4329\n",
      "Epoch [17/100], Loss: 0.4078\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3649\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3335\n",
      "Epoch [22/100], Loss: 0.3175\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2604\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2344\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2006\n",
      "Epoch [36/100], Loss: 0.1956\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1398\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1308\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1073\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0860\n",
      "Epoch [84/100], Loss: 0.0840\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0768\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0745\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1902\n",
      "Epoch [2/100], Loss: 2.5392\n",
      "Epoch [3/100], Loss: 1.8759\n",
      "Epoch [4/100], Loss: 1.4565\n",
      "Epoch [5/100], Loss: 1.2122\n",
      "Epoch [6/100], Loss: 1.0523\n",
      "Epoch [7/100], Loss: 0.9292\n",
      "Epoch [8/100], Loss: 0.8306\n",
      "Epoch [9/100], Loss: 0.7494\n",
      "Epoch [10/100], Loss: 0.6789\n",
      "Epoch [11/100], Loss: 0.6210\n",
      "Epoch [12/100], Loss: 0.5710\n",
      "Epoch [13/100], Loss: 0.5295\n",
      "Epoch [14/100], Loss: 0.4915\n",
      "Epoch [15/100], Loss: 0.4589\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4100\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3163\n",
      "Epoch [23/100], Loss: 0.3036\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2345\n",
      "Epoch [31/100], Loss: 0.2267\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1853\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1719\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1175\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1122\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0767\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.84%\n",
      "Epoch [1/100], Loss: 3.1835\n",
      "Epoch [2/100], Loss: 2.5446\n",
      "Epoch [3/100], Loss: 1.8889\n",
      "Epoch [4/100], Loss: 1.4582\n",
      "Epoch [5/100], Loss: 1.2101\n",
      "Epoch [6/100], Loss: 1.0512\n",
      "Epoch [7/100], Loss: 0.9322\n",
      "Epoch [8/100], Loss: 0.8346\n",
      "Epoch [9/100], Loss: 0.7513\n",
      "Epoch [10/100], Loss: 0.6814\n",
      "Epoch [11/100], Loss: 0.6197\n",
      "Epoch [12/100], Loss: 0.5694\n",
      "Epoch [13/100], Loss: 0.5291\n",
      "Epoch [14/100], Loss: 0.4927\n",
      "Epoch [15/100], Loss: 0.4594\n",
      "Epoch [16/100], Loss: 0.4307\n",
      "Epoch [17/100], Loss: 0.4066\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3178\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2904\n",
      "Epoch [25/100], Loss: 0.2788\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2584\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2269\n",
      "Epoch [32/100], Loss: 0.2197\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1636\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1195\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1150\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1074\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0750\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1936\n",
      "Epoch [2/100], Loss: 2.5185\n",
      "Epoch [3/100], Loss: 1.8779\n",
      "Epoch [4/100], Loss: 1.4612\n",
      "Epoch [5/100], Loss: 1.2111\n",
      "Epoch [6/100], Loss: 1.0455\n",
      "Epoch [7/100], Loss: 0.9342\n",
      "Epoch [8/100], Loss: 0.8337\n",
      "Epoch [9/100], Loss: 0.7491\n",
      "Epoch [10/100], Loss: 0.6793\n",
      "Epoch [11/100], Loss: 0.6218\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5296\n",
      "Epoch [14/100], Loss: 0.4935\n",
      "Epoch [15/100], Loss: 0.4601\n",
      "Epoch [16/100], Loss: 0.4325\n",
      "Epoch [17/100], Loss: 0.4067\n",
      "Epoch [18/100], Loss: 0.3845\n",
      "Epoch [19/100], Loss: 0.3669\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3329\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1357\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0761\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0735\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1975\n",
      "Epoch [2/100], Loss: 2.5281\n",
      "Epoch [3/100], Loss: 1.8808\n",
      "Epoch [4/100], Loss: 1.4706\n",
      "Epoch [5/100], Loss: 1.2091\n",
      "Epoch [6/100], Loss: 1.0471\n",
      "Epoch [7/100], Loss: 0.9270\n",
      "Epoch [8/100], Loss: 0.8329\n",
      "Epoch [9/100], Loss: 0.7476\n",
      "Epoch [10/100], Loss: 0.6771\n",
      "Epoch [11/100], Loss: 0.6207\n",
      "Epoch [12/100], Loss: 0.5696\n",
      "Epoch [13/100], Loss: 0.5294\n",
      "Epoch [14/100], Loss: 0.4942\n",
      "Epoch [15/100], Loss: 0.4609\n",
      "Epoch [16/100], Loss: 0.4330\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3870\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3043\n",
      "Epoch [24/100], Loss: 0.2932\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2139\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1471\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1309\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1186\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0948\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0847\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1866\n",
      "Epoch [2/100], Loss: 2.5399\n",
      "Epoch [3/100], Loss: 1.8843\n",
      "Epoch [4/100], Loss: 1.4648\n",
      "Epoch [5/100], Loss: 1.2147\n",
      "Epoch [6/100], Loss: 1.0536\n",
      "Epoch [7/100], Loss: 0.9295\n",
      "Epoch [8/100], Loss: 0.8290\n",
      "Epoch [9/100], Loss: 0.7488\n",
      "Epoch [10/100], Loss: 0.6803\n",
      "Epoch [11/100], Loss: 0.6178\n",
      "Epoch [12/100], Loss: 0.5728\n",
      "Epoch [13/100], Loss: 0.5285\n",
      "Epoch [14/100], Loss: 0.4929\n",
      "Epoch [15/100], Loss: 0.4611\n",
      "Epoch [16/100], Loss: 0.4328\n",
      "Epoch [17/100], Loss: 0.4110\n",
      "Epoch [18/100], Loss: 0.3860\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3330\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2927\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2584\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2116\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.1996\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1626\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1530\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1306\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1175\n",
      "Epoch [61/100], Loss: 0.1149\n",
      "Epoch [62/100], Loss: 0.1142\n",
      "Epoch [63/100], Loss: 0.1122\n",
      "Epoch [64/100], Loss: 0.1104\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0951\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0791\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0762\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0738\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1877\n",
      "Epoch [2/100], Loss: 2.5297\n",
      "Epoch [3/100], Loss: 1.8863\n",
      "Epoch [4/100], Loss: 1.4607\n",
      "Epoch [5/100], Loss: 1.2147\n",
      "Epoch [6/100], Loss: 1.0488\n",
      "Epoch [7/100], Loss: 0.9242\n",
      "Epoch [8/100], Loss: 0.8297\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6736\n",
      "Epoch [11/100], Loss: 0.6182\n",
      "Epoch [12/100], Loss: 0.5735\n",
      "Epoch [13/100], Loss: 0.5263\n",
      "Epoch [14/100], Loss: 0.4910\n",
      "Epoch [15/100], Loss: 0.4608\n",
      "Epoch [16/100], Loss: 0.4334\n",
      "Epoch [17/100], Loss: 0.4083\n",
      "Epoch [18/100], Loss: 0.3845\n",
      "Epoch [19/100], Loss: 0.3647\n",
      "Epoch [20/100], Loss: 0.3490\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3177\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2436\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2268\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2070\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1808\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1282\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1057\n",
      "Epoch [68/100], Loss: 0.1040\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0977\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0871\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0828\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1821\n",
      "Epoch [2/100], Loss: 2.5332\n",
      "Epoch [3/100], Loss: 1.8865\n",
      "Epoch [4/100], Loss: 1.4669\n",
      "Epoch [5/100], Loss: 1.2139\n",
      "Epoch [6/100], Loss: 1.0427\n",
      "Epoch [7/100], Loss: 0.9291\n",
      "Epoch [8/100], Loss: 0.8311\n",
      "Epoch [9/100], Loss: 0.7471\n",
      "Epoch [10/100], Loss: 0.6791\n",
      "Epoch [11/100], Loss: 0.6213\n",
      "Epoch [12/100], Loss: 0.5704\n",
      "Epoch [13/100], Loss: 0.5281\n",
      "Epoch [14/100], Loss: 0.4920\n",
      "Epoch [15/100], Loss: 0.4603\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3664\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3175\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2704\n",
      "Epoch [27/100], Loss: 0.2602\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2268\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2133\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.1998\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0934\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0910\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0829\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Epoch [91/100], Loss: 0.0783\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0722\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1858\n",
      "Epoch [2/100], Loss: 2.5358\n",
      "Epoch [3/100], Loss: 1.8787\n",
      "Epoch [4/100], Loss: 1.4638\n",
      "Epoch [5/100], Loss: 1.2117\n",
      "Epoch [6/100], Loss: 1.0475\n",
      "Epoch [7/100], Loss: 0.9300\n",
      "Epoch [8/100], Loss: 0.8341\n",
      "Epoch [9/100], Loss: 0.7476\n",
      "Epoch [10/100], Loss: 0.6763\n",
      "Epoch [11/100], Loss: 0.6193\n",
      "Epoch [12/100], Loss: 0.5724\n",
      "Epoch [13/100], Loss: 0.5279\n",
      "Epoch [14/100], Loss: 0.4938\n",
      "Epoch [15/100], Loss: 0.4609\n",
      "Epoch [16/100], Loss: 0.4310\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3845\n",
      "Epoch [19/100], Loss: 0.3648\n",
      "Epoch [20/100], Loss: 0.3478\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3189\n",
      "Epoch [23/100], Loss: 0.3045\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2700\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2412\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2128\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1520\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1416\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1306\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1229\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1011\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0802\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0766\n",
      "Epoch [94/100], Loss: 0.0757\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.82%\n",
      "Epoch [1/100], Loss: 3.1876\n",
      "Epoch [2/100], Loss: 2.5333\n",
      "Epoch [3/100], Loss: 1.8866\n",
      "Epoch [4/100], Loss: 1.4704\n",
      "Epoch [5/100], Loss: 1.2173\n",
      "Epoch [6/100], Loss: 1.0453\n",
      "Epoch [7/100], Loss: 0.9328\n",
      "Epoch [8/100], Loss: 0.8312\n",
      "Epoch [9/100], Loss: 0.7483\n",
      "Epoch [10/100], Loss: 0.6769\n",
      "Epoch [11/100], Loss: 0.6221\n",
      "Epoch [12/100], Loss: 0.5692\n",
      "Epoch [13/100], Loss: 0.5293\n",
      "Epoch [14/100], Loss: 0.4906\n",
      "Epoch [15/100], Loss: 0.4594\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4065\n",
      "Epoch [18/100], Loss: 0.3855\n",
      "Epoch [19/100], Loss: 0.3666\n",
      "Epoch [20/100], Loss: 0.3500\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3029\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2696\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2429\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2198\n",
      "Epoch [33/100], Loss: 0.2117\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1800\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1626\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1529\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1208\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1180\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1107\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1064\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1013\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0933\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0856\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0821\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0752\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.74%\n",
      "Epoch [1/100], Loss: 3.1815\n",
      "Epoch [2/100], Loss: 2.5368\n",
      "Epoch [3/100], Loss: 1.8816\n",
      "Epoch [4/100], Loss: 1.4656\n",
      "Epoch [5/100], Loss: 1.2031\n",
      "Epoch [6/100], Loss: 1.0444\n",
      "Epoch [7/100], Loss: 0.9290\n",
      "Epoch [8/100], Loss: 0.8292\n",
      "Epoch [9/100], Loss: 0.7483\n",
      "Epoch [10/100], Loss: 0.6779\n",
      "Epoch [11/100], Loss: 0.6185\n",
      "Epoch [12/100], Loss: 0.5668\n",
      "Epoch [13/100], Loss: 0.5287\n",
      "Epoch [14/100], Loss: 0.4921\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4324\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3868\n",
      "Epoch [19/100], Loss: 0.3658\n",
      "Epoch [20/100], Loss: 0.3482\n",
      "Epoch [21/100], Loss: 0.3323\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2195\n",
      "Epoch [33/100], Loss: 0.2133\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1890\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1804\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1399\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1346\n",
      "Epoch [53/100], Loss: 0.1329\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1106\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0777\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0757\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0730\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1863\n",
      "Epoch [2/100], Loss: 2.5290\n",
      "Epoch [3/100], Loss: 1.8809\n",
      "Epoch [4/100], Loss: 1.4718\n",
      "Epoch [5/100], Loss: 1.2157\n",
      "Epoch [6/100], Loss: 1.0478\n",
      "Epoch [7/100], Loss: 0.9301\n",
      "Epoch [8/100], Loss: 0.8314\n",
      "Epoch [9/100], Loss: 0.7474\n",
      "Epoch [10/100], Loss: 0.6788\n",
      "Epoch [11/100], Loss: 0.6227\n",
      "Epoch [12/100], Loss: 0.5693\n",
      "Epoch [13/100], Loss: 0.5300\n",
      "Epoch [14/100], Loss: 0.4930\n",
      "Epoch [15/100], Loss: 0.4593\n",
      "Epoch [16/100], Loss: 0.4308\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3464\n",
      "Epoch [21/100], Loss: 0.3305\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3049\n",
      "Epoch [24/100], Loss: 0.2904\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2513\n",
      "Epoch [29/100], Loss: 0.2436\n",
      "Epoch [30/100], Loss: 0.2342\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2212\n",
      "Epoch [33/100], Loss: 0.2135\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1757\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1143\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1080\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1033\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0977\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0958\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0757\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0738\n",
      "Epoch [97/100], Loss: 0.0736\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1882\n",
      "Epoch [2/100], Loss: 2.5320\n",
      "Epoch [3/100], Loss: 1.8810\n",
      "Epoch [4/100], Loss: 1.4660\n",
      "Epoch [5/100], Loss: 1.2179\n",
      "Epoch [6/100], Loss: 1.0548\n",
      "Epoch [7/100], Loss: 0.9317\n",
      "Epoch [8/100], Loss: 0.8313\n",
      "Epoch [9/100], Loss: 0.7498\n",
      "Epoch [10/100], Loss: 0.6822\n",
      "Epoch [11/100], Loss: 0.6183\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5280\n",
      "Epoch [14/100], Loss: 0.4896\n",
      "Epoch [15/100], Loss: 0.4595\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4066\n",
      "Epoch [18/100], Loss: 0.3869\n",
      "Epoch [19/100], Loss: 0.3637\n",
      "Epoch [20/100], Loss: 0.3466\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3161\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2905\n",
      "Epoch [25/100], Loss: 0.2809\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2068\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1942\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1847\n",
      "Epoch [39/100], Loss: 0.1805\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1503\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1409\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1333\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1169\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1114\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0998\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0860\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0837\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1903\n",
      "Epoch [2/100], Loss: 2.5245\n",
      "Epoch [3/100], Loss: 1.8761\n",
      "Epoch [4/100], Loss: 1.4581\n",
      "Epoch [5/100], Loss: 1.2146\n",
      "Epoch [6/100], Loss: 1.0501\n",
      "Epoch [7/100], Loss: 0.9248\n",
      "Epoch [8/100], Loss: 0.8334\n",
      "Epoch [9/100], Loss: 0.7481\n",
      "Epoch [10/100], Loss: 0.6788\n",
      "Epoch [11/100], Loss: 0.6199\n",
      "Epoch [12/100], Loss: 0.5699\n",
      "Epoch [13/100], Loss: 0.5278\n",
      "Epoch [14/100], Loss: 0.4924\n",
      "Epoch [15/100], Loss: 0.4598\n",
      "Epoch [16/100], Loss: 0.4314\n",
      "Epoch [17/100], Loss: 0.4070\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3668\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3322\n",
      "Epoch [22/100], Loss: 0.3177\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2917\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2703\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2265\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1838\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1138\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1033\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1012\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0951\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1831\n",
      "Epoch [2/100], Loss: 2.5400\n",
      "Epoch [3/100], Loss: 1.8876\n",
      "Epoch [4/100], Loss: 1.4560\n",
      "Epoch [5/100], Loss: 1.2097\n",
      "Epoch [6/100], Loss: 1.0493\n",
      "Epoch [7/100], Loss: 0.9313\n",
      "Epoch [8/100], Loss: 0.8296\n",
      "Epoch [9/100], Loss: 0.7480\n",
      "Epoch [10/100], Loss: 0.6781\n",
      "Epoch [11/100], Loss: 0.6196\n",
      "Epoch [12/100], Loss: 0.5704\n",
      "Epoch [13/100], Loss: 0.5316\n",
      "Epoch [14/100], Loss: 0.4949\n",
      "Epoch [15/100], Loss: 0.4607\n",
      "Epoch [16/100], Loss: 0.4308\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3866\n",
      "Epoch [19/100], Loss: 0.3642\n",
      "Epoch [20/100], Loss: 0.3469\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3027\n",
      "Epoch [24/100], Loss: 0.2905\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2422\n",
      "Epoch [30/100], Loss: 0.2336\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2195\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1889\n",
      "Epoch [38/100], Loss: 0.1855\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1663\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1571\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1488\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1348\n",
      "Epoch [53/100], Loss: 0.1321\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1139\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0945\n",
      "Epoch [76/100], Loss: 0.0926\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0914\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0891\n",
      "Epoch [81/100], Loss: 0.0877\n",
      "Epoch [82/100], Loss: 0.0868\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0766\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0735\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.73%\n",
      "Epoch [1/100], Loss: 3.1856\n",
      "Epoch [2/100], Loss: 2.5318\n",
      "Epoch [3/100], Loss: 1.8853\n",
      "Epoch [4/100], Loss: 1.4563\n",
      "Epoch [5/100], Loss: 1.2088\n",
      "Epoch [6/100], Loss: 1.0504\n",
      "Epoch [7/100], Loss: 0.9300\n",
      "Epoch [8/100], Loss: 0.8292\n",
      "Epoch [9/100], Loss: 0.7471\n",
      "Epoch [10/100], Loss: 0.6760\n",
      "Epoch [11/100], Loss: 0.6205\n",
      "Epoch [12/100], Loss: 0.5727\n",
      "Epoch [13/100], Loss: 0.5285\n",
      "Epoch [14/100], Loss: 0.4918\n",
      "Epoch [15/100], Loss: 0.4610\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4073\n",
      "Epoch [18/100], Loss: 0.3872\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2786\n",
      "Epoch [26/100], Loss: 0.2694\n",
      "Epoch [27/100], Loss: 0.2605\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2345\n",
      "Epoch [31/100], Loss: 0.2259\n",
      "Epoch [32/100], Loss: 0.2196\n",
      "Epoch [33/100], Loss: 0.2127\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1759\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1413\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1238\n",
      "Epoch [58/100], Loss: 0.1208\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1091\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1041\n",
      "Epoch [69/100], Loss: 0.1039\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0767\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1836\n",
      "Epoch [2/100], Loss: 2.5310\n",
      "Epoch [3/100], Loss: 1.8758\n",
      "Epoch [4/100], Loss: 1.4621\n",
      "Epoch [5/100], Loss: 1.2092\n",
      "Epoch [6/100], Loss: 1.0479\n",
      "Epoch [7/100], Loss: 0.9359\n",
      "Epoch [8/100], Loss: 0.8363\n",
      "Epoch [9/100], Loss: 0.7475\n",
      "Epoch [10/100], Loss: 0.6755\n",
      "Epoch [11/100], Loss: 0.6198\n",
      "Epoch [12/100], Loss: 0.5703\n",
      "Epoch [13/100], Loss: 0.5277\n",
      "Epoch [14/100], Loss: 0.4912\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4326\n",
      "Epoch [17/100], Loss: 0.4063\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3652\n",
      "Epoch [20/100], Loss: 0.3493\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3164\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2588\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1801\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1566\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1382\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1273\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1215\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1060\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0858\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.71%\n",
      "Epoch [1/100], Loss: 3.1840\n",
      "Epoch [2/100], Loss: 2.5367\n",
      "Epoch [3/100], Loss: 1.8934\n",
      "Epoch [4/100], Loss: 1.4621\n",
      "Epoch [5/100], Loss: 1.2163\n",
      "Epoch [6/100], Loss: 1.0425\n",
      "Epoch [7/100], Loss: 0.9284\n",
      "Epoch [8/100], Loss: 0.8310\n",
      "Epoch [9/100], Loss: 0.7487\n",
      "Epoch [10/100], Loss: 0.6790\n",
      "Epoch [11/100], Loss: 0.6178\n",
      "Epoch [12/100], Loss: 0.5692\n",
      "Epoch [13/100], Loss: 0.5281\n",
      "Epoch [14/100], Loss: 0.4897\n",
      "Epoch [15/100], Loss: 0.4593\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3494\n",
      "Epoch [21/100], Loss: 0.3310\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2695\n",
      "Epoch [27/100], Loss: 0.2595\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2131\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1955\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1850\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1562\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1408\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1241\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1176\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0886\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1917\n",
      "Epoch [2/100], Loss: 2.5348\n",
      "Epoch [3/100], Loss: 1.8782\n",
      "Epoch [4/100], Loss: 1.4663\n",
      "Epoch [5/100], Loss: 1.2137\n",
      "Epoch [6/100], Loss: 1.0448\n",
      "Epoch [7/100], Loss: 0.9320\n",
      "Epoch [8/100], Loss: 0.8329\n",
      "Epoch [9/100], Loss: 0.7502\n",
      "Epoch [10/100], Loss: 0.6772\n",
      "Epoch [11/100], Loss: 0.6209\n",
      "Epoch [12/100], Loss: 0.5721\n",
      "Epoch [13/100], Loss: 0.5297\n",
      "Epoch [14/100], Loss: 0.4935\n",
      "Epoch [15/100], Loss: 0.4590\n",
      "Epoch [16/100], Loss: 0.4316\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3040\n",
      "Epoch [24/100], Loss: 0.2913\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2075\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1801\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1714\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1595\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1464\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1379\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0935\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0751\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1886\n",
      "Epoch [2/100], Loss: 2.5314\n",
      "Epoch [3/100], Loss: 1.8855\n",
      "Epoch [4/100], Loss: 1.4709\n",
      "Epoch [5/100], Loss: 1.2115\n",
      "Epoch [6/100], Loss: 1.0548\n",
      "Epoch [7/100], Loss: 0.9340\n",
      "Epoch [8/100], Loss: 0.8328\n",
      "Epoch [9/100], Loss: 0.7453\n",
      "Epoch [10/100], Loss: 0.6778\n",
      "Epoch [11/100], Loss: 0.6196\n",
      "Epoch [12/100], Loss: 0.5702\n",
      "Epoch [13/100], Loss: 0.5281\n",
      "Epoch [14/100], Loss: 0.4923\n",
      "Epoch [15/100], Loss: 0.4604\n",
      "Epoch [16/100], Loss: 0.4315\n",
      "Epoch [17/100], Loss: 0.4071\n",
      "Epoch [18/100], Loss: 0.3868\n",
      "Epoch [19/100], Loss: 0.3653\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3306\n",
      "Epoch [22/100], Loss: 0.3177\n",
      "Epoch [23/100], Loss: 0.3056\n",
      "Epoch [24/100], Loss: 0.2900\n",
      "Epoch [25/100], Loss: 0.2784\n",
      "Epoch [26/100], Loss: 0.2686\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2497\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2342\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.1996\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1852\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1714\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1553\n",
      "Epoch [46/100], Loss: 0.1529\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0938\n",
      "Epoch [76/100], Loss: 0.0936\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0897\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0753\n",
      "Epoch [96/100], Loss: 0.0743\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0728\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.83%\n",
      "Epoch [1/100], Loss: 3.1893\n",
      "Epoch [2/100], Loss: 2.5362\n",
      "Epoch [3/100], Loss: 1.8921\n",
      "Epoch [4/100], Loss: 1.4676\n",
      "Epoch [5/100], Loss: 1.2132\n",
      "Epoch [6/100], Loss: 1.0512\n",
      "Epoch [7/100], Loss: 0.9294\n",
      "Epoch [8/100], Loss: 0.8306\n",
      "Epoch [9/100], Loss: 0.7478\n",
      "Epoch [10/100], Loss: 0.6814\n",
      "Epoch [11/100], Loss: 0.6197\n",
      "Epoch [12/100], Loss: 0.5686\n",
      "Epoch [13/100], Loss: 0.5305\n",
      "Epoch [14/100], Loss: 0.4911\n",
      "Epoch [15/100], Loss: 0.4592\n",
      "Epoch [16/100], Loss: 0.4313\n",
      "Epoch [17/100], Loss: 0.4081\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3657\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3308\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2803\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2195\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2068\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1309\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1261\n",
      "Epoch [57/100], Loss: 0.1236\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1179\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0947\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0921\n",
      "Epoch [78/100], Loss: 0.0916\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0810\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0791\n",
      "Epoch [91/100], Loss: 0.0788\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0761\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 5000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/100], Loss: 3.1805\n",
      "Epoch [2/100], Loss: 2.5380\n",
      "Epoch [3/100], Loss: 1.8829\n",
      "Epoch [4/100], Loss: 1.4619\n",
      "Epoch [5/100], Loss: 1.2138\n",
      "Epoch [6/100], Loss: 1.0483\n",
      "Epoch [7/100], Loss: 0.9269\n",
      "Epoch [8/100], Loss: 0.8340\n",
      "Epoch [9/100], Loss: 0.7470\n",
      "Epoch [10/100], Loss: 0.6774\n",
      "Epoch [11/100], Loss: 0.6237\n",
      "Epoch [12/100], Loss: 0.5687\n",
      "Epoch [13/100], Loss: 0.5280\n",
      "Epoch [14/100], Loss: 0.4923\n",
      "Epoch [15/100], Loss: 0.4590\n",
      "Epoch [16/100], Loss: 0.4314\n",
      "Epoch [17/100], Loss: 0.4082\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3656\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3330\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3025\n",
      "Epoch [24/100], Loss: 0.2919\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2517\n",
      "Epoch [29/100], Loss: 0.2424\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2261\n",
      "Epoch [32/100], Loss: 0.2183\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1957\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1805\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1705\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1284\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1238\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1188\n",
      "Epoch [60/100], Loss: 0.1181\n",
      "Epoch [61/100], Loss: 0.1158\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1122\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0912\n",
      "Epoch [79/100], Loss: 0.0899\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0866\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0809\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0745\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1833\n",
      "Epoch [2/100], Loss: 2.5286\n",
      "Epoch [3/100], Loss: 1.8722\n",
      "Epoch [4/100], Loss: 1.4787\n",
      "Epoch [5/100], Loss: 1.2179\n",
      "Epoch [6/100], Loss: 1.0543\n",
      "Epoch [7/100], Loss: 0.9362\n",
      "Epoch [8/100], Loss: 0.8296\n",
      "Epoch [9/100], Loss: 0.7477\n",
      "Epoch [10/100], Loss: 0.6796\n",
      "Epoch [11/100], Loss: 0.6177\n",
      "Epoch [12/100], Loss: 0.5705\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4908\n",
      "Epoch [15/100], Loss: 0.4608\n",
      "Epoch [16/100], Loss: 0.4331\n",
      "Epoch [17/100], Loss: 0.4070\n",
      "Epoch [18/100], Loss: 0.3859\n",
      "Epoch [19/100], Loss: 0.3651\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2585\n",
      "Epoch [28/100], Loss: 0.2493\n",
      "Epoch [29/100], Loss: 0.2421\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2080\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1954\n",
      "Epoch [37/100], Loss: 0.1901\n",
      "Epoch [38/100], Loss: 0.1851\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1664\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1568\n",
      "Epoch [46/100], Loss: 0.1532\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1405\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1088\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0769\n",
      "Epoch [93/100], Loss: 0.0772\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0745\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.74%\n",
      "Epoch [1/100], Loss: 3.1804\n",
      "Epoch [2/100], Loss: 2.5305\n",
      "Epoch [3/100], Loss: 1.8851\n",
      "Epoch [4/100], Loss: 1.4779\n",
      "Epoch [5/100], Loss: 1.2168\n",
      "Epoch [6/100], Loss: 1.0484\n",
      "Epoch [7/100], Loss: 0.9328\n",
      "Epoch [8/100], Loss: 0.8309\n",
      "Epoch [9/100], Loss: 0.7465\n",
      "Epoch [10/100], Loss: 0.6791\n",
      "Epoch [11/100], Loss: 0.6202\n",
      "Epoch [12/100], Loss: 0.5699\n",
      "Epoch [13/100], Loss: 0.5264\n",
      "Epoch [14/100], Loss: 0.4917\n",
      "Epoch [15/100], Loss: 0.4596\n",
      "Epoch [16/100], Loss: 0.4311\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3863\n",
      "Epoch [19/100], Loss: 0.3655\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3181\n",
      "Epoch [23/100], Loss: 0.3027\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2328\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2067\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1901\n",
      "Epoch [38/100], Loss: 0.1848\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1435\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1237\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1028\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0990\n",
      "Epoch [73/100], Loss: 0.0972\n",
      "Epoch [74/100], Loss: 0.0960\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0932\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0883\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0737\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0721\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.81%\n",
      "Epoch [1/100], Loss: 3.1838\n",
      "Epoch [2/100], Loss: 2.5288\n",
      "Epoch [3/100], Loss: 1.8807\n",
      "Epoch [4/100], Loss: 1.4642\n",
      "Epoch [5/100], Loss: 1.2216\n",
      "Epoch [6/100], Loss: 1.0468\n",
      "Epoch [7/100], Loss: 0.9292\n",
      "Epoch [8/100], Loss: 0.8321\n",
      "Epoch [9/100], Loss: 0.7495\n",
      "Epoch [10/100], Loss: 0.6803\n",
      "Epoch [11/100], Loss: 0.6182\n",
      "Epoch [12/100], Loss: 0.5705\n",
      "Epoch [13/100], Loss: 0.5263\n",
      "Epoch [14/100], Loss: 0.4899\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4305\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3658\n",
      "Epoch [20/100], Loss: 0.3466\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3174\n",
      "Epoch [23/100], Loss: 0.3030\n",
      "Epoch [24/100], Loss: 0.2918\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2698\n",
      "Epoch [27/100], Loss: 0.2598\n",
      "Epoch [28/100], Loss: 0.2492\n",
      "Epoch [29/100], Loss: 0.2430\n",
      "Epoch [30/100], Loss: 0.2345\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1897\n",
      "Epoch [38/100], Loss: 0.1839\n",
      "Epoch [39/100], Loss: 0.1793\n",
      "Epoch [40/100], Loss: 0.1754\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1468\n",
      "Epoch [49/100], Loss: 0.1440\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0876\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.74%\n",
      "Epoch [1/100], Loss: 3.1906\n",
      "Epoch [2/100], Loss: 2.5282\n",
      "Epoch [3/100], Loss: 1.8799\n",
      "Epoch [4/100], Loss: 1.4594\n",
      "Epoch [5/100], Loss: 1.2121\n",
      "Epoch [6/100], Loss: 1.0435\n",
      "Epoch [7/100], Loss: 0.9256\n",
      "Epoch [8/100], Loss: 0.8365\n",
      "Epoch [9/100], Loss: 0.7482\n",
      "Epoch [10/100], Loss: 0.6784\n",
      "Epoch [11/100], Loss: 0.6204\n",
      "Epoch [12/100], Loss: 0.5709\n",
      "Epoch [13/100], Loss: 0.5279\n",
      "Epoch [14/100], Loss: 0.4918\n",
      "Epoch [15/100], Loss: 0.4606\n",
      "Epoch [16/100], Loss: 0.4311\n",
      "Epoch [17/100], Loss: 0.4075\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3664\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3186\n",
      "Epoch [23/100], Loss: 0.3047\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2499\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2195\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1434\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1038\n",
      "Epoch [69/100], Loss: 0.1022\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.76%\n",
      "Epoch [1/100], Loss: 3.1865\n",
      "Epoch [2/100], Loss: 2.5327\n",
      "Epoch [3/100], Loss: 1.8827\n",
      "Epoch [4/100], Loss: 1.4633\n",
      "Epoch [5/100], Loss: 1.2089\n",
      "Epoch [6/100], Loss: 1.0474\n",
      "Epoch [7/100], Loss: 0.9289\n",
      "Epoch [8/100], Loss: 0.8329\n",
      "Epoch [9/100], Loss: 0.7450\n",
      "Epoch [10/100], Loss: 0.6790\n",
      "Epoch [11/100], Loss: 0.6220\n",
      "Epoch [12/100], Loss: 0.5717\n",
      "Epoch [13/100], Loss: 0.5284\n",
      "Epoch [14/100], Loss: 0.4917\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4310\n",
      "Epoch [17/100], Loss: 0.4064\n",
      "Epoch [18/100], Loss: 0.3851\n",
      "Epoch [19/100], Loss: 0.3665\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3316\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2804\n",
      "Epoch [26/100], Loss: 0.2685\n",
      "Epoch [27/100], Loss: 0.2606\n",
      "Epoch [28/100], Loss: 0.2492\n",
      "Epoch [29/100], Loss: 0.2421\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2265\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1715\n",
      "Epoch [42/100], Loss: 0.1680\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1565\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1069\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0875\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0709\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0709\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1843\n",
      "Epoch [2/100], Loss: 2.5298\n",
      "Epoch [3/100], Loss: 1.8871\n",
      "Epoch [4/100], Loss: 1.4662\n",
      "Epoch [5/100], Loss: 1.2131\n",
      "Epoch [6/100], Loss: 1.0479\n",
      "Epoch [7/100], Loss: 0.9307\n",
      "Epoch [8/100], Loss: 0.8302\n",
      "Epoch [9/100], Loss: 0.7474\n",
      "Epoch [10/100], Loss: 0.6760\n",
      "Epoch [11/100], Loss: 0.6189\n",
      "Epoch [12/100], Loss: 0.5707\n",
      "Epoch [13/100], Loss: 0.5286\n",
      "Epoch [14/100], Loss: 0.4923\n",
      "Epoch [15/100], Loss: 0.4610\n",
      "Epoch [16/100], Loss: 0.4321\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3666\n",
      "Epoch [20/100], Loss: 0.3484\n",
      "Epoch [21/100], Loss: 0.3327\n",
      "Epoch [22/100], Loss: 0.3178\n",
      "Epoch [23/100], Loss: 0.3044\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2697\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2425\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2061\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1712\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1596\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1497\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1295\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0784\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0759\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.74%\n",
      "Epoch [1/100], Loss: 3.1888\n",
      "Epoch [2/100], Loss: 2.5251\n",
      "Epoch [3/100], Loss: 1.8797\n",
      "Epoch [4/100], Loss: 1.4562\n",
      "Epoch [5/100], Loss: 1.2118\n",
      "Epoch [6/100], Loss: 1.0458\n",
      "Epoch [7/100], Loss: 0.9299\n",
      "Epoch [8/100], Loss: 0.8316\n",
      "Epoch [9/100], Loss: 0.7480\n",
      "Epoch [10/100], Loss: 0.6791\n",
      "Epoch [11/100], Loss: 0.6182\n",
      "Epoch [12/100], Loss: 0.5694\n",
      "Epoch [13/100], Loss: 0.5290\n",
      "Epoch [14/100], Loss: 0.4906\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4331\n",
      "Epoch [17/100], Loss: 0.4062\n",
      "Epoch [18/100], Loss: 0.3863\n",
      "Epoch [19/100], Loss: 0.3664\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3167\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2923\n",
      "Epoch [25/100], Loss: 0.2803\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2597\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2341\n",
      "Epoch [31/100], Loss: 0.2256\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2062\n",
      "Epoch [35/100], Loss: 0.1995\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1888\n",
      "Epoch [38/100], Loss: 0.1845\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1555\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1384\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1259\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1218\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1102\n",
      "Epoch [65/100], Loss: 0.1086\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0999\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0896\n",
      "Epoch [80/100], Loss: 0.0888\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.83%\n",
      "Epoch [1/100], Loss: 3.1809\n",
      "Epoch [2/100], Loss: 2.5328\n",
      "Epoch [3/100], Loss: 1.8814\n",
      "Epoch [4/100], Loss: 1.4659\n",
      "Epoch [5/100], Loss: 1.2119\n",
      "Epoch [6/100], Loss: 1.0450\n",
      "Epoch [7/100], Loss: 0.9291\n",
      "Epoch [8/100], Loss: 0.8309\n",
      "Epoch [9/100], Loss: 0.7495\n",
      "Epoch [10/100], Loss: 0.6796\n",
      "Epoch [11/100], Loss: 0.6176\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4901\n",
      "Epoch [15/100], Loss: 0.4610\n",
      "Epoch [16/100], Loss: 0.4316\n",
      "Epoch [17/100], Loss: 0.4055\n",
      "Epoch [18/100], Loss: 0.3889\n",
      "Epoch [19/100], Loss: 0.3685\n",
      "Epoch [20/100], Loss: 0.3500\n",
      "Epoch [21/100], Loss: 0.3337\n",
      "Epoch [22/100], Loss: 0.3184\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2798\n",
      "Epoch [26/100], Loss: 0.2698\n",
      "Epoch [27/100], Loss: 0.2603\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2422\n",
      "Epoch [30/100], Loss: 0.2340\n",
      "Epoch [31/100], Loss: 0.2265\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1958\n",
      "Epoch [37/100], Loss: 0.1889\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1714\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1080\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1024\n",
      "Epoch [70/100], Loss: 0.1010\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0951\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0848\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0743\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1859\n",
      "Epoch [2/100], Loss: 2.5454\n",
      "Epoch [3/100], Loss: 1.8833\n",
      "Epoch [4/100], Loss: 1.4633\n",
      "Epoch [5/100], Loss: 1.2104\n",
      "Epoch [6/100], Loss: 1.0446\n",
      "Epoch [7/100], Loss: 0.9305\n",
      "Epoch [8/100], Loss: 0.8304\n",
      "Epoch [9/100], Loss: 0.7487\n",
      "Epoch [10/100], Loss: 0.6787\n",
      "Epoch [11/100], Loss: 0.6188\n",
      "Epoch [12/100], Loss: 0.5724\n",
      "Epoch [13/100], Loss: 0.5295\n",
      "Epoch [14/100], Loss: 0.4912\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4080\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3664\n",
      "Epoch [20/100], Loss: 0.3471\n",
      "Epoch [21/100], Loss: 0.3321\n",
      "Epoch [22/100], Loss: 0.3177\n",
      "Epoch [23/100], Loss: 0.3025\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2417\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2266\n",
      "Epoch [32/100], Loss: 0.2184\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2072\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1889\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1707\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1649\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1466\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1380\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1327\n",
      "Epoch [54/100], Loss: 0.1304\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1105\n",
      "Epoch [65/100], Loss: 0.1080\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0856\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0817\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0721\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1912\n",
      "Epoch [2/100], Loss: 2.5393\n",
      "Epoch [3/100], Loss: 1.8816\n",
      "Epoch [4/100], Loss: 1.4547\n",
      "Epoch [5/100], Loss: 1.2053\n",
      "Epoch [6/100], Loss: 1.0423\n",
      "Epoch [7/100], Loss: 0.9254\n",
      "Epoch [8/100], Loss: 0.8307\n",
      "Epoch [9/100], Loss: 0.7512\n",
      "Epoch [10/100], Loss: 0.6800\n",
      "Epoch [11/100], Loss: 0.6193\n",
      "Epoch [12/100], Loss: 0.5680\n",
      "Epoch [13/100], Loss: 0.5290\n",
      "Epoch [14/100], Loss: 0.4917\n",
      "Epoch [15/100], Loss: 0.4602\n",
      "Epoch [16/100], Loss: 0.4312\n",
      "Epoch [17/100], Loss: 0.4080\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3506\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3178\n",
      "Epoch [23/100], Loss: 0.3041\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2608\n",
      "Epoch [28/100], Loss: 0.2498\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2188\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1903\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1807\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1670\n",
      "Epoch [43/100], Loss: 0.1627\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1526\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1429\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1303\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1250\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1070\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1019\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0866\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.79%\n",
      "Epoch [1/100], Loss: 3.1851\n",
      "Epoch [2/100], Loss: 2.5282\n",
      "Epoch [3/100], Loss: 1.8823\n",
      "Epoch [4/100], Loss: 1.4569\n",
      "Epoch [5/100], Loss: 1.2064\n",
      "Epoch [6/100], Loss: 1.0512\n",
      "Epoch [7/100], Loss: 0.9326\n",
      "Epoch [8/100], Loss: 0.8297\n",
      "Epoch [9/100], Loss: 0.7498\n",
      "Epoch [10/100], Loss: 0.6782\n",
      "Epoch [11/100], Loss: 0.6197\n",
      "Epoch [12/100], Loss: 0.5704\n",
      "Epoch [13/100], Loss: 0.5293\n",
      "Epoch [14/100], Loss: 0.4911\n",
      "Epoch [15/100], Loss: 0.4588\n",
      "Epoch [16/100], Loss: 0.4317\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3864\n",
      "Epoch [19/100], Loss: 0.3636\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3327\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3034\n",
      "Epoch [24/100], Loss: 0.2915\n",
      "Epoch [25/100], Loss: 0.2806\n",
      "Epoch [26/100], Loss: 0.2690\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2506\n",
      "Epoch [29/100], Loss: 0.2419\n",
      "Epoch [30/100], Loss: 0.2348\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1949\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1850\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1713\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1525\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1257\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1193\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1155\n",
      "Epoch [62/100], Loss: 0.1131\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0959\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0749\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.0739\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1850\n",
      "Epoch [2/100], Loss: 2.5313\n",
      "Epoch [3/100], Loss: 1.8737\n",
      "Epoch [4/100], Loss: 1.4686\n",
      "Epoch [5/100], Loss: 1.2123\n",
      "Epoch [6/100], Loss: 1.0521\n",
      "Epoch [7/100], Loss: 0.9360\n",
      "Epoch [8/100], Loss: 0.8376\n",
      "Epoch [9/100], Loss: 0.7498\n",
      "Epoch [10/100], Loss: 0.6791\n",
      "Epoch [11/100], Loss: 0.6196\n",
      "Epoch [12/100], Loss: 0.5685\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4907\n",
      "Epoch [15/100], Loss: 0.4593\n",
      "Epoch [16/100], Loss: 0.4315\n",
      "Epoch [17/100], Loss: 0.4074\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3658\n",
      "Epoch [20/100], Loss: 0.3488\n",
      "Epoch [21/100], Loss: 0.3309\n",
      "Epoch [22/100], Loss: 0.3165\n",
      "Epoch [23/100], Loss: 0.3042\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2805\n",
      "Epoch [26/100], Loss: 0.2688\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2337\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2190\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1944\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1838\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1330\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1100\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1026\n",
      "Epoch [70/100], Loss: 0.1012\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0978\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0850\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0786\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0750\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.84%\n",
      "Epoch [1/100], Loss: 3.1934\n",
      "Epoch [2/100], Loss: 2.5439\n",
      "Epoch [3/100], Loss: 1.8681\n",
      "Epoch [4/100], Loss: 1.4636\n",
      "Epoch [5/100], Loss: 1.2124\n",
      "Epoch [6/100], Loss: 1.0420\n",
      "Epoch [7/100], Loss: 0.9348\n",
      "Epoch [8/100], Loss: 0.8349\n",
      "Epoch [9/100], Loss: 0.7527\n",
      "Epoch [10/100], Loss: 0.6820\n",
      "Epoch [11/100], Loss: 0.6185\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5279\n",
      "Epoch [14/100], Loss: 0.4916\n",
      "Epoch [15/100], Loss: 0.4612\n",
      "Epoch [16/100], Loss: 0.4336\n",
      "Epoch [17/100], Loss: 0.4072\n",
      "Epoch [18/100], Loss: 0.3861\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3467\n",
      "Epoch [21/100], Loss: 0.3318\n",
      "Epoch [22/100], Loss: 0.3157\n",
      "Epoch [23/100], Loss: 0.3059\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2789\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2603\n",
      "Epoch [28/100], Loss: 0.2509\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2330\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2065\n",
      "Epoch [35/100], Loss: 0.2001\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1805\n",
      "Epoch [40/100], Loss: 0.1756\n",
      "Epoch [41/100], Loss: 0.1716\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1427\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0906\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0855\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0837\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0810\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0726\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1874\n",
      "Epoch [2/100], Loss: 2.5357\n",
      "Epoch [3/100], Loss: 1.8886\n",
      "Epoch [4/100], Loss: 1.4686\n",
      "Epoch [5/100], Loss: 1.2130\n",
      "Epoch [6/100], Loss: 1.0492\n",
      "Epoch [7/100], Loss: 0.9307\n",
      "Epoch [8/100], Loss: 0.8325\n",
      "Epoch [9/100], Loss: 0.7508\n",
      "Epoch [10/100], Loss: 0.6825\n",
      "Epoch [11/100], Loss: 0.6180\n",
      "Epoch [12/100], Loss: 0.5722\n",
      "Epoch [13/100], Loss: 0.5271\n",
      "Epoch [14/100], Loss: 0.4905\n",
      "Epoch [15/100], Loss: 0.4576\n",
      "Epoch [16/100], Loss: 0.4327\n",
      "Epoch [17/100], Loss: 0.4085\n",
      "Epoch [18/100], Loss: 0.3850\n",
      "Epoch [19/100], Loss: 0.3673\n",
      "Epoch [20/100], Loss: 0.3489\n",
      "Epoch [21/100], Loss: 0.3325\n",
      "Epoch [22/100], Loss: 0.3161\n",
      "Epoch [23/100], Loss: 0.3042\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2802\n",
      "Epoch [26/100], Loss: 0.2693\n",
      "Epoch [27/100], Loss: 0.2606\n",
      "Epoch [28/100], Loss: 0.2516\n",
      "Epoch [29/100], Loss: 0.2418\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2258\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2120\n",
      "Epoch [34/100], Loss: 0.2063\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1952\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1672\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1437\n",
      "Epoch [50/100], Loss: 0.1402\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1230\n",
      "Epoch [58/100], Loss: 0.1216\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1072\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0996\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0860\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0769\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0715\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0715\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1924\n",
      "Epoch [2/100], Loss: 2.5328\n",
      "Epoch [3/100], Loss: 1.8752\n",
      "Epoch [4/100], Loss: 1.4691\n",
      "Epoch [5/100], Loss: 1.2065\n",
      "Epoch [6/100], Loss: 1.0485\n",
      "Epoch [7/100], Loss: 0.9303\n",
      "Epoch [8/100], Loss: 0.8355\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6808\n",
      "Epoch [11/100], Loss: 0.6241\n",
      "Epoch [12/100], Loss: 0.5735\n",
      "Epoch [13/100], Loss: 0.5332\n",
      "Epoch [14/100], Loss: 0.4930\n",
      "Epoch [15/100], Loss: 0.4594\n",
      "Epoch [16/100], Loss: 0.4318\n",
      "Epoch [17/100], Loss: 0.4058\n",
      "Epoch [18/100], Loss: 0.3867\n",
      "Epoch [19/100], Loss: 0.3658\n",
      "Epoch [20/100], Loss: 0.3486\n",
      "Epoch [21/100], Loss: 0.3326\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2909\n",
      "Epoch [25/100], Loss: 0.2792\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2604\n",
      "Epoch [28/100], Loss: 0.2494\n",
      "Epoch [29/100], Loss: 0.2420\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2187\n",
      "Epoch [33/100], Loss: 0.2125\n",
      "Epoch [34/100], Loss: 0.2057\n",
      "Epoch [35/100], Loss: 0.1999\n",
      "Epoch [36/100], Loss: 0.1947\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1805\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1594\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1493\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1273\n",
      "Epoch [56/100], Loss: 0.1260\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1197\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1064\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0899\n",
      "Epoch [80/100], Loss: 0.0890\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0766\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1830\n",
      "Epoch [2/100], Loss: 2.5325\n",
      "Epoch [3/100], Loss: 1.8784\n",
      "Epoch [4/100], Loss: 1.4587\n",
      "Epoch [5/100], Loss: 1.2124\n",
      "Epoch [6/100], Loss: 1.0502\n",
      "Epoch [7/100], Loss: 0.9299\n",
      "Epoch [8/100], Loss: 0.8324\n",
      "Epoch [9/100], Loss: 0.7476\n",
      "Epoch [10/100], Loss: 0.6780\n",
      "Epoch [11/100], Loss: 0.6198\n",
      "Epoch [12/100], Loss: 0.5685\n",
      "Epoch [13/100], Loss: 0.5277\n",
      "Epoch [14/100], Loss: 0.4932\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4317\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3643\n",
      "Epoch [20/100], Loss: 0.3474\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2795\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2594\n",
      "Epoch [28/100], Loss: 0.2507\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2194\n",
      "Epoch [33/100], Loss: 0.2130\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1953\n",
      "Epoch [37/100], Loss: 0.1902\n",
      "Epoch [38/100], Loss: 0.1844\n",
      "Epoch [39/100], Loss: 0.1792\n",
      "Epoch [40/100], Loss: 0.1757\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1406\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1259\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1037\n",
      "Epoch [69/100], Loss: 0.1026\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0984\n",
      "Epoch [73/100], Loss: 0.0964\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0947\n",
      "Epoch [76/100], Loss: 0.0926\n",
      "Epoch [77/100], Loss: 0.0926\n",
      "Epoch [78/100], Loss: 0.0909\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0846\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0799\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0769\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1842\n",
      "Epoch [2/100], Loss: 2.5355\n",
      "Epoch [3/100], Loss: 1.8826\n",
      "Epoch [4/100], Loss: 1.4656\n",
      "Epoch [5/100], Loss: 1.2195\n",
      "Epoch [6/100], Loss: 1.0473\n",
      "Epoch [7/100], Loss: 0.9277\n",
      "Epoch [8/100], Loss: 0.8330\n",
      "Epoch [9/100], Loss: 0.7468\n",
      "Epoch [10/100], Loss: 0.6771\n",
      "Epoch [11/100], Loss: 0.6231\n",
      "Epoch [12/100], Loss: 0.5695\n",
      "Epoch [13/100], Loss: 0.5273\n",
      "Epoch [14/100], Loss: 0.4928\n",
      "Epoch [15/100], Loss: 0.4590\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4061\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3658\n",
      "Epoch [20/100], Loss: 0.3476\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3037\n",
      "Epoch [24/100], Loss: 0.2916\n",
      "Epoch [25/100], Loss: 0.2807\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2593\n",
      "Epoch [28/100], Loss: 0.2508\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2333\n",
      "Epoch [31/100], Loss: 0.2254\n",
      "Epoch [32/100], Loss: 0.2184\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1850\n",
      "Epoch [39/100], Loss: 0.1794\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1714\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1635\n",
      "Epoch [44/100], Loss: 0.1598\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1523\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1459\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1400\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1326\n",
      "Epoch [54/100], Loss: 0.1299\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1137\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0845\n",
      "Epoch [85/100], Loss: 0.0831\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0782\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0753\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0716\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.66%\n",
      "Epoch [1/100], Loss: 3.1860\n",
      "Epoch [2/100], Loss: 2.5291\n",
      "Epoch [3/100], Loss: 1.8825\n",
      "Epoch [4/100], Loss: 1.4655\n",
      "Epoch [5/100], Loss: 1.2116\n",
      "Epoch [6/100], Loss: 1.0453\n",
      "Epoch [7/100], Loss: 0.9278\n",
      "Epoch [8/100], Loss: 0.8319\n",
      "Epoch [9/100], Loss: 0.7486\n",
      "Epoch [10/100], Loss: 0.6820\n",
      "Epoch [11/100], Loss: 0.6212\n",
      "Epoch [12/100], Loss: 0.5736\n",
      "Epoch [13/100], Loss: 0.5283\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4599\n",
      "Epoch [16/100], Loss: 0.4318\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3654\n",
      "Epoch [20/100], Loss: 0.3479\n",
      "Epoch [21/100], Loss: 0.3321\n",
      "Epoch [22/100], Loss: 0.3166\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2684\n",
      "Epoch [27/100], Loss: 0.2589\n",
      "Epoch [28/100], Loss: 0.2514\n",
      "Epoch [29/100], Loss: 0.2410\n",
      "Epoch [30/100], Loss: 0.2334\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2185\n",
      "Epoch [33/100], Loss: 0.2119\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2019\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1893\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1798\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1711\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1629\n",
      "Epoch [44/100], Loss: 0.1590\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1494\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1329\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1259\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1051\n",
      "Epoch [68/100], Loss: 0.1033\n",
      "Epoch [69/100], Loss: 0.1028\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0944\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0806\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0781\n",
      "Epoch [92/100], Loss: 0.0772\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1938\n",
      "Epoch [2/100], Loss: 2.5266\n",
      "Epoch [3/100], Loss: 1.8772\n",
      "Epoch [4/100], Loss: 1.4626\n",
      "Epoch [5/100], Loss: 1.2043\n",
      "Epoch [6/100], Loss: 1.0457\n",
      "Epoch [7/100], Loss: 0.9306\n",
      "Epoch [8/100], Loss: 0.8316\n",
      "Epoch [9/100], Loss: 0.7518\n",
      "Epoch [10/100], Loss: 0.6800\n",
      "Epoch [11/100], Loss: 0.6191\n",
      "Epoch [12/100], Loss: 0.5711\n",
      "Epoch [13/100], Loss: 0.5284\n",
      "Epoch [14/100], Loss: 0.4922\n",
      "Epoch [15/100], Loss: 0.4591\n",
      "Epoch [16/100], Loss: 0.4326\n",
      "Epoch [17/100], Loss: 0.4078\n",
      "Epoch [18/100], Loss: 0.3858\n",
      "Epoch [19/100], Loss: 0.3647\n",
      "Epoch [20/100], Loss: 0.3480\n",
      "Epoch [21/100], Loss: 0.3308\n",
      "Epoch [22/100], Loss: 0.3164\n",
      "Epoch [23/100], Loss: 0.3038\n",
      "Epoch [24/100], Loss: 0.2906\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2698\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2496\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2130\n",
      "Epoch [34/100], Loss: 0.2066\n",
      "Epoch [35/100], Loss: 0.2009\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1861\n",
      "Epoch [39/100], Loss: 0.1801\n",
      "Epoch [40/100], Loss: 0.1753\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1665\n",
      "Epoch [43/100], Loss: 0.1637\n",
      "Epoch [44/100], Loss: 0.1601\n",
      "Epoch [45/100], Loss: 0.1568\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1465\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1347\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1213\n",
      "Epoch [59/100], Loss: 0.1194\n",
      "Epoch [60/100], Loss: 0.1174\n",
      "Epoch [61/100], Loss: 0.1156\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1082\n",
      "Epoch [66/100], Loss: 0.1068\n",
      "Epoch [67/100], Loss: 0.1055\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1009\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0952\n",
      "Epoch [75/100], Loss: 0.0939\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0916\n",
      "Epoch [78/100], Loss: 0.0908\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0836\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0820\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0778\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0753\n",
      "Epoch [95/100], Loss: 0.0746\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.78%\n",
      "Epoch [1/100], Loss: 3.1873\n",
      "Epoch [2/100], Loss: 2.5394\n",
      "Epoch [3/100], Loss: 1.8899\n",
      "Epoch [4/100], Loss: 1.4662\n",
      "Epoch [5/100], Loss: 1.2050\n",
      "Epoch [6/100], Loss: 1.0524\n",
      "Epoch [7/100], Loss: 0.9275\n",
      "Epoch [8/100], Loss: 0.8326\n",
      "Epoch [9/100], Loss: 0.7521\n",
      "Epoch [10/100], Loss: 0.6777\n",
      "Epoch [11/100], Loss: 0.6203\n",
      "Epoch [12/100], Loss: 0.5705\n",
      "Epoch [13/100], Loss: 0.5282\n",
      "Epoch [14/100], Loss: 0.4910\n",
      "Epoch [15/100], Loss: 0.4585\n",
      "Epoch [16/100], Loss: 0.4343\n",
      "Epoch [17/100], Loss: 0.4090\n",
      "Epoch [18/100], Loss: 0.3857\n",
      "Epoch [19/100], Loss: 0.3647\n",
      "Epoch [20/100], Loss: 0.3473\n",
      "Epoch [21/100], Loss: 0.3320\n",
      "Epoch [22/100], Loss: 0.3181\n",
      "Epoch [23/100], Loss: 0.3040\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2800\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2603\n",
      "Epoch [28/100], Loss: 0.2492\n",
      "Epoch [29/100], Loss: 0.2407\n",
      "Epoch [30/100], Loss: 0.2340\n",
      "Epoch [31/100], Loss: 0.2260\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.2003\n",
      "Epoch [36/100], Loss: 0.1950\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1757\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1634\n",
      "Epoch [44/100], Loss: 0.1592\n",
      "Epoch [45/100], Loss: 0.1554\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1497\n",
      "Epoch [48/100], Loss: 0.1461\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1323\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1210\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1133\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0969\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0942\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0835\n",
      "Epoch [86/100], Loss: 0.0827\n",
      "Epoch [87/100], Loss: 0.0819\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0788\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0761\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1806\n",
      "Epoch [2/100], Loss: 2.5342\n",
      "Epoch [3/100], Loss: 1.8873\n",
      "Epoch [4/100], Loss: 1.4679\n",
      "Epoch [5/100], Loss: 1.2114\n",
      "Epoch [6/100], Loss: 1.0513\n",
      "Epoch [7/100], Loss: 0.9324\n",
      "Epoch [8/100], Loss: 0.8340\n",
      "Epoch [9/100], Loss: 0.7485\n",
      "Epoch [10/100], Loss: 0.6771\n",
      "Epoch [11/100], Loss: 0.6204\n",
      "Epoch [12/100], Loss: 0.5710\n",
      "Epoch [13/100], Loss: 0.5288\n",
      "Epoch [14/100], Loss: 0.4947\n",
      "Epoch [15/100], Loss: 0.4579\n",
      "Epoch [16/100], Loss: 0.4335\n",
      "Epoch [17/100], Loss: 0.4081\n",
      "Epoch [18/100], Loss: 0.3848\n",
      "Epoch [19/100], Loss: 0.3666\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3329\n",
      "Epoch [22/100], Loss: 0.3178\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2904\n",
      "Epoch [25/100], Loss: 0.2790\n",
      "Epoch [26/100], Loss: 0.2697\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2495\n",
      "Epoch [29/100], Loss: 0.2423\n",
      "Epoch [30/100], Loss: 0.2339\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2060\n",
      "Epoch [35/100], Loss: 0.2002\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1852\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1632\n",
      "Epoch [44/100], Loss: 0.1599\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1530\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1404\n",
      "Epoch [51/100], Loss: 0.1383\n",
      "Epoch [52/100], Loss: 0.1353\n",
      "Epoch [53/100], Loss: 0.1319\n",
      "Epoch [54/100], Loss: 0.1298\n",
      "Epoch [55/100], Loss: 0.1276\n",
      "Epoch [56/100], Loss: 0.1251\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1190\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1080\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1026\n",
      "Epoch [70/100], Loss: 0.1005\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0955\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0915\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0769\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0756\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0745\n",
      "Epoch [97/100], Loss: 0.0736\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0717\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.84%\n",
      "Epoch [1/100], Loss: 3.1873\n",
      "Epoch [2/100], Loss: 2.5311\n",
      "Epoch [3/100], Loss: 1.8815\n",
      "Epoch [4/100], Loss: 1.4652\n",
      "Epoch [5/100], Loss: 1.2128\n",
      "Epoch [6/100], Loss: 1.0411\n",
      "Epoch [7/100], Loss: 0.9261\n",
      "Epoch [8/100], Loss: 0.8310\n",
      "Epoch [9/100], Loss: 0.7515\n",
      "Epoch [10/100], Loss: 0.6771\n",
      "Epoch [11/100], Loss: 0.6184\n",
      "Epoch [12/100], Loss: 0.5710\n",
      "Epoch [13/100], Loss: 0.5269\n",
      "Epoch [14/100], Loss: 0.4885\n",
      "Epoch [15/100], Loss: 0.4597\n",
      "Epoch [16/100], Loss: 0.4334\n",
      "Epoch [17/100], Loss: 0.4076\n",
      "Epoch [18/100], Loss: 0.3860\n",
      "Epoch [19/100], Loss: 0.3661\n",
      "Epoch [20/100], Loss: 0.3478\n",
      "Epoch [21/100], Loss: 0.3312\n",
      "Epoch [22/100], Loss: 0.3168\n",
      "Epoch [23/100], Loss: 0.3035\n",
      "Epoch [24/100], Loss: 0.2921\n",
      "Epoch [25/100], Loss: 0.2797\n",
      "Epoch [26/100], Loss: 0.2687\n",
      "Epoch [27/100], Loss: 0.2596\n",
      "Epoch [28/100], Loss: 0.2510\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2007\n",
      "Epoch [36/100], Loss: 0.1943\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1749\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1676\n",
      "Epoch [43/100], Loss: 0.1637\n",
      "Epoch [44/100], Loss: 0.1591\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1529\n",
      "Epoch [47/100], Loss: 0.1498\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1430\n",
      "Epoch [50/100], Loss: 0.1401\n",
      "Epoch [51/100], Loss: 0.1374\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1301\n",
      "Epoch [55/100], Loss: 0.1288\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1212\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1153\n",
      "Epoch [62/100], Loss: 0.1132\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1085\n",
      "Epoch [66/100], Loss: 0.1066\n",
      "Epoch [67/100], Loss: 0.1050\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0956\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0927\n",
      "Epoch [77/100], Loss: 0.0918\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0864\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0842\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0810\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0790\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0770\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0716\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.75%\n",
      "Epoch [1/100], Loss: 3.1820\n",
      "Epoch [2/100], Loss: 2.5321\n",
      "Epoch [3/100], Loss: 1.8905\n",
      "Epoch [4/100], Loss: 1.4601\n",
      "Epoch [5/100], Loss: 1.2088\n",
      "Epoch [6/100], Loss: 1.0446\n",
      "Epoch [7/100], Loss: 0.9321\n",
      "Epoch [8/100], Loss: 0.8335\n",
      "Epoch [9/100], Loss: 0.7492\n",
      "Epoch [10/100], Loss: 0.6797\n",
      "Epoch [11/100], Loss: 0.6203\n",
      "Epoch [12/100], Loss: 0.5706\n",
      "Epoch [13/100], Loss: 0.5298\n",
      "Epoch [14/100], Loss: 0.4924\n",
      "Epoch [15/100], Loss: 0.4625\n",
      "Epoch [16/100], Loss: 0.4312\n",
      "Epoch [17/100], Loss: 0.4078\n",
      "Epoch [18/100], Loss: 0.3856\n",
      "Epoch [19/100], Loss: 0.3680\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3301\n",
      "Epoch [22/100], Loss: 0.3170\n",
      "Epoch [23/100], Loss: 0.3043\n",
      "Epoch [24/100], Loss: 0.2907\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2692\n",
      "Epoch [27/100], Loss: 0.2591\n",
      "Epoch [28/100], Loss: 0.2501\n",
      "Epoch [29/100], Loss: 0.2416\n",
      "Epoch [30/100], Loss: 0.2332\n",
      "Epoch [31/100], Loss: 0.2255\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2068\n",
      "Epoch [35/100], Loss: 0.2012\n",
      "Epoch [36/100], Loss: 0.1941\n",
      "Epoch [37/100], Loss: 0.1894\n",
      "Epoch [38/100], Loss: 0.1846\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1709\n",
      "Epoch [42/100], Loss: 0.1671\n",
      "Epoch [43/100], Loss: 0.1633\n",
      "Epoch [44/100], Loss: 0.1603\n",
      "Epoch [45/100], Loss: 0.1560\n",
      "Epoch [46/100], Loss: 0.1528\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1457\n",
      "Epoch [49/100], Loss: 0.1431\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1373\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1274\n",
      "Epoch [56/100], Loss: 0.1251\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1214\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1144\n",
      "Epoch [63/100], Loss: 0.1119\n",
      "Epoch [64/100], Loss: 0.1098\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1025\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0994\n",
      "Epoch [72/100], Loss: 0.0987\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0951\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0931\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0843\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0822\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0727\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0710\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0710\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1860\n",
      "Epoch [2/100], Loss: 2.5343\n",
      "Epoch [3/100], Loss: 1.8849\n",
      "Epoch [4/100], Loss: 1.4590\n",
      "Epoch [5/100], Loss: 1.2140\n",
      "Epoch [6/100], Loss: 1.0448\n",
      "Epoch [7/100], Loss: 0.9264\n",
      "Epoch [8/100], Loss: 0.8337\n",
      "Epoch [9/100], Loss: 0.7503\n",
      "Epoch [10/100], Loss: 0.6773\n",
      "Epoch [11/100], Loss: 0.6210\n",
      "Epoch [12/100], Loss: 0.5700\n",
      "Epoch [13/100], Loss: 0.5277\n",
      "Epoch [14/100], Loss: 0.4939\n",
      "Epoch [15/100], Loss: 0.4608\n",
      "Epoch [16/100], Loss: 0.4322\n",
      "Epoch [17/100], Loss: 0.4087\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3650\n",
      "Epoch [20/100], Loss: 0.3483\n",
      "Epoch [21/100], Loss: 0.3308\n",
      "Epoch [22/100], Loss: 0.3171\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2911\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2691\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2503\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2263\n",
      "Epoch [32/100], Loss: 0.2189\n",
      "Epoch [33/100], Loss: 0.2126\n",
      "Epoch [34/100], Loss: 0.2059\n",
      "Epoch [35/100], Loss: 0.2000\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1892\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1675\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1559\n",
      "Epoch [46/100], Loss: 0.1527\n",
      "Epoch [47/100], Loss: 0.1496\n",
      "Epoch [48/100], Loss: 0.1463\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1408\n",
      "Epoch [51/100], Loss: 0.1372\n",
      "Epoch [52/100], Loss: 0.1352\n",
      "Epoch [53/100], Loss: 0.1328\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1173\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1136\n",
      "Epoch [63/100], Loss: 0.1117\n",
      "Epoch [64/100], Loss: 0.1103\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1006\n",
      "Epoch [71/100], Loss: 0.0991\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0905\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0883\n",
      "Epoch [81/100], Loss: 0.0872\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0851\n",
      "Epoch [84/100], Loss: 0.0846\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0816\n",
      "Epoch [88/100], Loss: 0.0803\n",
      "Epoch [89/100], Loss: 0.0796\n",
      "Epoch [90/100], Loss: 0.0789\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0764\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0748\n",
      "Epoch [96/100], Loss: 0.0741\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0719\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1803\n",
      "Epoch [2/100], Loss: 2.5326\n",
      "Epoch [3/100], Loss: 1.8830\n",
      "Epoch [4/100], Loss: 1.4581\n",
      "Epoch [5/100], Loss: 1.2140\n",
      "Epoch [6/100], Loss: 1.0471\n",
      "Epoch [7/100], Loss: 0.9329\n",
      "Epoch [8/100], Loss: 0.8345\n",
      "Epoch [9/100], Loss: 0.7520\n",
      "Epoch [10/100], Loss: 0.6795\n",
      "Epoch [11/100], Loss: 0.6195\n",
      "Epoch [12/100], Loss: 0.5725\n",
      "Epoch [13/100], Loss: 0.5296\n",
      "Epoch [14/100], Loss: 0.4930\n",
      "Epoch [15/100], Loss: 0.4601\n",
      "Epoch [16/100], Loss: 0.4335\n",
      "Epoch [17/100], Loss: 0.4071\n",
      "Epoch [18/100], Loss: 0.3854\n",
      "Epoch [19/100], Loss: 0.3648\n",
      "Epoch [20/100], Loss: 0.3481\n",
      "Epoch [21/100], Loss: 0.3313\n",
      "Epoch [22/100], Loss: 0.3169\n",
      "Epoch [23/100], Loss: 0.3032\n",
      "Epoch [24/100], Loss: 0.2914\n",
      "Epoch [25/100], Loss: 0.2794\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2592\n",
      "Epoch [28/100], Loss: 0.2508\n",
      "Epoch [29/100], Loss: 0.2434\n",
      "Epoch [30/100], Loss: 0.2338\n",
      "Epoch [31/100], Loss: 0.2262\n",
      "Epoch [32/100], Loss: 0.2197\n",
      "Epoch [33/100], Loss: 0.2123\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2006\n",
      "Epoch [36/100], Loss: 0.1945\n",
      "Epoch [37/100], Loss: 0.1898\n",
      "Epoch [38/100], Loss: 0.1843\n",
      "Epoch [39/100], Loss: 0.1795\n",
      "Epoch [40/100], Loss: 0.1752\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1666\n",
      "Epoch [43/100], Loss: 0.1634\n",
      "Epoch [44/100], Loss: 0.1605\n",
      "Epoch [45/100], Loss: 0.1564\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1490\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1412\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1350\n",
      "Epoch [53/100], Loss: 0.1322\n",
      "Epoch [54/100], Loss: 0.1297\n",
      "Epoch [55/100], Loss: 0.1278\n",
      "Epoch [56/100], Loss: 0.1252\n",
      "Epoch [57/100], Loss: 0.1232\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1204\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1144\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1104\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1053\n",
      "Epoch [68/100], Loss: 0.1036\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0981\n",
      "Epoch [73/100], Loss: 0.0965\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0943\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0898\n",
      "Epoch [80/100], Loss: 0.0882\n",
      "Epoch [81/100], Loss: 0.0874\n",
      "Epoch [82/100], Loss: 0.0865\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0841\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0825\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0807\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0758\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.77%\n",
      "Epoch [1/100], Loss: 3.1929\n",
      "Epoch [2/100], Loss: 2.5261\n",
      "Epoch [3/100], Loss: 1.8721\n",
      "Epoch [4/100], Loss: 1.4648\n",
      "Epoch [5/100], Loss: 1.2082\n",
      "Epoch [6/100], Loss: 1.0480\n",
      "Epoch [7/100], Loss: 0.9294\n",
      "Epoch [8/100], Loss: 0.8338\n",
      "Epoch [9/100], Loss: 0.7464\n",
      "Epoch [10/100], Loss: 0.6772\n",
      "Epoch [11/100], Loss: 0.6198\n",
      "Epoch [12/100], Loss: 0.5686\n",
      "Epoch [13/100], Loss: 0.5287\n",
      "Epoch [14/100], Loss: 0.4925\n",
      "Epoch [15/100], Loss: 0.4607\n",
      "Epoch [16/100], Loss: 0.4315\n",
      "Epoch [17/100], Loss: 0.4064\n",
      "Epoch [18/100], Loss: 0.3853\n",
      "Epoch [19/100], Loss: 0.3647\n",
      "Epoch [20/100], Loss: 0.3490\n",
      "Epoch [21/100], Loss: 0.3319\n",
      "Epoch [22/100], Loss: 0.3173\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2801\n",
      "Epoch [26/100], Loss: 0.2689\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2504\n",
      "Epoch [29/100], Loss: 0.2414\n",
      "Epoch [30/100], Loss: 0.2341\n",
      "Epoch [31/100], Loss: 0.2270\n",
      "Epoch [32/100], Loss: 0.2192\n",
      "Epoch [33/100], Loss: 0.2133\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1946\n",
      "Epoch [37/100], Loss: 0.1891\n",
      "Epoch [38/100], Loss: 0.1851\n",
      "Epoch [39/100], Loss: 0.1802\n",
      "Epoch [40/100], Loss: 0.1761\n",
      "Epoch [41/100], Loss: 0.1708\n",
      "Epoch [42/100], Loss: 0.1667\n",
      "Epoch [43/100], Loss: 0.1628\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1556\n",
      "Epoch [46/100], Loss: 0.1522\n",
      "Epoch [47/100], Loss: 0.1492\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1432\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1375\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1302\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1255\n",
      "Epoch [57/100], Loss: 0.1233\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1192\n",
      "Epoch [60/100], Loss: 0.1170\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1139\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1101\n",
      "Epoch [65/100], Loss: 0.1083\n",
      "Epoch [66/100], Loss: 0.1065\n",
      "Epoch [67/100], Loss: 0.1054\n",
      "Epoch [68/100], Loss: 0.1034\n",
      "Epoch [69/100], Loss: 0.1021\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0995\n",
      "Epoch [72/100], Loss: 0.0982\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0930\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0895\n",
      "Epoch [80/100], Loss: 0.0885\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0861\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0833\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0815\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0797\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0734\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0712\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0712\n",
      "Test Accuracy Logit Lipschitz: 51.72%\n",
      "Epoch [1/100], Loss: 3.1931\n",
      "Epoch [2/100], Loss: 2.5335\n",
      "Epoch [3/100], Loss: 1.8873\n",
      "Epoch [4/100], Loss: 1.4577\n",
      "Epoch [5/100], Loss: 1.2146\n",
      "Epoch [6/100], Loss: 1.0498\n",
      "Epoch [7/100], Loss: 0.9346\n",
      "Epoch [8/100], Loss: 0.8331\n",
      "Epoch [9/100], Loss: 0.7455\n",
      "Epoch [10/100], Loss: 0.6766\n",
      "Epoch [11/100], Loss: 0.6172\n",
      "Epoch [12/100], Loss: 0.5690\n",
      "Epoch [13/100], Loss: 0.5285\n",
      "Epoch [14/100], Loss: 0.4905\n",
      "Epoch [15/100], Loss: 0.4587\n",
      "Epoch [16/100], Loss: 0.4341\n",
      "Epoch [17/100], Loss: 0.4079\n",
      "Epoch [18/100], Loss: 0.3846\n",
      "Epoch [19/100], Loss: 0.3648\n",
      "Epoch [20/100], Loss: 0.3477\n",
      "Epoch [21/100], Loss: 0.3314\n",
      "Epoch [22/100], Loss: 0.3172\n",
      "Epoch [23/100], Loss: 0.3033\n",
      "Epoch [24/100], Loss: 0.2912\n",
      "Epoch [25/100], Loss: 0.2799\n",
      "Epoch [26/100], Loss: 0.2709\n",
      "Epoch [27/100], Loss: 0.2587\n",
      "Epoch [28/100], Loss: 0.2505\n",
      "Epoch [29/100], Loss: 0.2411\n",
      "Epoch [30/100], Loss: 0.2327\n",
      "Epoch [31/100], Loss: 0.2264\n",
      "Epoch [32/100], Loss: 0.2195\n",
      "Epoch [33/100], Loss: 0.2124\n",
      "Epoch [34/100], Loss: 0.2081\n",
      "Epoch [35/100], Loss: 0.2008\n",
      "Epoch [36/100], Loss: 0.1939\n",
      "Epoch [37/100], Loss: 0.1896\n",
      "Epoch [38/100], Loss: 0.1840\n",
      "Epoch [39/100], Loss: 0.1797\n",
      "Epoch [40/100], Loss: 0.1751\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1668\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1600\n",
      "Epoch [45/100], Loss: 0.1558\n",
      "Epoch [46/100], Loss: 0.1536\n",
      "Epoch [47/100], Loss: 0.1491\n",
      "Epoch [48/100], Loss: 0.1460\n",
      "Epoch [49/100], Loss: 0.1442\n",
      "Epoch [50/100], Loss: 0.1399\n",
      "Epoch [51/100], Loss: 0.1376\n",
      "Epoch [52/100], Loss: 0.1354\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1300\n",
      "Epoch [55/100], Loss: 0.1275\n",
      "Epoch [56/100], Loss: 0.1254\n",
      "Epoch [57/100], Loss: 0.1231\n",
      "Epoch [58/100], Loss: 0.1208\n",
      "Epoch [59/100], Loss: 0.1187\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1151\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1115\n",
      "Epoch [64/100], Loss: 0.1099\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1067\n",
      "Epoch [67/100], Loss: 0.1057\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1020\n",
      "Epoch [70/100], Loss: 0.1007\n",
      "Epoch [71/100], Loss: 0.0993\n",
      "Epoch [72/100], Loss: 0.0980\n",
      "Epoch [73/100], Loss: 0.0968\n",
      "Epoch [74/100], Loss: 0.0959\n",
      "Epoch [75/100], Loss: 0.0941\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0919\n",
      "Epoch [78/100], Loss: 0.0904\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0887\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0852\n",
      "Epoch [84/100], Loss: 0.0840\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0823\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0805\n",
      "Epoch [89/100], Loss: 0.0795\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0765\n",
      "Epoch [94/100], Loss: 0.0755\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0740\n",
      "Epoch [97/100], Loss: 0.0731\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.80%\n",
      "Epoch [1/100], Loss: 3.1896\n",
      "Epoch [2/100], Loss: 2.5427\n",
      "Epoch [3/100], Loss: 1.8799\n",
      "Epoch [4/100], Loss: 1.4559\n",
      "Epoch [5/100], Loss: 1.2089\n",
      "Epoch [6/100], Loss: 1.0505\n",
      "Epoch [7/100], Loss: 0.9305\n",
      "Epoch [8/100], Loss: 0.8344\n",
      "Epoch [9/100], Loss: 0.7480\n",
      "Epoch [10/100], Loss: 0.6805\n",
      "Epoch [11/100], Loss: 0.6200\n",
      "Epoch [12/100], Loss: 0.5683\n",
      "Epoch [13/100], Loss: 0.5305\n",
      "Epoch [14/100], Loss: 0.4908\n",
      "Epoch [15/100], Loss: 0.4601\n",
      "Epoch [16/100], Loss: 0.4319\n",
      "Epoch [17/100], Loss: 0.4063\n",
      "Epoch [18/100], Loss: 0.3843\n",
      "Epoch [19/100], Loss: 0.3672\n",
      "Epoch [20/100], Loss: 0.3485\n",
      "Epoch [21/100], Loss: 0.3315\n",
      "Epoch [22/100], Loss: 0.3178\n",
      "Epoch [23/100], Loss: 0.3039\n",
      "Epoch [24/100], Loss: 0.2908\n",
      "Epoch [25/100], Loss: 0.2793\n",
      "Epoch [26/100], Loss: 0.2682\n",
      "Epoch [27/100], Loss: 0.2590\n",
      "Epoch [28/100], Loss: 0.2500\n",
      "Epoch [29/100], Loss: 0.2413\n",
      "Epoch [30/100], Loss: 0.2331\n",
      "Epoch [31/100], Loss: 0.2269\n",
      "Epoch [32/100], Loss: 0.2191\n",
      "Epoch [33/100], Loss: 0.2121\n",
      "Epoch [34/100], Loss: 0.2064\n",
      "Epoch [35/100], Loss: 0.2005\n",
      "Epoch [36/100], Loss: 0.1948\n",
      "Epoch [37/100], Loss: 0.1899\n",
      "Epoch [38/100], Loss: 0.1841\n",
      "Epoch [39/100], Loss: 0.1796\n",
      "Epoch [40/100], Loss: 0.1750\n",
      "Epoch [41/100], Loss: 0.1710\n",
      "Epoch [42/100], Loss: 0.1673\n",
      "Epoch [43/100], Loss: 0.1631\n",
      "Epoch [44/100], Loss: 0.1597\n",
      "Epoch [45/100], Loss: 0.1557\n",
      "Epoch [46/100], Loss: 0.1524\n",
      "Epoch [47/100], Loss: 0.1489\n",
      "Epoch [48/100], Loss: 0.1462\n",
      "Epoch [49/100], Loss: 0.1428\n",
      "Epoch [50/100], Loss: 0.1407\n",
      "Epoch [51/100], Loss: 0.1377\n",
      "Epoch [52/100], Loss: 0.1351\n",
      "Epoch [53/100], Loss: 0.1325\n",
      "Epoch [54/100], Loss: 0.1296\n",
      "Epoch [55/100], Loss: 0.1277\n",
      "Epoch [56/100], Loss: 0.1256\n",
      "Epoch [57/100], Loss: 0.1235\n",
      "Epoch [58/100], Loss: 0.1209\n",
      "Epoch [59/100], Loss: 0.1189\n",
      "Epoch [60/100], Loss: 0.1172\n",
      "Epoch [61/100], Loss: 0.1154\n",
      "Epoch [62/100], Loss: 0.1134\n",
      "Epoch [63/100], Loss: 0.1118\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1081\n",
      "Epoch [66/100], Loss: 0.1071\n",
      "Epoch [67/100], Loss: 0.1049\n",
      "Epoch [68/100], Loss: 0.1035\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0992\n",
      "Epoch [72/100], Loss: 0.0979\n",
      "Epoch [73/100], Loss: 0.0966\n",
      "Epoch [74/100], Loss: 0.0953\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0929\n",
      "Epoch [77/100], Loss: 0.0917\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0894\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0863\n",
      "Epoch [83/100], Loss: 0.0853\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0834\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0779\n",
      "Epoch [92/100], Loss: 0.0771\n",
      "Epoch [93/100], Loss: 0.0762\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0739\n",
      "Epoch [97/100], Loss: 0.0733\n",
      "Epoch [98/100], Loss: 0.0725\n",
      "Epoch [99/100], Loss: 0.0720\n",
      "Epoch [100/100], Loss: 0.0713\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0713\n",
      "Test Accuracy Logit Lipschitz: 51.83%\n",
      "Epoch [1/100], Loss: 3.1916\n",
      "Epoch [2/100], Loss: 2.5303\n",
      "Epoch [3/100], Loss: 1.8793\n",
      "Epoch [4/100], Loss: 1.4530\n",
      "Epoch [5/100], Loss: 1.2088\n",
      "Epoch [6/100], Loss: 1.0456\n",
      "Epoch [7/100], Loss: 0.9315\n",
      "Epoch [8/100], Loss: 0.8312\n",
      "Epoch [9/100], Loss: 0.7492\n",
      "Epoch [10/100], Loss: 0.6785\n",
      "Epoch [11/100], Loss: 0.6202\n",
      "Epoch [12/100], Loss: 0.5713\n",
      "Epoch [13/100], Loss: 0.5274\n",
      "Epoch [14/100], Loss: 0.4928\n",
      "Epoch [15/100], Loss: 0.4584\n",
      "Epoch [16/100], Loss: 0.4318\n",
      "Epoch [17/100], Loss: 0.4069\n",
      "Epoch [18/100], Loss: 0.3847\n",
      "Epoch [19/100], Loss: 0.3642\n",
      "Epoch [20/100], Loss: 0.3468\n",
      "Epoch [21/100], Loss: 0.3311\n",
      "Epoch [22/100], Loss: 0.3163\n",
      "Epoch [23/100], Loss: 0.3031\n",
      "Epoch [24/100], Loss: 0.2910\n",
      "Epoch [25/100], Loss: 0.2796\n",
      "Epoch [26/100], Loss: 0.2700\n",
      "Epoch [27/100], Loss: 0.2586\n",
      "Epoch [28/100], Loss: 0.2502\n",
      "Epoch [29/100], Loss: 0.2415\n",
      "Epoch [30/100], Loss: 0.2335\n",
      "Epoch [31/100], Loss: 0.2257\n",
      "Epoch [32/100], Loss: 0.2193\n",
      "Epoch [33/100], Loss: 0.2122\n",
      "Epoch [34/100], Loss: 0.2058\n",
      "Epoch [35/100], Loss: 0.2004\n",
      "Epoch [36/100], Loss: 0.1942\n",
      "Epoch [37/100], Loss: 0.1895\n",
      "Epoch [38/100], Loss: 0.1842\n",
      "Epoch [39/100], Loss: 0.1799\n",
      "Epoch [40/100], Loss: 0.1755\n",
      "Epoch [41/100], Loss: 0.1706\n",
      "Epoch [42/100], Loss: 0.1669\n",
      "Epoch [43/100], Loss: 0.1630\n",
      "Epoch [44/100], Loss: 0.1593\n",
      "Epoch [45/100], Loss: 0.1561\n",
      "Epoch [46/100], Loss: 0.1521\n",
      "Epoch [47/100], Loss: 0.1495\n",
      "Epoch [48/100], Loss: 0.1466\n",
      "Epoch [49/100], Loss: 0.1433\n",
      "Epoch [50/100], Loss: 0.1403\n",
      "Epoch [51/100], Loss: 0.1378\n",
      "Epoch [52/100], Loss: 0.1349\n",
      "Epoch [53/100], Loss: 0.1324\n",
      "Epoch [54/100], Loss: 0.1306\n",
      "Epoch [55/100], Loss: 0.1279\n",
      "Epoch [56/100], Loss: 0.1253\n",
      "Epoch [57/100], Loss: 0.1234\n",
      "Epoch [58/100], Loss: 0.1211\n",
      "Epoch [59/100], Loss: 0.1191\n",
      "Epoch [60/100], Loss: 0.1171\n",
      "Epoch [61/100], Loss: 0.1152\n",
      "Epoch [62/100], Loss: 0.1135\n",
      "Epoch [63/100], Loss: 0.1116\n",
      "Epoch [64/100], Loss: 0.1097\n",
      "Epoch [65/100], Loss: 0.1084\n",
      "Epoch [66/100], Loss: 0.1073\n",
      "Epoch [67/100], Loss: 0.1052\n",
      "Epoch [68/100], Loss: 0.1039\n",
      "Epoch [69/100], Loss: 0.1023\n",
      "Epoch [70/100], Loss: 0.1008\n",
      "Epoch [71/100], Loss: 0.0998\n",
      "Epoch [72/100], Loss: 0.0984\n",
      "Epoch [73/100], Loss: 0.0967\n",
      "Epoch [74/100], Loss: 0.0954\n",
      "Epoch [75/100], Loss: 0.0940\n",
      "Epoch [76/100], Loss: 0.0928\n",
      "Epoch [77/100], Loss: 0.0920\n",
      "Epoch [78/100], Loss: 0.0907\n",
      "Epoch [79/100], Loss: 0.0893\n",
      "Epoch [80/100], Loss: 0.0884\n",
      "Epoch [81/100], Loss: 0.0873\n",
      "Epoch [82/100], Loss: 0.0862\n",
      "Epoch [83/100], Loss: 0.0854\n",
      "Epoch [84/100], Loss: 0.0844\n",
      "Epoch [85/100], Loss: 0.0832\n",
      "Epoch [86/100], Loss: 0.0824\n",
      "Epoch [87/100], Loss: 0.0814\n",
      "Epoch [88/100], Loss: 0.0804\n",
      "Epoch [89/100], Loss: 0.0798\n",
      "Epoch [90/100], Loss: 0.0787\n",
      "Epoch [91/100], Loss: 0.0780\n",
      "Epoch [92/100], Loss: 0.0773\n",
      "Epoch [93/100], Loss: 0.0763\n",
      "Epoch [94/100], Loss: 0.0754\n",
      "Epoch [95/100], Loss: 0.0747\n",
      "Epoch [96/100], Loss: 0.0742\n",
      "Epoch [97/100], Loss: 0.0732\n",
      "Epoch [98/100], Loss: 0.0724\n",
      "Epoch [99/100], Loss: 0.0718\n",
      "Epoch [100/100], Loss: 0.0711\n",
      "Subset 1000, Epoch [100/100], Loss: 0.0711\n",
      "Test Accuracy Logit Lipschitz: 51.83%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-4  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "\n",
    "logit_accuracy_lipschitz = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "    print(f\"Training with subset size: {subset_size}\")\n",
    "\n",
    "    for _ in range(30):\n",
    "        logit_model_lipschitz = LogisticRegression(input_dim, num_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(logit_model_lipschitz.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = Subset(train_set, subset_indices)\n",
    "        train_loader = DataLoader(subset, batch_size=128, shuffle=True)\n",
    "\n",
    "        previous_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            logit_model_lipschitz.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = logit_model_lipschitz(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                L = max(param.grad.abs().max().item() for param in logit_model_lipschitz.parameters() if param.grad is not None)\n",
    "                adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "                # Update optimizer's learning rate\n",
    "                for param_group in optimizer_sdg.param_groups:\n",
    "                    param_group['lr'] = adaptive_lr\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "            # Check for early stopping\n",
    "            if previous_loss - current_loss < tolerance:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                    break\n",
    "            else:\n",
    "                epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "            previous_loss = current_loss\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        logit_model_lipschitz.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                images = images.view(-1, 28*28)\n",
    "                outputs = logit_model_lipschitz(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy Logit Lipschitz: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy_lipschitz[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000    51.780449\n",
       " 50000    51.764904\n",
       " 10000    51.779808\n",
       " 5000     51.775962\n",
       " 1000     51.780288\n",
       " dtype: float64,\n",
       " 75000    0.047666\n",
       " 50000    0.042748\n",
       " 10000    0.026556\n",
       " 5000     0.038701\n",
       " 1000     0.038278\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_accuracy_lipschitz_df = pd.DataFrame(logit_accuracy_lipschitz)\n",
    "logit_accuracy_lipschitz_df.mean(), logit_accuracy_lipschitz_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 75000\n",
      "Epoch [1/75], Loss: 3.3893\n",
      "Epoch [1/75], Val Loss: 3.2876\n",
      "Epoch [2/75], Loss: 3.2100\n",
      "Epoch [2/75], Val Loss: 3.1802\n",
      "Epoch [3/75], Loss: 3.0715\n",
      "Epoch [3/75], Val Loss: 3.0702\n",
      "Epoch [4/75], Loss: 2.9060\n",
      "Epoch [4/75], Val Loss: 2.9066\n",
      "Epoch [5/75], Loss: 2.6687\n",
      "Epoch [5/75], Val Loss: 2.6574\n",
      "Epoch [6/75], Loss: 2.3342\n",
      "Epoch [6/75], Val Loss: 2.3375\n",
      "Epoch [7/75], Loss: 1.9452\n",
      "Epoch [7/75], Val Loss: 2.0189\n",
      "Epoch [8/75], Loss: 1.6195\n",
      "Epoch [8/75], Val Loss: 1.8608\n",
      "Epoch [9/75], Loss: 1.3972\n",
      "Epoch [9/75], Val Loss: 1.7338\n",
      "Epoch [10/75], Loss: 1.2567\n",
      "Epoch [10/75], Val Loss: 1.7310\n",
      "Epoch [11/75], Loss: 1.1465\n",
      "Epoch [11/75], Val Loss: 1.7162\n",
      "Epoch [12/75], Loss: 1.0624\n",
      "Epoch [12/75], Val Loss: 1.7663\n",
      "Epoch [13/75], Loss: 0.9794\n",
      "Epoch [13/75], Val Loss: 1.6637\n",
      "Epoch [14/75], Loss: 0.8889\n",
      "Epoch [14/75], Val Loss: 1.5926\n",
      "Epoch [15/75], Loss: 0.7530\n",
      "Epoch [15/75], Val Loss: 1.5128\n",
      "Epoch [16/75], Loss: 0.6511\n",
      "Epoch [16/75], Val Loss: 1.5327\n",
      "Epoch [17/75], Loss: 0.5483\n",
      "Epoch [17/75], Val Loss: 1.4716\n",
      "Epoch [18/75], Loss: 0.4678\n",
      "Epoch [18/75], Val Loss: 1.5498\n",
      "Epoch [19/75], Loss: 0.4185\n",
      "Epoch [19/75], Val Loss: 1.4906\n",
      "Epoch [20/75], Loss: 0.3489\n",
      "Epoch [20/75], Val Loss: 1.5887\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [20/75], Loss: 0.3489\n",
      "Test Accuracy Base CNN: 59.98%\n",
      "Epoch [1/75], Loss: 3.4204\n",
      "Epoch [1/75], Val Loss: 3.3112\n",
      "Epoch [2/75], Loss: 3.2272\n",
      "Epoch [2/75], Val Loss: 3.2118\n",
      "Epoch [3/75], Loss: 3.1124\n",
      "Epoch [3/75], Val Loss: 3.1382\n",
      "Epoch [4/75], Loss: 2.9914\n",
      "Epoch [4/75], Val Loss: 3.0210\n",
      "Epoch [5/75], Loss: 2.8041\n",
      "Epoch [5/75], Val Loss: 2.8405\n",
      "Epoch [6/75], Loss: 2.5430\n",
      "Epoch [6/75], Val Loss: 2.5727\n",
      "Epoch [7/75], Loss: 2.2060\n",
      "Epoch [7/75], Val Loss: 2.2394\n",
      "Epoch [8/75], Loss: 1.8430\n",
      "Epoch [8/75], Val Loss: 1.9460\n",
      "Epoch [9/75], Loss: 1.5289\n",
      "Epoch [9/75], Val Loss: 1.7668\n",
      "Epoch [10/75], Loss: 1.3422\n",
      "Epoch [10/75], Val Loss: 1.6960\n",
      "Epoch [11/75], Loss: 1.1873\n",
      "Epoch [11/75], Val Loss: 1.6709\n",
      "Epoch [12/75], Loss: 1.1328\n",
      "Epoch [12/75], Val Loss: 1.6969\n",
      "Epoch [13/75], Loss: 1.0515\n",
      "Epoch [13/75], Val Loss: 1.6898\n",
      "Epoch [14/75], Loss: 1.0117\n",
      "Epoch [14/75], Val Loss: 1.5804\n",
      "Epoch [15/75], Loss: 0.8882\n",
      "Epoch [15/75], Val Loss: 1.6499\n",
      "Epoch [16/75], Loss: 0.8560\n",
      "Epoch [16/75], Val Loss: 1.4893\n",
      "Epoch [17/75], Loss: 0.7524\n",
      "Epoch [17/75], Val Loss: 1.5845\n",
      "Epoch [18/75], Loss: 0.6617\n",
      "Epoch [18/75], Val Loss: 1.6340\n",
      "Epoch [19/75], Loss: 0.5701\n",
      "Epoch [19/75], Val Loss: 1.4961\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [19/75], Loss: 0.5701\n",
      "Test Accuracy Base CNN: 59.93%\n",
      "Epoch [1/75], Loss: 3.4382\n",
      "Epoch [1/75], Val Loss: 3.2686\n",
      "Epoch [2/75], Loss: 3.2048\n",
      "Epoch [2/75], Val Loss: 3.1710\n",
      "Epoch [3/75], Loss: 3.0573\n",
      "Epoch [3/75], Val Loss: 3.0806\n",
      "Epoch [4/75], Loss: 2.9083\n",
      "Epoch [4/75], Val Loss: 2.9178\n",
      "Epoch [5/75], Loss: 2.6860\n",
      "Epoch [5/75], Val Loss: 2.6752\n",
      "Epoch [6/75], Loss: 2.3876\n",
      "Epoch [6/75], Val Loss: 2.3755\n",
      "Epoch [7/75], Loss: 2.0212\n",
      "Epoch [7/75], Val Loss: 2.0424\n",
      "Epoch [8/75], Loss: 1.6834\n",
      "Epoch [8/75], Val Loss: 1.8259\n",
      "Epoch [9/75], Loss: 1.4458\n",
      "Epoch [9/75], Val Loss: 1.6855\n",
      "Epoch [10/75], Loss: 1.2885\n",
      "Epoch [10/75], Val Loss: 1.6843\n",
      "Epoch [11/75], Loss: 1.2043\n",
      "Epoch [11/75], Val Loss: 1.6680\n",
      "Epoch [12/75], Loss: 1.0772\n",
      "Epoch [12/75], Val Loss: 1.6676\n",
      "Epoch [13/75], Loss: 0.9345\n",
      "Epoch [13/75], Val Loss: 1.6085\n",
      "Epoch [14/75], Loss: 0.8621\n",
      "Epoch [14/75], Val Loss: 1.5995\n",
      "Epoch [15/75], Loss: 0.7860\n",
      "Epoch [15/75], Val Loss: 1.5648\n",
      "Epoch [16/75], Loss: 0.6789\n",
      "Epoch [16/75], Val Loss: 1.5261\n",
      "Epoch [17/75], Loss: 0.5988\n",
      "Epoch [17/75], Val Loss: 1.5437\n",
      "Epoch [18/75], Loss: 0.5411\n",
      "Epoch [18/75], Val Loss: 1.5305\n",
      "Epoch [19/75], Loss: 0.5120\n",
      "Epoch [19/75], Val Loss: 1.5532\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [19/75], Loss: 0.5120\n",
      "Test Accuracy Base CNN: 59.93%\n",
      "Epoch [1/75], Loss: 3.3621\n",
      "Epoch [1/75], Val Loss: 3.2887\n",
      "Epoch [2/75], Loss: 3.2106\n",
      "Epoch [2/75], Val Loss: 3.1858\n",
      "Epoch [3/75], Loss: 3.0705\n",
      "Epoch [3/75], Val Loss: 3.0940\n",
      "Epoch [4/75], Loss: 2.9139\n",
      "Epoch [4/75], Val Loss: 2.9527\n",
      "Epoch [5/75], Loss: 2.6948\n",
      "Epoch [5/75], Val Loss: 2.7366\n",
      "Epoch [6/75], Loss: 2.3815\n",
      "Epoch [6/75], Val Loss: 2.4388\n",
      "Epoch [7/75], Loss: 2.0108\n",
      "Epoch [7/75], Val Loss: 2.1195\n",
      "Epoch [8/75], Loss: 1.6786\n",
      "Epoch [8/75], Val Loss: 1.9098\n",
      "Epoch [9/75], Loss: 1.4309\n",
      "Epoch [9/75], Val Loss: 1.7636\n",
      "Epoch [10/75], Loss: 1.2601\n",
      "Epoch [10/75], Val Loss: 1.7229\n",
      "Epoch [11/75], Loss: 1.1716\n",
      "Epoch [11/75], Val Loss: 1.6784\n",
      "Epoch [12/75], Loss: 1.0180\n",
      "Epoch [12/75], Val Loss: 1.6408\n",
      "Epoch [13/75], Loss: 0.8898\n",
      "Epoch [13/75], Val Loss: 1.7568\n",
      "Epoch [14/75], Loss: 0.9567\n",
      "Epoch [14/75], Val Loss: 1.6052\n",
      "Epoch [15/75], Loss: 0.7753\n",
      "Epoch [15/75], Val Loss: 1.6265\n",
      "Epoch [16/75], Loss: 0.6876\n",
      "Epoch [16/75], Val Loss: 1.5289\n",
      "Epoch [17/75], Loss: 0.5692\n",
      "Epoch [17/75], Val Loss: 1.5702\n",
      "Epoch [18/75], Loss: 0.4957\n",
      "Epoch [18/75], Val Loss: 1.5120\n",
      "Epoch [19/75], Loss: 0.4715\n",
      "Epoch [19/75], Val Loss: 1.5215\n",
      "Epoch [20/75], Loss: 0.4097\n",
      "Epoch [20/75], Val Loss: 1.6246\n",
      "Epoch [21/75], Loss: 0.3609\n",
      "Epoch [21/75], Val Loss: 1.6243\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [21/75], Loss: 0.3609\n",
      "Test Accuracy Base CNN: 59.03%\n",
      "Epoch [1/75], Loss: 3.3068\n",
      "Epoch [1/75], Val Loss: 3.2174\n",
      "Epoch [2/75], Loss: 3.1573\n",
      "Epoch [2/75], Val Loss: 3.1265\n",
      "Epoch [3/75], Loss: 3.0120\n",
      "Epoch [3/75], Val Loss: 3.0297\n",
      "Epoch [4/75], Loss: 2.8248\n",
      "Epoch [4/75], Val Loss: 2.8448\n",
      "Epoch [5/75], Loss: 2.5658\n",
      "Epoch [5/75], Val Loss: 2.5691\n",
      "Epoch [6/75], Loss: 2.2203\n",
      "Epoch [6/75], Val Loss: 2.2418\n",
      "Epoch [7/75], Loss: 1.8582\n",
      "Epoch [7/75], Val Loss: 1.9603\n",
      "Epoch [8/75], Loss: 1.5437\n",
      "Epoch [8/75], Val Loss: 1.8015\n",
      "Epoch [9/75], Loss: 1.4053\n",
      "Epoch [9/75], Val Loss: 1.7118\n",
      "Epoch [10/75], Loss: 1.2895\n",
      "Epoch [10/75], Val Loss: 1.6922\n",
      "Epoch [11/75], Loss: 1.1411\n",
      "Epoch [11/75], Val Loss: 1.7193\n",
      "Epoch [12/75], Loss: 1.0372\n",
      "Epoch [12/75], Val Loss: 1.7340\n",
      "Epoch [13/75], Loss: 0.9448\n",
      "Epoch [13/75], Val Loss: 1.6787\n",
      "Epoch [14/75], Loss: 0.8720\n",
      "Epoch [14/75], Val Loss: 1.6251\n",
      "Epoch [15/75], Loss: 0.8192\n",
      "Epoch [15/75], Val Loss: 1.5147\n",
      "Epoch [16/75], Loss: 0.7286\n",
      "Epoch [16/75], Val Loss: 1.5725\n",
      "Epoch [17/75], Loss: 0.5912\n",
      "Epoch [17/75], Val Loss: 1.5213\n",
      "Epoch [18/75], Loss: 0.5141\n",
      "Epoch [18/75], Val Loss: 1.5166\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [18/75], Loss: 0.5141\n",
      "Test Accuracy Base CNN: 59.20%\n",
      "Epoch [1/75], Loss: 3.3186\n",
      "Epoch [1/75], Val Loss: 3.2588\n",
      "Epoch [2/75], Loss: 3.1800\n",
      "Epoch [2/75], Val Loss: 3.1789\n",
      "Epoch [3/75], Loss: 3.0570\n",
      "Epoch [3/75], Val Loss: 3.0988\n",
      "Epoch [4/75], Loss: 2.9185\n",
      "Epoch [4/75], Val Loss: 2.9768\n",
      "Epoch [5/75], Loss: 2.7156\n",
      "Epoch [5/75], Val Loss: 2.7744\n",
      "Epoch [6/75], Loss: 2.4352\n",
      "Epoch [6/75], Val Loss: 2.5051\n",
      "Epoch [7/75], Loss: 2.0839\n",
      "Epoch [7/75], Val Loss: 2.1872\n",
      "Epoch [8/75], Loss: 1.7349\n",
      "Epoch [8/75], Val Loss: 1.9021\n",
      "Epoch [9/75], Loss: 1.4989\n",
      "Epoch [9/75], Val Loss: 1.7757\n",
      "Epoch [10/75], Loss: 1.3182\n",
      "Epoch [10/75], Val Loss: 1.7334\n",
      "Epoch [11/75], Loss: 1.2515\n",
      "Epoch [11/75], Val Loss: 1.6951\n",
      "Epoch [12/75], Loss: 1.1191\n",
      "Epoch [12/75], Val Loss: 1.6464\n",
      "Epoch [13/75], Loss: 0.9958\n",
      "Epoch [13/75], Val Loss: 1.6236\n",
      "Epoch [14/75], Loss: 0.9148\n",
      "Epoch [14/75], Val Loss: 1.6212\n",
      "Epoch [15/75], Loss: 0.8753\n",
      "Epoch [15/75], Val Loss: 1.5708\n",
      "Epoch [16/75], Loss: 0.7465\n",
      "Epoch [16/75], Val Loss: 1.5381\n",
      "Epoch [17/75], Loss: 0.6532\n",
      "Epoch [17/75], Val Loss: 1.5066\n",
      "Epoch [18/75], Loss: 0.5964\n",
      "Epoch [18/75], Val Loss: 1.5403\n",
      "Epoch [19/75], Loss: 0.5011\n",
      "Epoch [19/75], Val Loss: 1.4810\n",
      "Epoch [20/75], Loss: 0.4957\n",
      "Epoch [20/75], Val Loss: 1.5482\n",
      "Epoch [21/75], Loss: 0.4148\n",
      "Epoch [21/75], Val Loss: 1.5366\n",
      "Epoch [22/75], Loss: 0.3513\n",
      "Epoch [22/75], Val Loss: 1.5688\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [22/75], Loss: 0.3513\n",
      "Test Accuracy Base CNN: 60.20%\n",
      "Epoch [1/75], Loss: 3.3910\n",
      "Epoch [1/75], Val Loss: 3.3109\n",
      "Epoch [2/75], Loss: 3.2494\n",
      "Epoch [2/75], Val Loss: 3.2288\n",
      "Epoch [3/75], Loss: 3.1419\n",
      "Epoch [3/75], Val Loss: 3.1678\n",
      "Epoch [4/75], Loss: 3.0503\n",
      "Epoch [4/75], Val Loss: 3.0904\n",
      "Epoch [5/75], Loss: 2.9351\n",
      "Epoch [5/75], Val Loss: 2.9831\n",
      "Epoch [6/75], Loss: 2.7717\n",
      "Epoch [6/75], Val Loss: 2.8140\n",
      "Epoch [7/75], Loss: 2.5283\n",
      "Epoch [7/75], Val Loss: 2.5638\n",
      "Epoch [8/75], Loss: 2.2182\n",
      "Epoch [8/75], Val Loss: 2.2513\n",
      "Epoch [9/75], Loss: 1.8592\n",
      "Epoch [9/75], Val Loss: 1.9580\n",
      "Epoch [10/75], Loss: 1.5677\n",
      "Epoch [10/75], Val Loss: 1.7801\n",
      "Epoch [11/75], Loss: 1.3686\n",
      "Epoch [11/75], Val Loss: 1.7020\n",
      "Epoch [12/75], Loss: 1.2131\n",
      "Epoch [12/75], Val Loss: 1.6734\n",
      "Epoch [13/75], Loss: 1.1045\n",
      "Epoch [13/75], Val Loss: 1.7059\n",
      "Epoch [14/75], Loss: 1.0321\n",
      "Epoch [14/75], Val Loss: 1.6747\n",
      "Epoch [15/75], Loss: 0.9069\n",
      "Epoch [15/75], Val Loss: 1.6989\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [15/75], Loss: 0.9069\n",
      "Test Accuracy Base CNN: 55.09%\n",
      "Epoch [1/75], Loss: 3.3282\n",
      "Epoch [1/75], Val Loss: 3.2819\n",
      "Epoch [2/75], Loss: 3.1973\n",
      "Epoch [2/75], Val Loss: 3.1988\n",
      "Epoch [3/75], Loss: 3.0745\n",
      "Epoch [3/75], Val Loss: 3.1266\n",
      "Epoch [4/75], Loss: 2.9367\n",
      "Epoch [4/75], Val Loss: 3.0137\n",
      "Epoch [5/75], Loss: 2.7443\n",
      "Epoch [5/75], Val Loss: 2.8245\n",
      "Epoch [6/75], Loss: 2.4768\n",
      "Epoch [6/75], Val Loss: 2.5549\n",
      "Epoch [7/75], Loss: 2.1521\n",
      "Epoch [7/75], Val Loss: 2.2613\n",
      "Epoch [8/75], Loss: 1.8163\n",
      "Epoch [8/75], Val Loss: 1.9973\n",
      "Epoch [9/75], Loss: 1.5362\n",
      "Epoch [9/75], Val Loss: 1.8233\n",
      "Epoch [10/75], Loss: 1.3287\n",
      "Epoch [10/75], Val Loss: 1.8138\n",
      "Epoch [11/75], Loss: 1.2153\n",
      "Epoch [11/75], Val Loss: 1.7914\n",
      "Epoch [12/75], Loss: 1.1607\n",
      "Epoch [12/75], Val Loss: 1.8690\n",
      "Epoch [13/75], Loss: 1.0977\n",
      "Epoch [13/75], Val Loss: 1.8103\n",
      "Epoch [14/75], Loss: 0.9856\n",
      "Epoch [14/75], Val Loss: 1.7705\n",
      "Epoch [15/75], Loss: 0.9849\n",
      "Epoch [15/75], Val Loss: 1.7597\n",
      "Epoch [16/75], Loss: 0.8045\n",
      "Epoch [16/75], Val Loss: 1.6934\n",
      "Epoch [17/75], Loss: 0.6773\n",
      "Epoch [17/75], Val Loss: 1.5546\n",
      "Epoch [18/75], Loss: 0.6374\n",
      "Epoch [18/75], Val Loss: 1.5078\n",
      "Epoch [19/75], Loss: 0.5398\n",
      "Epoch [19/75], Val Loss: 1.5954\n",
      "Epoch [20/75], Loss: 0.4678\n",
      "Epoch [20/75], Val Loss: 1.4833\n",
      "Epoch [21/75], Loss: 0.3903\n",
      "Epoch [21/75], Val Loss: 1.5785\n",
      "Epoch [22/75], Loss: 0.3806\n",
      "Epoch [22/75], Val Loss: 1.5737\n",
      "Epoch [23/75], Loss: 0.3160\n",
      "Epoch [23/75], Val Loss: 1.5957\n",
      "Stopping early at epoch 23 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [23/75], Loss: 0.3160\n",
      "Test Accuracy Base CNN: 59.86%\n",
      "Epoch [1/75], Loss: 3.2905\n",
      "Epoch [1/75], Val Loss: 3.2159\n",
      "Epoch [2/75], Loss: 3.1042\n",
      "Epoch [2/75], Val Loss: 3.0852\n",
      "Epoch [3/75], Loss: 2.9101\n",
      "Epoch [3/75], Val Loss: 2.9303\n",
      "Epoch [4/75], Loss: 2.6418\n",
      "Epoch [4/75], Val Loss: 2.6536\n",
      "Epoch [5/75], Loss: 2.2674\n",
      "Epoch [5/75], Val Loss: 2.2576\n",
      "Epoch [6/75], Loss: 1.8170\n",
      "Epoch [6/75], Val Loss: 1.9410\n",
      "Epoch [7/75], Loss: 1.5139\n",
      "Epoch [7/75], Val Loss: 1.7741\n",
      "Epoch [8/75], Loss: 1.3616\n",
      "Epoch [8/75], Val Loss: 1.9913\n",
      "Epoch [9/75], Loss: 1.2452\n",
      "Epoch [9/75], Val Loss: 1.8293\n",
      "Epoch [10/75], Loss: 1.0670\n",
      "Epoch [10/75], Val Loss: 1.6871\n",
      "Epoch [11/75], Loss: 0.9494\n",
      "Epoch [11/75], Val Loss: 1.6042\n",
      "Epoch [12/75], Loss: 0.8171\n",
      "Epoch [12/75], Val Loss: 1.6507\n",
      "Epoch [13/75], Loss: 0.7214\n",
      "Epoch [13/75], Val Loss: 1.6850\n",
      "Epoch [14/75], Loss: 0.7465\n",
      "Epoch [14/75], Val Loss: 1.5654\n",
      "Epoch [15/75], Loss: 0.5983\n",
      "Epoch [15/75], Val Loss: 1.5839\n",
      "Epoch [16/75], Loss: 0.5270\n",
      "Epoch [16/75], Val Loss: 1.5092\n",
      "Epoch [17/75], Loss: 0.4566\n",
      "Epoch [17/75], Val Loss: 1.4595\n",
      "Epoch [18/75], Loss: 0.3736\n",
      "Epoch [18/75], Val Loss: 1.5037\n",
      "Epoch [19/75], Loss: 0.3433\n",
      "Epoch [19/75], Val Loss: 1.5154\n",
      "Epoch [20/75], Loss: 0.2606\n",
      "Epoch [20/75], Val Loss: 1.5558\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [20/75], Loss: 0.2606\n",
      "Test Accuracy Base CNN: 60.87%\n",
      "Epoch [1/75], Loss: 3.3004\n",
      "Epoch [1/75], Val Loss: 3.2654\n",
      "Epoch [2/75], Loss: 3.2214\n",
      "Epoch [2/75], Val Loss: 3.2022\n",
      "Epoch [3/75], Loss: 3.1275\n",
      "Epoch [3/75], Val Loss: 3.1377\n",
      "Epoch [4/75], Loss: 3.0222\n",
      "Epoch [4/75], Val Loss: 3.0503\n",
      "Epoch [5/75], Loss: 2.8808\n",
      "Epoch [5/75], Val Loss: 2.9151\n",
      "Epoch [6/75], Loss: 2.6847\n",
      "Epoch [6/75], Val Loss: 2.7182\n",
      "Epoch [7/75], Loss: 2.4313\n",
      "Epoch [7/75], Val Loss: 2.4503\n",
      "Epoch [8/75], Loss: 2.0971\n",
      "Epoch [8/75], Val Loss: 2.1366\n",
      "Epoch [9/75], Loss: 1.7611\n",
      "Epoch [9/75], Val Loss: 1.8644\n",
      "Epoch [10/75], Loss: 1.4953\n",
      "Epoch [10/75], Val Loss: 1.7309\n",
      "Epoch [11/75], Loss: 1.3365\n",
      "Epoch [11/75], Val Loss: 1.6819\n",
      "Epoch [12/75], Loss: 1.1715\n",
      "Epoch [12/75], Val Loss: 1.6748\n",
      "Epoch [13/75], Loss: 1.0941\n",
      "Epoch [13/75], Val Loss: 1.6301\n",
      "Epoch [14/75], Loss: 0.9871\n",
      "Epoch [14/75], Val Loss: 1.7580\n",
      "Epoch [15/75], Loss: 0.9998\n",
      "Epoch [15/75], Val Loss: 1.6656\n",
      "Epoch [16/75], Loss: 0.9354\n",
      "Epoch [16/75], Val Loss: 1.6377\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [16/75], Loss: 0.9354\n",
      "Test Accuracy Base CNN: 56.00%\n",
      "Epoch [1/75], Loss: 3.3334\n",
      "Epoch [1/75], Val Loss: 3.2502\n",
      "Epoch [2/75], Loss: 3.1669\n",
      "Epoch [2/75], Val Loss: 3.1257\n",
      "Epoch [3/75], Loss: 3.0023\n",
      "Epoch [3/75], Val Loss: 2.9802\n",
      "Epoch [4/75], Loss: 2.7711\n",
      "Epoch [4/75], Val Loss: 2.7664\n",
      "Epoch [5/75], Loss: 2.4607\n",
      "Epoch [5/75], Val Loss: 2.4673\n",
      "Epoch [6/75], Loss: 2.0541\n",
      "Epoch [6/75], Val Loss: 2.1097\n",
      "Epoch [7/75], Loss: 1.7001\n",
      "Epoch [7/75], Val Loss: 1.8994\n",
      "Epoch [8/75], Loss: 1.4616\n",
      "Epoch [8/75], Val Loss: 1.8280\n",
      "Epoch [9/75], Loss: 1.3297\n",
      "Epoch [9/75], Val Loss: 1.7371\n",
      "Epoch [10/75], Loss: 1.1638\n",
      "Epoch [10/75], Val Loss: 1.7093\n",
      "Epoch [11/75], Loss: 1.0483\n",
      "Epoch [11/75], Val Loss: 1.6877\n",
      "Epoch [12/75], Loss: 1.0144\n",
      "Epoch [12/75], Val Loss: 1.6471\n",
      "Epoch [13/75], Loss: 0.8869\n",
      "Epoch [13/75], Val Loss: 1.6390\n",
      "Epoch [14/75], Loss: 0.7592\n",
      "Epoch [14/75], Val Loss: 1.7214\n",
      "Epoch [15/75], Loss: 0.7322\n",
      "Epoch [15/75], Val Loss: 1.6248\n",
      "Epoch [16/75], Loss: 0.6627\n",
      "Epoch [16/75], Val Loss: 1.5717\n",
      "Epoch [17/75], Loss: 0.5453\n",
      "Epoch [17/75], Val Loss: 1.5192\n",
      "Epoch [18/75], Loss: 0.4830\n",
      "Epoch [18/75], Val Loss: 1.5674\n",
      "Epoch [19/75], Loss: 0.4366\n",
      "Epoch [19/75], Val Loss: 1.5521\n",
      "Epoch [20/75], Loss: 0.4019\n",
      "Epoch [20/75], Val Loss: 1.5231\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [20/75], Loss: 0.4019\n",
      "Test Accuracy Base CNN: 61.44%\n",
      "Epoch [1/75], Loss: 3.3448\n",
      "Epoch [1/75], Val Loss: 3.2517\n",
      "Epoch [2/75], Loss: 3.1776\n",
      "Epoch [2/75], Val Loss: 3.1396\n",
      "Epoch [3/75], Loss: 3.0145\n",
      "Epoch [3/75], Val Loss: 3.0280\n",
      "Epoch [4/75], Loss: 2.8399\n",
      "Epoch [4/75], Val Loss: 2.8461\n",
      "Epoch [5/75], Loss: 2.5725\n",
      "Epoch [5/75], Val Loss: 2.5635\n",
      "Epoch [6/75], Loss: 2.2298\n",
      "Epoch [6/75], Val Loss: 2.2137\n",
      "Epoch [7/75], Loss: 1.8262\n",
      "Epoch [7/75], Val Loss: 1.8983\n",
      "Epoch [8/75], Loss: 1.5303\n",
      "Epoch [8/75], Val Loss: 1.7549\n",
      "Epoch [9/75], Loss: 1.3150\n",
      "Epoch [9/75], Val Loss: 1.6833\n",
      "Epoch [10/75], Loss: 1.1928\n",
      "Epoch [10/75], Val Loss: 1.7289\n",
      "Epoch [11/75], Loss: 1.1246\n",
      "Epoch [11/75], Val Loss: 1.6555\n",
      "Epoch [12/75], Loss: 1.0456\n",
      "Epoch [12/75], Val Loss: 1.7512\n",
      "Epoch [13/75], Loss: 0.9222\n",
      "Epoch [13/75], Val Loss: 1.5637\n",
      "Epoch [14/75], Loss: 0.8094\n",
      "Epoch [14/75], Val Loss: 1.5966\n",
      "Epoch [15/75], Loss: 0.7029\n",
      "Epoch [15/75], Val Loss: 1.5226\n",
      "Epoch [16/75], Loss: 0.6158\n",
      "Epoch [16/75], Val Loss: 1.5215\n",
      "Epoch [17/75], Loss: 0.5677\n",
      "Epoch [17/75], Val Loss: 1.5600\n",
      "Epoch [18/75], Loss: 0.5179\n",
      "Epoch [18/75], Val Loss: 1.5394\n",
      "Epoch [19/75], Loss: 0.4800\n",
      "Epoch [19/75], Val Loss: 1.5642\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [19/75], Loss: 0.4800\n",
      "Test Accuracy Base CNN: 59.57%\n",
      "Epoch [1/75], Loss: 3.3532\n",
      "Epoch [1/75], Val Loss: 3.2621\n",
      "Epoch [2/75], Loss: 3.1840\n",
      "Epoch [2/75], Val Loss: 3.1700\n",
      "Epoch [3/75], Loss: 3.0315\n",
      "Epoch [3/75], Val Loss: 3.0590\n",
      "Epoch [4/75], Loss: 2.8289\n",
      "Epoch [4/75], Val Loss: 2.8520\n",
      "Epoch [5/75], Loss: 2.5318\n",
      "Epoch [5/75], Val Loss: 2.5384\n",
      "Epoch [6/75], Loss: 2.1579\n",
      "Epoch [6/75], Val Loss: 2.2025\n",
      "Epoch [7/75], Loss: 1.7873\n",
      "Epoch [7/75], Val Loss: 1.9235\n",
      "Epoch [8/75], Loss: 1.5198\n",
      "Epoch [8/75], Val Loss: 1.7979\n",
      "Epoch [9/75], Loss: 1.3245\n",
      "Epoch [9/75], Val Loss: 1.7597\n",
      "Epoch [10/75], Loss: 1.1802\n",
      "Epoch [10/75], Val Loss: 1.7062\n",
      "Epoch [11/75], Loss: 1.0424\n",
      "Epoch [11/75], Val Loss: 1.6716\n",
      "Epoch [12/75], Loss: 0.9578\n",
      "Epoch [12/75], Val Loss: 1.6616\n",
      "Epoch [13/75], Loss: 0.9241\n",
      "Epoch [13/75], Val Loss: 1.7164\n",
      "Epoch [14/75], Loss: 0.9182\n",
      "Epoch [14/75], Val Loss: 1.7249\n",
      "Epoch [15/75], Loss: 0.8265\n",
      "Epoch [15/75], Val Loss: 1.5856\n",
      "Epoch [16/75], Loss: 0.7016\n",
      "Epoch [16/75], Val Loss: 1.5621\n",
      "Epoch [17/75], Loss: 0.5770\n",
      "Epoch [17/75], Val Loss: 1.5089\n",
      "Epoch [18/75], Loss: 0.5173\n",
      "Epoch [18/75], Val Loss: 1.4910\n",
      "Epoch [19/75], Loss: 0.4489\n",
      "Epoch [19/75], Val Loss: 1.5735\n",
      "Epoch [20/75], Loss: 0.3901\n",
      "Epoch [20/75], Val Loss: 1.5808\n",
      "Epoch [21/75], Loss: 0.3445\n",
      "Epoch [21/75], Val Loss: 1.5706\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [21/75], Loss: 0.3445\n",
      "Test Accuracy Base CNN: 61.37%\n",
      "Epoch [1/75], Loss: 3.3340\n",
      "Epoch [1/75], Val Loss: 3.2914\n",
      "Epoch [2/75], Loss: 3.2010\n",
      "Epoch [2/75], Val Loss: 3.1900\n",
      "Epoch [3/75], Loss: 3.0567\n",
      "Epoch [3/75], Val Loss: 3.0692\n",
      "Epoch [4/75], Loss: 2.8674\n",
      "Epoch [4/75], Val Loss: 2.8828\n",
      "Epoch [5/75], Loss: 2.5964\n",
      "Epoch [5/75], Val Loss: 2.6333\n",
      "Epoch [6/75], Loss: 2.2548\n",
      "Epoch [6/75], Val Loss: 2.2962\n",
      "Epoch [7/75], Loss: 1.8401\n",
      "Epoch [7/75], Val Loss: 1.9622\n",
      "Epoch [8/75], Loss: 1.5117\n",
      "Epoch [8/75], Val Loss: 1.7786\n",
      "Epoch [9/75], Loss: 1.3140\n",
      "Epoch [9/75], Val Loss: 1.7573\n",
      "Epoch [10/75], Loss: 1.1533\n",
      "Epoch [10/75], Val Loss: 1.6451\n",
      "Epoch [11/75], Loss: 1.0048\n",
      "Epoch [11/75], Val Loss: 1.6669\n",
      "Epoch [12/75], Loss: 0.9790\n",
      "Epoch [12/75], Val Loss: 1.6796\n",
      "Epoch [13/75], Loss: 0.9074\n",
      "Epoch [13/75], Val Loss: 1.6416\n",
      "Epoch [14/75], Loss: 0.7793\n",
      "Epoch [14/75], Val Loss: 1.5496\n",
      "Epoch [15/75], Loss: 0.6284\n",
      "Epoch [15/75], Val Loss: 1.4922\n",
      "Epoch [16/75], Loss: 0.5747\n",
      "Epoch [16/75], Val Loss: 1.4870\n",
      "Epoch [17/75], Loss: 0.4978\n",
      "Epoch [17/75], Val Loss: 1.4859\n",
      "Epoch [18/75], Loss: 0.4383\n",
      "Epoch [18/75], Val Loss: 1.5124\n",
      "Epoch [19/75], Loss: 0.3951\n",
      "Epoch [19/75], Val Loss: 1.4945\n",
      "Epoch [20/75], Loss: 0.3158\n",
      "Epoch [20/75], Val Loss: 1.4665\n",
      "Epoch [21/75], Loss: 0.2605\n",
      "Epoch [21/75], Val Loss: 1.5027\n",
      "Epoch [22/75], Loss: 0.2351\n",
      "Epoch [22/75], Val Loss: 1.5414\n",
      "Epoch [23/75], Loss: 0.1896\n",
      "Epoch [23/75], Val Loss: 1.5303\n",
      "Stopping early at epoch 23 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [23/75], Loss: 0.1896\n",
      "Test Accuracy Base CNN: 63.73%\n",
      "Epoch [1/75], Loss: 3.3298\n",
      "Epoch [1/75], Val Loss: 3.2677\n",
      "Epoch [2/75], Loss: 3.2297\n",
      "Epoch [2/75], Val Loss: 3.1894\n",
      "Epoch [3/75], Loss: 3.1170\n",
      "Epoch [3/75], Val Loss: 3.1168\n",
      "Epoch [4/75], Loss: 3.0109\n",
      "Epoch [4/75], Val Loss: 3.0286\n",
      "Epoch [5/75], Loss: 2.8655\n",
      "Epoch [5/75], Val Loss: 2.8830\n",
      "Epoch [6/75], Loss: 2.6606\n",
      "Epoch [6/75], Val Loss: 2.6636\n",
      "Epoch [7/75], Loss: 2.3783\n",
      "Epoch [7/75], Val Loss: 2.3601\n",
      "Epoch [8/75], Loss: 2.0136\n",
      "Epoch [8/75], Val Loss: 2.0422\n",
      "Epoch [9/75], Loss: 1.6757\n",
      "Epoch [9/75], Val Loss: 1.7719\n",
      "Epoch [10/75], Loss: 1.4318\n",
      "Epoch [10/75], Val Loss: 1.6613\n",
      "Epoch [11/75], Loss: 1.2639\n",
      "Epoch [11/75], Val Loss: 1.6377\n",
      "Epoch [12/75], Loss: 1.1724\n",
      "Epoch [12/75], Val Loss: 1.5931\n",
      "Epoch [13/75], Loss: 1.0693\n",
      "Epoch [13/75], Val Loss: 1.5712\n",
      "Epoch [14/75], Loss: 0.9826\n",
      "Epoch [14/75], Val Loss: 1.5620\n",
      "Epoch [15/75], Loss: 0.9466\n",
      "Epoch [15/75], Val Loss: 1.5702\n",
      "Epoch [16/75], Loss: 0.8220\n",
      "Epoch [16/75], Val Loss: 1.5237\n",
      "Epoch [17/75], Loss: 0.6944\n",
      "Epoch [17/75], Val Loss: 1.5105\n",
      "Epoch [18/75], Loss: 0.6750\n",
      "Epoch [18/75], Val Loss: 1.5511\n",
      "Epoch [19/75], Loss: 0.5783\n",
      "Epoch [19/75], Val Loss: 1.4767\n",
      "Epoch [20/75], Loss: 0.4879\n",
      "Epoch [20/75], Val Loss: 1.4820\n",
      "Epoch [21/75], Loss: 0.4248\n",
      "Epoch [21/75], Val Loss: 1.5091\n",
      "Epoch [22/75], Loss: 0.3897\n",
      "Epoch [22/75], Val Loss: 1.5252\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [22/75], Loss: 0.3897\n",
      "Test Accuracy Base CNN: 61.15%\n",
      "Epoch [1/75], Loss: 3.3438\n",
      "Epoch [1/75], Val Loss: 3.2603\n",
      "Epoch [2/75], Loss: 3.1853\n",
      "Epoch [2/75], Val Loss: 3.1490\n",
      "Epoch [3/75], Loss: 3.0276\n",
      "Epoch [3/75], Val Loss: 3.0347\n",
      "Epoch [4/75], Loss: 2.8470\n",
      "Epoch [4/75], Val Loss: 2.8663\n",
      "Epoch [5/75], Loss: 2.5869\n",
      "Epoch [5/75], Val Loss: 2.6150\n",
      "Epoch [6/75], Loss: 2.2658\n",
      "Epoch [6/75], Val Loss: 2.2893\n",
      "Epoch [7/75], Loss: 1.9085\n",
      "Epoch [7/75], Val Loss: 2.0073\n",
      "Epoch [8/75], Loss: 1.6076\n",
      "Epoch [8/75], Val Loss: 1.8099\n",
      "Epoch [9/75], Loss: 1.3691\n",
      "Epoch [9/75], Val Loss: 1.7429\n",
      "Epoch [10/75], Loss: 1.2000\n",
      "Epoch [10/75], Val Loss: 1.7268\n",
      "Epoch [11/75], Loss: 1.0702\n",
      "Epoch [11/75], Val Loss: 1.6788\n",
      "Epoch [12/75], Loss: 1.0446\n",
      "Epoch [12/75], Val Loss: 1.7302\n",
      "Epoch [13/75], Loss: 0.9328\n",
      "Epoch [13/75], Val Loss: 1.7221\n",
      "Epoch [14/75], Loss: 0.8631\n",
      "Epoch [14/75], Val Loss: 1.7028\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [14/75], Loss: 0.8631\n",
      "Test Accuracy Base CNN: 53.39%\n",
      "Epoch [1/75], Loss: 3.3844\n",
      "Epoch [1/75], Val Loss: 3.2898\n",
      "Epoch [2/75], Loss: 3.2076\n",
      "Epoch [2/75], Val Loss: 3.1943\n",
      "Epoch [3/75], Loss: 3.0631\n",
      "Epoch [3/75], Val Loss: 3.0973\n",
      "Epoch [4/75], Loss: 2.9046\n",
      "Epoch [4/75], Val Loss: 2.9498\n",
      "Epoch [5/75], Loss: 2.6764\n",
      "Epoch [5/75], Val Loss: 2.7196\n",
      "Epoch [6/75], Loss: 2.3367\n",
      "Epoch [6/75], Val Loss: 2.3858\n",
      "Epoch [7/75], Loss: 1.9465\n",
      "Epoch [7/75], Val Loss: 2.0369\n",
      "Epoch [8/75], Loss: 1.5754\n",
      "Epoch [8/75], Val Loss: 1.8268\n",
      "Epoch [9/75], Loss: 1.3331\n",
      "Epoch [9/75], Val Loss: 1.7530\n",
      "Epoch [10/75], Loss: 1.2066\n",
      "Epoch [10/75], Val Loss: 1.7663\n",
      "Epoch [11/75], Loss: 1.0985\n",
      "Epoch [11/75], Val Loss: 1.6255\n",
      "Epoch [12/75], Loss: 0.9595\n",
      "Epoch [12/75], Val Loss: 1.7017\n",
      "Epoch [13/75], Loss: 0.9046\n",
      "Epoch [13/75], Val Loss: 1.6474\n",
      "Epoch [14/75], Loss: 0.8368\n",
      "Epoch [14/75], Val Loss: 1.6582\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [14/75], Loss: 0.8368\n",
      "Test Accuracy Base CNN: 55.52%\n",
      "Epoch [1/75], Loss: 3.2962\n",
      "Epoch [1/75], Val Loss: 3.2657\n",
      "Epoch [2/75], Loss: 3.1963\n",
      "Epoch [2/75], Val Loss: 3.1772\n",
      "Epoch [3/75], Loss: 3.0725\n",
      "Epoch [3/75], Val Loss: 3.0821\n",
      "Epoch [4/75], Loss: 2.9232\n",
      "Epoch [4/75], Val Loss: 2.9436\n",
      "Epoch [5/75], Loss: 2.7096\n",
      "Epoch [5/75], Val Loss: 2.7369\n",
      "Epoch [6/75], Loss: 2.4005\n",
      "Epoch [6/75], Val Loss: 2.4125\n",
      "Epoch [7/75], Loss: 2.0287\n",
      "Epoch [7/75], Val Loss: 2.0859\n",
      "Epoch [8/75], Loss: 1.6938\n",
      "Epoch [8/75], Val Loss: 1.9121\n",
      "Epoch [9/75], Loss: 1.4671\n",
      "Epoch [9/75], Val Loss: 1.8034\n",
      "Epoch [10/75], Loss: 1.3181\n",
      "Epoch [10/75], Val Loss: 1.8140\n",
      "Epoch [11/75], Loss: 1.2475\n",
      "Epoch [11/75], Val Loss: 1.7297\n",
      "Epoch [12/75], Loss: 1.1235\n",
      "Epoch [12/75], Val Loss: 1.7516\n",
      "Epoch [13/75], Loss: 0.9976\n",
      "Epoch [13/75], Val Loss: 1.7062\n",
      "Epoch [14/75], Loss: 0.9020\n",
      "Epoch [14/75], Val Loss: 1.5620\n",
      "Epoch [15/75], Loss: 0.8092\n",
      "Epoch [15/75], Val Loss: 1.5298\n",
      "Epoch [16/75], Loss: 0.7278\n",
      "Epoch [16/75], Val Loss: 1.5008\n",
      "Epoch [17/75], Loss: 0.6709\n",
      "Epoch [17/75], Val Loss: 1.5146\n",
      "Epoch [18/75], Loss: 0.6113\n",
      "Epoch [18/75], Val Loss: 1.5573\n",
      "Epoch [19/75], Loss: 0.5185\n",
      "Epoch [19/75], Val Loss: 1.6089\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [19/75], Loss: 0.5185\n",
      "Test Accuracy Base CNN: 57.37%\n",
      "Epoch [1/75], Loss: 3.2977\n",
      "Epoch [1/75], Val Loss: 3.2470\n",
      "Epoch [2/75], Loss: 3.1554\n",
      "Epoch [2/75], Val Loss: 3.1437\n",
      "Epoch [3/75], Loss: 2.9984\n",
      "Epoch [3/75], Val Loss: 3.0163\n",
      "Epoch [4/75], Loss: 2.8070\n",
      "Epoch [4/75], Val Loss: 2.8298\n",
      "Epoch [5/75], Loss: 2.5445\n",
      "Epoch [5/75], Val Loss: 2.5587\n",
      "Epoch [6/75], Loss: 2.1898\n",
      "Epoch [6/75], Val Loss: 2.2339\n",
      "Epoch [7/75], Loss: 1.8208\n",
      "Epoch [7/75], Val Loss: 1.9393\n",
      "Epoch [8/75], Loss: 1.5198\n",
      "Epoch [8/75], Val Loss: 1.7938\n",
      "Epoch [9/75], Loss: 1.3247\n",
      "Epoch [9/75], Val Loss: 1.7316\n",
      "Epoch [10/75], Loss: 1.2139\n",
      "Epoch [10/75], Val Loss: 1.7334\n",
      "Epoch [11/75], Loss: 1.1096\n",
      "Epoch [11/75], Val Loss: 1.7016\n",
      "Epoch [12/75], Loss: 1.0423\n",
      "Epoch [12/75], Val Loss: 1.6020\n",
      "Epoch [13/75], Loss: 0.8913\n",
      "Epoch [13/75], Val Loss: 1.7876\n",
      "Epoch [14/75], Loss: 0.8272\n",
      "Epoch [14/75], Val Loss: 1.6069\n",
      "Epoch [15/75], Loss: 0.7106\n",
      "Epoch [15/75], Val Loss: 1.5862\n",
      "Epoch [16/75], Loss: 0.6166\n",
      "Epoch [16/75], Val Loss: 1.5263\n",
      "Epoch [17/75], Loss: 0.5685\n",
      "Epoch [17/75], Val Loss: 1.5509\n",
      "Epoch [18/75], Loss: 0.4801\n",
      "Epoch [18/75], Val Loss: 1.5360\n",
      "Epoch [19/75], Loss: 0.4736\n",
      "Epoch [19/75], Val Loss: 1.5935\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [19/75], Loss: 0.4736\n",
      "Test Accuracy Base CNN: 58.02%\n",
      "Epoch [1/75], Loss: 3.3786\n",
      "Epoch [1/75], Val Loss: 3.2754\n",
      "Epoch [2/75], Loss: 3.2140\n",
      "Epoch [2/75], Val Loss: 3.1915\n",
      "Epoch [3/75], Loss: 3.0904\n",
      "Epoch [3/75], Val Loss: 3.1085\n",
      "Epoch [4/75], Loss: 2.9598\n",
      "Epoch [4/75], Val Loss: 2.9842\n",
      "Epoch [5/75], Loss: 2.7630\n",
      "Epoch [5/75], Val Loss: 2.7773\n",
      "Epoch [6/75], Loss: 2.4878\n",
      "Epoch [6/75], Val Loss: 2.4870\n",
      "Epoch [7/75], Loss: 2.1147\n",
      "Epoch [7/75], Val Loss: 2.1499\n",
      "Epoch [8/75], Loss: 1.7435\n",
      "Epoch [8/75], Val Loss: 1.8914\n",
      "Epoch [9/75], Loss: 1.4673\n",
      "Epoch [9/75], Val Loss: 1.7966\n",
      "Epoch [10/75], Loss: 1.2973\n",
      "Epoch [10/75], Val Loss: 1.6562\n",
      "Epoch [11/75], Loss: 1.1803\n",
      "Epoch [11/75], Val Loss: 1.7474\n",
      "Epoch [12/75], Loss: 1.0886\n",
      "Epoch [12/75], Val Loss: 1.6372\n",
      "Epoch [13/75], Loss: 0.9979\n",
      "Epoch [13/75], Val Loss: 1.7581\n",
      "Epoch [14/75], Loss: 0.9243\n",
      "Epoch [14/75], Val Loss: 1.5922\n",
      "Epoch [15/75], Loss: 0.8193\n",
      "Epoch [15/75], Val Loss: 1.6940\n",
      "Epoch [16/75], Loss: 0.7544\n",
      "Epoch [16/75], Val Loss: 1.5477\n",
      "Epoch [17/75], Loss: 0.6359\n",
      "Epoch [17/75], Val Loss: 1.5705\n",
      "Epoch [18/75], Loss: 0.5678\n",
      "Epoch [18/75], Val Loss: 1.5440\n",
      "Epoch [19/75], Loss: 0.5279\n",
      "Epoch [19/75], Val Loss: 1.5493\n",
      "Epoch [20/75], Loss: 0.4403\n",
      "Epoch [20/75], Val Loss: 1.5006\n",
      "Epoch [21/75], Loss: 0.3895\n",
      "Epoch [21/75], Val Loss: 1.5099\n",
      "Epoch [22/75], Loss: 0.3197\n",
      "Epoch [22/75], Val Loss: 1.5848\n",
      "Epoch [23/75], Loss: 0.2915\n",
      "Epoch [23/75], Val Loss: 1.5586\n",
      "Stopping early at epoch 23 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [23/75], Loss: 0.2915\n",
      "Test Accuracy Base CNN: 61.19%\n",
      "Epoch [1/75], Loss: 3.3570\n",
      "Epoch [1/75], Val Loss: 3.2964\n",
      "Epoch [2/75], Loss: 3.2173\n",
      "Epoch [2/75], Val Loss: 3.2058\n",
      "Epoch [3/75], Loss: 3.0915\n",
      "Epoch [3/75], Val Loss: 3.1209\n",
      "Epoch [4/75], Loss: 2.9480\n",
      "Epoch [4/75], Val Loss: 2.9922\n",
      "Epoch [5/75], Loss: 2.7343\n",
      "Epoch [5/75], Val Loss: 2.7827\n",
      "Epoch [6/75], Loss: 2.4229\n",
      "Epoch [6/75], Val Loss: 2.4567\n",
      "Epoch [7/75], Loss: 2.0405\n",
      "Epoch [7/75], Val Loss: 2.1309\n",
      "Epoch [8/75], Loss: 1.6609\n",
      "Epoch [8/75], Val Loss: 1.8949\n",
      "Epoch [9/75], Loss: 1.4732\n",
      "Epoch [9/75], Val Loss: 1.8543\n",
      "Epoch [10/75], Loss: 1.3107\n",
      "Epoch [10/75], Val Loss: 1.8428\n",
      "Epoch [11/75], Loss: 1.2235\n",
      "Epoch [11/75], Val Loss: 1.7376\n",
      "Epoch [12/75], Loss: 1.1500\n",
      "Epoch [12/75], Val Loss: 1.8245\n",
      "Epoch [13/75], Loss: 1.0402\n",
      "Epoch [13/75], Val Loss: 1.5508\n",
      "Epoch [14/75], Loss: 0.8852\n",
      "Epoch [14/75], Val Loss: 1.5765\n",
      "Epoch [15/75], Loss: 0.7722\n",
      "Epoch [15/75], Val Loss: 1.5275\n",
      "Epoch [16/75], Loss: 0.6896\n",
      "Epoch [16/75], Val Loss: 1.4773\n",
      "Epoch [17/75], Loss: 0.6178\n",
      "Epoch [17/75], Val Loss: 1.5329\n",
      "Epoch [18/75], Loss: 0.5368\n",
      "Epoch [18/75], Val Loss: 1.5330\n",
      "Epoch [19/75], Loss: 0.5172\n",
      "Epoch [19/75], Val Loss: 1.5178\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [19/75], Loss: 0.5172\n",
      "Test Accuracy Base CNN: 60.37%\n",
      "Epoch [1/75], Loss: 3.3891\n",
      "Epoch [1/75], Val Loss: 3.2324\n",
      "Epoch [2/75], Loss: 3.1225\n",
      "Epoch [2/75], Val Loss: 3.1295\n",
      "Epoch [3/75], Loss: 2.9614\n",
      "Epoch [3/75], Val Loss: 2.9734\n",
      "Epoch [4/75], Loss: 2.7295\n",
      "Epoch [4/75], Val Loss: 2.7134\n",
      "Epoch [5/75], Loss: 2.3860\n",
      "Epoch [5/75], Val Loss: 2.3914\n",
      "Epoch [6/75], Loss: 1.9988\n",
      "Epoch [6/75], Val Loss: 2.0546\n",
      "Epoch [7/75], Loss: 1.6580\n",
      "Epoch [7/75], Val Loss: 1.8499\n",
      "Epoch [8/75], Loss: 1.4214\n",
      "Epoch [8/75], Val Loss: 1.7324\n",
      "Epoch [9/75], Loss: 1.2374\n",
      "Epoch [9/75], Val Loss: 1.6337\n",
      "Epoch [10/75], Loss: 1.0378\n",
      "Epoch [10/75], Val Loss: 1.6499\n",
      "Epoch [11/75], Loss: 0.9705\n",
      "Epoch [11/75], Val Loss: 1.6265\n",
      "Epoch [12/75], Loss: 0.8591\n",
      "Epoch [12/75], Val Loss: 1.6595\n",
      "Epoch [13/75], Loss: 0.7193\n",
      "Epoch [13/75], Val Loss: 1.6028\n",
      "Epoch [14/75], Loss: 0.6872\n",
      "Epoch [14/75], Val Loss: 1.5608\n",
      "Epoch [15/75], Loss: 0.6021\n",
      "Epoch [15/75], Val Loss: 1.5459\n",
      "Epoch [16/75], Loss: 0.5016\n",
      "Epoch [16/75], Val Loss: 1.5738\n",
      "Epoch [17/75], Loss: 0.4372\n",
      "Epoch [17/75], Val Loss: 1.5825\n",
      "Epoch [18/75], Loss: 0.4399\n",
      "Epoch [18/75], Val Loss: 1.6367\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [18/75], Loss: 0.4399\n",
      "Test Accuracy Base CNN: 59.43%\n",
      "Epoch [1/75], Loss: 3.3609\n",
      "Epoch [1/75], Val Loss: 3.2789\n",
      "Epoch [2/75], Loss: 3.1951\n",
      "Epoch [2/75], Val Loss: 3.1746\n",
      "Epoch [3/75], Loss: 3.0531\n",
      "Epoch [3/75], Val Loss: 3.0631\n",
      "Epoch [4/75], Loss: 2.8856\n",
      "Epoch [4/75], Val Loss: 2.8845\n",
      "Epoch [5/75], Loss: 2.6400\n",
      "Epoch [5/75], Val Loss: 2.6405\n",
      "Epoch [6/75], Loss: 2.3009\n",
      "Epoch [6/75], Val Loss: 2.3261\n",
      "Epoch [7/75], Loss: 1.9207\n",
      "Epoch [7/75], Val Loss: 2.0202\n",
      "Epoch [8/75], Loss: 1.6274\n",
      "Epoch [8/75], Val Loss: 1.8573\n",
      "Epoch [9/75], Loss: 1.3791\n",
      "Epoch [9/75], Val Loss: 1.8265\n",
      "Epoch [10/75], Loss: 1.2563\n",
      "Epoch [10/75], Val Loss: 1.7524\n",
      "Epoch [11/75], Loss: 1.1792\n",
      "Epoch [11/75], Val Loss: 1.7556\n",
      "Epoch [12/75], Loss: 1.1132\n",
      "Epoch [12/75], Val Loss: 1.6944\n",
      "Epoch [13/75], Loss: 0.9689\n",
      "Epoch [13/75], Val Loss: 1.6360\n",
      "Epoch [14/75], Loss: 0.8362\n",
      "Epoch [14/75], Val Loss: 1.5888\n",
      "Epoch [15/75], Loss: 0.7596\n",
      "Epoch [15/75], Val Loss: 1.5853\n",
      "Epoch [16/75], Loss: 0.6770\n",
      "Epoch [16/75], Val Loss: 1.4824\n",
      "Epoch [17/75], Loss: 0.5904\n",
      "Epoch [17/75], Val Loss: 1.5398\n",
      "Epoch [18/75], Loss: 0.5303\n",
      "Epoch [18/75], Val Loss: 1.5316\n",
      "Epoch [19/75], Loss: 0.4683\n",
      "Epoch [19/75], Val Loss: 1.5007\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [19/75], Loss: 0.4683\n",
      "Test Accuracy Base CNN: 60.43%\n",
      "Epoch [1/75], Loss: 3.2860\n",
      "Epoch [1/75], Val Loss: 3.2808\n",
      "Epoch [2/75], Loss: 3.2128\n",
      "Epoch [2/75], Val Loss: 3.2089\n",
      "Epoch [3/75], Loss: 3.1145\n",
      "Epoch [3/75], Val Loss: 3.1328\n",
      "Epoch [4/75], Loss: 2.9997\n",
      "Epoch [4/75], Val Loss: 3.0378\n",
      "Epoch [5/75], Loss: 2.8516\n",
      "Epoch [5/75], Val Loss: 2.9044\n",
      "Epoch [6/75], Loss: 2.6406\n",
      "Epoch [6/75], Val Loss: 2.7058\n",
      "Epoch [7/75], Loss: 2.3673\n",
      "Epoch [7/75], Val Loss: 2.4409\n",
      "Epoch [8/75], Loss: 2.0198\n",
      "Epoch [8/75], Val Loss: 2.1140\n",
      "Epoch [9/75], Loss: 1.6956\n",
      "Epoch [9/75], Val Loss: 1.8668\n",
      "Epoch [10/75], Loss: 1.4258\n",
      "Epoch [10/75], Val Loss: 1.6976\n",
      "Epoch [11/75], Loss: 1.2432\n",
      "Epoch [11/75], Val Loss: 1.7612\n",
      "Epoch [12/75], Loss: 1.1698\n",
      "Epoch [12/75], Val Loss: 1.8430\n",
      "Epoch [13/75], Loss: 1.0974\n",
      "Epoch [13/75], Val Loss: 1.8315\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [13/75], Loss: 1.0974\n",
      "Test Accuracy Base CNN: 50.74%\n",
      "Epoch [1/75], Loss: 3.3473\n",
      "Epoch [1/75], Val Loss: 3.2511\n",
      "Epoch [2/75], Loss: 3.1769\n",
      "Epoch [2/75], Val Loss: 3.1518\n",
      "Epoch [3/75], Loss: 3.0185\n",
      "Epoch [3/75], Val Loss: 3.0266\n",
      "Epoch [4/75], Loss: 2.8262\n",
      "Epoch [4/75], Val Loss: 2.8282\n",
      "Epoch [5/75], Loss: 2.5387\n",
      "Epoch [5/75], Val Loss: 2.5355\n",
      "Epoch [6/75], Loss: 2.1786\n",
      "Epoch [6/75], Val Loss: 2.1778\n",
      "Epoch [7/75], Loss: 1.8073\n",
      "Epoch [7/75], Val Loss: 1.8811\n",
      "Epoch [8/75], Loss: 1.5007\n",
      "Epoch [8/75], Val Loss: 1.7452\n",
      "Epoch [9/75], Loss: 1.3118\n",
      "Epoch [9/75], Val Loss: 1.7276\n",
      "Epoch [10/75], Loss: 1.2378\n",
      "Epoch [10/75], Val Loss: 1.8789\n",
      "Epoch [11/75], Loss: 1.1531\n",
      "Epoch [11/75], Val Loss: 1.7894\n",
      "Epoch [12/75], Loss: 1.0784\n",
      "Epoch [12/75], Val Loss: 1.8130\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [12/75], Loss: 1.0784\n",
      "Test Accuracy Base CNN: 50.82%\n",
      "Epoch [1/75], Loss: 3.3583\n",
      "Epoch [1/75], Val Loss: 3.2848\n",
      "Epoch [2/75], Loss: 3.1863\n",
      "Epoch [2/75], Val Loss: 3.1652\n",
      "Epoch [3/75], Loss: 3.0255\n",
      "Epoch [3/75], Val Loss: 3.0516\n",
      "Epoch [4/75], Loss: 2.8378\n",
      "Epoch [4/75], Val Loss: 2.8670\n",
      "Epoch [5/75], Loss: 2.5751\n",
      "Epoch [5/75], Val Loss: 2.6120\n",
      "Epoch [6/75], Loss: 2.2402\n",
      "Epoch [6/75], Val Loss: 2.2837\n",
      "Epoch [7/75], Loss: 1.8853\n",
      "Epoch [7/75], Val Loss: 1.9733\n",
      "Epoch [8/75], Loss: 1.5857\n",
      "Epoch [8/75], Val Loss: 1.8386\n",
      "Epoch [9/75], Loss: 1.4081\n",
      "Epoch [9/75], Val Loss: 1.7795\n",
      "Epoch [10/75], Loss: 1.3038\n",
      "Epoch [10/75], Val Loss: 1.7553\n",
      "Epoch [11/75], Loss: 1.1742\n",
      "Epoch [11/75], Val Loss: 1.6782\n",
      "Epoch [12/75], Loss: 1.0183\n",
      "Epoch [12/75], Val Loss: 1.5636\n",
      "Epoch [13/75], Loss: 0.9091\n",
      "Epoch [13/75], Val Loss: 1.5773\n",
      "Epoch [14/75], Loss: 0.8071\n",
      "Epoch [14/75], Val Loss: 1.5698\n",
      "Epoch [15/75], Loss: 0.8003\n",
      "Epoch [15/75], Val Loss: 1.5721\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [15/75], Loss: 0.8003\n",
      "Test Accuracy Base CNN: 56.48%\n",
      "Epoch [1/75], Loss: 3.3382\n",
      "Epoch [1/75], Val Loss: 3.2768\n",
      "Epoch [2/75], Loss: 3.1888\n",
      "Epoch [2/75], Val Loss: 3.1660\n",
      "Epoch [3/75], Loss: 3.0410\n",
      "Epoch [3/75], Val Loss: 3.0488\n",
      "Epoch [4/75], Loss: 2.8514\n",
      "Epoch [4/75], Val Loss: 2.8481\n",
      "Epoch [5/75], Loss: 2.5772\n",
      "Epoch [5/75], Val Loss: 2.5595\n",
      "Epoch [6/75], Loss: 2.1899\n",
      "Epoch [6/75], Val Loss: 2.1753\n",
      "Epoch [7/75], Loss: 1.7628\n",
      "Epoch [7/75], Val Loss: 1.8580\n",
      "Epoch [8/75], Loss: 1.4675\n",
      "Epoch [8/75], Val Loss: 1.7048\n",
      "Epoch [9/75], Loss: 1.2583\n",
      "Epoch [9/75], Val Loss: 1.7376\n",
      "Epoch [10/75], Loss: 1.2096\n",
      "Epoch [10/75], Val Loss: 1.6473\n",
      "Epoch [11/75], Loss: 1.0499\n",
      "Epoch [11/75], Val Loss: 1.8321\n",
      "Epoch [12/75], Loss: 1.0049\n",
      "Epoch [12/75], Val Loss: 1.6498\n",
      "Epoch [13/75], Loss: 0.9331\n",
      "Epoch [13/75], Val Loss: 1.6084\n",
      "Epoch [14/75], Loss: 0.8191\n",
      "Epoch [14/75], Val Loss: 1.6197\n",
      "Epoch [15/75], Loss: 0.7391\n",
      "Epoch [15/75], Val Loss: 1.4921\n",
      "Epoch [16/75], Loss: 0.6780\n",
      "Epoch [16/75], Val Loss: 1.6688\n",
      "Epoch [17/75], Loss: 0.6038\n",
      "Epoch [17/75], Val Loss: 1.5310\n",
      "Epoch [18/75], Loss: 0.5393\n",
      "Epoch [18/75], Val Loss: 1.5937\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [18/75], Loss: 0.5393\n",
      "Test Accuracy Base CNN: 58.50%\n",
      "Epoch [1/75], Loss: 3.4335\n",
      "Epoch [1/75], Val Loss: 3.2858\n",
      "Epoch [2/75], Loss: 3.1759\n",
      "Epoch [2/75], Val Loss: 3.1583\n",
      "Epoch [3/75], Loss: 3.0117\n",
      "Epoch [3/75], Val Loss: 3.0243\n",
      "Epoch [4/75], Loss: 2.8039\n",
      "Epoch [4/75], Val Loss: 2.7988\n",
      "Epoch [5/75], Loss: 2.4992\n",
      "Epoch [5/75], Val Loss: 2.5021\n",
      "Epoch [6/75], Loss: 2.1095\n",
      "Epoch [6/75], Val Loss: 2.1777\n",
      "Epoch [7/75], Loss: 1.7343\n",
      "Epoch [7/75], Val Loss: 1.8665\n",
      "Epoch [8/75], Loss: 1.4436\n",
      "Epoch [8/75], Val Loss: 1.7642\n",
      "Epoch [9/75], Loss: 1.3119\n",
      "Epoch [9/75], Val Loss: 1.7326\n",
      "Epoch [10/75], Loss: 1.2031\n",
      "Epoch [10/75], Val Loss: 1.8008\n",
      "Epoch [11/75], Loss: 1.0841\n",
      "Epoch [11/75], Val Loss: 1.7788\n",
      "Epoch [12/75], Loss: 1.0494\n",
      "Epoch [12/75], Val Loss: 1.6704\n",
      "Epoch [13/75], Loss: 0.8858\n",
      "Epoch [13/75], Val Loss: 1.5658\n",
      "Epoch [14/75], Loss: 0.8023\n",
      "Epoch [14/75], Val Loss: 1.5337\n",
      "Epoch [15/75], Loss: 0.7043\n",
      "Epoch [15/75], Val Loss: 1.5026\n",
      "Epoch [16/75], Loss: 0.5792\n",
      "Epoch [16/75], Val Loss: 1.4939\n",
      "Epoch [17/75], Loss: 0.4854\n",
      "Epoch [17/75], Val Loss: 1.4677\n",
      "Epoch [18/75], Loss: 0.4333\n",
      "Epoch [18/75], Val Loss: 1.4745\n",
      "Epoch [19/75], Loss: 0.3658\n",
      "Epoch [19/75], Val Loss: 1.5201\n",
      "Epoch [20/75], Loss: 0.3228\n",
      "Epoch [20/75], Val Loss: 1.5512\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [20/75], Loss: 0.3228\n",
      "Test Accuracy Base CNN: 60.24%\n",
      "Epoch [1/75], Loss: 3.3820\n",
      "Epoch [1/75], Val Loss: 3.2569\n",
      "Epoch [2/75], Loss: 3.2092\n",
      "Epoch [2/75], Val Loss: 3.1832\n",
      "Epoch [3/75], Loss: 3.0812\n",
      "Epoch [3/75], Val Loss: 3.0952\n",
      "Epoch [4/75], Loss: 2.9249\n",
      "Epoch [4/75], Val Loss: 2.9167\n",
      "Epoch [5/75], Loss: 2.6769\n",
      "Epoch [5/75], Val Loss: 2.6533\n",
      "Epoch [6/75], Loss: 2.3487\n",
      "Epoch [6/75], Val Loss: 2.3372\n",
      "Epoch [7/75], Loss: 1.9554\n",
      "Epoch [7/75], Val Loss: 2.0056\n",
      "Epoch [8/75], Loss: 1.6254\n",
      "Epoch [8/75], Val Loss: 1.8750\n",
      "Epoch [9/75], Loss: 1.4512\n",
      "Epoch [9/75], Val Loss: 1.7551\n",
      "Epoch [10/75], Loss: 1.2979\n",
      "Epoch [10/75], Val Loss: 1.7420\n",
      "Epoch [11/75], Loss: 1.2692\n",
      "Epoch [11/75], Val Loss: 1.7585\n",
      "Epoch [12/75], Loss: 1.2124\n",
      "Epoch [12/75], Val Loss: 1.8865\n",
      "Epoch [13/75], Loss: 1.0738\n",
      "Epoch [13/75], Val Loss: 1.5458\n",
      "Epoch [14/75], Loss: 0.9235\n",
      "Epoch [14/75], Val Loss: 1.4902\n",
      "Epoch [15/75], Loss: 0.8559\n",
      "Epoch [15/75], Val Loss: 1.5190\n",
      "Epoch [16/75], Loss: 0.7351\n",
      "Epoch [16/75], Val Loss: 1.4962\n",
      "Epoch [17/75], Loss: 0.6546\n",
      "Epoch [17/75], Val Loss: 1.4680\n",
      "Epoch [18/75], Loss: 0.5855\n",
      "Epoch [18/75], Val Loss: 1.5170\n",
      "Epoch [19/75], Loss: 0.4712\n",
      "Epoch [19/75], Val Loss: 1.4845\n",
      "Epoch [20/75], Loss: 0.4866\n",
      "Epoch [20/75], Val Loss: 1.5314\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [20/75], Loss: 0.4866\n",
      "Test Accuracy Base CNN: 60.31%\n",
      "Epoch [1/75], Loss: 3.4271\n",
      "Epoch [1/75], Val Loss: 3.2820\n",
      "Epoch [2/75], Loss: 3.1918\n",
      "Epoch [2/75], Val Loss: 3.1662\n",
      "Epoch [3/75], Loss: 3.0276\n",
      "Epoch [3/75], Val Loss: 3.0314\n",
      "Epoch [4/75], Loss: 2.8318\n",
      "Epoch [4/75], Val Loss: 2.8245\n",
      "Epoch [5/75], Loss: 2.5648\n",
      "Epoch [5/75], Val Loss: 2.5504\n",
      "Epoch [6/75], Loss: 2.2077\n",
      "Epoch [6/75], Val Loss: 2.2130\n",
      "Epoch [7/75], Loss: 1.8279\n",
      "Epoch [7/75], Val Loss: 1.9123\n",
      "Epoch [8/75], Loss: 1.5545\n",
      "Epoch [8/75], Val Loss: 1.8050\n",
      "Epoch [9/75], Loss: 1.3601\n",
      "Epoch [9/75], Val Loss: 1.7527\n",
      "Epoch [10/75], Loss: 1.1997\n",
      "Epoch [10/75], Val Loss: 1.6964\n",
      "Epoch [11/75], Loss: 1.1165\n",
      "Epoch [11/75], Val Loss: 1.8105\n",
      "Epoch [12/75], Loss: 1.0033\n",
      "Epoch [12/75], Val Loss: 1.7486\n",
      "Epoch [13/75], Loss: 0.9454\n",
      "Epoch [13/75], Val Loss: 1.6684\n",
      "Epoch [14/75], Loss: 0.8788\n",
      "Epoch [14/75], Val Loss: 1.6058\n",
      "Epoch [15/75], Loss: 0.7925\n",
      "Epoch [15/75], Val Loss: 1.6593\n",
      "Epoch [16/75], Loss: 0.7250\n",
      "Epoch [16/75], Val Loss: 1.5671\n",
      "Epoch [17/75], Loss: 0.6511\n",
      "Epoch [17/75], Val Loss: 1.5915\n",
      "Epoch [18/75], Loss: 0.5674\n",
      "Epoch [18/75], Val Loss: 1.4808\n",
      "Epoch [19/75], Loss: 0.5023\n",
      "Epoch [19/75], Val Loss: 1.4879\n",
      "Epoch [20/75], Loss: 0.4116\n",
      "Epoch [20/75], Val Loss: 1.5720\n",
      "Epoch [21/75], Loss: 0.3953\n",
      "Epoch [21/75], Val Loss: 1.5184\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [21/75], Loss: 0.3953\n",
      "Test Accuracy Base CNN: 60.60%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/75], Loss: 3.3329\n",
      "Epoch [1/75], Val Loss: 3.2684\n",
      "Epoch [2/75], Loss: 3.1641\n",
      "Epoch [2/75], Val Loss: 3.1421\n",
      "Epoch [3/75], Loss: 2.9831\n",
      "Epoch [3/75], Val Loss: 2.9692\n",
      "Epoch [4/75], Loss: 2.7384\n",
      "Epoch [4/75], Val Loss: 2.7206\n",
      "Epoch [5/75], Loss: 2.3967\n",
      "Epoch [5/75], Val Loss: 2.3813\n",
      "Epoch [6/75], Loss: 1.9462\n",
      "Epoch [6/75], Val Loss: 2.0074\n",
      "Epoch [7/75], Loss: 1.5988\n",
      "Epoch [7/75], Val Loss: 1.7732\n",
      "Epoch [8/75], Loss: 1.3627\n",
      "Epoch [8/75], Val Loss: 1.6595\n",
      "Epoch [9/75], Loss: 1.1719\n",
      "Epoch [9/75], Val Loss: 1.7266\n",
      "Epoch [10/75], Loss: 1.0936\n",
      "Epoch [10/75], Val Loss: 1.7168\n",
      "Epoch [11/75], Loss: 1.0660\n",
      "Epoch [11/75], Val Loss: 1.7514\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [11/75], Loss: 1.0660\n",
      "Test Accuracy Base CNN: 53.03%\n",
      "Epoch [1/75], Loss: 3.3896\n",
      "Epoch [1/75], Val Loss: 3.2896\n",
      "Epoch [2/75], Loss: 3.2234\n",
      "Epoch [2/75], Val Loss: 3.1906\n",
      "Epoch [3/75], Loss: 3.0896\n",
      "Epoch [3/75], Val Loss: 3.1018\n",
      "Epoch [4/75], Loss: 2.9367\n",
      "Epoch [4/75], Val Loss: 2.9669\n",
      "Epoch [5/75], Loss: 2.7131\n",
      "Epoch [5/75], Val Loss: 2.7324\n",
      "Epoch [6/75], Loss: 2.3975\n",
      "Epoch [6/75], Val Loss: 2.4323\n",
      "Epoch [7/75], Loss: 2.0509\n",
      "Epoch [7/75], Val Loss: 2.1549\n",
      "Epoch [8/75], Loss: 1.7067\n",
      "Epoch [8/75], Val Loss: 1.9273\n",
      "Epoch [9/75], Loss: 1.4572\n",
      "Epoch [9/75], Val Loss: 1.7832\n",
      "Epoch [10/75], Loss: 1.2699\n",
      "Epoch [10/75], Val Loss: 1.7825\n",
      "Epoch [11/75], Loss: 1.1294\n",
      "Epoch [11/75], Val Loss: 1.6529\n",
      "Epoch [12/75], Loss: 1.0846\n",
      "Epoch [12/75], Val Loss: 1.6384\n",
      "Epoch [13/75], Loss: 0.9496\n",
      "Epoch [13/75], Val Loss: 1.6240\n",
      "Epoch [14/75], Loss: 0.8166\n",
      "Epoch [14/75], Val Loss: 1.5833\n",
      "Epoch [15/75], Loss: 0.7179\n",
      "Epoch [15/75], Val Loss: 1.5818\n",
      "Epoch [16/75], Loss: 0.6283\n",
      "Epoch [16/75], Val Loss: 1.5921\n",
      "Epoch [17/75], Loss: 0.6230\n",
      "Epoch [17/75], Val Loss: 1.5494\n",
      "Epoch [18/75], Loss: 0.5396\n",
      "Epoch [18/75], Val Loss: 1.5874\n",
      "Epoch [19/75], Loss: 0.4409\n",
      "Epoch [19/75], Val Loss: 1.5801\n",
      "Epoch [20/75], Loss: 0.3789\n",
      "Epoch [20/75], Val Loss: 1.5751\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [20/75], Loss: 0.3789\n",
      "Test Accuracy Base CNN: 60.12%\n",
      "Epoch [1/75], Loss: 3.3163\n",
      "Epoch [1/75], Val Loss: 3.2740\n",
      "Epoch [2/75], Loss: 3.1490\n",
      "Epoch [2/75], Val Loss: 3.1464\n",
      "Epoch [3/75], Loss: 2.9808\n",
      "Epoch [3/75], Val Loss: 3.0134\n",
      "Epoch [4/75], Loss: 2.7605\n",
      "Epoch [4/75], Val Loss: 2.7960\n",
      "Epoch [5/75], Loss: 2.4536\n",
      "Epoch [5/75], Val Loss: 2.4811\n",
      "Epoch [6/75], Loss: 2.0694\n",
      "Epoch [6/75], Val Loss: 2.1639\n",
      "Epoch [7/75], Loss: 1.7358\n",
      "Epoch [7/75], Val Loss: 1.9337\n",
      "Epoch [8/75], Loss: 1.4896\n",
      "Epoch [8/75], Val Loss: 1.8275\n",
      "Epoch [9/75], Loss: 1.2973\n",
      "Epoch [9/75], Val Loss: 1.7686\n",
      "Epoch [10/75], Loss: 1.1737\n",
      "Epoch [10/75], Val Loss: 1.9534\n",
      "Epoch [11/75], Loss: 1.2032\n",
      "Epoch [11/75], Val Loss: 1.8841\n",
      "Epoch [12/75], Loss: 1.0565\n",
      "Epoch [12/75], Val Loss: 1.7653\n",
      "Epoch [13/75], Loss: 0.9679\n",
      "Epoch [13/75], Val Loss: 1.6344\n",
      "Epoch [14/75], Loss: 0.8494\n",
      "Epoch [14/75], Val Loss: 1.5650\n",
      "Epoch [15/75], Loss: 0.7236\n",
      "Epoch [15/75], Val Loss: 1.5920\n",
      "Epoch [16/75], Loss: 0.6505\n",
      "Epoch [16/75], Val Loss: 1.4702\n",
      "Epoch [17/75], Loss: 0.5456\n",
      "Epoch [17/75], Val Loss: 1.5388\n",
      "Epoch [18/75], Loss: 0.4925\n",
      "Epoch [18/75], Val Loss: 1.4966\n",
      "Epoch [19/75], Loss: 0.4217\n",
      "Epoch [19/75], Val Loss: 1.5139\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [19/75], Loss: 0.4217\n",
      "Test Accuracy Base CNN: 60.92%\n",
      "Epoch [1/75], Loss: 3.3140\n",
      "Epoch [1/75], Val Loss: 3.2475\n",
      "Epoch [2/75], Loss: 3.1784\n",
      "Epoch [2/75], Val Loss: 3.1527\n",
      "Epoch [3/75], Loss: 3.0411\n",
      "Epoch [3/75], Val Loss: 3.0549\n",
      "Epoch [4/75], Loss: 2.8699\n",
      "Epoch [4/75], Val Loss: 2.8807\n",
      "Epoch [5/75], Loss: 2.6149\n",
      "Epoch [5/75], Val Loss: 2.6182\n",
      "Epoch [6/75], Loss: 2.2764\n",
      "Epoch [6/75], Val Loss: 2.2879\n",
      "Epoch [7/75], Loss: 1.8949\n",
      "Epoch [7/75], Val Loss: 1.9982\n",
      "Epoch [8/75], Loss: 1.5986\n",
      "Epoch [8/75], Val Loss: 1.7903\n",
      "Epoch [9/75], Loss: 1.3940\n",
      "Epoch [9/75], Val Loss: 1.7752\n",
      "Epoch [10/75], Loss: 1.2553\n",
      "Epoch [10/75], Val Loss: 1.7092\n",
      "Epoch [11/75], Loss: 1.0796\n",
      "Epoch [11/75], Val Loss: 1.6649\n",
      "Epoch [12/75], Loss: 0.9722\n",
      "Epoch [12/75], Val Loss: 1.6943\n",
      "Epoch [13/75], Loss: 0.8800\n",
      "Epoch [13/75], Val Loss: 1.6095\n",
      "Epoch [14/75], Loss: 0.8003\n",
      "Epoch [14/75], Val Loss: 1.5829\n",
      "Epoch [15/75], Loss: 0.7096\n",
      "Epoch [15/75], Val Loss: 1.6523\n",
      "Epoch [16/75], Loss: 0.6441\n",
      "Epoch [16/75], Val Loss: 1.6595\n",
      "Epoch [17/75], Loss: 0.6189\n",
      "Epoch [17/75], Val Loss: 1.7068\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [17/75], Loss: 0.6189\n",
      "Test Accuracy Base CNN: 56.18%\n",
      "Epoch [1/75], Loss: 3.3676\n",
      "Epoch [1/75], Val Loss: 3.2678\n",
      "Epoch [2/75], Loss: 3.1816\n",
      "Epoch [2/75], Val Loss: 3.1313\n",
      "Epoch [3/75], Loss: 3.0058\n",
      "Epoch [3/75], Val Loss: 2.9948\n",
      "Epoch [4/75], Loss: 2.8003\n",
      "Epoch [4/75], Val Loss: 2.7813\n",
      "Epoch [5/75], Loss: 2.5048\n",
      "Epoch [5/75], Val Loss: 2.4700\n",
      "Epoch [6/75], Loss: 2.1118\n",
      "Epoch [6/75], Val Loss: 2.1235\n",
      "Epoch [7/75], Loss: 1.7423\n",
      "Epoch [7/75], Val Loss: 1.8441\n",
      "Epoch [8/75], Loss: 1.4674\n",
      "Epoch [8/75], Val Loss: 1.7335\n",
      "Epoch [9/75], Loss: 1.3627\n",
      "Epoch [9/75], Val Loss: 1.7154\n",
      "Epoch [10/75], Loss: 1.2298\n",
      "Epoch [10/75], Val Loss: 1.8041\n",
      "Epoch [11/75], Loss: 1.1292\n",
      "Epoch [11/75], Val Loss: 1.7199\n",
      "Epoch [12/75], Loss: 1.0435\n",
      "Epoch [12/75], Val Loss: 1.7540\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [12/75], Loss: 1.0435\n",
      "Test Accuracy Base CNN: 51.75%\n",
      "Epoch [1/75], Loss: 3.3748\n",
      "Epoch [1/75], Val Loss: 3.2595\n",
      "Epoch [2/75], Loss: 3.1742\n",
      "Epoch [2/75], Val Loss: 3.1533\n",
      "Epoch [3/75], Loss: 3.0233\n",
      "Epoch [3/75], Val Loss: 3.0447\n",
      "Epoch [4/75], Loss: 2.8418\n",
      "Epoch [4/75], Val Loss: 2.8541\n",
      "Epoch [5/75], Loss: 2.5852\n",
      "Epoch [5/75], Val Loss: 2.5804\n",
      "Epoch [6/75], Loss: 2.2312\n",
      "Epoch [6/75], Val Loss: 2.2521\n",
      "Epoch [7/75], Loss: 1.8497\n",
      "Epoch [7/75], Val Loss: 1.9821\n",
      "Epoch [8/75], Loss: 1.5310\n",
      "Epoch [8/75], Val Loss: 1.7739\n",
      "Epoch [9/75], Loss: 1.3048\n",
      "Epoch [9/75], Val Loss: 1.7621\n",
      "Epoch [10/75], Loss: 1.2385\n",
      "Epoch [10/75], Val Loss: 1.7555\n",
      "Epoch [11/75], Loss: 1.0916\n",
      "Epoch [11/75], Val Loss: 1.6997\n",
      "Epoch [12/75], Loss: 1.0670\n",
      "Epoch [12/75], Val Loss: 1.8008\n",
      "Epoch [13/75], Loss: 0.9320\n",
      "Epoch [13/75], Val Loss: 1.6736\n",
      "Epoch [14/75], Loss: 0.9102\n",
      "Epoch [14/75], Val Loss: 1.5872\n",
      "Epoch [15/75], Loss: 0.8353\n",
      "Epoch [15/75], Val Loss: 1.7033\n",
      "Epoch [16/75], Loss: 0.8770\n",
      "Epoch [16/75], Val Loss: 1.4662\n",
      "Epoch [17/75], Loss: 0.6470\n",
      "Epoch [17/75], Val Loss: 1.5655\n",
      "Epoch [18/75], Loss: 0.5807\n",
      "Epoch [18/75], Val Loss: 1.4715\n",
      "Epoch [19/75], Loss: 0.4597\n",
      "Epoch [19/75], Val Loss: 1.4749\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [19/75], Loss: 0.4597\n",
      "Test Accuracy Base CNN: 60.04%\n",
      "Epoch [1/75], Loss: 3.3893\n",
      "Epoch [1/75], Val Loss: 3.2378\n",
      "Epoch [2/75], Loss: 3.1590\n",
      "Epoch [2/75], Val Loss: 3.1165\n",
      "Epoch [3/75], Loss: 2.9819\n",
      "Epoch [3/75], Val Loss: 2.9714\n",
      "Epoch [4/75], Loss: 2.7371\n",
      "Epoch [4/75], Val Loss: 2.6943\n",
      "Epoch [5/75], Loss: 2.3771\n",
      "Epoch [5/75], Val Loss: 2.3350\n",
      "Epoch [6/75], Loss: 1.9836\n",
      "Epoch [6/75], Val Loss: 2.0221\n",
      "Epoch [7/75], Loss: 1.6246\n",
      "Epoch [7/75], Val Loss: 1.8639\n",
      "Epoch [8/75], Loss: 1.4748\n",
      "Epoch [8/75], Val Loss: 1.9618\n",
      "Epoch [9/75], Loss: 1.3636\n",
      "Epoch [9/75], Val Loss: 1.6961\n",
      "Epoch [10/75], Loss: 1.1546\n",
      "Epoch [10/75], Val Loss: 1.6793\n",
      "Epoch [11/75], Loss: 1.1026\n",
      "Epoch [11/75], Val Loss: 1.7355\n",
      "Epoch [12/75], Loss: 1.0270\n",
      "Epoch [12/75], Val Loss: 1.5689\n",
      "Epoch [13/75], Loss: 0.8735\n",
      "Epoch [13/75], Val Loss: 1.5717\n",
      "Epoch [14/75], Loss: 0.7698\n",
      "Epoch [14/75], Val Loss: 1.5483\n",
      "Epoch [15/75], Loss: 0.6542\n",
      "Epoch [15/75], Val Loss: 1.4726\n",
      "Epoch [16/75], Loss: 0.6002\n",
      "Epoch [16/75], Val Loss: 1.4902\n",
      "Epoch [17/75], Loss: 0.5167\n",
      "Epoch [17/75], Val Loss: 1.4986\n",
      "Epoch [18/75], Loss: 0.4858\n",
      "Epoch [18/75], Val Loss: 1.5050\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [18/75], Loss: 0.4858\n",
      "Test Accuracy Base CNN: 60.39%\n",
      "Epoch [1/75], Loss: 3.3805\n",
      "Epoch [1/75], Val Loss: 3.2982\n",
      "Epoch [2/75], Loss: 3.2268\n",
      "Epoch [2/75], Val Loss: 3.2088\n",
      "Epoch [3/75], Loss: 3.1086\n",
      "Epoch [3/75], Val Loss: 3.1320\n",
      "Epoch [4/75], Loss: 2.9771\n",
      "Epoch [4/75], Val Loss: 3.0113\n",
      "Epoch [5/75], Loss: 2.7804\n",
      "Epoch [5/75], Val Loss: 2.8131\n",
      "Epoch [6/75], Loss: 2.4986\n",
      "Epoch [6/75], Val Loss: 2.5242\n",
      "Epoch [7/75], Loss: 2.1153\n",
      "Epoch [7/75], Val Loss: 2.1639\n",
      "Epoch [8/75], Loss: 1.7272\n",
      "Epoch [8/75], Val Loss: 1.8836\n",
      "Epoch [9/75], Loss: 1.4304\n",
      "Epoch [9/75], Val Loss: 1.7407\n",
      "Epoch [10/75], Loss: 1.2435\n",
      "Epoch [10/75], Val Loss: 1.6885\n",
      "Epoch [11/75], Loss: 1.1105\n",
      "Epoch [11/75], Val Loss: 1.6716\n",
      "Epoch [12/75], Loss: 0.9621\n",
      "Epoch [12/75], Val Loss: 1.6063\n",
      "Epoch [13/75], Loss: 0.8789\n",
      "Epoch [13/75], Val Loss: 1.5602\n",
      "Epoch [14/75], Loss: 0.8295\n",
      "Epoch [14/75], Val Loss: 1.5850\n",
      "Epoch [15/75], Loss: 0.7567\n",
      "Epoch [15/75], Val Loss: 1.5429\n",
      "Epoch [16/75], Loss: 0.6897\n",
      "Epoch [16/75], Val Loss: 1.5696\n",
      "Epoch [17/75], Loss: 0.5691\n",
      "Epoch [17/75], Val Loss: 1.5595\n",
      "Epoch [18/75], Loss: 0.5172\n",
      "Epoch [18/75], Val Loss: 1.5466\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [18/75], Loss: 0.5172\n",
      "Test Accuracy Base CNN: 60.17%\n",
      "Epoch [1/75], Loss: 3.3213\n",
      "Epoch [1/75], Val Loss: 3.2481\n",
      "Epoch [2/75], Loss: 3.1571\n",
      "Epoch [2/75], Val Loss: 3.1437\n",
      "Epoch [3/75], Loss: 3.0045\n",
      "Epoch [3/75], Val Loss: 3.0196\n",
      "Epoch [4/75], Loss: 2.8243\n",
      "Epoch [4/75], Val Loss: 2.8384\n",
      "Epoch [5/75], Loss: 2.5534\n",
      "Epoch [5/75], Val Loss: 2.5481\n",
      "Epoch [6/75], Loss: 2.1884\n",
      "Epoch [6/75], Val Loss: 2.2234\n",
      "Epoch [7/75], Loss: 1.8076\n",
      "Epoch [7/75], Val Loss: 1.9234\n",
      "Epoch [8/75], Loss: 1.5068\n",
      "Epoch [8/75], Val Loss: 1.8132\n",
      "Epoch [9/75], Loss: 1.3193\n",
      "Epoch [9/75], Val Loss: 1.7015\n",
      "Epoch [10/75], Loss: 1.2275\n",
      "Epoch [10/75], Val Loss: 1.7680\n",
      "Epoch [11/75], Loss: 1.1415\n",
      "Epoch [11/75], Val Loss: 1.6728\n",
      "Epoch [12/75], Loss: 0.9718\n",
      "Epoch [12/75], Val Loss: 1.6285\n",
      "Epoch [13/75], Loss: 0.8526\n",
      "Epoch [13/75], Val Loss: 1.7733\n",
      "Epoch [14/75], Loss: 0.9119\n",
      "Epoch [14/75], Val Loss: 1.5662\n",
      "Epoch [15/75], Loss: 0.7806\n",
      "Epoch [15/75], Val Loss: 1.7433\n",
      "Epoch [16/75], Loss: 0.6956\n",
      "Epoch [16/75], Val Loss: 1.5959\n",
      "Epoch [17/75], Loss: 0.6195\n",
      "Epoch [17/75], Val Loss: 1.6232\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [17/75], Loss: 0.6195\n",
      "Test Accuracy Base CNN: 56.31%\n",
      "Epoch [1/75], Loss: 3.3171\n",
      "Epoch [1/75], Val Loss: 3.2848\n",
      "Epoch [2/75], Loss: 3.2161\n",
      "Epoch [2/75], Val Loss: 3.2003\n",
      "Epoch [3/75], Loss: 3.1103\n",
      "Epoch [3/75], Val Loss: 3.1293\n",
      "Epoch [4/75], Loss: 3.0046\n",
      "Epoch [4/75], Val Loss: 3.0362\n",
      "Epoch [5/75], Loss: 2.8424\n",
      "Epoch [5/75], Val Loss: 2.8766\n",
      "Epoch [6/75], Loss: 2.6200\n",
      "Epoch [6/75], Val Loss: 2.6606\n",
      "Epoch [7/75], Loss: 2.3198\n",
      "Epoch [7/75], Val Loss: 2.3733\n",
      "Epoch [8/75], Loss: 1.9629\n",
      "Epoch [8/75], Val Loss: 2.0421\n",
      "Epoch [9/75], Loss: 1.6351\n",
      "Epoch [9/75], Val Loss: 1.8154\n",
      "Epoch [10/75], Loss: 1.4139\n",
      "Epoch [10/75], Val Loss: 1.6707\n",
      "Epoch [11/75], Loss: 1.2077\n",
      "Epoch [11/75], Val Loss: 1.6717\n",
      "Epoch [12/75], Loss: 1.1264\n",
      "Epoch [12/75], Val Loss: 1.5724\n",
      "Epoch [13/75], Loss: 1.0013\n",
      "Epoch [13/75], Val Loss: 1.7227\n",
      "Epoch [14/75], Loss: 0.9023\n",
      "Epoch [14/75], Val Loss: 1.5992\n",
      "Epoch [15/75], Loss: 0.8232\n",
      "Epoch [15/75], Val Loss: 1.5827\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [15/75], Loss: 0.8232\n",
      "Test Accuracy Base CNN: 56.95%\n",
      "Epoch [1/75], Loss: 3.3848\n",
      "Epoch [1/75], Val Loss: 3.2677\n",
      "Epoch [2/75], Loss: 3.1724\n",
      "Epoch [2/75], Val Loss: 3.1555\n",
      "Epoch [3/75], Loss: 3.0333\n",
      "Epoch [3/75], Val Loss: 3.0280\n",
      "Epoch [4/75], Loss: 2.8365\n",
      "Epoch [4/75], Val Loss: 2.8367\n",
      "Epoch [5/75], Loss: 2.5691\n",
      "Epoch [5/75], Val Loss: 2.5317\n",
      "Epoch [6/75], Loss: 2.1962\n",
      "Epoch [6/75], Val Loss: 2.1465\n",
      "Epoch [7/75], Loss: 1.7868\n",
      "Epoch [7/75], Val Loss: 1.8704\n",
      "Epoch [8/75], Loss: 1.4757\n",
      "Epoch [8/75], Val Loss: 1.6937\n",
      "Epoch [9/75], Loss: 1.3038\n",
      "Epoch [9/75], Val Loss: 1.7179\n",
      "Epoch [10/75], Loss: 1.2450\n",
      "Epoch [10/75], Val Loss: 1.7263\n",
      "Epoch [11/75], Loss: 1.1290\n",
      "Epoch [11/75], Val Loss: 1.7766\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [11/75], Loss: 1.1290\n",
      "Test Accuracy Base CNN: 50.98%\n",
      "Epoch [1/75], Loss: 3.3379\n",
      "Epoch [1/75], Val Loss: 3.2878\n",
      "Epoch [2/75], Loss: 3.2205\n",
      "Epoch [2/75], Val Loss: 3.1994\n",
      "Epoch [3/75], Loss: 3.1103\n",
      "Epoch [3/75], Val Loss: 3.1213\n",
      "Epoch [4/75], Loss: 2.9808\n",
      "Epoch [4/75], Val Loss: 3.0050\n",
      "Epoch [5/75], Loss: 2.8028\n",
      "Epoch [5/75], Val Loss: 2.8270\n",
      "Epoch [6/75], Loss: 2.5541\n",
      "Epoch [6/75], Val Loss: 2.5539\n",
      "Epoch [7/75], Loss: 2.2143\n",
      "Epoch [7/75], Val Loss: 2.2368\n",
      "Epoch [8/75], Loss: 1.8695\n",
      "Epoch [8/75], Val Loss: 1.9693\n",
      "Epoch [9/75], Loss: 1.5651\n",
      "Epoch [9/75], Val Loss: 1.7889\n",
      "Epoch [10/75], Loss: 1.3407\n",
      "Epoch [10/75], Val Loss: 1.7771\n",
      "Epoch [11/75], Loss: 1.2620\n",
      "Epoch [11/75], Val Loss: 1.6323\n",
      "Epoch [12/75], Loss: 1.1675\n",
      "Epoch [12/75], Val Loss: 1.6344\n",
      "Epoch [13/75], Loss: 1.0086\n",
      "Epoch [13/75], Val Loss: 1.6685\n",
      "Epoch [14/75], Loss: 0.8759\n",
      "Epoch [14/75], Val Loss: 1.6137\n",
      "Epoch [15/75], Loss: 0.8301\n",
      "Epoch [15/75], Val Loss: 1.6133\n",
      "Epoch [16/75], Loss: 0.7338\n",
      "Epoch [16/75], Val Loss: 1.5756\n",
      "Epoch [17/75], Loss: 0.6697\n",
      "Epoch [17/75], Val Loss: 1.5112\n",
      "Epoch [18/75], Loss: 0.6442\n",
      "Epoch [18/75], Val Loss: 1.5521\n",
      "Epoch [19/75], Loss: 0.5846\n",
      "Epoch [19/75], Val Loss: 1.4761\n",
      "Epoch [20/75], Loss: 0.5394\n",
      "Epoch [20/75], Val Loss: 1.5059\n",
      "Epoch [21/75], Loss: 0.4625\n",
      "Epoch [21/75], Val Loss: 1.5440\n",
      "Epoch [22/75], Loss: 0.4336\n",
      "Epoch [22/75], Val Loss: 1.5636\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [22/75], Loss: 0.4336\n",
      "Test Accuracy Base CNN: 60.46%\n",
      "Epoch [1/75], Loss: 3.3492\n",
      "Epoch [1/75], Val Loss: 3.2702\n",
      "Epoch [2/75], Loss: 3.1994\n",
      "Epoch [2/75], Val Loss: 3.1678\n",
      "Epoch [3/75], Loss: 3.0678\n",
      "Epoch [3/75], Val Loss: 3.0694\n",
      "Epoch [4/75], Loss: 2.9032\n",
      "Epoch [4/75], Val Loss: 2.9203\n",
      "Epoch [5/75], Loss: 2.6823\n",
      "Epoch [5/75], Val Loss: 2.7060\n",
      "Epoch [6/75], Loss: 2.3997\n",
      "Epoch [6/75], Val Loss: 2.4022\n",
      "Epoch [7/75], Loss: 2.0495\n",
      "Epoch [7/75], Val Loss: 2.0753\n",
      "Epoch [8/75], Loss: 1.7441\n",
      "Epoch [8/75], Val Loss: 1.8645\n",
      "Epoch [9/75], Loss: 1.5182\n",
      "Epoch [9/75], Val Loss: 1.7633\n",
      "Epoch [10/75], Loss: 1.3559\n",
      "Epoch [10/75], Val Loss: 1.6745\n",
      "Epoch [11/75], Loss: 1.2527\n",
      "Epoch [11/75], Val Loss: 1.6235\n",
      "Epoch [12/75], Loss: 1.1537\n",
      "Epoch [12/75], Val Loss: 1.6559\n",
      "Epoch [13/75], Loss: 1.0032\n",
      "Epoch [13/75], Val Loss: 1.5953\n",
      "Epoch [14/75], Loss: 0.9221\n",
      "Epoch [14/75], Val Loss: 1.6233\n",
      "Epoch [15/75], Loss: 0.8504\n",
      "Epoch [15/75], Val Loss: 1.5762\n",
      "Epoch [16/75], Loss: 0.7908\n",
      "Epoch [16/75], Val Loss: 1.5655\n",
      "Epoch [17/75], Loss: 0.7415\n",
      "Epoch [17/75], Val Loss: 1.5475\n",
      "Epoch [18/75], Loss: 0.6475\n",
      "Epoch [18/75], Val Loss: 1.6569\n",
      "Epoch [19/75], Loss: 0.5807\n",
      "Epoch [19/75], Val Loss: 1.5171\n",
      "Epoch [20/75], Loss: 0.5083\n",
      "Epoch [20/75], Val Loss: 1.5573\n",
      "Epoch [21/75], Loss: 0.4392\n",
      "Epoch [21/75], Val Loss: 1.5535\n",
      "Epoch [22/75], Loss: 0.3882\n",
      "Epoch [22/75], Val Loss: 1.5698\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [22/75], Loss: 0.3882\n",
      "Test Accuracy Base CNN: 59.86%\n",
      "Epoch [1/75], Loss: 3.3474\n",
      "Epoch [1/75], Val Loss: 3.2772\n",
      "Epoch [2/75], Loss: 3.1687\n",
      "Epoch [2/75], Val Loss: 3.1703\n",
      "Epoch [3/75], Loss: 3.0149\n",
      "Epoch [3/75], Val Loss: 3.0433\n",
      "Epoch [4/75], Loss: 2.8149\n",
      "Epoch [4/75], Val Loss: 2.8399\n",
      "Epoch [5/75], Loss: 2.5254\n",
      "Epoch [5/75], Val Loss: 2.5522\n",
      "Epoch [6/75], Loss: 2.1545\n",
      "Epoch [6/75], Val Loss: 2.2326\n",
      "Epoch [7/75], Loss: 1.7885\n",
      "Epoch [7/75], Val Loss: 1.9570\n",
      "Epoch [8/75], Loss: 1.5331\n",
      "Epoch [8/75], Val Loss: 1.8717\n",
      "Epoch [9/75], Loss: 1.3858\n",
      "Epoch [9/75], Val Loss: 1.8839\n",
      "Epoch [10/75], Loss: 1.2600\n",
      "Epoch [10/75], Val Loss: 1.8133\n",
      "Epoch [11/75], Loss: 1.1738\n",
      "Epoch [11/75], Val Loss: 1.7881\n",
      "Epoch [12/75], Loss: 1.0881\n",
      "Epoch [12/75], Val Loss: 1.7521\n",
      "Epoch [13/75], Loss: 0.9389\n",
      "Epoch [13/75], Val Loss: 1.6220\n",
      "Epoch [14/75], Loss: 0.8010\n",
      "Epoch [14/75], Val Loss: 1.6209\n",
      "Epoch [15/75], Loss: 0.7358\n",
      "Epoch [15/75], Val Loss: 1.5791\n",
      "Epoch [16/75], Loss: 0.6972\n",
      "Epoch [16/75], Val Loss: 1.5668\n",
      "Epoch [17/75], Loss: 0.6494\n",
      "Epoch [17/75], Val Loss: 1.4470\n",
      "Epoch [18/75], Loss: 0.5269\n",
      "Epoch [18/75], Val Loss: 1.5238\n",
      "Epoch [19/75], Loss: 0.4975\n",
      "Epoch [19/75], Val Loss: 1.4808\n",
      "Epoch [20/75], Loss: 0.4069\n",
      "Epoch [20/75], Val Loss: 1.5142\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [20/75], Loss: 0.4069\n",
      "Test Accuracy Base CNN: 60.10%\n",
      "Epoch [1/75], Loss: 3.3083\n",
      "Epoch [1/75], Val Loss: 3.2863\n",
      "Epoch [2/75], Loss: 3.2058\n",
      "Epoch [2/75], Val Loss: 3.2090\n",
      "Epoch [3/75], Loss: 3.0925\n",
      "Epoch [3/75], Val Loss: 3.1317\n",
      "Epoch [4/75], Loss: 2.9653\n",
      "Epoch [4/75], Val Loss: 3.0159\n",
      "Epoch [5/75], Loss: 2.7861\n",
      "Epoch [5/75], Val Loss: 2.8316\n",
      "Epoch [6/75], Loss: 2.5391\n",
      "Epoch [6/75], Val Loss: 2.5713\n",
      "Epoch [7/75], Loss: 2.2002\n",
      "Epoch [7/75], Val Loss: 2.2259\n",
      "Epoch [8/75], Loss: 1.8236\n",
      "Epoch [8/75], Val Loss: 1.9376\n",
      "Epoch [9/75], Loss: 1.5325\n",
      "Epoch [9/75], Val Loss: 1.8178\n",
      "Epoch [10/75], Loss: 1.3755\n",
      "Epoch [10/75], Val Loss: 1.8273\n",
      "Epoch [11/75], Loss: 1.2906\n",
      "Epoch [11/75], Val Loss: 1.7665\n",
      "Epoch [12/75], Loss: 1.1967\n",
      "Epoch [12/75], Val Loss: 1.9485\n",
      "Epoch [13/75], Loss: 1.1075\n",
      "Epoch [13/75], Val Loss: 1.6518\n",
      "Epoch [14/75], Loss: 0.9677\n",
      "Epoch [14/75], Val Loss: 1.6972\n",
      "Epoch [15/75], Loss: 0.8530\n",
      "Epoch [15/75], Val Loss: 1.6452\n",
      "Epoch [16/75], Loss: 0.8173\n",
      "Epoch [16/75], Val Loss: 1.5517\n",
      "Epoch [17/75], Loss: 0.6775\n",
      "Epoch [17/75], Val Loss: 1.4716\n",
      "Epoch [18/75], Loss: 0.5981\n",
      "Epoch [18/75], Val Loss: 1.5360\n",
      "Epoch [19/75], Loss: 0.5426\n",
      "Epoch [19/75], Val Loss: 1.5099\n",
      "Epoch [20/75], Loss: 0.4777\n",
      "Epoch [20/75], Val Loss: 1.5461\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [20/75], Loss: 0.4777\n",
      "Test Accuracy Base CNN: 57.98%\n",
      "Epoch [1/75], Loss: 3.3063\n",
      "Epoch [1/75], Val Loss: 3.2549\n",
      "Epoch [2/75], Loss: 3.1643\n",
      "Epoch [2/75], Val Loss: 3.1472\n",
      "Epoch [3/75], Loss: 3.0297\n",
      "Epoch [3/75], Val Loss: 3.0396\n",
      "Epoch [4/75], Loss: 2.8720\n",
      "Epoch [4/75], Val Loss: 2.8667\n",
      "Epoch [5/75], Loss: 2.6382\n",
      "Epoch [5/75], Val Loss: 2.6160\n",
      "Epoch [6/75], Loss: 2.3255\n",
      "Epoch [6/75], Val Loss: 2.3225\n",
      "Epoch [7/75], Loss: 1.9536\n",
      "Epoch [7/75], Val Loss: 1.9970\n",
      "Epoch [8/75], Loss: 1.6412\n",
      "Epoch [8/75], Val Loss: 1.8248\n",
      "Epoch [9/75], Loss: 1.4487\n",
      "Epoch [9/75], Val Loss: 1.7201\n",
      "Epoch [10/75], Loss: 1.2933\n",
      "Epoch [10/75], Val Loss: 1.7123\n",
      "Epoch [11/75], Loss: 1.1913\n",
      "Epoch [11/75], Val Loss: 1.6438\n",
      "Epoch [12/75], Loss: 1.0477\n",
      "Epoch [12/75], Val Loss: 1.5624\n",
      "Epoch [13/75], Loss: 0.8925\n",
      "Epoch [13/75], Val Loss: 1.5615\n",
      "Epoch [14/75], Loss: 0.8265\n",
      "Epoch [14/75], Val Loss: 1.5747\n",
      "Epoch [15/75], Loss: 0.7432\n",
      "Epoch [15/75], Val Loss: 1.4973\n",
      "Epoch [16/75], Loss: 0.6511\n",
      "Epoch [16/75], Val Loss: 1.4687\n",
      "Epoch [17/75], Loss: 0.5888\n",
      "Epoch [17/75], Val Loss: 1.4728\n",
      "Epoch [18/75], Loss: 0.5168\n",
      "Epoch [18/75], Val Loss: 1.4932\n",
      "Epoch [19/75], Loss: 0.4780\n",
      "Epoch [19/75], Val Loss: 1.5250\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [19/75], Loss: 0.4780\n",
      "Test Accuracy Base CNN: 59.85%\n",
      "Epoch [1/75], Loss: 3.3313\n",
      "Epoch [1/75], Val Loss: 3.2974\n",
      "Epoch [2/75], Loss: 3.1930\n",
      "Epoch [2/75], Val Loss: 3.1928\n",
      "Epoch [3/75], Loss: 3.0462\n",
      "Epoch [3/75], Val Loss: 3.0744\n",
      "Epoch [4/75], Loss: 2.8567\n",
      "Epoch [4/75], Val Loss: 2.9021\n",
      "Epoch [5/75], Loss: 2.6036\n",
      "Epoch [5/75], Val Loss: 2.6466\n",
      "Epoch [6/75], Loss: 2.2583\n",
      "Epoch [6/75], Val Loss: 2.3229\n",
      "Epoch [7/75], Loss: 1.8630\n",
      "Epoch [7/75], Val Loss: 2.0117\n",
      "Epoch [8/75], Loss: 1.5561\n",
      "Epoch [8/75], Val Loss: 1.8392\n",
      "Epoch [9/75], Loss: 1.3322\n",
      "Epoch [9/75], Val Loss: 1.7530\n",
      "Epoch [10/75], Loss: 1.2314\n",
      "Epoch [10/75], Val Loss: 1.6877\n",
      "Epoch [11/75], Loss: 1.1424\n",
      "Epoch [11/75], Val Loss: 1.8501\n",
      "Epoch [12/75], Loss: 1.0135\n",
      "Epoch [12/75], Val Loss: 1.7062\n",
      "Epoch [13/75], Loss: 0.9307\n",
      "Epoch [13/75], Val Loss: 1.6895\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [13/75], Loss: 0.9307\n",
      "Test Accuracy Base CNN: 53.47%\n",
      "Epoch [1/75], Loss: 3.3564\n",
      "Epoch [1/75], Val Loss: 3.2670\n",
      "Epoch [2/75], Loss: 3.1657\n",
      "Epoch [2/75], Val Loss: 3.1590\n",
      "Epoch [3/75], Loss: 3.0016\n",
      "Epoch [3/75], Val Loss: 3.0258\n",
      "Epoch [4/75], Loss: 2.7972\n",
      "Epoch [4/75], Val Loss: 2.8192\n",
      "Epoch [5/75], Loss: 2.5039\n",
      "Epoch [5/75], Val Loss: 2.5119\n",
      "Epoch [6/75], Loss: 2.1370\n",
      "Epoch [6/75], Val Loss: 2.1667\n",
      "Epoch [7/75], Loss: 1.7402\n",
      "Epoch [7/75], Val Loss: 1.9207\n",
      "Epoch [8/75], Loss: 1.4476\n",
      "Epoch [8/75], Val Loss: 1.7582\n",
      "Epoch [9/75], Loss: 1.2962\n",
      "Epoch [9/75], Val Loss: 1.9012\n",
      "Epoch [10/75], Loss: 1.2829\n",
      "Epoch [10/75], Val Loss: 1.6958\n",
      "Epoch [11/75], Loss: 1.0887\n",
      "Epoch [11/75], Val Loss: 1.7318\n",
      "Epoch [12/75], Loss: 0.9606\n",
      "Epoch [12/75], Val Loss: 1.6089\n",
      "Epoch [13/75], Loss: 0.8091\n",
      "Epoch [13/75], Val Loss: 1.5793\n",
      "Epoch [14/75], Loss: 0.7619\n",
      "Epoch [14/75], Val Loss: 1.5731\n",
      "Epoch [15/75], Loss: 0.6809\n",
      "Epoch [15/75], Val Loss: 1.5470\n",
      "Epoch [16/75], Loss: 0.6353\n",
      "Epoch [16/75], Val Loss: 1.5761\n",
      "Epoch [17/75], Loss: 0.5391\n",
      "Epoch [17/75], Val Loss: 1.5144\n",
      "Epoch [18/75], Loss: 0.4623\n",
      "Epoch [18/75], Val Loss: 1.4984\n",
      "Epoch [19/75], Loss: 0.4004\n",
      "Epoch [19/75], Val Loss: 1.5549\n",
      "Epoch [20/75], Loss: 0.3467\n",
      "Epoch [20/75], Val Loss: 1.5673\n",
      "Epoch [21/75], Loss: 0.3055\n",
      "Epoch [21/75], Val Loss: 1.5461\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [21/75], Loss: 0.3055\n",
      "Test Accuracy Base CNN: 61.81%\n",
      "Epoch [1/75], Loss: 3.3559\n",
      "Epoch [1/75], Val Loss: 3.2942\n",
      "Epoch [2/75], Loss: 3.2095\n",
      "Epoch [2/75], Val Loss: 3.1902\n",
      "Epoch [3/75], Loss: 3.0745\n",
      "Epoch [3/75], Val Loss: 3.0907\n",
      "Epoch [4/75], Loss: 2.9292\n",
      "Epoch [4/75], Val Loss: 2.9517\n",
      "Epoch [5/75], Loss: 2.7195\n",
      "Epoch [5/75], Val Loss: 2.7402\n",
      "Epoch [6/75], Loss: 2.4376\n",
      "Epoch [6/75], Val Loss: 2.4544\n",
      "Epoch [7/75], Loss: 2.0728\n",
      "Epoch [7/75], Val Loss: 2.1228\n",
      "Epoch [8/75], Loss: 1.7258\n",
      "Epoch [8/75], Val Loss: 1.8881\n",
      "Epoch [9/75], Loss: 1.4861\n",
      "Epoch [9/75], Val Loss: 1.8414\n",
      "Epoch [10/75], Loss: 1.3362\n",
      "Epoch [10/75], Val Loss: 1.7931\n",
      "Epoch [11/75], Loss: 1.2365\n",
      "Epoch [11/75], Val Loss: 1.7165\n",
      "Epoch [12/75], Loss: 1.1260\n",
      "Epoch [12/75], Val Loss: 1.7504\n",
      "Epoch [13/75], Loss: 1.0051\n",
      "Epoch [13/75], Val Loss: 1.7466\n",
      "Epoch [14/75], Loss: 0.9105\n",
      "Epoch [14/75], Val Loss: 1.6921\n",
      "Epoch [15/75], Loss: 0.8468\n",
      "Epoch [15/75], Val Loss: 1.5709\n",
      "Epoch [16/75], Loss: 0.7165\n",
      "Epoch [16/75], Val Loss: 1.6087\n",
      "Epoch [17/75], Loss: 0.6511\n",
      "Epoch [17/75], Val Loss: 1.5311\n",
      "Epoch [18/75], Loss: 0.5741\n",
      "Epoch [18/75], Val Loss: 1.4838\n",
      "Epoch [19/75], Loss: 0.5215\n",
      "Epoch [19/75], Val Loss: 1.4769\n",
      "Epoch [20/75], Loss: 0.4645\n",
      "Epoch [20/75], Val Loss: 1.5506\n",
      "Epoch [21/75], Loss: 0.3961\n",
      "Epoch [21/75], Val Loss: 1.5297\n",
      "Epoch [22/75], Loss: 0.3758\n",
      "Epoch [22/75], Val Loss: 1.6211\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [22/75], Loss: 0.3758\n",
      "Test Accuracy Base CNN: 58.95%\n",
      "Epoch [1/75], Loss: 3.2940\n",
      "Epoch [1/75], Val Loss: 3.2658\n",
      "Epoch [2/75], Loss: 3.1675\n",
      "Epoch [2/75], Val Loss: 3.1553\n",
      "Epoch [3/75], Loss: 3.0314\n",
      "Epoch [3/75], Val Loss: 3.0296\n",
      "Epoch [4/75], Loss: 2.8514\n",
      "Epoch [4/75], Val Loss: 2.8306\n",
      "Epoch [5/75], Loss: 2.5920\n",
      "Epoch [5/75], Val Loss: 2.5614\n",
      "Epoch [6/75], Loss: 2.2419\n",
      "Epoch [6/75], Val Loss: 2.2380\n",
      "Epoch [7/75], Loss: 1.8776\n",
      "Epoch [7/75], Val Loss: 1.9220\n",
      "Epoch [8/75], Loss: 1.5749\n",
      "Epoch [8/75], Val Loss: 1.7661\n",
      "Epoch [9/75], Loss: 1.3642\n",
      "Epoch [9/75], Val Loss: 1.8346\n",
      "Epoch [10/75], Loss: 1.2624\n",
      "Epoch [10/75], Val Loss: 1.8010\n",
      "Epoch [11/75], Loss: 1.2102\n",
      "Epoch [11/75], Val Loss: 1.7788\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [11/75], Loss: 1.2102\n",
      "Test Accuracy Base CNN: 51.05%\n",
      "Epoch [1/75], Loss: 3.3568\n",
      "Epoch [1/75], Val Loss: 3.2615\n",
      "Epoch [2/75], Loss: 3.1692\n",
      "Epoch [2/75], Val Loss: 3.1577\n",
      "Epoch [3/75], Loss: 3.0187\n",
      "Epoch [3/75], Val Loss: 3.0307\n",
      "Epoch [4/75], Loss: 2.8201\n",
      "Epoch [4/75], Val Loss: 2.8395\n",
      "Epoch [5/75], Loss: 2.5497\n",
      "Epoch [5/75], Val Loss: 2.5752\n",
      "Epoch [6/75], Loss: 2.1946\n",
      "Epoch [6/75], Val Loss: 2.2497\n",
      "Epoch [7/75], Loss: 1.8164\n",
      "Epoch [7/75], Val Loss: 1.9538\n",
      "Epoch [8/75], Loss: 1.5157\n",
      "Epoch [8/75], Val Loss: 1.7959\n",
      "Epoch [9/75], Loss: 1.3153\n",
      "Epoch [9/75], Val Loss: 1.8258\n",
      "Epoch [10/75], Loss: 1.2240\n",
      "Epoch [10/75], Val Loss: 1.6935\n",
      "Epoch [11/75], Loss: 1.0861\n",
      "Epoch [11/75], Val Loss: 1.7028\n",
      "Epoch [12/75], Loss: 0.9781\n",
      "Epoch [12/75], Val Loss: 1.6526\n",
      "Epoch [13/75], Loss: 0.9511\n",
      "Epoch [13/75], Val Loss: 1.6278\n",
      "Epoch [14/75], Loss: 0.8534\n",
      "Epoch [14/75], Val Loss: 1.7357\n",
      "Epoch [15/75], Loss: 0.8191\n",
      "Epoch [15/75], Val Loss: 1.6456\n",
      "Epoch [16/75], Loss: 0.7094\n",
      "Epoch [16/75], Val Loss: 1.5780\n",
      "Epoch [17/75], Loss: 0.6296\n",
      "Epoch [17/75], Val Loss: 1.4831\n",
      "Epoch [18/75], Loss: 0.5728\n",
      "Epoch [18/75], Val Loss: 1.6846\n",
      "Epoch [19/75], Loss: 0.5425\n",
      "Epoch [19/75], Val Loss: 1.6606\n",
      "Epoch [20/75], Loss: 0.5081\n",
      "Epoch [20/75], Val Loss: 1.6190\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [20/75], Loss: 0.5081\n",
      "Test Accuracy Base CNN: 59.12%\n",
      "Epoch [1/75], Loss: 3.3743\n",
      "Epoch [1/75], Val Loss: 3.3093\n",
      "Epoch [2/75], Loss: 3.2433\n",
      "Epoch [2/75], Val Loss: 3.2204\n",
      "Epoch [3/75], Loss: 3.1312\n",
      "Epoch [3/75], Val Loss: 3.1425\n",
      "Epoch [4/75], Loss: 3.0080\n",
      "Epoch [4/75], Val Loss: 3.0426\n",
      "Epoch [5/75], Loss: 2.8355\n",
      "Epoch [5/75], Val Loss: 2.8722\n",
      "Epoch [6/75], Loss: 2.5761\n",
      "Epoch [6/75], Val Loss: 2.6160\n",
      "Epoch [7/75], Loss: 2.2349\n",
      "Epoch [7/75], Val Loss: 2.2984\n",
      "Epoch [8/75], Loss: 1.8764\n",
      "Epoch [8/75], Val Loss: 1.9710\n",
      "Epoch [9/75], Loss: 1.5765\n",
      "Epoch [9/75], Val Loss: 1.7885\n",
      "Epoch [10/75], Loss: 1.3264\n",
      "Epoch [10/75], Val Loss: 1.7009\n",
      "Epoch [11/75], Loss: 1.1888\n",
      "Epoch [11/75], Val Loss: 1.7262\n",
      "Epoch [12/75], Loss: 1.0697\n",
      "Epoch [12/75], Val Loss: 1.6310\n",
      "Epoch [13/75], Loss: 0.9530\n",
      "Epoch [13/75], Val Loss: 1.6410\n",
      "Epoch [14/75], Loss: 0.8633\n",
      "Epoch [14/75], Val Loss: 1.6229\n",
      "Epoch [15/75], Loss: 0.7881\n",
      "Epoch [15/75], Val Loss: 1.6092\n",
      "Epoch [16/75], Loss: 0.7058\n",
      "Epoch [16/75], Val Loss: 1.5089\n",
      "Epoch [17/75], Loss: 0.6048\n",
      "Epoch [17/75], Val Loss: 1.4584\n",
      "Epoch [18/75], Loss: 0.5739\n",
      "Epoch [18/75], Val Loss: 1.5485\n",
      "Epoch [19/75], Loss: 0.4806\n",
      "Epoch [19/75], Val Loss: 1.4510\n",
      "Epoch [20/75], Loss: 0.4036\n",
      "Epoch [20/75], Val Loss: 1.5071\n",
      "Epoch [21/75], Loss: 0.3859\n",
      "Epoch [21/75], Val Loss: 1.5174\n",
      "Epoch [22/75], Loss: 0.3199\n",
      "Epoch [22/75], Val Loss: 1.5589\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [22/75], Loss: 0.3199\n",
      "Test Accuracy Base CNN: 61.18%\n",
      "Epoch [1/75], Loss: 3.3508\n",
      "Epoch [1/75], Val Loss: 3.2801\n",
      "Epoch [2/75], Loss: 3.2293\n",
      "Epoch [2/75], Val Loss: 3.2021\n",
      "Epoch [3/75], Loss: 3.1223\n",
      "Epoch [3/75], Val Loss: 3.1256\n",
      "Epoch [4/75], Loss: 3.0075\n",
      "Epoch [4/75], Val Loss: 3.0223\n",
      "Epoch [5/75], Loss: 2.8478\n",
      "Epoch [5/75], Val Loss: 2.8578\n",
      "Epoch [6/75], Loss: 2.6151\n",
      "Epoch [6/75], Val Loss: 2.6207\n",
      "Epoch [7/75], Loss: 2.2939\n",
      "Epoch [7/75], Val Loss: 2.2983\n",
      "Epoch [8/75], Loss: 1.9388\n",
      "Epoch [8/75], Val Loss: 1.9851\n",
      "Epoch [9/75], Loss: 1.5872\n",
      "Epoch [9/75], Val Loss: 1.7811\n",
      "Epoch [10/75], Loss: 1.3518\n",
      "Epoch [10/75], Val Loss: 1.7223\n",
      "Epoch [11/75], Loss: 1.2422\n",
      "Epoch [11/75], Val Loss: 1.7250\n",
      "Epoch [12/75], Loss: 1.1726\n",
      "Epoch [12/75], Val Loss: 1.6331\n",
      "Epoch [13/75], Loss: 1.0449\n",
      "Epoch [13/75], Val Loss: 1.8190\n",
      "Epoch [14/75], Loss: 1.0371\n",
      "Epoch [14/75], Val Loss: 1.6016\n",
      "Epoch [15/75], Loss: 0.8919\n",
      "Epoch [15/75], Val Loss: 1.6078\n",
      "Epoch [16/75], Loss: 0.7399\n",
      "Epoch [16/75], Val Loss: 1.5931\n",
      "Epoch [17/75], Loss: 0.6752\n",
      "Epoch [17/75], Val Loss: 1.5743\n",
      "Epoch [18/75], Loss: 0.5848\n",
      "Epoch [18/75], Val Loss: 1.5339\n",
      "Epoch [19/75], Loss: 0.5473\n",
      "Epoch [19/75], Val Loss: 1.4986\n",
      "Epoch [20/75], Loss: 0.4647\n",
      "Epoch [20/75], Val Loss: 1.5687\n",
      "Epoch [21/75], Loss: 0.4220\n",
      "Epoch [21/75], Val Loss: 1.6028\n",
      "Epoch [22/75], Loss: 0.3940\n",
      "Epoch [22/75], Val Loss: 1.5767\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [22/75], Loss: 0.3940\n",
      "Test Accuracy Base CNN: 60.68%\n",
      "Epoch [1/75], Loss: 3.3074\n",
      "Epoch [1/75], Val Loss: 3.2349\n",
      "Epoch [2/75], Loss: 3.1630\n",
      "Epoch [2/75], Val Loss: 3.1373\n",
      "Epoch [3/75], Loss: 3.0288\n",
      "Epoch [3/75], Val Loss: 3.0282\n",
      "Epoch [4/75], Loss: 2.8564\n",
      "Epoch [4/75], Val Loss: 2.8748\n",
      "Epoch [5/75], Loss: 2.6185\n",
      "Epoch [5/75], Val Loss: 2.6126\n",
      "Epoch [6/75], Loss: 2.3163\n",
      "Epoch [6/75], Val Loss: 2.3196\n",
      "Epoch [7/75], Loss: 1.9449\n",
      "Epoch [7/75], Val Loss: 2.0358\n",
      "Epoch [8/75], Loss: 1.6296\n",
      "Epoch [8/75], Val Loss: 1.8188\n",
      "Epoch [9/75], Loss: 1.4106\n",
      "Epoch [9/75], Val Loss: 1.8617\n",
      "Epoch [10/75], Loss: 1.3296\n",
      "Epoch [10/75], Val Loss: 1.7128\n",
      "Epoch [11/75], Loss: 1.2272\n",
      "Epoch [11/75], Val Loss: 1.7376\n",
      "Epoch [12/75], Loss: 1.0904\n",
      "Epoch [12/75], Val Loss: 1.6503\n",
      "Epoch [13/75], Loss: 0.9515\n",
      "Epoch [13/75], Val Loss: 1.6657\n",
      "Epoch [14/75], Loss: 0.8911\n",
      "Epoch [14/75], Val Loss: 1.6727\n",
      "Epoch [15/75], Loss: 0.8559\n",
      "Epoch [15/75], Val Loss: 1.5751\n",
      "Epoch [16/75], Loss: 0.7689\n",
      "Epoch [16/75], Val Loss: 1.6623\n",
      "Epoch [17/75], Loss: 0.7728\n",
      "Epoch [17/75], Val Loss: 1.5875\n",
      "Epoch [18/75], Loss: 0.6858\n",
      "Epoch [18/75], Val Loss: 1.6113\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [18/75], Loss: 0.6858\n",
      "Test Accuracy Base CNN: 57.52%\n",
      "Epoch [1/75], Loss: 3.3237\n",
      "Epoch [1/75], Val Loss: 3.2667\n",
      "Epoch [2/75], Loss: 3.2162\n",
      "Epoch [2/75], Val Loss: 3.1894\n",
      "Epoch [3/75], Loss: 3.1046\n",
      "Epoch [3/75], Val Loss: 3.1171\n",
      "Epoch [4/75], Loss: 2.9831\n",
      "Epoch [4/75], Val Loss: 3.0160\n",
      "Epoch [5/75], Loss: 2.8180\n",
      "Epoch [5/75], Val Loss: 2.8489\n",
      "Epoch [6/75], Loss: 2.5726\n",
      "Epoch [6/75], Val Loss: 2.6011\n",
      "Epoch [7/75], Loss: 2.2315\n",
      "Epoch [7/75], Val Loss: 2.2842\n",
      "Epoch [8/75], Loss: 1.8730\n",
      "Epoch [8/75], Val Loss: 1.9830\n",
      "Epoch [9/75], Loss: 1.5486\n",
      "Epoch [9/75], Val Loss: 1.7797\n",
      "Epoch [10/75], Loss: 1.3373\n",
      "Epoch [10/75], Val Loss: 1.7455\n",
      "Epoch [11/75], Loss: 1.2316\n",
      "Epoch [11/75], Val Loss: 1.7019\n",
      "Epoch [12/75], Loss: 1.0772\n",
      "Epoch [12/75], Val Loss: 1.6512\n",
      "Epoch [13/75], Loss: 1.0346\n",
      "Epoch [13/75], Val Loss: 1.6145\n",
      "Epoch [14/75], Loss: 0.8927\n",
      "Epoch [14/75], Val Loss: 1.5773\n",
      "Epoch [15/75], Loss: 0.7847\n",
      "Epoch [15/75], Val Loss: 1.5220\n",
      "Epoch [16/75], Loss: 0.6933\n",
      "Epoch [16/75], Val Loss: 1.5291\n",
      "Epoch [17/75], Loss: 0.6148\n",
      "Epoch [17/75], Val Loss: 1.5772\n",
      "Epoch [18/75], Loss: 0.5532\n",
      "Epoch [18/75], Val Loss: 1.4711\n",
      "Epoch [19/75], Loss: 0.4687\n",
      "Epoch [19/75], Val Loss: 1.4944\n",
      "Epoch [20/75], Loss: 0.4270\n",
      "Epoch [20/75], Val Loss: 1.5535\n",
      "Epoch [21/75], Loss: 0.3888\n",
      "Epoch [21/75], Val Loss: 1.5684\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [21/75], Loss: 0.3888\n",
      "Test Accuracy Base CNN: 60.20%\n",
      "Epoch [1/75], Loss: 3.4043\n",
      "Epoch [1/75], Val Loss: 3.2979\n",
      "Epoch [2/75], Loss: 3.2177\n",
      "Epoch [2/75], Val Loss: 3.1921\n",
      "Epoch [3/75], Loss: 3.0801\n",
      "Epoch [3/75], Val Loss: 3.0873\n",
      "Epoch [4/75], Loss: 2.9214\n",
      "Epoch [4/75], Val Loss: 2.9410\n",
      "Epoch [5/75], Loss: 2.7048\n",
      "Epoch [5/75], Val Loss: 2.7177\n",
      "Epoch [6/75], Loss: 2.3997\n",
      "Epoch [6/75], Val Loss: 2.4039\n",
      "Epoch [7/75], Loss: 2.0191\n",
      "Epoch [7/75], Val Loss: 2.1178\n",
      "Epoch [8/75], Loss: 1.6935\n",
      "Epoch [8/75], Val Loss: 1.8928\n",
      "Epoch [9/75], Loss: 1.4418\n",
      "Epoch [9/75], Val Loss: 1.7922\n",
      "Epoch [10/75], Loss: 1.2730\n",
      "Epoch [10/75], Val Loss: 1.8181\n",
      "Epoch [11/75], Loss: 1.2282\n",
      "Epoch [11/75], Val Loss: 1.8294\n",
      "Epoch [12/75], Loss: 1.2134\n",
      "Epoch [12/75], Val Loss: 1.8123\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [12/75], Loss: 1.2134\n",
      "Test Accuracy Base CNN: 51.52%\n",
      "Epoch [1/75], Loss: 3.3483\n",
      "Epoch [1/75], Val Loss: 3.2818\n",
      "Epoch [2/75], Loss: 3.2345\n",
      "Epoch [2/75], Val Loss: 3.2022\n",
      "Epoch [3/75], Loss: 3.1276\n",
      "Epoch [3/75], Val Loss: 3.1291\n",
      "Epoch [4/75], Loss: 3.0116\n",
      "Epoch [4/75], Val Loss: 3.0282\n",
      "Epoch [5/75], Loss: 2.8578\n",
      "Epoch [5/75], Val Loss: 2.8757\n",
      "Epoch [6/75], Loss: 2.6357\n",
      "Epoch [6/75], Val Loss: 2.6354\n",
      "Epoch [7/75], Loss: 2.3271\n",
      "Epoch [7/75], Val Loss: 2.3155\n",
      "Epoch [8/75], Loss: 1.9655\n",
      "Epoch [8/75], Val Loss: 1.9892\n",
      "Epoch [9/75], Loss: 1.6401\n",
      "Epoch [9/75], Val Loss: 1.7585\n",
      "Epoch [10/75], Loss: 1.3759\n",
      "Epoch [10/75], Val Loss: 1.6603\n",
      "Epoch [11/75], Loss: 1.2297\n",
      "Epoch [11/75], Val Loss: 1.6571\n",
      "Epoch [12/75], Loss: 1.1443\n",
      "Epoch [12/75], Val Loss: 1.5961\n",
      "Epoch [13/75], Loss: 1.0340\n",
      "Epoch [13/75], Val Loss: 1.5905\n",
      "Epoch [14/75], Loss: 0.9225\n",
      "Epoch [14/75], Val Loss: 1.5816\n",
      "Epoch [15/75], Loss: 0.7725\n",
      "Epoch [15/75], Val Loss: 1.5237\n",
      "Epoch [16/75], Loss: 0.6654\n",
      "Epoch [16/75], Val Loss: 1.5059\n",
      "Epoch [17/75], Loss: 0.6315\n",
      "Epoch [17/75], Val Loss: 1.5870\n",
      "Epoch [18/75], Loss: 0.6234\n",
      "Epoch [18/75], Val Loss: 1.6366\n",
      "Epoch [19/75], Loss: 0.6062\n",
      "Epoch [19/75], Val Loss: 1.5876\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [19/75], Loss: 0.6062\n",
      "Test Accuracy Base CNN: 58.41%\n",
      "Epoch [1/75], Loss: 3.3106\n",
      "Epoch [1/75], Val Loss: 3.2838\n",
      "Epoch [2/75], Loss: 3.1932\n",
      "Epoch [2/75], Val Loss: 3.1814\n",
      "Epoch [3/75], Loss: 3.0605\n",
      "Epoch [3/75], Val Loss: 3.0833\n",
      "Epoch [4/75], Loss: 2.9069\n",
      "Epoch [4/75], Val Loss: 2.9418\n",
      "Epoch [5/75], Loss: 2.6865\n",
      "Epoch [5/75], Val Loss: 2.6994\n",
      "Epoch [6/75], Loss: 2.3763\n",
      "Epoch [6/75], Val Loss: 2.4016\n",
      "Epoch [7/75], Loss: 2.0277\n",
      "Epoch [7/75], Val Loss: 2.0998\n",
      "Epoch [8/75], Loss: 1.6961\n",
      "Epoch [8/75], Val Loss: 1.8667\n",
      "Epoch [9/75], Loss: 1.4386\n",
      "Epoch [9/75], Val Loss: 1.7813\n",
      "Epoch [10/75], Loss: 1.2872\n",
      "Epoch [10/75], Val Loss: 1.7015\n",
      "Epoch [11/75], Loss: 1.2086\n",
      "Epoch [11/75], Val Loss: 1.6846\n",
      "Epoch [12/75], Loss: 1.0483\n",
      "Epoch [12/75], Val Loss: 1.9049\n",
      "Epoch [13/75], Loss: 1.0990\n",
      "Epoch [13/75], Val Loss: 1.6736\n",
      "Epoch [14/75], Loss: 0.9112\n",
      "Epoch [14/75], Val Loss: 1.7977\n",
      "Epoch [15/75], Loss: 0.8240\n",
      "Epoch [15/75], Val Loss: 1.5742\n",
      "Epoch [16/75], Loss: 0.7390\n",
      "Epoch [16/75], Val Loss: 1.5968\n",
      "Epoch [17/75], Loss: 0.6319\n",
      "Epoch [17/75], Val Loss: 1.5192\n",
      "Epoch [18/75], Loss: 0.5705\n",
      "Epoch [18/75], Val Loss: 1.5015\n",
      "Epoch [19/75], Loss: 0.4708\n",
      "Epoch [19/75], Val Loss: 1.5934\n",
      "Epoch [20/75], Loss: 0.3954\n",
      "Epoch [20/75], Val Loss: 1.5205\n",
      "Epoch [21/75], Loss: 0.3577\n",
      "Epoch [21/75], Val Loss: 1.6173\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [21/75], Loss: 0.3577\n",
      "Test Accuracy Base CNN: 60.21%\n",
      "Epoch [1/75], Loss: 3.3553\n",
      "Epoch [1/75], Val Loss: 3.2678\n",
      "Epoch [2/75], Loss: 3.2035\n",
      "Epoch [2/75], Val Loss: 3.1635\n",
      "Epoch [3/75], Loss: 3.0729\n",
      "Epoch [3/75], Val Loss: 3.0679\n",
      "Epoch [4/75], Loss: 2.9312\n",
      "Epoch [4/75], Val Loss: 2.9304\n",
      "Epoch [5/75], Loss: 2.7266\n",
      "Epoch [5/75], Val Loss: 2.7206\n",
      "Epoch [6/75], Loss: 2.4427\n",
      "Epoch [6/75], Val Loss: 2.4174\n",
      "Epoch [7/75], Loss: 2.1000\n",
      "Epoch [7/75], Val Loss: 2.0884\n",
      "Epoch [8/75], Loss: 1.7426\n",
      "Epoch [8/75], Val Loss: 1.8508\n",
      "Epoch [9/75], Loss: 1.4554\n",
      "Epoch [9/75], Val Loss: 1.6897\n",
      "Epoch [10/75], Loss: 1.2893\n",
      "Epoch [10/75], Val Loss: 1.6745\n",
      "Epoch [11/75], Loss: 1.1461\n",
      "Epoch [11/75], Val Loss: 1.6936\n",
      "Epoch [12/75], Loss: 1.0815\n",
      "Epoch [12/75], Val Loss: 1.6749\n",
      "Epoch [13/75], Loss: 1.0080\n",
      "Epoch [13/75], Val Loss: 1.6309\n",
      "Epoch [14/75], Loss: 0.9192\n",
      "Epoch [14/75], Val Loss: 1.5498\n",
      "Epoch [15/75], Loss: 0.8440\n",
      "Epoch [15/75], Val Loss: 1.5171\n",
      "Epoch [16/75], Loss: 0.7834\n",
      "Epoch [16/75], Val Loss: 1.5312\n",
      "Epoch [17/75], Loss: 0.7237\n",
      "Epoch [17/75], Val Loss: 1.6198\n",
      "Epoch [18/75], Loss: 0.6203\n",
      "Epoch [18/75], Val Loss: 1.4971\n",
      "Epoch [19/75], Loss: 0.5487\n",
      "Epoch [19/75], Val Loss: 1.4556\n",
      "Epoch [20/75], Loss: 0.4826\n",
      "Epoch [20/75], Val Loss: 1.4786\n",
      "Epoch [21/75], Loss: 0.4098\n",
      "Epoch [21/75], Val Loss: 1.4581\n",
      "Epoch [22/75], Loss: 0.3569\n",
      "Epoch [22/75], Val Loss: 1.4820\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [22/75], Loss: 0.3569\n",
      "Test Accuracy Base CNN: 61.96%\n",
      "Epoch [1/75], Loss: 3.3838\n",
      "Epoch [1/75], Val Loss: 3.2245\n",
      "Epoch [2/75], Loss: 3.1311\n",
      "Epoch [2/75], Val Loss: 3.1086\n",
      "Epoch [3/75], Loss: 2.9751\n",
      "Epoch [3/75], Val Loss: 2.9851\n",
      "Epoch [4/75], Loss: 2.7580\n",
      "Epoch [4/75], Val Loss: 2.7673\n",
      "Epoch [5/75], Loss: 2.4564\n",
      "Epoch [5/75], Val Loss: 2.4380\n",
      "Epoch [6/75], Loss: 2.0729\n",
      "Epoch [6/75], Val Loss: 2.1040\n",
      "Epoch [7/75], Loss: 1.7234\n",
      "Epoch [7/75], Val Loss: 1.8886\n",
      "Epoch [8/75], Loss: 1.4592\n",
      "Epoch [8/75], Val Loss: 1.7795\n",
      "Epoch [9/75], Loss: 1.3435\n",
      "Epoch [9/75], Val Loss: 1.8445\n",
      "Epoch [10/75], Loss: 1.2515\n",
      "Epoch [10/75], Val Loss: 1.7649\n",
      "Epoch [11/75], Loss: 1.0922\n",
      "Epoch [11/75], Val Loss: 1.6772\n",
      "Epoch [12/75], Loss: 0.9775\n",
      "Epoch [12/75], Val Loss: 1.6741\n",
      "Epoch [13/75], Loss: 0.9092\n",
      "Epoch [13/75], Val Loss: 1.5857\n",
      "Epoch [14/75], Loss: 0.8317\n",
      "Epoch [14/75], Val Loss: 1.6011\n",
      "Epoch [15/75], Loss: 0.7595\n",
      "Epoch [15/75], Val Loss: 1.5568\n",
      "Epoch [16/75], Loss: 0.6882\n",
      "Epoch [16/75], Val Loss: 1.5503\n",
      "Epoch [17/75], Loss: 0.6284\n",
      "Epoch [17/75], Val Loss: 1.5237\n",
      "Epoch [18/75], Loss: 0.5653\n",
      "Epoch [18/75], Val Loss: 1.4623\n",
      "Epoch [19/75], Loss: 0.4522\n",
      "Epoch [19/75], Val Loss: 1.4960\n",
      "Epoch [20/75], Loss: 0.4003\n",
      "Epoch [20/75], Val Loss: 1.4759\n",
      "Epoch [21/75], Loss: 0.3551\n",
      "Epoch [21/75], Val Loss: 1.4758\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [21/75], Loss: 0.3551\n",
      "Test Accuracy Base CNN: 62.23%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/75], Loss: 3.3390\n",
      "Epoch [1/75], Val Loss: 3.2748\n",
      "Epoch [2/75], Loss: 3.2174\n",
      "Epoch [2/75], Val Loss: 3.2054\n",
      "Epoch [3/75], Loss: 3.1179\n",
      "Epoch [3/75], Val Loss: 3.1399\n",
      "Epoch [4/75], Loss: 3.0095\n",
      "Epoch [4/75], Val Loss: 3.0275\n",
      "Epoch [5/75], Loss: 2.8518\n",
      "Epoch [5/75], Val Loss: 2.8670\n",
      "Epoch [6/75], Loss: 2.6427\n",
      "Epoch [6/75], Val Loss: 2.6549\n",
      "Epoch [7/75], Loss: 2.3735\n",
      "Epoch [7/75], Val Loss: 2.4130\n",
      "Epoch [8/75], Loss: 2.0335\n",
      "Epoch [8/75], Val Loss: 2.1028\n",
      "Epoch [9/75], Loss: 1.7122\n",
      "Epoch [9/75], Val Loss: 1.8936\n",
      "Epoch [10/75], Loss: 1.4501\n",
      "Epoch [10/75], Val Loss: 1.7600\n",
      "Epoch [11/75], Loss: 1.3369\n",
      "Epoch [11/75], Val Loss: 1.6935\n",
      "Epoch [12/75], Loss: 1.1636\n",
      "Epoch [12/75], Val Loss: 1.6256\n",
      "Epoch [13/75], Loss: 1.1304\n",
      "Epoch [13/75], Val Loss: 1.6155\n",
      "Epoch [14/75], Loss: 1.0200\n",
      "Epoch [14/75], Val Loss: 1.5120\n",
      "Epoch [15/75], Loss: 0.8638\n",
      "Epoch [15/75], Val Loss: 1.5420\n",
      "Epoch [16/75], Loss: 0.7810\n",
      "Epoch [16/75], Val Loss: 1.5427\n",
      "Epoch [17/75], Loss: 0.7142\n",
      "Epoch [17/75], Val Loss: 1.4813\n",
      "Epoch [18/75], Loss: 0.6536\n",
      "Epoch [18/75], Val Loss: 1.4972\n",
      "Epoch [19/75], Loss: 0.5755\n",
      "Epoch [19/75], Val Loss: 1.4578\n",
      "Epoch [20/75], Loss: 0.5104\n",
      "Epoch [20/75], Val Loss: 1.5349\n",
      "Epoch [21/75], Loss: 0.4689\n",
      "Epoch [21/75], Val Loss: 1.5015\n",
      "Epoch [22/75], Loss: 0.4287\n",
      "Epoch [22/75], Val Loss: 1.5576\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [22/75], Loss: 0.4287\n",
      "Test Accuracy Base CNN: 60.11%\n",
      "Epoch [1/75], Loss: 3.3002\n",
      "Epoch [1/75], Val Loss: 3.2362\n",
      "Epoch [2/75], Loss: 3.1540\n",
      "Epoch [2/75], Val Loss: 3.1209\n",
      "Epoch [3/75], Loss: 2.9830\n",
      "Epoch [3/75], Val Loss: 2.9947\n",
      "Epoch [4/75], Loss: 2.7835\n",
      "Epoch [4/75], Val Loss: 2.8047\n",
      "Epoch [5/75], Loss: 2.4895\n",
      "Epoch [5/75], Val Loss: 2.4934\n",
      "Epoch [6/75], Loss: 2.1162\n",
      "Epoch [6/75], Val Loss: 2.1599\n",
      "Epoch [7/75], Loss: 1.7562\n",
      "Epoch [7/75], Val Loss: 1.9169\n",
      "Epoch [8/75], Loss: 1.4814\n",
      "Epoch [8/75], Val Loss: 1.8002\n",
      "Epoch [9/75], Loss: 1.3238\n",
      "Epoch [9/75], Val Loss: 1.8190\n",
      "Epoch [10/75], Loss: 1.2880\n",
      "Epoch [10/75], Val Loss: 1.7476\n",
      "Epoch [11/75], Loss: 1.0722\n",
      "Epoch [11/75], Val Loss: 1.6837\n",
      "Epoch [12/75], Loss: 0.9999\n",
      "Epoch [12/75], Val Loss: 1.6559\n",
      "Epoch [13/75], Loss: 0.8772\n",
      "Epoch [13/75], Val Loss: 1.6472\n",
      "Epoch [14/75], Loss: 0.7678\n",
      "Epoch [14/75], Val Loss: 1.7577\n",
      "Epoch [15/75], Loss: 0.7345\n",
      "Epoch [15/75], Val Loss: 1.6455\n",
      "Epoch [16/75], Loss: 0.6970\n",
      "Epoch [16/75], Val Loss: 1.6114\n",
      "Epoch [17/75], Loss: 0.6525\n",
      "Epoch [17/75], Val Loss: 1.6600\n",
      "Epoch [18/75], Loss: 0.6175\n",
      "Epoch [18/75], Val Loss: 1.5946\n",
      "Epoch [19/75], Loss: 0.5025\n",
      "Epoch [19/75], Val Loss: 1.6089\n",
      "Epoch [20/75], Loss: 0.4573\n",
      "Epoch [20/75], Val Loss: 1.5280\n",
      "Epoch [21/75], Loss: 0.3960\n",
      "Epoch [21/75], Val Loss: 1.5251\n",
      "Epoch [22/75], Loss: 0.3543\n",
      "Epoch [22/75], Val Loss: 1.4997\n",
      "Epoch [23/75], Loss: 0.2982\n",
      "Epoch [23/75], Val Loss: 1.6549\n",
      "Epoch [24/75], Loss: 0.2566\n",
      "Epoch [24/75], Val Loss: 1.5497\n",
      "Epoch [25/75], Loss: 0.2385\n",
      "Epoch [25/75], Val Loss: 1.6466\n",
      "Stopping early at epoch 25 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [25/75], Loss: 0.2385\n",
      "Test Accuracy Base CNN: 60.67%\n",
      "Epoch [1/75], Loss: 3.3448\n",
      "Epoch [1/75], Val Loss: 3.2893\n",
      "Epoch [2/75], Loss: 3.2349\n",
      "Epoch [2/75], Val Loss: 3.2110\n",
      "Epoch [3/75], Loss: 3.1210\n",
      "Epoch [3/75], Val Loss: 3.1344\n",
      "Epoch [4/75], Loss: 3.0019\n",
      "Epoch [4/75], Val Loss: 3.0262\n",
      "Epoch [5/75], Loss: 2.8281\n",
      "Epoch [5/75], Val Loss: 2.8576\n",
      "Epoch [6/75], Loss: 2.5845\n",
      "Epoch [6/75], Val Loss: 2.6012\n",
      "Epoch [7/75], Loss: 2.2454\n",
      "Epoch [7/75], Val Loss: 2.2846\n",
      "Epoch [8/75], Loss: 1.8805\n",
      "Epoch [8/75], Val Loss: 1.9748\n",
      "Epoch [9/75], Loss: 1.5716\n",
      "Epoch [9/75], Val Loss: 1.7666\n",
      "Epoch [10/75], Loss: 1.3435\n",
      "Epoch [10/75], Val Loss: 1.6736\n",
      "Epoch [11/75], Loss: 1.1956\n",
      "Epoch [11/75], Val Loss: 1.7037\n",
      "Epoch [12/75], Loss: 1.0640\n",
      "Epoch [12/75], Val Loss: 1.6478\n",
      "Epoch [13/75], Loss: 0.9960\n",
      "Epoch [13/75], Val Loss: 1.6876\n",
      "Epoch [14/75], Loss: 0.9308\n",
      "Epoch [14/75], Val Loss: 1.5898\n",
      "Epoch [15/75], Loss: 0.7811\n",
      "Epoch [15/75], Val Loss: 1.5587\n",
      "Epoch [16/75], Loss: 0.7061\n",
      "Epoch [16/75], Val Loss: 1.5775\n",
      "Epoch [17/75], Loss: 0.6391\n",
      "Epoch [17/75], Val Loss: 1.5696\n",
      "Epoch [18/75], Loss: 0.5644\n",
      "Epoch [18/75], Val Loss: 1.4742\n",
      "Epoch [19/75], Loss: 0.4929\n",
      "Epoch [19/75], Val Loss: 1.5070\n",
      "Epoch [20/75], Loss: 0.4225\n",
      "Epoch [20/75], Val Loss: 1.6071\n",
      "Epoch [21/75], Loss: 0.3932\n",
      "Epoch [21/75], Val Loss: 1.5955\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [21/75], Loss: 0.3932\n",
      "Test Accuracy Base CNN: 59.80%\n",
      "Epoch [1/75], Loss: 3.3229\n",
      "Epoch [1/75], Val Loss: 3.2540\n",
      "Epoch [2/75], Loss: 3.1703\n",
      "Epoch [2/75], Val Loss: 3.1506\n",
      "Epoch [3/75], Loss: 3.0228\n",
      "Epoch [3/75], Val Loss: 3.0397\n",
      "Epoch [4/75], Loss: 2.8510\n",
      "Epoch [4/75], Val Loss: 2.8589\n",
      "Epoch [5/75], Loss: 2.5907\n",
      "Epoch [5/75], Val Loss: 2.5771\n",
      "Epoch [6/75], Loss: 2.2440\n",
      "Epoch [6/75], Val Loss: 2.2392\n",
      "Epoch [7/75], Loss: 1.8438\n",
      "Epoch [7/75], Val Loss: 1.9349\n",
      "Epoch [8/75], Loss: 1.5834\n",
      "Epoch [8/75], Val Loss: 1.7847\n",
      "Epoch [9/75], Loss: 1.3727\n",
      "Epoch [9/75], Val Loss: 1.7685\n",
      "Epoch [10/75], Loss: 1.3123\n",
      "Epoch [10/75], Val Loss: 1.9085\n",
      "Epoch [11/75], Loss: 1.2511\n",
      "Epoch [11/75], Val Loss: 1.6476\n",
      "Epoch [12/75], Loss: 1.0582\n",
      "Epoch [12/75], Val Loss: 1.6258\n",
      "Epoch [13/75], Loss: 0.9464\n",
      "Epoch [13/75], Val Loss: 1.5877\n",
      "Epoch [14/75], Loss: 0.8481\n",
      "Epoch [14/75], Val Loss: 1.5678\n",
      "Epoch [15/75], Loss: 0.7531\n",
      "Epoch [15/75], Val Loss: 1.5557\n",
      "Epoch [16/75], Loss: 0.6891\n",
      "Epoch [16/75], Val Loss: 1.5230\n",
      "Epoch [17/75], Loss: 0.6034\n",
      "Epoch [17/75], Val Loss: 1.4980\n",
      "Epoch [18/75], Loss: 0.5581\n",
      "Epoch [18/75], Val Loss: 1.5600\n",
      "Epoch [19/75], Loss: 0.5151\n",
      "Epoch [19/75], Val Loss: 1.5086\n",
      "Epoch [20/75], Loss: 0.4509\n",
      "Epoch [20/75], Val Loss: 1.5034\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [20/75], Loss: 0.4509\n",
      "Test Accuracy Base CNN: 60.79%\n",
      "Epoch [1/75], Loss: 3.3195\n",
      "Epoch [1/75], Val Loss: 3.2851\n",
      "Epoch [2/75], Loss: 3.2023\n",
      "Epoch [2/75], Val Loss: 3.1802\n",
      "Epoch [3/75], Loss: 3.0758\n",
      "Epoch [3/75], Val Loss: 3.0781\n",
      "Epoch [4/75], Loss: 2.9254\n",
      "Epoch [4/75], Val Loss: 2.9425\n",
      "Epoch [5/75], Loss: 2.7107\n",
      "Epoch [5/75], Val Loss: 2.7376\n",
      "Epoch [6/75], Loss: 2.4187\n",
      "Epoch [6/75], Val Loss: 2.4599\n",
      "Epoch [7/75], Loss: 2.0693\n",
      "Epoch [7/75], Val Loss: 2.1228\n",
      "Epoch [8/75], Loss: 1.7032\n",
      "Epoch [8/75], Val Loss: 1.9021\n",
      "Epoch [9/75], Loss: 1.4690\n",
      "Epoch [9/75], Val Loss: 1.7984\n",
      "Epoch [10/75], Loss: 1.2599\n",
      "Epoch [10/75], Val Loss: 1.7502\n",
      "Epoch [11/75], Loss: 1.0953\n",
      "Epoch [11/75], Val Loss: 1.6155\n",
      "Epoch [12/75], Loss: 1.0019\n",
      "Epoch [12/75], Val Loss: 1.6612\n",
      "Epoch [13/75], Loss: 0.8843\n",
      "Epoch [13/75], Val Loss: 1.6400\n",
      "Epoch [14/75], Loss: 0.8215\n",
      "Epoch [14/75], Val Loss: 1.5781\n",
      "Epoch [15/75], Loss: 0.7417\n",
      "Epoch [15/75], Val Loss: 1.5913\n",
      "Epoch [16/75], Loss: 0.6649\n",
      "Epoch [16/75], Val Loss: 1.4959\n",
      "Epoch [17/75], Loss: 0.5613\n",
      "Epoch [17/75], Val Loss: 1.6064\n",
      "Epoch [18/75], Loss: 0.5452\n",
      "Epoch [18/75], Val Loss: 1.5888\n",
      "Epoch [19/75], Loss: 0.5332\n",
      "Epoch [19/75], Val Loss: 1.6084\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [19/75], Loss: 0.5332\n",
      "Test Accuracy Base CNN: 58.93%\n",
      "Epoch [1/75], Loss: 3.3711\n",
      "Epoch [1/75], Val Loss: 3.2534\n",
      "Epoch [2/75], Loss: 3.1825\n",
      "Epoch [2/75], Val Loss: 3.1388\n",
      "Epoch [3/75], Loss: 3.0205\n",
      "Epoch [3/75], Val Loss: 3.0216\n",
      "Epoch [4/75], Loss: 2.8321\n",
      "Epoch [4/75], Val Loss: 2.8349\n",
      "Epoch [5/75], Loss: 2.5582\n",
      "Epoch [5/75], Val Loss: 2.5605\n",
      "Epoch [6/75], Loss: 2.2172\n",
      "Epoch [6/75], Val Loss: 2.2297\n",
      "Epoch [7/75], Loss: 1.8571\n",
      "Epoch [7/75], Val Loss: 1.9401\n",
      "Epoch [8/75], Loss: 1.5918\n",
      "Epoch [8/75], Val Loss: 1.7861\n",
      "Epoch [9/75], Loss: 1.4258\n",
      "Epoch [9/75], Val Loss: 1.7538\n",
      "Epoch [10/75], Loss: 1.3123\n",
      "Epoch [10/75], Val Loss: 1.7337\n",
      "Epoch [11/75], Loss: 1.1610\n",
      "Epoch [11/75], Val Loss: 1.6637\n",
      "Epoch [12/75], Loss: 1.0029\n",
      "Epoch [12/75], Val Loss: 1.6824\n",
      "Epoch [13/75], Loss: 0.8662\n",
      "Epoch [13/75], Val Loss: 1.5904\n",
      "Epoch [14/75], Loss: 0.7702\n",
      "Epoch [14/75], Val Loss: 1.5521\n",
      "Epoch [15/75], Loss: 0.6829\n",
      "Epoch [15/75], Val Loss: 1.5151\n",
      "Epoch [16/75], Loss: 0.5885\n",
      "Epoch [16/75], Val Loss: 1.5033\n",
      "Epoch [17/75], Loss: 0.5442\n",
      "Epoch [17/75], Val Loss: 1.5935\n",
      "Epoch [18/75], Loss: 0.5199\n",
      "Epoch [18/75], Val Loss: 1.5619\n",
      "Epoch [19/75], Loss: 0.4811\n",
      "Epoch [19/75], Val Loss: 1.5491\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [19/75], Loss: 0.4811\n",
      "Test Accuracy Base CNN: 59.90%\n",
      "Epoch [1/75], Loss: 3.3594\n",
      "Epoch [1/75], Val Loss: 3.3061\n",
      "Epoch [2/75], Loss: 3.2525\n",
      "Epoch [2/75], Val Loss: 3.2298\n",
      "Epoch [3/75], Loss: 3.1466\n",
      "Epoch [3/75], Val Loss: 3.1625\n",
      "Epoch [4/75], Loss: 3.0396\n",
      "Epoch [4/75], Val Loss: 3.0697\n",
      "Epoch [5/75], Loss: 2.8888\n",
      "Epoch [5/75], Val Loss: 2.9346\n",
      "Epoch [6/75], Loss: 2.6774\n",
      "Epoch [6/75], Val Loss: 2.7136\n",
      "Epoch [7/75], Loss: 2.3776\n",
      "Epoch [7/75], Val Loss: 2.4062\n",
      "Epoch [8/75], Loss: 2.0186\n",
      "Epoch [8/75], Val Loss: 2.0564\n",
      "Epoch [9/75], Loss: 1.6669\n",
      "Epoch [9/75], Val Loss: 1.8242\n",
      "Epoch [10/75], Loss: 1.4558\n",
      "Epoch [10/75], Val Loss: 1.7235\n",
      "Epoch [11/75], Loss: 1.3629\n",
      "Epoch [11/75], Val Loss: 1.8155\n",
      "Epoch [12/75], Loss: 1.2327\n",
      "Epoch [12/75], Val Loss: 1.6316\n",
      "Epoch [13/75], Loss: 1.0653\n",
      "Epoch [13/75], Val Loss: 1.5921\n",
      "Epoch [14/75], Loss: 0.9404\n",
      "Epoch [14/75], Val Loss: 1.5158\n",
      "Epoch [15/75], Loss: 0.8154\n",
      "Epoch [15/75], Val Loss: 1.4446\n",
      "Epoch [16/75], Loss: 0.7250\n",
      "Epoch [16/75], Val Loss: 1.4992\n",
      "Epoch [17/75], Loss: 0.7114\n",
      "Epoch [17/75], Val Loss: 1.5397\n",
      "Epoch [18/75], Loss: 0.6356\n",
      "Epoch [18/75], Val Loss: 1.4878\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [18/75], Loss: 0.6356\n",
      "Test Accuracy Base CNN: 59.05%\n",
      "Epoch [1/75], Loss: 3.4103\n",
      "Epoch [1/75], Val Loss: 3.3087\n",
      "Epoch [2/75], Loss: 3.2434\n",
      "Epoch [2/75], Val Loss: 3.2005\n",
      "Epoch [3/75], Loss: 3.1037\n",
      "Epoch [3/75], Val Loss: 3.1072\n",
      "Epoch [4/75], Loss: 2.9626\n",
      "Epoch [4/75], Val Loss: 2.9770\n",
      "Epoch [5/75], Loss: 2.7720\n",
      "Epoch [5/75], Val Loss: 2.7799\n",
      "Epoch [6/75], Loss: 2.5065\n",
      "Epoch [6/75], Val Loss: 2.5032\n",
      "Epoch [7/75], Loss: 2.1581\n",
      "Epoch [7/75], Val Loss: 2.1681\n",
      "Epoch [8/75], Loss: 1.7677\n",
      "Epoch [8/75], Val Loss: 1.8896\n",
      "Epoch [9/75], Loss: 1.4942\n",
      "Epoch [9/75], Val Loss: 1.7028\n",
      "Epoch [10/75], Loss: 1.3173\n",
      "Epoch [10/75], Val Loss: 1.7167\n",
      "Epoch [11/75], Loss: 1.1623\n",
      "Epoch [11/75], Val Loss: 1.6590\n",
      "Epoch [12/75], Loss: 1.0973\n",
      "Epoch [12/75], Val Loss: 1.6896\n",
      "Epoch [13/75], Loss: 1.0295\n",
      "Epoch [13/75], Val Loss: 1.5654\n",
      "Epoch [14/75], Loss: 0.8838\n",
      "Epoch [14/75], Val Loss: 1.6245\n",
      "Epoch [15/75], Loss: 0.8264\n",
      "Epoch [15/75], Val Loss: 1.5374\n",
      "Epoch [16/75], Loss: 0.7759\n",
      "Epoch [16/75], Val Loss: 1.5535\n",
      "Epoch [17/75], Loss: 0.6995\n",
      "Epoch [17/75], Val Loss: 1.5426\n",
      "Epoch [18/75], Loss: 0.6830\n",
      "Epoch [18/75], Val Loss: 1.5190\n",
      "Epoch [19/75], Loss: 0.6154\n",
      "Epoch [19/75], Val Loss: 1.4835\n",
      "Epoch [20/75], Loss: 0.5286\n",
      "Epoch [20/75], Val Loss: 1.5217\n",
      "Epoch [21/75], Loss: 0.4675\n",
      "Epoch [21/75], Val Loss: 1.5179\n",
      "Epoch [22/75], Loss: 0.4089\n",
      "Epoch [22/75], Val Loss: 1.5355\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [22/75], Loss: 0.4089\n",
      "Test Accuracy Base CNN: 60.75%\n",
      "Epoch [1/75], Loss: 3.3203\n",
      "Epoch [1/75], Val Loss: 3.2806\n",
      "Epoch [2/75], Loss: 3.2320\n",
      "Epoch [2/75], Val Loss: 3.2154\n",
      "Epoch [3/75], Loss: 3.1371\n",
      "Epoch [3/75], Val Loss: 3.1613\n",
      "Epoch [4/75], Loss: 3.0413\n",
      "Epoch [4/75], Val Loss: 3.0856\n",
      "Epoch [5/75], Loss: 2.9186\n",
      "Epoch [5/75], Val Loss: 2.9730\n",
      "Epoch [6/75], Loss: 2.7592\n",
      "Epoch [6/75], Val Loss: 2.8069\n",
      "Epoch [7/75], Loss: 2.5230\n",
      "Epoch [7/75], Val Loss: 2.5570\n",
      "Epoch [8/75], Loss: 2.2218\n",
      "Epoch [8/75], Val Loss: 2.2545\n",
      "Epoch [9/75], Loss: 1.8654\n",
      "Epoch [9/75], Val Loss: 1.9506\n",
      "Epoch [10/75], Loss: 1.5375\n",
      "Epoch [10/75], Val Loss: 1.7425\n",
      "Epoch [11/75], Loss: 1.3275\n",
      "Epoch [11/75], Val Loss: 1.6204\n",
      "Epoch [12/75], Loss: 1.1355\n",
      "Epoch [12/75], Val Loss: 1.6065\n",
      "Epoch [13/75], Loss: 1.0316\n",
      "Epoch [13/75], Val Loss: 1.5310\n",
      "Epoch [14/75], Loss: 0.9432\n",
      "Epoch [14/75], Val Loss: 1.5599\n",
      "Epoch [15/75], Loss: 0.8283\n",
      "Epoch [15/75], Val Loss: 1.5132\n",
      "Epoch [16/75], Loss: 0.7390\n",
      "Epoch [16/75], Val Loss: 1.5318\n",
      "Epoch [17/75], Loss: 0.6894\n",
      "Epoch [17/75], Val Loss: 1.5174\n",
      "Epoch [18/75], Loss: 0.7015\n",
      "Epoch [18/75], Val Loss: 1.4594\n",
      "Epoch [19/75], Loss: 0.6295\n",
      "Epoch [19/75], Val Loss: 1.5129\n",
      "Epoch [20/75], Loss: 0.6341\n",
      "Epoch [20/75], Val Loss: 1.4978\n",
      "Epoch [21/75], Loss: 0.4829\n",
      "Epoch [21/75], Val Loss: 1.4493\n",
      "Epoch [22/75], Loss: 0.4277\n",
      "Epoch [22/75], Val Loss: 1.4447\n",
      "Epoch [23/75], Loss: 0.4109\n",
      "Epoch [23/75], Val Loss: 1.4297\n",
      "Epoch [24/75], Loss: 0.3269\n",
      "Epoch [24/75], Val Loss: 1.4523\n",
      "Epoch [25/75], Loss: 0.3038\n",
      "Epoch [25/75], Val Loss: 1.4569\n",
      "Epoch [26/75], Loss: 0.2659\n",
      "Epoch [26/75], Val Loss: 1.4499\n",
      "Stopping early at epoch 26 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [26/75], Loss: 0.2659\n",
      "Test Accuracy Base CNN: 63.63%\n",
      "Epoch [1/75], Loss: 3.3489\n",
      "Epoch [1/75], Val Loss: 3.2939\n",
      "Epoch [2/75], Loss: 3.2370\n",
      "Epoch [2/75], Val Loss: 3.2074\n",
      "Epoch [3/75], Loss: 3.1248\n",
      "Epoch [3/75], Val Loss: 3.1308\n",
      "Epoch [4/75], Loss: 3.0093\n",
      "Epoch [4/75], Val Loss: 3.0371\n",
      "Epoch [5/75], Loss: 2.8498\n",
      "Epoch [5/75], Val Loss: 2.8978\n",
      "Epoch [6/75], Loss: 2.6345\n",
      "Epoch [6/75], Val Loss: 2.6936\n",
      "Epoch [7/75], Loss: 2.3528\n",
      "Epoch [7/75], Val Loss: 2.4268\n",
      "Epoch [8/75], Loss: 2.0381\n",
      "Epoch [8/75], Val Loss: 2.1303\n",
      "Epoch [9/75], Loss: 1.7173\n",
      "Epoch [9/75], Val Loss: 1.9318\n",
      "Epoch [10/75], Loss: 1.4687\n",
      "Epoch [10/75], Val Loss: 1.7745\n",
      "Epoch [11/75], Loss: 1.3063\n",
      "Epoch [11/75], Val Loss: 1.7035\n",
      "Epoch [12/75], Loss: 1.2173\n",
      "Epoch [12/75], Val Loss: 1.7752\n",
      "Epoch [13/75], Loss: 1.1063\n",
      "Epoch [13/75], Val Loss: 1.7864\n",
      "Epoch [14/75], Loss: 1.0572\n",
      "Epoch [14/75], Val Loss: 1.6440\n",
      "Epoch [15/75], Loss: 0.9488\n",
      "Epoch [15/75], Val Loss: 1.6383\n",
      "Epoch [16/75], Loss: 0.8147\n",
      "Epoch [16/75], Val Loss: 1.5952\n",
      "Epoch [17/75], Loss: 0.7827\n",
      "Epoch [17/75], Val Loss: 1.5087\n",
      "Epoch [18/75], Loss: 0.6460\n",
      "Epoch [18/75], Val Loss: 1.6128\n",
      "Epoch [19/75], Loss: 0.5808\n",
      "Epoch [19/75], Val Loss: 1.4376\n",
      "Epoch [20/75], Loss: 0.5267\n",
      "Epoch [20/75], Val Loss: 1.5162\n",
      "Epoch [21/75], Loss: 0.4613\n",
      "Epoch [21/75], Val Loss: 1.5115\n",
      "Epoch [22/75], Loss: 0.4453\n",
      "Epoch [22/75], Val Loss: 1.5272\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [22/75], Loss: 0.4453\n",
      "Test Accuracy Base CNN: 60.66%\n",
      "Epoch [1/75], Loss: 3.3800\n",
      "Epoch [1/75], Val Loss: 3.2720\n",
      "Epoch [2/75], Loss: 3.2201\n",
      "Epoch [2/75], Val Loss: 3.1892\n",
      "Epoch [3/75], Loss: 3.0967\n",
      "Epoch [3/75], Val Loss: 3.0999\n",
      "Epoch [4/75], Loss: 2.9553\n",
      "Epoch [4/75], Val Loss: 2.9584\n",
      "Epoch [5/75], Loss: 2.7567\n",
      "Epoch [5/75], Val Loss: 2.7482\n",
      "Epoch [6/75], Loss: 2.4627\n",
      "Epoch [6/75], Val Loss: 2.4372\n",
      "Epoch [7/75], Loss: 2.0887\n",
      "Epoch [7/75], Val Loss: 2.1253\n",
      "Epoch [8/75], Loss: 1.7159\n",
      "Epoch [8/75], Val Loss: 1.9029\n",
      "Epoch [9/75], Loss: 1.5098\n",
      "Epoch [9/75], Val Loss: 1.7792\n",
      "Epoch [10/75], Loss: 1.3349\n",
      "Epoch [10/75], Val Loss: 1.6746\n",
      "Epoch [11/75], Loss: 1.2089\n",
      "Epoch [11/75], Val Loss: 1.8275\n",
      "Epoch [12/75], Loss: 1.1556\n",
      "Epoch [12/75], Val Loss: 1.7809\n",
      "Epoch [13/75], Loss: 1.0371\n",
      "Epoch [13/75], Val Loss: 1.6569\n",
      "Epoch [14/75], Loss: 0.9242\n",
      "Epoch [14/75], Val Loss: 1.6005\n",
      "Epoch [15/75], Loss: 0.8100\n",
      "Epoch [15/75], Val Loss: 1.7001\n",
      "Epoch [16/75], Loss: 0.7825\n",
      "Epoch [16/75], Val Loss: 1.5317\n",
      "Epoch [17/75], Loss: 0.7185\n",
      "Epoch [17/75], Val Loss: 1.4553\n",
      "Epoch [18/75], Loss: 0.6295\n",
      "Epoch [18/75], Val Loss: 1.6772\n",
      "Epoch [19/75], Loss: 0.5527\n",
      "Epoch [19/75], Val Loss: 1.4359\n",
      "Epoch [20/75], Loss: 0.4657\n",
      "Epoch [20/75], Val Loss: 1.5117\n",
      "Epoch [21/75], Loss: 0.4099\n",
      "Epoch [21/75], Val Loss: 1.4593\n",
      "Epoch [22/75], Loss: 0.3716\n",
      "Epoch [22/75], Val Loss: 1.4966\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [22/75], Loss: 0.3716\n",
      "Test Accuracy Base CNN: 62.28%\n",
      "Epoch [1/75], Loss: 3.3364\n",
      "Epoch [1/75], Val Loss: 3.3042\n",
      "Epoch [2/75], Loss: 3.2102\n",
      "Epoch [2/75], Val Loss: 3.1968\n",
      "Epoch [3/75], Loss: 3.0757\n",
      "Epoch [3/75], Val Loss: 3.0839\n",
      "Epoch [4/75], Loss: 2.8999\n",
      "Epoch [4/75], Val Loss: 2.9182\n",
      "Epoch [5/75], Loss: 2.6554\n",
      "Epoch [5/75], Val Loss: 2.6800\n",
      "Epoch [6/75], Loss: 2.3390\n",
      "Epoch [6/75], Val Loss: 2.3743\n",
      "Epoch [7/75], Loss: 1.9745\n",
      "Epoch [7/75], Val Loss: 2.0330\n",
      "Epoch [8/75], Loss: 1.6676\n",
      "Epoch [8/75], Val Loss: 1.8606\n",
      "Epoch [9/75], Loss: 1.4541\n",
      "Epoch [9/75], Val Loss: 1.7469\n",
      "Epoch [10/75], Loss: 1.2191\n",
      "Epoch [10/75], Val Loss: 1.7185\n",
      "Epoch [11/75], Loss: 1.1562\n",
      "Epoch [11/75], Val Loss: 1.6235\n",
      "Epoch [12/75], Loss: 0.9834\n",
      "Epoch [12/75], Val Loss: 1.7087\n",
      "Epoch [13/75], Loss: 0.8997\n",
      "Epoch [13/75], Val Loss: 1.6237\n",
      "Epoch [14/75], Loss: 0.8331\n",
      "Epoch [14/75], Val Loss: 1.6240\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [14/75], Loss: 0.8331\n",
      "Test Accuracy Base CNN: 56.01%\n",
      "Epoch [1/75], Loss: 3.3356\n",
      "Epoch [1/75], Val Loss: 3.2879\n",
      "Epoch [2/75], Loss: 3.2035\n",
      "Epoch [2/75], Val Loss: 3.1840\n",
      "Epoch [3/75], Loss: 3.0678\n",
      "Epoch [3/75], Val Loss: 3.0778\n",
      "Epoch [4/75], Loss: 2.9001\n",
      "Epoch [4/75], Val Loss: 2.9255\n",
      "Epoch [5/75], Loss: 2.6529\n",
      "Epoch [5/75], Val Loss: 2.6930\n",
      "Epoch [6/75], Loss: 2.3279\n",
      "Epoch [6/75], Val Loss: 2.3744\n",
      "Epoch [7/75], Loss: 1.9475\n",
      "Epoch [7/75], Val Loss: 2.0546\n",
      "Epoch [8/75], Loss: 1.6043\n",
      "Epoch [8/75], Val Loss: 1.8672\n",
      "Epoch [9/75], Loss: 1.3964\n",
      "Epoch [9/75], Val Loss: 1.8434\n",
      "Epoch [10/75], Loss: 1.2807\n",
      "Epoch [10/75], Val Loss: 1.8503\n",
      "Epoch [11/75], Loss: 1.1777\n",
      "Epoch [11/75], Val Loss: 1.6875\n",
      "Epoch [12/75], Loss: 0.9936\n",
      "Epoch [12/75], Val Loss: 1.6545\n",
      "Epoch [13/75], Loss: 0.9502\n",
      "Epoch [13/75], Val Loss: 1.6863\n",
      "Epoch [14/75], Loss: 0.8365\n",
      "Epoch [14/75], Val Loss: 1.6057\n",
      "Epoch [15/75], Loss: 0.7500\n",
      "Epoch [15/75], Val Loss: 1.6155\n",
      "Epoch [16/75], Loss: 0.6741\n",
      "Epoch [16/75], Val Loss: 1.5019\n",
      "Epoch [17/75], Loss: 0.5728\n",
      "Epoch [17/75], Val Loss: 1.4884\n",
      "Epoch [18/75], Loss: 0.4908\n",
      "Epoch [18/75], Val Loss: 1.6091\n",
      "Epoch [19/75], Loss: 0.4579\n",
      "Epoch [19/75], Val Loss: 1.4817\n",
      "Epoch [20/75], Loss: 0.4316\n",
      "Epoch [20/75], Val Loss: 1.5382\n",
      "Epoch [21/75], Loss: 0.3834\n",
      "Epoch [21/75], Val Loss: 1.6065\n",
      "Epoch [22/75], Loss: 0.3601\n",
      "Epoch [22/75], Val Loss: 1.6284\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [22/75], Loss: 0.3601\n",
      "Test Accuracy Base CNN: 59.90%\n",
      "Epoch [1/75], Loss: 3.3499\n",
      "Epoch [1/75], Val Loss: 3.2716\n",
      "Epoch [2/75], Loss: 3.1920\n",
      "Epoch [2/75], Val Loss: 3.1757\n",
      "Epoch [3/75], Loss: 3.0727\n",
      "Epoch [3/75], Val Loss: 3.0853\n",
      "Epoch [4/75], Loss: 2.9214\n",
      "Epoch [4/75], Val Loss: 2.9392\n",
      "Epoch [5/75], Loss: 2.7225\n",
      "Epoch [5/75], Val Loss: 2.7511\n",
      "Epoch [6/75], Loss: 2.4689\n",
      "Epoch [6/75], Val Loss: 2.4965\n",
      "Epoch [7/75], Loss: 2.1240\n",
      "Epoch [7/75], Val Loss: 2.1824\n",
      "Epoch [8/75], Loss: 1.7904\n",
      "Epoch [8/75], Val Loss: 1.9400\n",
      "Epoch [9/75], Loss: 1.5222\n",
      "Epoch [9/75], Val Loss: 1.7948\n",
      "Epoch [10/75], Loss: 1.3484\n",
      "Epoch [10/75], Val Loss: 1.7100\n",
      "Epoch [11/75], Loss: 1.2236\n",
      "Epoch [11/75], Val Loss: 1.6254\n",
      "Epoch [12/75], Loss: 1.0923\n",
      "Epoch [12/75], Val Loss: 1.5766\n",
      "Epoch [13/75], Loss: 0.9432\n",
      "Epoch [13/75], Val Loss: 1.6369\n",
      "Epoch [14/75], Loss: 0.9298\n",
      "Epoch [14/75], Val Loss: 1.5412\n",
      "Epoch [15/75], Loss: 0.8101\n",
      "Epoch [15/75], Val Loss: 1.5109\n",
      "Epoch [16/75], Loss: 0.7767\n",
      "Epoch [16/75], Val Loss: 1.5287\n",
      "Epoch [17/75], Loss: 0.6523\n",
      "Epoch [17/75], Val Loss: 1.4880\n",
      "Epoch [18/75], Loss: 0.6277\n",
      "Epoch [18/75], Val Loss: 1.5490\n",
      "Epoch [19/75], Loss: 0.5267\n",
      "Epoch [19/75], Val Loss: 1.4632\n",
      "Epoch [20/75], Loss: 0.4188\n",
      "Epoch [20/75], Val Loss: 1.5036\n",
      "Epoch [21/75], Loss: 0.3657\n",
      "Epoch [21/75], Val Loss: 1.4536\n",
      "Epoch [22/75], Loss: 0.3313\n",
      "Epoch [22/75], Val Loss: 1.5070\n",
      "Epoch [23/75], Loss: 0.2947\n",
      "Epoch [23/75], Val Loss: 1.5002\n",
      "Epoch [24/75], Loss: 0.2519\n",
      "Epoch [24/75], Val Loss: 1.5515\n",
      "Stopping early at epoch 24 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [24/75], Loss: 0.2519\n",
      "Test Accuracy Base CNN: 61.82%\n",
      "Epoch [1/75], Loss: 3.3225\n",
      "Epoch [1/75], Val Loss: 3.2251\n",
      "Epoch [2/75], Loss: 3.1350\n",
      "Epoch [2/75], Val Loss: 3.1083\n",
      "Epoch [3/75], Loss: 2.9537\n",
      "Epoch [3/75], Val Loss: 2.9706\n",
      "Epoch [4/75], Loss: 2.7210\n",
      "Epoch [4/75], Val Loss: 2.7203\n",
      "Epoch [5/75], Loss: 2.3666\n",
      "Epoch [5/75], Val Loss: 2.3477\n",
      "Epoch [6/75], Loss: 1.9742\n",
      "Epoch [6/75], Val Loss: 2.0276\n",
      "Epoch [7/75], Loss: 1.6201\n",
      "Epoch [7/75], Val Loss: 1.8691\n",
      "Epoch [8/75], Loss: 1.4393\n",
      "Epoch [8/75], Val Loss: 1.7857\n",
      "Epoch [9/75], Loss: 1.2970\n",
      "Epoch [9/75], Val Loss: 1.7663\n",
      "Epoch [10/75], Loss: 1.1968\n",
      "Epoch [10/75], Val Loss: 1.7573\n",
      "Epoch [11/75], Loss: 1.0488\n",
      "Epoch [11/75], Val Loss: 1.5837\n",
      "Epoch [12/75], Loss: 0.8917\n",
      "Epoch [12/75], Val Loss: 1.5713\n",
      "Epoch [13/75], Loss: 0.8147\n",
      "Epoch [13/75], Val Loss: 1.6785\n",
      "Epoch [14/75], Loss: 0.8116\n",
      "Epoch [14/75], Val Loss: 1.6748\n",
      "Epoch [15/75], Loss: 0.7107\n",
      "Epoch [15/75], Val Loss: 1.6277\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.7107\n",
      "Test Accuracy Base CNN: 56.59%\n",
      "Epoch [1/75], Loss: 3.2950\n",
      "Epoch [1/75], Val Loss: 3.2483\n",
      "Epoch [2/75], Loss: 3.1418\n",
      "Epoch [2/75], Val Loss: 3.1214\n",
      "Epoch [3/75], Loss: 2.9660\n",
      "Epoch [3/75], Val Loss: 2.9459\n",
      "Epoch [4/75], Loss: 2.6992\n",
      "Epoch [4/75], Val Loss: 2.6628\n",
      "Epoch [5/75], Loss: 2.3230\n",
      "Epoch [5/75], Val Loss: 2.3136\n",
      "Epoch [6/75], Loss: 1.8940\n",
      "Epoch [6/75], Val Loss: 1.9776\n",
      "Epoch [7/75], Loss: 1.5860\n",
      "Epoch [7/75], Val Loss: 1.8465\n",
      "Epoch [8/75], Loss: 1.3970\n",
      "Epoch [8/75], Val Loss: 1.7489\n",
      "Epoch [9/75], Loss: 1.2270\n",
      "Epoch [9/75], Val Loss: 1.9147\n",
      "Epoch [10/75], Loss: 1.1848\n",
      "Epoch [10/75], Val Loss: 1.9172\n",
      "Epoch [11/75], Loss: 1.0876\n",
      "Epoch [11/75], Val Loss: 1.7691\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [11/75], Loss: 1.0876\n",
      "Test Accuracy Base CNN: 50.61%\n",
      "Epoch [1/75], Loss: 3.3419\n",
      "Epoch [1/75], Val Loss: 3.2858\n",
      "Epoch [2/75], Loss: 3.1981\n",
      "Epoch [2/75], Val Loss: 3.1873\n",
      "Epoch [3/75], Loss: 3.0768\n",
      "Epoch [3/75], Val Loss: 3.0948\n",
      "Epoch [4/75], Loss: 2.9301\n",
      "Epoch [4/75], Val Loss: 2.9620\n",
      "Epoch [5/75], Loss: 2.7268\n",
      "Epoch [5/75], Val Loss: 2.7533\n",
      "Epoch [6/75], Loss: 2.4349\n",
      "Epoch [6/75], Val Loss: 2.4574\n",
      "Epoch [7/75], Loss: 2.0763\n",
      "Epoch [7/75], Val Loss: 2.1320\n",
      "Epoch [8/75], Loss: 1.7355\n",
      "Epoch [8/75], Val Loss: 1.8632\n",
      "Epoch [9/75], Loss: 1.4694\n",
      "Epoch [9/75], Val Loss: 1.7669\n",
      "Epoch [10/75], Loss: 1.2934\n",
      "Epoch [10/75], Val Loss: 1.7221\n",
      "Epoch [11/75], Loss: 1.1941\n",
      "Epoch [11/75], Val Loss: 1.6852\n",
      "Epoch [12/75], Loss: 1.1056\n",
      "Epoch [12/75], Val Loss: 1.6288\n",
      "Epoch [13/75], Loss: 0.9909\n",
      "Epoch [13/75], Val Loss: 1.6486\n",
      "Epoch [14/75], Loss: 0.9348\n",
      "Epoch [14/75], Val Loss: 1.5414\n",
      "Epoch [15/75], Loss: 0.7705\n",
      "Epoch [15/75], Val Loss: 1.5275\n",
      "Epoch [16/75], Loss: 0.7138\n",
      "Epoch [16/75], Val Loss: 1.5251\n",
      "Epoch [17/75], Loss: 0.6240\n",
      "Epoch [17/75], Val Loss: 1.4355\n",
      "Epoch [18/75], Loss: 0.5182\n",
      "Epoch [18/75], Val Loss: 1.3896\n",
      "Epoch [19/75], Loss: 0.4523\n",
      "Epoch [19/75], Val Loss: 1.4147\n",
      "Epoch [20/75], Loss: 0.3840\n",
      "Epoch [20/75], Val Loss: 1.4476\n",
      "Epoch [21/75], Loss: 0.3230\n",
      "Epoch [21/75], Val Loss: 1.4382\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [21/75], Loss: 0.3230\n",
      "Test Accuracy Base CNN: 62.42%\n",
      "Epoch [1/75], Loss: 3.3348\n",
      "Epoch [1/75], Val Loss: 3.3010\n",
      "Epoch [2/75], Loss: 3.2402\n",
      "Epoch [2/75], Val Loss: 3.2278\n",
      "Epoch [3/75], Loss: 3.1402\n",
      "Epoch [3/75], Val Loss: 3.1676\n",
      "Epoch [4/75], Loss: 3.0432\n",
      "Epoch [4/75], Val Loss: 3.0912\n",
      "Epoch [5/75], Loss: 2.9093\n",
      "Epoch [5/75], Val Loss: 2.9618\n",
      "Epoch [6/75], Loss: 2.7217\n",
      "Epoch [6/75], Val Loss: 2.7658\n",
      "Epoch [7/75], Loss: 2.4604\n",
      "Epoch [7/75], Val Loss: 2.5037\n",
      "Epoch [8/75], Loss: 2.1144\n",
      "Epoch [8/75], Val Loss: 2.1976\n",
      "Epoch [9/75], Loss: 1.7492\n",
      "Epoch [9/75], Val Loss: 1.9224\n",
      "Epoch [10/75], Loss: 1.4548\n",
      "Epoch [10/75], Val Loss: 1.7385\n",
      "Epoch [11/75], Loss: 1.2759\n",
      "Epoch [11/75], Val Loss: 1.7310\n",
      "Epoch [12/75], Loss: 1.1296\n",
      "Epoch [12/75], Val Loss: 1.6019\n",
      "Epoch [13/75], Loss: 1.0500\n",
      "Epoch [13/75], Val Loss: 1.6893\n",
      "Epoch [14/75], Loss: 0.9223\n",
      "Epoch [14/75], Val Loss: 1.6877\n",
      "Epoch [15/75], Loss: 0.8516\n",
      "Epoch [15/75], Val Loss: 1.5926\n",
      "Epoch [16/75], Loss: 0.7416\n",
      "Epoch [16/75], Val Loss: 1.5560\n",
      "Epoch [17/75], Loss: 0.6748\n",
      "Epoch [17/75], Val Loss: 1.5445\n",
      "Epoch [18/75], Loss: 0.5836\n",
      "Epoch [18/75], Val Loss: 1.5894\n",
      "Epoch [19/75], Loss: 0.5676\n",
      "Epoch [19/75], Val Loss: 1.4774\n",
      "Epoch [20/75], Loss: 0.4740\n",
      "Epoch [20/75], Val Loss: 1.4908\n",
      "Epoch [21/75], Loss: 0.4169\n",
      "Epoch [21/75], Val Loss: 1.4826\n",
      "Epoch [22/75], Loss: 0.3950\n",
      "Epoch [22/75], Val Loss: 1.5740\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [22/75], Loss: 0.3950\n",
      "Test Accuracy Base CNN: 60.52%\n",
      "Epoch [1/75], Loss: 3.3443\n",
      "Epoch [1/75], Val Loss: 3.3035\n",
      "Epoch [2/75], Loss: 3.2522\n",
      "Epoch [2/75], Val Loss: 3.2333\n",
      "Epoch [3/75], Loss: 3.1657\n",
      "Epoch [3/75], Val Loss: 3.1731\n",
      "Epoch [4/75], Loss: 3.0777\n",
      "Epoch [4/75], Val Loss: 3.0960\n",
      "Epoch [5/75], Loss: 2.9604\n",
      "Epoch [5/75], Val Loss: 2.9905\n",
      "Epoch [6/75], Loss: 2.8073\n",
      "Epoch [6/75], Val Loss: 2.8397\n",
      "Epoch [7/75], Loss: 2.5994\n",
      "Epoch [7/75], Val Loss: 2.6232\n",
      "Epoch [8/75], Loss: 2.3057\n",
      "Epoch [8/75], Val Loss: 2.3390\n",
      "Epoch [9/75], Loss: 1.9686\n",
      "Epoch [9/75], Val Loss: 2.0301\n",
      "Epoch [10/75], Loss: 1.6409\n",
      "Epoch [10/75], Val Loss: 1.7904\n",
      "Epoch [11/75], Loss: 1.4104\n",
      "Epoch [11/75], Val Loss: 1.6899\n",
      "Epoch [12/75], Loss: 1.2305\n",
      "Epoch [12/75], Val Loss: 1.6642\n",
      "Epoch [13/75], Loss: 1.1218\n",
      "Epoch [13/75], Val Loss: 1.6264\n",
      "Epoch [14/75], Loss: 1.0031\n",
      "Epoch [14/75], Val Loss: 1.5597\n",
      "Epoch [15/75], Loss: 0.9264\n",
      "Epoch [15/75], Val Loss: 1.6569\n",
      "Epoch [16/75], Loss: 0.8818\n",
      "Epoch [16/75], Val Loss: 1.4333\n",
      "Epoch [17/75], Loss: 0.8018\n",
      "Epoch [17/75], Val Loss: 1.6218\n",
      "Epoch [18/75], Loss: 0.8214\n",
      "Epoch [18/75], Val Loss: 1.5782\n",
      "Epoch [19/75], Loss: 0.6631\n",
      "Epoch [19/75], Val Loss: 1.5697\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [19/75], Loss: 0.6631\n",
      "Test Accuracy Base CNN: 56.75%\n",
      "Epoch [1/75], Loss: 3.2929\n",
      "Epoch [1/75], Val Loss: 3.2389\n",
      "Epoch [2/75], Loss: 3.1446\n",
      "Epoch [2/75], Val Loss: 3.1422\n",
      "Epoch [3/75], Loss: 3.0088\n",
      "Epoch [3/75], Val Loss: 3.0353\n",
      "Epoch [4/75], Loss: 2.8398\n",
      "Epoch [4/75], Val Loss: 2.8611\n",
      "Epoch [5/75], Loss: 2.5937\n",
      "Epoch [5/75], Val Loss: 2.6072\n",
      "Epoch [6/75], Loss: 2.2676\n",
      "Epoch [6/75], Val Loss: 2.2904\n",
      "Epoch [7/75], Loss: 1.9074\n",
      "Epoch [7/75], Val Loss: 1.9887\n",
      "Epoch [8/75], Loss: 1.6019\n",
      "Epoch [8/75], Val Loss: 1.7844\n",
      "Epoch [9/75], Loss: 1.3937\n",
      "Epoch [9/75], Val Loss: 1.7289\n",
      "Epoch [10/75], Loss: 1.1997\n",
      "Epoch [10/75], Val Loss: 1.7638\n",
      "Epoch [11/75], Loss: 1.1348\n",
      "Epoch [11/75], Val Loss: 1.6649\n",
      "Epoch [12/75], Loss: 1.0128\n",
      "Epoch [12/75], Val Loss: 1.7481\n",
      "Epoch [13/75], Loss: 0.9088\n",
      "Epoch [13/75], Val Loss: 1.5196\n",
      "Epoch [14/75], Loss: 0.8392\n",
      "Epoch [14/75], Val Loss: 1.6653\n",
      "Epoch [15/75], Loss: 0.8010\n",
      "Epoch [15/75], Val Loss: 1.5446\n",
      "Epoch [16/75], Loss: 0.6662\n",
      "Epoch [16/75], Val Loss: 1.5114\n",
      "Epoch [17/75], Loss: 0.6431\n",
      "Epoch [17/75], Val Loss: 1.5374\n",
      "Epoch [18/75], Loss: 0.5486\n",
      "Epoch [18/75], Val Loss: 1.4994\n",
      "Epoch [19/75], Loss: 0.5168\n",
      "Epoch [19/75], Val Loss: 1.4930\n",
      "Epoch [20/75], Loss: 0.4119\n",
      "Epoch [20/75], Val Loss: 1.6037\n",
      "Epoch [21/75], Loss: 0.4089\n",
      "Epoch [21/75], Val Loss: 1.5971\n",
      "Epoch [22/75], Loss: 0.3439\n",
      "Epoch [22/75], Val Loss: 1.5317\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [22/75], Loss: 0.3439\n",
      "Test Accuracy Base CNN: 61.02%\n",
      "Epoch [1/75], Loss: 3.3840\n",
      "Epoch [1/75], Val Loss: 3.2694\n",
      "Epoch [2/75], Loss: 3.2233\n",
      "Epoch [2/75], Val Loss: 3.1899\n",
      "Epoch [3/75], Loss: 3.1079\n",
      "Epoch [3/75], Val Loss: 3.1190\n",
      "Epoch [4/75], Loss: 2.9724\n",
      "Epoch [4/75], Val Loss: 2.9928\n",
      "Epoch [5/75], Loss: 2.7897\n",
      "Epoch [5/75], Val Loss: 2.7966\n",
      "Epoch [6/75], Loss: 2.5169\n",
      "Epoch [6/75], Val Loss: 2.5053\n",
      "Epoch [7/75], Loss: 2.1690\n",
      "Epoch [7/75], Val Loss: 2.1675\n",
      "Epoch [8/75], Loss: 1.8081\n",
      "Epoch [8/75], Val Loss: 1.9169\n",
      "Epoch [9/75], Loss: 1.5575\n",
      "Epoch [9/75], Val Loss: 1.8310\n",
      "Epoch [10/75], Loss: 1.3582\n",
      "Epoch [10/75], Val Loss: 1.7444\n",
      "Epoch [11/75], Loss: 1.2938\n",
      "Epoch [11/75], Val Loss: 1.8869\n",
      "Epoch [12/75], Loss: 1.2199\n",
      "Epoch [12/75], Val Loss: 1.6567\n",
      "Epoch [13/75], Loss: 1.0549\n",
      "Epoch [13/75], Val Loss: 1.7468\n",
      "Epoch [14/75], Loss: 0.9478\n",
      "Epoch [14/75], Val Loss: 1.7222\n",
      "Epoch [15/75], Loss: 0.8541\n",
      "Epoch [15/75], Val Loss: 1.6215\n",
      "Epoch [16/75], Loss: 0.7745\n",
      "Epoch [16/75], Val Loss: 1.6124\n",
      "Epoch [17/75], Loss: 0.7677\n",
      "Epoch [17/75], Val Loss: 1.6379\n",
      "Epoch [18/75], Loss: 0.6539\n",
      "Epoch [18/75], Val Loss: 1.5889\n",
      "Epoch [19/75], Loss: 0.5963\n",
      "Epoch [19/75], Val Loss: 1.5863\n",
      "Epoch [20/75], Loss: 0.5124\n",
      "Epoch [20/75], Val Loss: 1.6469\n",
      "Epoch [21/75], Loss: 0.4651\n",
      "Epoch [21/75], Val Loss: 1.5516\n",
      "Epoch [22/75], Loss: 0.4369\n",
      "Epoch [22/75], Val Loss: 1.5694\n",
      "Epoch [23/75], Loss: 0.3523\n",
      "Epoch [23/75], Val Loss: 1.6102\n",
      "Epoch [24/75], Loss: 0.3164\n",
      "Epoch [24/75], Val Loss: 1.6885\n",
      "Stopping early at epoch 24 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [24/75], Loss: 0.3164\n",
      "Test Accuracy Base CNN: 60.11%\n",
      "Epoch [1/75], Loss: 3.3508\n",
      "Epoch [1/75], Val Loss: 3.2659\n",
      "Epoch [2/75], Loss: 3.2139\n",
      "Epoch [2/75], Val Loss: 3.1839\n",
      "Epoch [3/75], Loss: 3.0922\n",
      "Epoch [3/75], Val Loss: 3.1135\n",
      "Epoch [4/75], Loss: 2.9621\n",
      "Epoch [4/75], Val Loss: 2.9987\n",
      "Epoch [5/75], Loss: 2.7695\n",
      "Epoch [5/75], Val Loss: 2.8044\n",
      "Epoch [6/75], Loss: 2.5019\n",
      "Epoch [6/75], Val Loss: 2.5333\n",
      "Epoch [7/75], Loss: 2.1601\n",
      "Epoch [7/75], Val Loss: 2.2134\n",
      "Epoch [8/75], Loss: 1.8007\n",
      "Epoch [8/75], Val Loss: 1.9663\n",
      "Epoch [9/75], Loss: 1.5578\n",
      "Epoch [9/75], Val Loss: 1.8156\n",
      "Epoch [10/75], Loss: 1.3674\n",
      "Epoch [10/75], Val Loss: 1.7378\n",
      "Epoch [11/75], Loss: 1.2293\n",
      "Epoch [11/75], Val Loss: 1.7943\n",
      "Epoch [12/75], Loss: 1.1444\n",
      "Epoch [12/75], Val Loss: 1.7559\n",
      "Epoch [13/75], Loss: 1.0969\n",
      "Epoch [13/75], Val Loss: 1.7469\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [13/75], Loss: 1.0969\n",
      "Test Accuracy Base CNN: 53.06%\n",
      "Epoch [1/75], Loss: 3.2875\n",
      "Epoch [1/75], Val Loss: 3.2584\n",
      "Epoch [2/75], Loss: 3.1833\n",
      "Epoch [2/75], Val Loss: 3.1725\n",
      "Epoch [3/75], Loss: 3.0611\n",
      "Epoch [3/75], Val Loss: 3.0679\n",
      "Epoch [4/75], Loss: 2.9060\n",
      "Epoch [4/75], Val Loss: 2.9152\n",
      "Epoch [5/75], Loss: 2.6756\n",
      "Epoch [5/75], Val Loss: 2.6794\n",
      "Epoch [6/75], Loss: 2.3695\n",
      "Epoch [6/75], Val Loss: 2.3748\n",
      "Epoch [7/75], Loss: 1.9916\n",
      "Epoch [7/75], Val Loss: 2.0430\n",
      "Epoch [8/75], Loss: 1.6367\n",
      "Epoch [8/75], Val Loss: 1.8175\n",
      "Epoch [9/75], Loss: 1.4108\n",
      "Epoch [9/75], Val Loss: 1.7291\n",
      "Epoch [10/75], Loss: 1.2217\n",
      "Epoch [10/75], Val Loss: 1.6881\n",
      "Epoch [11/75], Loss: 1.0843\n",
      "Epoch [11/75], Val Loss: 1.6457\n",
      "Epoch [12/75], Loss: 0.9900\n",
      "Epoch [12/75], Val Loss: 1.6671\n",
      "Epoch [13/75], Loss: 0.8839\n",
      "Epoch [13/75], Val Loss: 1.5682\n",
      "Epoch [14/75], Loss: 0.7974\n",
      "Epoch [14/75], Val Loss: 1.5873\n",
      "Epoch [15/75], Loss: 0.7350\n",
      "Epoch [15/75], Val Loss: 1.6070\n",
      "Epoch [16/75], Loss: 0.6564\n",
      "Epoch [16/75], Val Loss: 1.5908\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [16/75], Loss: 0.6564\n",
      "Test Accuracy Base CNN: 57.32%\n",
      "Epoch [1/75], Loss: 3.3594\n",
      "Epoch [1/75], Val Loss: 3.2679\n",
      "Epoch [2/75], Loss: 3.2185\n",
      "Epoch [2/75], Val Loss: 3.1715\n",
      "Epoch [3/75], Loss: 3.0833\n",
      "Epoch [3/75], Val Loss: 3.0675\n",
      "Epoch [4/75], Loss: 2.9208\n",
      "Epoch [4/75], Val Loss: 2.9197\n",
      "Epoch [5/75], Loss: 2.7093\n",
      "Epoch [5/75], Val Loss: 2.6994\n",
      "Epoch [6/75], Loss: 2.4017\n",
      "Epoch [6/75], Val Loss: 2.3860\n",
      "Epoch [7/75], Loss: 2.0214\n",
      "Epoch [7/75], Val Loss: 2.0509\n",
      "Epoch [8/75], Loss: 1.6676\n",
      "Epoch [8/75], Val Loss: 1.8400\n",
      "Epoch [9/75], Loss: 1.4683\n",
      "Epoch [9/75], Val Loss: 1.7003\n",
      "Epoch [10/75], Loss: 1.2703\n",
      "Epoch [10/75], Val Loss: 1.6874\n",
      "Epoch [11/75], Loss: 1.1134\n",
      "Epoch [11/75], Val Loss: 1.7115\n",
      "Epoch [12/75], Loss: 1.0239\n",
      "Epoch [12/75], Val Loss: 1.7111\n",
      "Epoch [13/75], Loss: 0.9673\n",
      "Epoch [13/75], Val Loss: 1.6235\n",
      "Epoch [14/75], Loss: 0.8454\n",
      "Epoch [14/75], Val Loss: 1.5787\n",
      "Epoch [15/75], Loss: 0.7571\n",
      "Epoch [15/75], Val Loss: 1.6337\n",
      "Epoch [16/75], Loss: 0.7553\n",
      "Epoch [16/75], Val Loss: 1.5727\n",
      "Epoch [17/75], Loss: 0.6527\n",
      "Epoch [17/75], Val Loss: 1.5593\n",
      "Epoch [18/75], Loss: 0.6335\n",
      "Epoch [18/75], Val Loss: 1.4949\n",
      "Epoch [19/75], Loss: 0.5648\n",
      "Epoch [19/75], Val Loss: 1.5002\n",
      "Epoch [20/75], Loss: 0.5284\n",
      "Epoch [20/75], Val Loss: 1.4990\n",
      "Epoch [21/75], Loss: 0.4262\n",
      "Epoch [21/75], Val Loss: 1.5342\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [21/75], Loss: 0.4262\n",
      "Test Accuracy Base CNN: 60.23%\n",
      "Epoch [1/75], Loss: 3.3124\n",
      "Epoch [1/75], Val Loss: 3.2448\n",
      "Epoch [2/75], Loss: 3.1598\n",
      "Epoch [2/75], Val Loss: 3.1319\n",
      "Epoch [3/75], Loss: 3.0115\n",
      "Epoch [3/75], Val Loss: 3.0073\n",
      "Epoch [4/75], Loss: 2.8058\n",
      "Epoch [4/75], Val Loss: 2.8019\n",
      "Epoch [5/75], Loss: 2.5170\n",
      "Epoch [5/75], Val Loss: 2.5123\n",
      "Epoch [6/75], Loss: 2.1423\n",
      "Epoch [6/75], Val Loss: 2.1789\n",
      "Epoch [7/75], Loss: 1.7335\n",
      "Epoch [7/75], Val Loss: 1.8824\n",
      "Epoch [8/75], Loss: 1.4364\n",
      "Epoch [8/75], Val Loss: 1.7673\n",
      "Epoch [9/75], Loss: 1.2549\n",
      "Epoch [9/75], Val Loss: 1.7642\n",
      "Epoch [10/75], Loss: 1.2012\n",
      "Epoch [10/75], Val Loss: 1.6720\n",
      "Epoch [11/75], Loss: 1.1131\n",
      "Epoch [11/75], Val Loss: 1.7243\n",
      "Epoch [12/75], Loss: 1.0347\n",
      "Epoch [12/75], Val Loss: 1.6956\n",
      "Epoch [13/75], Loss: 0.9189\n",
      "Epoch [13/75], Val Loss: 1.6447\n",
      "Epoch [14/75], Loss: 0.8108\n",
      "Epoch [14/75], Val Loss: 1.6234\n",
      "Epoch [15/75], Loss: 0.8087\n",
      "Epoch [15/75], Val Loss: 1.6911\n",
      "Epoch [16/75], Loss: 0.6799\n",
      "Epoch [16/75], Val Loss: 1.4624\n",
      "Epoch [17/75], Loss: 0.5243\n",
      "Epoch [17/75], Val Loss: 1.5264\n",
      "Epoch [18/75], Loss: 0.4739\n",
      "Epoch [18/75], Val Loss: 1.4657\n",
      "Epoch [19/75], Loss: 0.4017\n",
      "Epoch [19/75], Val Loss: 1.4879\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [19/75], Loss: 0.4017\n",
      "Test Accuracy Base CNN: 61.38%\n",
      "Epoch [1/75], Loss: 3.3657\n",
      "Epoch [1/75], Val Loss: 3.2361\n",
      "Epoch [2/75], Loss: 3.1189\n",
      "Epoch [2/75], Val Loss: 3.1413\n",
      "Epoch [3/75], Loss: 2.9581\n",
      "Epoch [3/75], Val Loss: 2.9841\n",
      "Epoch [4/75], Loss: 2.7165\n",
      "Epoch [4/75], Val Loss: 2.7000\n",
      "Epoch [5/75], Loss: 2.3606\n",
      "Epoch [5/75], Val Loss: 2.3402\n",
      "Epoch [6/75], Loss: 1.9229\n",
      "Epoch [6/75], Val Loss: 2.0320\n",
      "Epoch [7/75], Loss: 1.6008\n",
      "Epoch [7/75], Val Loss: 1.8422\n",
      "Epoch [8/75], Loss: 1.4415\n",
      "Epoch [8/75], Val Loss: 1.8619\n",
      "Epoch [9/75], Loss: 1.3402\n",
      "Epoch [9/75], Val Loss: 1.6900\n",
      "Epoch [10/75], Loss: 1.1408\n",
      "Epoch [10/75], Val Loss: 1.6609\n",
      "Epoch [11/75], Loss: 1.0294\n",
      "Epoch [11/75], Val Loss: 1.6468\n",
      "Epoch [12/75], Loss: 0.8843\n",
      "Epoch [12/75], Val Loss: 1.7047\n",
      "Epoch [13/75], Loss: 0.8224\n",
      "Epoch [13/75], Val Loss: 1.5787\n",
      "Epoch [14/75], Loss: 0.7766\n",
      "Epoch [14/75], Val Loss: 1.6005\n",
      "Epoch [15/75], Loss: 0.7307\n",
      "Epoch [15/75], Val Loss: 1.5463\n",
      "Epoch [16/75], Loss: 0.6204\n",
      "Epoch [16/75], Val Loss: 1.4364\n",
      "Epoch [17/75], Loss: 0.5112\n",
      "Epoch [17/75], Val Loss: 1.5556\n",
      "Epoch [18/75], Loss: 0.4937\n",
      "Epoch [18/75], Val Loss: 1.5659\n",
      "Epoch [19/75], Loss: 0.4381\n",
      "Epoch [19/75], Val Loss: 1.4708\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [19/75], Loss: 0.4381\n",
      "Test Accuracy Base CNN: 61.01%\n",
      "Epoch [1/75], Loss: 3.4056\n",
      "Epoch [1/75], Val Loss: 3.2706\n",
      "Epoch [2/75], Loss: 3.2019\n",
      "Epoch [2/75], Val Loss: 3.1914\n",
      "Epoch [3/75], Loss: 3.0914\n",
      "Epoch [3/75], Val Loss: 3.1091\n",
      "Epoch [4/75], Loss: 2.9642\n",
      "Epoch [4/75], Val Loss: 2.9929\n",
      "Epoch [5/75], Loss: 2.7886\n",
      "Epoch [5/75], Val Loss: 2.8243\n",
      "Epoch [6/75], Loss: 2.5450\n",
      "Epoch [6/75], Val Loss: 2.5685\n",
      "Epoch [7/75], Loss: 2.2106\n",
      "Epoch [7/75], Val Loss: 2.2746\n",
      "Epoch [8/75], Loss: 1.8655\n",
      "Epoch [8/75], Val Loss: 1.9924\n",
      "Epoch [9/75], Loss: 1.5935\n",
      "Epoch [9/75], Val Loss: 1.8631\n",
      "Epoch [10/75], Loss: 1.4161\n",
      "Epoch [10/75], Val Loss: 1.7454\n",
      "Epoch [11/75], Loss: 1.2672\n",
      "Epoch [11/75], Val Loss: 1.7923\n",
      "Epoch [12/75], Loss: 1.2258\n",
      "Epoch [12/75], Val Loss: 1.8449\n",
      "Epoch [13/75], Loss: 1.1031\n",
      "Epoch [13/75], Val Loss: 1.6428\n",
      "Epoch [14/75], Loss: 1.0062\n",
      "Epoch [14/75], Val Loss: 1.6536\n",
      "Epoch [15/75], Loss: 0.8857\n",
      "Epoch [15/75], Val Loss: 1.5726\n",
      "Epoch [16/75], Loss: 0.7993\n",
      "Epoch [16/75], Val Loss: 1.4625\n",
      "Epoch [17/75], Loss: 0.7102\n",
      "Epoch [17/75], Val Loss: 1.4837\n",
      "Epoch [18/75], Loss: 0.6561\n",
      "Epoch [18/75], Val Loss: 1.5244\n",
      "Epoch [19/75], Loss: 0.5186\n",
      "Epoch [19/75], Val Loss: 1.4905\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [19/75], Loss: 0.5186\n",
      "Test Accuracy Base CNN: 58.89%\n",
      "Epoch [1/75], Loss: 3.3423\n",
      "Epoch [1/75], Val Loss: 3.2629\n",
      "Epoch [2/75], Loss: 3.1980\n",
      "Epoch [2/75], Val Loss: 3.1732\n",
      "Epoch [3/75], Loss: 3.0820\n",
      "Epoch [3/75], Val Loss: 3.0896\n",
      "Epoch [4/75], Loss: 2.9428\n",
      "Epoch [4/75], Val Loss: 2.9576\n",
      "Epoch [5/75], Loss: 2.7382\n",
      "Epoch [5/75], Val Loss: 2.7506\n",
      "Epoch [6/75], Loss: 2.4449\n",
      "Epoch [6/75], Val Loss: 2.4448\n",
      "Epoch [7/75], Loss: 2.0806\n",
      "Epoch [7/75], Val Loss: 2.1358\n",
      "Epoch [8/75], Loss: 1.7190\n",
      "Epoch [8/75], Val Loss: 1.8886\n",
      "Epoch [9/75], Loss: 1.4521\n",
      "Epoch [9/75], Val Loss: 1.7591\n",
      "Epoch [10/75], Loss: 1.2479\n",
      "Epoch [10/75], Val Loss: 1.6533\n",
      "Epoch [11/75], Loss: 1.1243\n",
      "Epoch [11/75], Val Loss: 1.6488\n",
      "Epoch [12/75], Loss: 1.0179\n",
      "Epoch [12/75], Val Loss: 1.6400\n",
      "Epoch [13/75], Loss: 0.9422\n",
      "Epoch [13/75], Val Loss: 1.7489\n",
      "Epoch [14/75], Loss: 0.9289\n",
      "Epoch [14/75], Val Loss: 1.7567\n",
      "Epoch [15/75], Loss: 0.8734\n",
      "Epoch [15/75], Val Loss: 1.8074\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.8734\n",
      "Test Accuracy Base CNN: 53.18%\n",
      "Epoch [1/75], Loss: 3.3206\n",
      "Epoch [1/75], Val Loss: 3.2642\n",
      "Epoch [2/75], Loss: 3.1670\n",
      "Epoch [2/75], Val Loss: 3.1398\n",
      "Epoch [3/75], Loss: 2.9990\n",
      "Epoch [3/75], Val Loss: 3.0190\n",
      "Epoch [4/75], Loss: 2.8038\n",
      "Epoch [4/75], Val Loss: 2.8077\n",
      "Epoch [5/75], Loss: 2.5157\n",
      "Epoch [5/75], Val Loss: 2.5043\n",
      "Epoch [6/75], Loss: 2.1504\n",
      "Epoch [6/75], Val Loss: 2.1414\n",
      "Epoch [7/75], Loss: 1.7511\n",
      "Epoch [7/75], Val Loss: 1.8841\n",
      "Epoch [8/75], Loss: 1.4715\n",
      "Epoch [8/75], Val Loss: 1.8053\n",
      "Epoch [9/75], Loss: 1.3108\n",
      "Epoch [9/75], Val Loss: 1.7354\n",
      "Epoch [10/75], Loss: 1.1324\n",
      "Epoch [10/75], Val Loss: 1.6252\n",
      "Epoch [11/75], Loss: 0.9875\n",
      "Epoch [11/75], Val Loss: 1.6545\n",
      "Epoch [12/75], Loss: 0.9010\n",
      "Epoch [12/75], Val Loss: 1.5965\n",
      "Epoch [13/75], Loss: 0.7739\n",
      "Epoch [13/75], Val Loss: 1.5698\n",
      "Epoch [14/75], Loss: 0.7189\n",
      "Epoch [14/75], Val Loss: 1.5184\n",
      "Epoch [15/75], Loss: 0.6918\n",
      "Epoch [15/75], Val Loss: 1.6545\n",
      "Epoch [16/75], Loss: 0.6232\n",
      "Epoch [16/75], Val Loss: 1.5485\n",
      "Epoch [17/75], Loss: 0.5514\n",
      "Epoch [17/75], Val Loss: 1.6537\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [17/75], Loss: 0.5514\n",
      "Test Accuracy Base CNN: 57.21%\n",
      "Epoch [1/75], Loss: 3.3056\n",
      "Epoch [1/75], Val Loss: 3.2826\n",
      "Epoch [2/75], Loss: 3.2043\n",
      "Epoch [2/75], Val Loss: 3.2029\n",
      "Epoch [3/75], Loss: 3.0904\n",
      "Epoch [3/75], Val Loss: 3.1097\n",
      "Epoch [4/75], Loss: 2.9473\n",
      "Epoch [4/75], Val Loss: 2.9583\n",
      "Epoch [5/75], Loss: 2.7266\n",
      "Epoch [5/75], Val Loss: 2.7231\n",
      "Epoch [6/75], Loss: 2.4196\n",
      "Epoch [6/75], Val Loss: 2.3937\n",
      "Epoch [7/75], Loss: 2.0397\n",
      "Epoch [7/75], Val Loss: 2.0491\n",
      "Epoch [8/75], Loss: 1.6597\n",
      "Epoch [8/75], Val Loss: 1.8045\n",
      "Epoch [9/75], Loss: 1.4126\n",
      "Epoch [9/75], Val Loss: 1.7089\n",
      "Epoch [10/75], Loss: 1.2570\n",
      "Epoch [10/75], Val Loss: 1.6883\n",
      "Epoch [11/75], Loss: 1.1931\n",
      "Epoch [11/75], Val Loss: 1.7327\n",
      "Epoch [12/75], Loss: 1.1802\n",
      "Epoch [12/75], Val Loss: 1.6973\n",
      "Epoch [13/75], Loss: 0.9923\n",
      "Epoch [13/75], Val Loss: 1.7118\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [13/75], Loss: 0.9923\n",
      "Test Accuracy Base CNN: 54.21%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/75], Loss: 3.3508\n",
      "Epoch [1/75], Val Loss: 3.3069\n",
      "Epoch [2/75], Loss: 3.2430\n",
      "Epoch [2/75], Val Loss: 3.2236\n",
      "Epoch [3/75], Loss: 3.1444\n",
      "Epoch [3/75], Val Loss: 3.1539\n",
      "Epoch [4/75], Loss: 3.0350\n",
      "Epoch [4/75], Val Loss: 3.0702\n",
      "Epoch [5/75], Loss: 2.9052\n",
      "Epoch [5/75], Val Loss: 2.9477\n",
      "Epoch [6/75], Loss: 2.7138\n",
      "Epoch [6/75], Val Loss: 2.7514\n",
      "Epoch [7/75], Loss: 2.4602\n",
      "Epoch [7/75], Val Loss: 2.4819\n",
      "Epoch [8/75], Loss: 2.1209\n",
      "Epoch [8/75], Val Loss: 2.1661\n",
      "Epoch [9/75], Loss: 1.7857\n",
      "Epoch [9/75], Val Loss: 1.9147\n",
      "Epoch [10/75], Loss: 1.5613\n",
      "Epoch [10/75], Val Loss: 1.7466\n",
      "Epoch [11/75], Loss: 1.4174\n",
      "Epoch [11/75], Val Loss: 1.7417\n",
      "Epoch [12/75], Loss: 1.2450\n",
      "Epoch [12/75], Val Loss: 1.6690\n",
      "Epoch [13/75], Loss: 1.1553\n",
      "Epoch [13/75], Val Loss: 1.7239\n",
      "Epoch [14/75], Loss: 1.0330\n",
      "Epoch [14/75], Val Loss: 1.6688\n",
      "Epoch [15/75], Loss: 0.9353\n",
      "Epoch [15/75], Val Loss: 1.7893\n",
      "Epoch [16/75], Loss: 0.9141\n",
      "Epoch [16/75], Val Loss: 1.6062\n",
      "Epoch [17/75], Loss: 0.8005\n",
      "Epoch [17/75], Val Loss: 1.6129\n",
      "Epoch [18/75], Loss: 0.7005\n",
      "Epoch [18/75], Val Loss: 1.5768\n",
      "Epoch [19/75], Loss: 0.6424\n",
      "Epoch [19/75], Val Loss: 1.4501\n",
      "Epoch [20/75], Loss: 0.5340\n",
      "Epoch [20/75], Val Loss: 1.5428\n",
      "Epoch [21/75], Loss: 0.4600\n",
      "Epoch [21/75], Val Loss: 1.4268\n",
      "Epoch [22/75], Loss: 0.3963\n",
      "Epoch [22/75], Val Loss: 1.5391\n",
      "Epoch [23/75], Loss: 0.3547\n",
      "Epoch [23/75], Val Loss: 1.5246\n",
      "Epoch [24/75], Loss: 0.2971\n",
      "Epoch [24/75], Val Loss: 1.5720\n",
      "Stopping early at epoch 24 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [24/75], Loss: 0.2971\n",
      "Test Accuracy Base CNN: 60.76%\n",
      "Epoch [1/75], Loss: 3.3243\n",
      "Epoch [1/75], Val Loss: 3.2568\n",
      "Epoch [2/75], Loss: 3.1582\n",
      "Epoch [2/75], Val Loss: 3.1390\n",
      "Epoch [3/75], Loss: 3.0076\n",
      "Epoch [3/75], Val Loss: 2.9863\n",
      "Epoch [4/75], Loss: 2.7952\n",
      "Epoch [4/75], Val Loss: 2.7602\n",
      "Epoch [5/75], Loss: 2.5031\n",
      "Epoch [5/75], Val Loss: 2.4591\n",
      "Epoch [6/75], Loss: 2.1255\n",
      "Epoch [6/75], Val Loss: 2.1025\n",
      "Epoch [7/75], Loss: 1.7352\n",
      "Epoch [7/75], Val Loss: 1.7916\n",
      "Epoch [8/75], Loss: 1.4623\n",
      "Epoch [8/75], Val Loss: 1.7516\n",
      "Epoch [9/75], Loss: 1.3765\n",
      "Epoch [9/75], Val Loss: 1.7791\n",
      "Epoch [10/75], Loss: 1.2568\n",
      "Epoch [10/75], Val Loss: 1.7408\n",
      "Epoch [11/75], Loss: 1.1636\n",
      "Epoch [11/75], Val Loss: 1.7131\n",
      "Epoch [12/75], Loss: 1.0838\n",
      "Epoch [12/75], Val Loss: 1.6130\n",
      "Epoch [13/75], Loss: 0.9337\n",
      "Epoch [13/75], Val Loss: 1.6013\n",
      "Epoch [14/75], Loss: 0.8121\n",
      "Epoch [14/75], Val Loss: 1.5304\n",
      "Epoch [15/75], Loss: 0.8042\n",
      "Epoch [15/75], Val Loss: 1.5610\n",
      "Epoch [16/75], Loss: 0.7136\n",
      "Epoch [16/75], Val Loss: 1.5017\n",
      "Epoch [17/75], Loss: 0.6283\n",
      "Epoch [17/75], Val Loss: 1.4959\n",
      "Epoch [18/75], Loss: 0.5411\n",
      "Epoch [18/75], Val Loss: 1.5313\n",
      "Epoch [19/75], Loss: 0.5042\n",
      "Epoch [19/75], Val Loss: 1.4398\n",
      "Epoch [20/75], Loss: 0.4567\n",
      "Epoch [20/75], Val Loss: 1.5432\n",
      "Epoch [21/75], Loss: 0.4023\n",
      "Epoch [21/75], Val Loss: 1.5166\n",
      "Epoch [22/75], Loss: 0.3334\n",
      "Epoch [22/75], Val Loss: 1.4991\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [22/75], Loss: 0.3334\n",
      "Test Accuracy Base CNN: 61.64%\n",
      "Epoch [1/75], Loss: 3.3627\n",
      "Epoch [1/75], Val Loss: 3.2679\n",
      "Epoch [2/75], Loss: 3.1947\n",
      "Epoch [2/75], Val Loss: 3.1613\n",
      "Epoch [3/75], Loss: 3.0624\n",
      "Epoch [3/75], Val Loss: 3.0771\n",
      "Epoch [4/75], Loss: 2.9074\n",
      "Epoch [4/75], Val Loss: 2.9339\n",
      "Epoch [5/75], Loss: 2.6827\n",
      "Epoch [5/75], Val Loss: 2.7006\n",
      "Epoch [6/75], Loss: 2.3849\n",
      "Epoch [6/75], Val Loss: 2.3971\n",
      "Epoch [7/75], Loss: 2.0305\n",
      "Epoch [7/75], Val Loss: 2.0939\n",
      "Epoch [8/75], Loss: 1.6598\n",
      "Epoch [8/75], Val Loss: 1.8376\n",
      "Epoch [9/75], Loss: 1.4080\n",
      "Epoch [9/75], Val Loss: 1.7748\n",
      "Epoch [10/75], Loss: 1.3259\n",
      "Epoch [10/75], Val Loss: 1.7112\n",
      "Epoch [11/75], Loss: 1.1426\n",
      "Epoch [11/75], Val Loss: 1.6888\n",
      "Epoch [12/75], Loss: 1.1192\n",
      "Epoch [12/75], Val Loss: 1.6276\n",
      "Epoch [13/75], Loss: 0.9096\n",
      "Epoch [13/75], Val Loss: 1.6623\n",
      "Epoch [14/75], Loss: 0.8518\n",
      "Epoch [14/75], Val Loss: 1.6280\n",
      "Epoch [15/75], Loss: 0.7923\n",
      "Epoch [15/75], Val Loss: 1.6011\n",
      "Epoch [16/75], Loss: 0.7453\n",
      "Epoch [16/75], Val Loss: 1.6499\n",
      "Epoch [17/75], Loss: 0.6619\n",
      "Epoch [17/75], Val Loss: 1.5762\n",
      "Epoch [18/75], Loss: 0.5893\n",
      "Epoch [18/75], Val Loss: 1.5588\n",
      "Epoch [19/75], Loss: 0.4915\n",
      "Epoch [19/75], Val Loss: 1.4967\n",
      "Epoch [20/75], Loss: 0.4431\n",
      "Epoch [20/75], Val Loss: 1.5125\n",
      "Epoch [21/75], Loss: 0.4421\n",
      "Epoch [21/75], Val Loss: 1.5389\n",
      "Epoch [22/75], Loss: 0.3952\n",
      "Epoch [22/75], Val Loss: 1.6440\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [22/75], Loss: 0.3952\n",
      "Test Accuracy Base CNN: 58.94%\n",
      "Epoch [1/75], Loss: 3.3537\n",
      "Epoch [1/75], Val Loss: 3.2787\n",
      "Epoch [2/75], Loss: 3.1912\n",
      "Epoch [2/75], Val Loss: 3.1765\n",
      "Epoch [3/75], Loss: 3.0614\n",
      "Epoch [3/75], Val Loss: 3.0893\n",
      "Epoch [4/75], Loss: 2.9200\n",
      "Epoch [4/75], Val Loss: 2.9553\n",
      "Epoch [5/75], Loss: 2.6993\n",
      "Epoch [5/75], Val Loss: 2.7188\n",
      "Epoch [6/75], Loss: 2.4004\n",
      "Epoch [6/75], Val Loss: 2.4197\n",
      "Epoch [7/75], Loss: 2.0219\n",
      "Epoch [7/75], Val Loss: 2.0653\n",
      "Epoch [8/75], Loss: 1.6640\n",
      "Epoch [8/75], Val Loss: 1.8514\n",
      "Epoch [9/75], Loss: 1.4368\n",
      "Epoch [9/75], Val Loss: 1.7398\n",
      "Epoch [10/75], Loss: 1.2886\n",
      "Epoch [10/75], Val Loss: 1.7409\n",
      "Epoch [11/75], Loss: 1.2034\n",
      "Epoch [11/75], Val Loss: 1.7165\n",
      "Epoch [12/75], Loss: 1.0872\n",
      "Epoch [12/75], Val Loss: 1.6521\n",
      "Epoch [13/75], Loss: 0.8881\n",
      "Epoch [13/75], Val Loss: 1.5988\n",
      "Epoch [14/75], Loss: 0.7887\n",
      "Epoch [14/75], Val Loss: 1.5870\n",
      "Epoch [15/75], Loss: 0.7636\n",
      "Epoch [15/75], Val Loss: 1.4972\n",
      "Epoch [16/75], Loss: 0.6635\n",
      "Epoch [16/75], Val Loss: 1.5777\n",
      "Epoch [17/75], Loss: 0.5938\n",
      "Epoch [17/75], Val Loss: 1.5262\n",
      "Epoch [18/75], Loss: 0.5404\n",
      "Epoch [18/75], Val Loss: 1.7031\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [18/75], Loss: 0.5404\n",
      "Test Accuracy Base CNN: 54.88%\n",
      "Epoch [1/75], Loss: 3.3190\n",
      "Epoch [1/75], Val Loss: 3.2218\n",
      "Epoch [2/75], Loss: 3.1615\n",
      "Epoch [2/75], Val Loss: 3.1287\n",
      "Epoch [3/75], Loss: 3.0103\n",
      "Epoch [3/75], Val Loss: 3.0135\n",
      "Epoch [4/75], Loss: 2.8257\n",
      "Epoch [4/75], Val Loss: 2.8398\n",
      "Epoch [5/75], Loss: 2.5666\n",
      "Epoch [5/75], Val Loss: 2.5832\n",
      "Epoch [6/75], Loss: 2.2231\n",
      "Epoch [6/75], Val Loss: 2.2445\n",
      "Epoch [7/75], Loss: 1.8590\n",
      "Epoch [7/75], Val Loss: 1.9542\n",
      "Epoch [8/75], Loss: 1.5811\n",
      "Epoch [8/75], Val Loss: 1.8417\n",
      "Epoch [9/75], Loss: 1.4034\n",
      "Epoch [9/75], Val Loss: 1.7454\n",
      "Epoch [10/75], Loss: 1.2780\n",
      "Epoch [10/75], Val Loss: 1.6421\n",
      "Epoch [11/75], Loss: 1.0510\n",
      "Epoch [11/75], Val Loss: 1.7096\n",
      "Epoch [12/75], Loss: 1.0562\n",
      "Epoch [12/75], Val Loss: 1.6721\n",
      "Epoch [13/75], Loss: 1.0003\n",
      "Epoch [13/75], Val Loss: 1.7588\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [13/75], Loss: 1.0003\n",
      "Test Accuracy Base CNN: 52.00%\n",
      "Epoch [1/75], Loss: 3.4219\n",
      "Epoch [1/75], Val Loss: 3.2705\n",
      "Epoch [2/75], Loss: 3.2005\n",
      "Epoch [2/75], Val Loss: 3.1359\n",
      "Epoch [3/75], Loss: 3.0274\n",
      "Epoch [3/75], Val Loss: 3.0144\n",
      "Epoch [4/75], Loss: 2.8225\n",
      "Epoch [4/75], Val Loss: 2.8173\n",
      "Epoch [5/75], Loss: 2.5215\n",
      "Epoch [5/75], Val Loss: 2.5245\n",
      "Epoch [6/75], Loss: 2.1483\n",
      "Epoch [6/75], Val Loss: 2.1677\n",
      "Epoch [7/75], Loss: 1.7772\n",
      "Epoch [7/75], Val Loss: 1.9504\n",
      "Epoch [8/75], Loss: 1.5780\n",
      "Epoch [8/75], Val Loss: 1.8037\n",
      "Epoch [9/75], Loss: 1.4024\n",
      "Epoch [9/75], Val Loss: 1.7965\n",
      "Epoch [10/75], Loss: 1.2195\n",
      "Epoch [10/75], Val Loss: 1.7452\n",
      "Epoch [11/75], Loss: 1.1207\n",
      "Epoch [11/75], Val Loss: 1.7386\n",
      "Epoch [12/75], Loss: 0.9842\n",
      "Epoch [12/75], Val Loss: 1.6473\n",
      "Epoch [13/75], Loss: 0.8904\n",
      "Epoch [13/75], Val Loss: 1.5693\n",
      "Epoch [14/75], Loss: 0.7408\n",
      "Epoch [14/75], Val Loss: 1.5144\n",
      "Epoch [15/75], Loss: 0.6624\n",
      "Epoch [15/75], Val Loss: 1.4105\n",
      "Epoch [16/75], Loss: 0.5708\n",
      "Epoch [16/75], Val Loss: 1.5004\n",
      "Epoch [17/75], Loss: 0.5204\n",
      "Epoch [17/75], Val Loss: 1.4762\n",
      "Epoch [18/75], Loss: 0.4604\n",
      "Epoch [18/75], Val Loss: 1.5161\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [18/75], Loss: 0.4604\n",
      "Test Accuracy Base CNN: 60.42%\n",
      "Epoch [1/75], Loss: 3.3492\n",
      "Epoch [1/75], Val Loss: 3.2496\n",
      "Epoch [2/75], Loss: 3.1908\n",
      "Epoch [2/75], Val Loss: 3.1641\n",
      "Epoch [3/75], Loss: 3.0620\n",
      "Epoch [3/75], Val Loss: 3.0726\n",
      "Epoch [4/75], Loss: 2.9113\n",
      "Epoch [4/75], Val Loss: 2.9246\n",
      "Epoch [5/75], Loss: 2.6817\n",
      "Epoch [5/75], Val Loss: 2.6867\n",
      "Epoch [6/75], Loss: 2.3757\n",
      "Epoch [6/75], Val Loss: 2.3494\n",
      "Epoch [7/75], Loss: 2.0060\n",
      "Epoch [7/75], Val Loss: 2.0371\n",
      "Epoch [8/75], Loss: 1.6535\n",
      "Epoch [8/75], Val Loss: 1.8161\n",
      "Epoch [9/75], Loss: 1.4031\n",
      "Epoch [9/75], Val Loss: 1.6930\n",
      "Epoch [10/75], Loss: 1.2382\n",
      "Epoch [10/75], Val Loss: 1.6322\n",
      "Epoch [11/75], Loss: 1.1036\n",
      "Epoch [11/75], Val Loss: 1.6025\n",
      "Epoch [12/75], Loss: 1.0208\n",
      "Epoch [12/75], Val Loss: 1.6158\n",
      "Epoch [13/75], Loss: 0.9064\n",
      "Epoch [13/75], Val Loss: 1.5993\n",
      "Epoch [14/75], Loss: 0.8683\n",
      "Epoch [14/75], Val Loss: 1.5724\n",
      "Epoch [15/75], Loss: 0.8033\n",
      "Epoch [15/75], Val Loss: 1.6240\n",
      "Epoch [16/75], Loss: 0.7509\n",
      "Epoch [16/75], Val Loss: 1.6708\n",
      "Epoch [17/75], Loss: 0.6699\n",
      "Epoch [17/75], Val Loss: 1.5210\n",
      "Epoch [18/75], Loss: 0.5594\n",
      "Epoch [18/75], Val Loss: 1.5296\n",
      "Epoch [19/75], Loss: 0.5103\n",
      "Epoch [19/75], Val Loss: 1.4522\n",
      "Epoch [20/75], Loss: 0.4738\n",
      "Epoch [20/75], Val Loss: 1.5170\n",
      "Epoch [21/75], Loss: 0.3965\n",
      "Epoch [21/75], Val Loss: 1.4939\n",
      "Epoch [22/75], Loss: 0.3024\n",
      "Epoch [22/75], Val Loss: 1.4598\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [22/75], Loss: 0.3024\n",
      "Test Accuracy Base CNN: 62.64%\n",
      "Epoch [1/75], Loss: 3.3362\n",
      "Epoch [1/75], Val Loss: 3.2694\n",
      "Epoch [2/75], Loss: 3.1805\n",
      "Epoch [2/75], Val Loss: 3.1728\n",
      "Epoch [3/75], Loss: 3.0536\n",
      "Epoch [3/75], Val Loss: 3.0737\n",
      "Epoch [4/75], Loss: 2.8922\n",
      "Epoch [4/75], Val Loss: 2.9062\n",
      "Epoch [5/75], Loss: 2.6524\n",
      "Epoch [5/75], Val Loss: 2.6742\n",
      "Epoch [6/75], Loss: 2.3392\n",
      "Epoch [6/75], Val Loss: 2.3775\n",
      "Epoch [7/75], Loss: 1.9754\n",
      "Epoch [7/75], Val Loss: 2.0603\n",
      "Epoch [8/75], Loss: 1.6743\n",
      "Epoch [8/75], Val Loss: 1.8400\n",
      "Epoch [9/75], Loss: 1.3972\n",
      "Epoch [9/75], Val Loss: 1.7496\n",
      "Epoch [10/75], Loss: 1.2269\n",
      "Epoch [10/75], Val Loss: 1.6759\n",
      "Epoch [11/75], Loss: 1.1300\n",
      "Epoch [11/75], Val Loss: 1.6552\n",
      "Epoch [12/75], Loss: 1.0003\n",
      "Epoch [12/75], Val Loss: 1.6901\n",
      "Epoch [13/75], Loss: 0.9322\n",
      "Epoch [13/75], Val Loss: 1.6565\n",
      "Epoch [14/75], Loss: 0.8066\n",
      "Epoch [14/75], Val Loss: 1.6253\n",
      "Epoch [15/75], Loss: 0.7291\n",
      "Epoch [15/75], Val Loss: 1.5172\n",
      "Epoch [16/75], Loss: 0.6931\n",
      "Epoch [16/75], Val Loss: 1.5282\n",
      "Epoch [17/75], Loss: 0.6006\n",
      "Epoch [17/75], Val Loss: 1.4503\n",
      "Epoch [18/75], Loss: 0.4860\n",
      "Epoch [18/75], Val Loss: 1.5224\n",
      "Epoch [19/75], Loss: 0.4065\n",
      "Epoch [19/75], Val Loss: 1.5298\n",
      "Epoch [20/75], Loss: 0.3492\n",
      "Epoch [20/75], Val Loss: 1.4750\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [20/75], Loss: 0.3492\n",
      "Test Accuracy Base CNN: 61.53%\n",
      "Epoch [1/75], Loss: 3.3084\n",
      "Epoch [1/75], Val Loss: 3.2666\n",
      "Epoch [2/75], Loss: 3.2137\n",
      "Epoch [2/75], Val Loss: 3.1843\n",
      "Epoch [3/75], Loss: 3.1040\n",
      "Epoch [3/75], Val Loss: 3.0990\n",
      "Epoch [4/75], Loss: 2.9757\n",
      "Epoch [4/75], Val Loss: 2.9783\n",
      "Epoch [5/75], Loss: 2.7967\n",
      "Epoch [5/75], Val Loss: 2.7986\n",
      "Epoch [6/75], Loss: 2.5442\n",
      "Epoch [6/75], Val Loss: 2.5350\n",
      "Epoch [7/75], Loss: 2.2365\n",
      "Epoch [7/75], Val Loss: 2.2009\n",
      "Epoch [8/75], Loss: 1.8673\n",
      "Epoch [8/75], Val Loss: 1.9284\n",
      "Epoch [9/75], Loss: 1.5843\n",
      "Epoch [9/75], Val Loss: 1.7411\n",
      "Epoch [10/75], Loss: 1.3921\n",
      "Epoch [10/75], Val Loss: 1.6602\n",
      "Epoch [11/75], Loss: 1.2099\n",
      "Epoch [11/75], Val Loss: 1.7697\n",
      "Epoch [12/75], Loss: 1.2062\n",
      "Epoch [12/75], Val Loss: 1.6705\n",
      "Epoch [13/75], Loss: 1.1054\n",
      "Epoch [13/75], Val Loss: 1.7027\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [13/75], Loss: 1.1054\n",
      "Test Accuracy Base CNN: 52.56%\n",
      "Epoch [1/75], Loss: 3.3831\n",
      "Epoch [1/75], Val Loss: 3.2970\n",
      "Epoch [2/75], Loss: 3.2400\n",
      "Epoch [2/75], Val Loss: 3.2216\n",
      "Epoch [3/75], Loss: 3.1187\n",
      "Epoch [3/75], Val Loss: 3.1498\n",
      "Epoch [4/75], Loss: 2.9911\n",
      "Epoch [4/75], Val Loss: 3.0255\n",
      "Epoch [5/75], Loss: 2.8114\n",
      "Epoch [5/75], Val Loss: 2.8301\n",
      "Epoch [6/75], Loss: 2.5527\n",
      "Epoch [6/75], Val Loss: 2.5600\n",
      "Epoch [7/75], Loss: 2.2038\n",
      "Epoch [7/75], Val Loss: 2.2663\n",
      "Epoch [8/75], Loss: 1.8709\n",
      "Epoch [8/75], Val Loss: 2.0109\n",
      "Epoch [9/75], Loss: 1.5755\n",
      "Epoch [9/75], Val Loss: 1.8539\n",
      "Epoch [10/75], Loss: 1.4035\n",
      "Epoch [10/75], Val Loss: 1.7796\n",
      "Epoch [11/75], Loss: 1.2250\n",
      "Epoch [11/75], Val Loss: 1.7476\n",
      "Epoch [12/75], Loss: 1.1043\n",
      "Epoch [12/75], Val Loss: 1.7905\n",
      "Epoch [13/75], Loss: 0.9645\n",
      "Epoch [13/75], Val Loss: 1.6870\n",
      "Epoch [14/75], Loss: 0.8755\n",
      "Epoch [14/75], Val Loss: 1.6478\n",
      "Epoch [15/75], Loss: 0.8399\n",
      "Epoch [15/75], Val Loss: 1.6172\n",
      "Epoch [16/75], Loss: 0.7864\n",
      "Epoch [16/75], Val Loss: 1.6299\n",
      "Epoch [17/75], Loss: 0.6707\n",
      "Epoch [17/75], Val Loss: 1.6768\n",
      "Epoch [18/75], Loss: 0.5723\n",
      "Epoch [18/75], Val Loss: 1.6341\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [18/75], Loss: 0.5723\n",
      "Test Accuracy Base CNN: 55.59%\n",
      "Epoch [1/75], Loss: 3.3554\n",
      "Epoch [1/75], Val Loss: 3.2656\n",
      "Epoch [2/75], Loss: 3.1786\n",
      "Epoch [2/75], Val Loss: 3.1503\n",
      "Epoch [3/75], Loss: 3.0266\n",
      "Epoch [3/75], Val Loss: 3.0432\n",
      "Epoch [4/75], Loss: 2.8514\n",
      "Epoch [4/75], Val Loss: 2.8641\n",
      "Epoch [5/75], Loss: 2.5773\n",
      "Epoch [5/75], Val Loss: 2.5613\n",
      "Epoch [6/75], Loss: 2.2068\n",
      "Epoch [6/75], Val Loss: 2.2272\n",
      "Epoch [7/75], Loss: 1.8053\n",
      "Epoch [7/75], Val Loss: 1.9088\n",
      "Epoch [8/75], Loss: 1.5215\n",
      "Epoch [8/75], Val Loss: 1.7496\n",
      "Epoch [9/75], Loss: 1.3519\n",
      "Epoch [9/75], Val Loss: 1.7099\n",
      "Epoch [10/75], Loss: 1.2123\n",
      "Epoch [10/75], Val Loss: 1.7051\n",
      "Epoch [11/75], Loss: 1.1505\n",
      "Epoch [11/75], Val Loss: 1.7026\n",
      "Epoch [12/75], Loss: 1.0605\n",
      "Epoch [12/75], Val Loss: 1.6443\n",
      "Epoch [13/75], Loss: 0.9332\n",
      "Epoch [13/75], Val Loss: 1.5909\n",
      "Epoch [14/75], Loss: 0.8847\n",
      "Epoch [14/75], Val Loss: 1.5779\n",
      "Epoch [15/75], Loss: 0.7826\n",
      "Epoch [15/75], Val Loss: 1.6101\n",
      "Epoch [16/75], Loss: 0.6842\n",
      "Epoch [16/75], Val Loss: 1.5335\n",
      "Epoch [17/75], Loss: 0.6192\n",
      "Epoch [17/75], Val Loss: 1.4833\n",
      "Epoch [18/75], Loss: 0.5509\n",
      "Epoch [18/75], Val Loss: 1.6034\n",
      "Epoch [19/75], Loss: 0.4808\n",
      "Epoch [19/75], Val Loss: 1.4760\n",
      "Epoch [20/75], Loss: 0.4595\n",
      "Epoch [20/75], Val Loss: 1.5229\n",
      "Epoch [21/75], Loss: 0.4020\n",
      "Epoch [21/75], Val Loss: 1.5082\n",
      "Epoch [22/75], Loss: 0.3235\n",
      "Epoch [22/75], Val Loss: 1.5228\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [22/75], Loss: 0.3235\n",
      "Test Accuracy Base CNN: 61.20%\n",
      "Epoch [1/75], Loss: 3.3171\n",
      "Epoch [1/75], Val Loss: 3.2768\n",
      "Epoch [2/75], Loss: 3.2454\n",
      "Epoch [2/75], Val Loss: 3.2176\n",
      "Epoch [3/75], Loss: 3.1656\n",
      "Epoch [3/75], Val Loss: 3.1626\n",
      "Epoch [4/75], Loss: 3.0806\n",
      "Epoch [4/75], Val Loss: 3.0910\n",
      "Epoch [5/75], Loss: 2.9753\n",
      "Epoch [5/75], Val Loss: 2.9892\n",
      "Epoch [6/75], Loss: 2.8238\n",
      "Epoch [6/75], Val Loss: 2.8265\n",
      "Epoch [7/75], Loss: 2.6007\n",
      "Epoch [7/75], Val Loss: 2.5916\n",
      "Epoch [8/75], Loss: 2.3095\n",
      "Epoch [8/75], Val Loss: 2.2896\n",
      "Epoch [9/75], Loss: 1.9641\n",
      "Epoch [9/75], Val Loss: 1.9811\n",
      "Epoch [10/75], Loss: 1.6463\n",
      "Epoch [10/75], Val Loss: 1.7856\n",
      "Epoch [11/75], Loss: 1.4453\n",
      "Epoch [11/75], Val Loss: 1.7256\n",
      "Epoch [12/75], Loss: 1.3069\n",
      "Epoch [12/75], Val Loss: 1.6955\n",
      "Epoch [13/75], Loss: 1.1798\n",
      "Epoch [13/75], Val Loss: 1.6097\n",
      "Epoch [14/75], Loss: 1.0436\n",
      "Epoch [14/75], Val Loss: 1.6870\n",
      "Epoch [15/75], Loss: 0.9351\n",
      "Epoch [15/75], Val Loss: 1.6151\n",
      "Epoch [16/75], Loss: 0.8872\n",
      "Epoch [16/75], Val Loss: 1.5926\n",
      "Epoch [17/75], Loss: 0.7818\n",
      "Epoch [17/75], Val Loss: 1.5555\n",
      "Epoch [18/75], Loss: 0.7067\n",
      "Epoch [18/75], Val Loss: 1.5424\n",
      "Epoch [19/75], Loss: 0.6455\n",
      "Epoch [19/75], Val Loss: 1.5225\n",
      "Epoch [20/75], Loss: 0.5519\n",
      "Epoch [20/75], Val Loss: 1.5918\n",
      "Epoch [21/75], Loss: 0.5074\n",
      "Epoch [21/75], Val Loss: 1.5086\n",
      "Epoch [22/75], Loss: 0.3929\n",
      "Epoch [22/75], Val Loss: 1.5538\n",
      "Epoch [23/75], Loss: 0.3531\n",
      "Epoch [23/75], Val Loss: 1.5161\n",
      "Epoch [24/75], Loss: 0.3052\n",
      "Epoch [24/75], Val Loss: 1.5692\n",
      "Stopping early at epoch 24 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [24/75], Loss: 0.3052\n",
      "Test Accuracy Base CNN: 60.95%\n",
      "Epoch [1/75], Loss: 3.3927\n",
      "Epoch [1/75], Val Loss: 3.2900\n",
      "Epoch [2/75], Loss: 3.2219\n",
      "Epoch [2/75], Val Loss: 3.1830\n",
      "Epoch [3/75], Loss: 3.0860\n",
      "Epoch [3/75], Val Loss: 3.0927\n",
      "Epoch [4/75], Loss: 2.9340\n",
      "Epoch [4/75], Val Loss: 2.9489\n",
      "Epoch [5/75], Loss: 2.7200\n",
      "Epoch [5/75], Val Loss: 2.7226\n",
      "Epoch [6/75], Loss: 2.4165\n",
      "Epoch [6/75], Val Loss: 2.4064\n",
      "Epoch [7/75], Loss: 2.0386\n",
      "Epoch [7/75], Val Loss: 2.0726\n",
      "Epoch [8/75], Loss: 1.6688\n",
      "Epoch [8/75], Val Loss: 1.8562\n",
      "Epoch [9/75], Loss: 1.4422\n",
      "Epoch [9/75], Val Loss: 1.7289\n",
      "Epoch [10/75], Loss: 1.2919\n",
      "Epoch [10/75], Val Loss: 1.7763\n",
      "Epoch [11/75], Loss: 1.1715\n",
      "Epoch [11/75], Val Loss: 1.6700\n",
      "Epoch [12/75], Loss: 0.9880\n",
      "Epoch [12/75], Val Loss: 1.7544\n",
      "Epoch [13/75], Loss: 0.9748\n",
      "Epoch [13/75], Val Loss: 1.7049\n",
      "Epoch [14/75], Loss: 0.8861\n",
      "Epoch [14/75], Val Loss: 1.6645\n",
      "Epoch [15/75], Loss: 0.8467\n",
      "Epoch [15/75], Val Loss: 1.5818\n",
      "Epoch [16/75], Loss: 0.6789\n",
      "Epoch [16/75], Val Loss: 1.5929\n",
      "Epoch [17/75], Loss: 0.6327\n",
      "Epoch [17/75], Val Loss: 1.5342\n",
      "Epoch [18/75], Loss: 0.5457\n",
      "Epoch [18/75], Val Loss: 1.5779\n",
      "Epoch [19/75], Loss: 0.5254\n",
      "Epoch [19/75], Val Loss: 1.5765\n",
      "Epoch [20/75], Loss: 0.4911\n",
      "Epoch [20/75], Val Loss: 1.6084\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [20/75], Loss: 0.4911\n",
      "Test Accuracy Base CNN: 58.99%\n",
      "Epoch [1/75], Loss: 3.4003\n",
      "Epoch [1/75], Val Loss: 3.2985\n",
      "Epoch [2/75], Loss: 3.2117\n",
      "Epoch [2/75], Val Loss: 3.1868\n",
      "Epoch [3/75], Loss: 3.0697\n",
      "Epoch [3/75], Val Loss: 3.0921\n",
      "Epoch [4/75], Loss: 2.9199\n",
      "Epoch [4/75], Val Loss: 2.9554\n",
      "Epoch [5/75], Loss: 2.7107\n",
      "Epoch [5/75], Val Loss: 2.7475\n",
      "Epoch [6/75], Loss: 2.4233\n",
      "Epoch [6/75], Val Loss: 2.4561\n",
      "Epoch [7/75], Loss: 2.0754\n",
      "Epoch [7/75], Val Loss: 2.1536\n",
      "Epoch [8/75], Loss: 1.7228\n",
      "Epoch [8/75], Val Loss: 1.9101\n",
      "Epoch [9/75], Loss: 1.4682\n",
      "Epoch [9/75], Val Loss: 1.7202\n",
      "Epoch [10/75], Loss: 1.2787\n",
      "Epoch [10/75], Val Loss: 1.6482\n",
      "Epoch [11/75], Loss: 1.0838\n",
      "Epoch [11/75], Val Loss: 1.6070\n",
      "Epoch [12/75], Loss: 0.9451\n",
      "Epoch [12/75], Val Loss: 1.5922\n",
      "Epoch [13/75], Loss: 0.8899\n",
      "Epoch [13/75], Val Loss: 1.5555\n",
      "Epoch [14/75], Loss: 0.8283\n",
      "Epoch [14/75], Val Loss: 1.7599\n",
      "Epoch [15/75], Loss: 0.7622\n",
      "Epoch [15/75], Val Loss: 1.5847\n",
      "Epoch [16/75], Loss: 0.6844\n",
      "Epoch [16/75], Val Loss: 1.6377\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [16/75], Loss: 0.6844\n",
      "Test Accuracy Base CNN: 56.70%\n",
      "Epoch [1/75], Loss: 3.3190\n",
      "Epoch [1/75], Val Loss: 3.2703\n",
      "Epoch [2/75], Loss: 3.1929\n",
      "Epoch [2/75], Val Loss: 3.1878\n",
      "Epoch [3/75], Loss: 3.0725\n",
      "Epoch [3/75], Val Loss: 3.1003\n",
      "Epoch [4/75], Loss: 2.9346\n",
      "Epoch [4/75], Val Loss: 2.9584\n",
      "Epoch [5/75], Loss: 2.7417\n",
      "Epoch [5/75], Val Loss: 2.7502\n",
      "Epoch [6/75], Loss: 2.4842\n",
      "Epoch [6/75], Val Loss: 2.4963\n",
      "Epoch [7/75], Loss: 2.1671\n",
      "Epoch [7/75], Val Loss: 2.1925\n",
      "Epoch [8/75], Loss: 1.8339\n",
      "Epoch [8/75], Val Loss: 1.9660\n",
      "Epoch [9/75], Loss: 1.5796\n",
      "Epoch [9/75], Val Loss: 1.8132\n",
      "Epoch [10/75], Loss: 1.3698\n",
      "Epoch [10/75], Val Loss: 1.7324\n",
      "Epoch [11/75], Loss: 1.2465\n",
      "Epoch [11/75], Val Loss: 1.7164\n",
      "Epoch [12/75], Loss: 1.1135\n",
      "Epoch [12/75], Val Loss: 1.5698\n",
      "Epoch [13/75], Loss: 1.0006\n",
      "Epoch [13/75], Val Loss: 1.6750\n",
      "Epoch [14/75], Loss: 0.9245\n",
      "Epoch [14/75], Val Loss: 1.6069\n",
      "Epoch [15/75], Loss: 0.9188\n",
      "Epoch [15/75], Val Loss: 1.5973\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.9188\n",
      "Test Accuracy Base CNN: 56.70%\n",
      "Epoch [1/75], Loss: 3.3084\n",
      "Epoch [1/75], Val Loss: 3.2339\n",
      "Epoch [2/75], Loss: 3.1586\n",
      "Epoch [2/75], Val Loss: 3.1308\n",
      "Epoch [3/75], Loss: 2.9975\n",
      "Epoch [3/75], Val Loss: 3.0138\n",
      "Epoch [4/75], Loss: 2.7963\n",
      "Epoch [4/75], Val Loss: 2.8109\n",
      "Epoch [5/75], Loss: 2.5158\n",
      "Epoch [5/75], Val Loss: 2.5177\n",
      "Epoch [6/75], Loss: 2.1568\n",
      "Epoch [6/75], Val Loss: 2.2081\n",
      "Epoch [7/75], Loss: 1.7468\n",
      "Epoch [7/75], Val Loss: 1.9111\n",
      "Epoch [8/75], Loss: 1.4608\n",
      "Epoch [8/75], Val Loss: 1.7848\n",
      "Epoch [9/75], Loss: 1.2915\n",
      "Epoch [9/75], Val Loss: 1.6875\n",
      "Epoch [10/75], Loss: 1.1702\n",
      "Epoch [10/75], Val Loss: 1.7342\n",
      "Epoch [11/75], Loss: 1.1313\n",
      "Epoch [11/75], Val Loss: 1.7189\n",
      "Epoch [12/75], Loss: 1.0258\n",
      "Epoch [12/75], Val Loss: 1.6670\n",
      "Epoch [13/75], Loss: 0.9266\n",
      "Epoch [13/75], Val Loss: 1.5901\n",
      "Epoch [14/75], Loss: 0.8206\n",
      "Epoch [14/75], Val Loss: 1.5520\n",
      "Epoch [15/75], Loss: 0.7230\n",
      "Epoch [15/75], Val Loss: 1.5535\n",
      "Epoch [16/75], Loss: 0.6901\n",
      "Epoch [16/75], Val Loss: 1.5716\n",
      "Epoch [17/75], Loss: 0.5439\n",
      "Epoch [17/75], Val Loss: 1.5216\n",
      "Epoch [18/75], Loss: 0.5077\n",
      "Epoch [18/75], Val Loss: 1.5077\n",
      "Epoch [19/75], Loss: 0.4176\n",
      "Epoch [19/75], Val Loss: 1.4820\n",
      "Epoch [20/75], Loss: 0.3698\n",
      "Epoch [20/75], Val Loss: 1.4881\n",
      "Epoch [21/75], Loss: 0.3132\n",
      "Epoch [21/75], Val Loss: 1.5606\n",
      "Epoch [22/75], Loss: 0.3319\n",
      "Epoch [22/75], Val Loss: 1.5122\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [22/75], Loss: 0.3319\n",
      "Test Accuracy Base CNN: 62.00%\n",
      "Epoch [1/75], Loss: 3.3203\n",
      "Epoch [1/75], Val Loss: 3.2663\n",
      "Epoch [2/75], Loss: 3.2020\n",
      "Epoch [2/75], Val Loss: 3.1760\n",
      "Epoch [3/75], Loss: 3.0824\n",
      "Epoch [3/75], Val Loss: 3.0818\n",
      "Epoch [4/75], Loss: 2.9334\n",
      "Epoch [4/75], Val Loss: 2.9411\n",
      "Epoch [5/75], Loss: 2.7207\n",
      "Epoch [5/75], Val Loss: 2.7287\n",
      "Epoch [6/75], Loss: 2.4181\n",
      "Epoch [6/75], Val Loss: 2.4254\n",
      "Epoch [7/75], Loss: 2.0440\n",
      "Epoch [7/75], Val Loss: 2.0942\n",
      "Epoch [8/75], Loss: 1.7411\n",
      "Epoch [8/75], Val Loss: 1.8376\n",
      "Epoch [9/75], Loss: 1.4550\n",
      "Epoch [9/75], Val Loss: 1.7541\n",
      "Epoch [10/75], Loss: 1.2949\n",
      "Epoch [10/75], Val Loss: 1.7123\n",
      "Epoch [11/75], Loss: 1.1632\n",
      "Epoch [11/75], Val Loss: 1.6893\n",
      "Epoch [12/75], Loss: 1.0636\n",
      "Epoch [12/75], Val Loss: 1.5959\n",
      "Epoch [13/75], Loss: 0.9284\n",
      "Epoch [13/75], Val Loss: 1.6399\n",
      "Epoch [14/75], Loss: 0.8720\n",
      "Epoch [14/75], Val Loss: 1.6022\n",
      "Epoch [15/75], Loss: 0.7859\n",
      "Epoch [15/75], Val Loss: 1.6953\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.7859\n",
      "Test Accuracy Base CNN: 54.83%\n",
      "Epoch [1/75], Loss: 3.3913\n",
      "Epoch [1/75], Val Loss: 3.3143\n",
      "Epoch [2/75], Loss: 3.2369\n",
      "Epoch [2/75], Val Loss: 3.2043\n",
      "Epoch [3/75], Loss: 3.0969\n",
      "Epoch [3/75], Val Loss: 3.1090\n",
      "Epoch [4/75], Loss: 2.9507\n",
      "Epoch [4/75], Val Loss: 2.9731\n",
      "Epoch [5/75], Loss: 2.7529\n",
      "Epoch [5/75], Val Loss: 2.7634\n",
      "Epoch [6/75], Loss: 2.4485\n",
      "Epoch [6/75], Val Loss: 2.4544\n",
      "Epoch [7/75], Loss: 2.0682\n",
      "Epoch [7/75], Val Loss: 2.1105\n",
      "Epoch [8/75], Loss: 1.6990\n",
      "Epoch [8/75], Val Loss: 1.8732\n",
      "Epoch [9/75], Loss: 1.4157\n",
      "Epoch [9/75], Val Loss: 1.7375\n",
      "Epoch [10/75], Loss: 1.2997\n",
      "Epoch [10/75], Val Loss: 1.7744\n",
      "Epoch [11/75], Loss: 1.2542\n",
      "Epoch [11/75], Val Loss: 1.7809\n",
      "Epoch [12/75], Loss: 1.2137\n",
      "Epoch [12/75], Val Loss: 1.7950\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [12/75], Loss: 1.2137\n",
      "Test Accuracy Base CNN: 52.99%\n",
      "Epoch [1/75], Loss: 3.3286\n",
      "Epoch [1/75], Val Loss: 3.2725\n",
      "Epoch [2/75], Loss: 3.1803\n",
      "Epoch [2/75], Val Loss: 3.1575\n",
      "Epoch [3/75], Loss: 3.0415\n",
      "Epoch [3/75], Val Loss: 3.0454\n",
      "Epoch [4/75], Loss: 2.8722\n",
      "Epoch [4/75], Val Loss: 2.8692\n",
      "Epoch [5/75], Loss: 2.6361\n",
      "Epoch [5/75], Val Loss: 2.6293\n",
      "Epoch [6/75], Loss: 2.2951\n",
      "Epoch [6/75], Val Loss: 2.3065\n",
      "Epoch [7/75], Loss: 1.9011\n",
      "Epoch [7/75], Val Loss: 1.9533\n",
      "Epoch [8/75], Loss: 1.5821\n",
      "Epoch [8/75], Val Loss: 1.7585\n",
      "Epoch [9/75], Loss: 1.3768\n",
      "Epoch [9/75], Val Loss: 1.7152\n",
      "Epoch [10/75], Loss: 1.2913\n",
      "Epoch [10/75], Val Loss: 1.6659\n",
      "Epoch [11/75], Loss: 1.1731\n",
      "Epoch [11/75], Val Loss: 1.6757\n",
      "Epoch [12/75], Loss: 1.0701\n",
      "Epoch [12/75], Val Loss: 1.7602\n",
      "Epoch [13/75], Loss: 1.0461\n",
      "Epoch [13/75], Val Loss: 1.5825\n",
      "Epoch [14/75], Loss: 0.7948\n",
      "Epoch [14/75], Val Loss: 1.4869\n",
      "Epoch [15/75], Loss: 0.6767\n",
      "Epoch [15/75], Val Loss: 1.5153\n",
      "Epoch [16/75], Loss: 0.6144\n",
      "Epoch [16/75], Val Loss: 1.5037\n",
      "Epoch [17/75], Loss: 0.5205\n",
      "Epoch [17/75], Val Loss: 1.5501\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [17/75], Loss: 0.5205\n",
      "Test Accuracy Base CNN: 58.20%\n",
      "Epoch [1/75], Loss: 3.3480\n",
      "Epoch [1/75], Val Loss: 3.2185\n",
      "Epoch [2/75], Loss: 3.1096\n",
      "Epoch [2/75], Val Loss: 3.1120\n",
      "Epoch [3/75], Loss: 2.9466\n",
      "Epoch [3/75], Val Loss: 2.9611\n",
      "Epoch [4/75], Loss: 2.6950\n",
      "Epoch [4/75], Val Loss: 2.6933\n",
      "Epoch [5/75], Loss: 2.3565\n",
      "Epoch [5/75], Val Loss: 2.3487\n",
      "Epoch [6/75], Loss: 1.9507\n",
      "Epoch [6/75], Val Loss: 2.0253\n",
      "Epoch [7/75], Loss: 1.6422\n",
      "Epoch [7/75], Val Loss: 1.8849\n",
      "Epoch [8/75], Loss: 1.4569\n",
      "Epoch [8/75], Val Loss: 1.8315\n",
      "Epoch [9/75], Loss: 1.2464\n",
      "Epoch [9/75], Val Loss: 1.7345\n",
      "Epoch [10/75], Loss: 1.1828\n",
      "Epoch [10/75], Val Loss: 1.6442\n",
      "Epoch [11/75], Loss: 1.0491\n",
      "Epoch [11/75], Val Loss: 1.6426\n",
      "Epoch [12/75], Loss: 0.8942\n",
      "Epoch [12/75], Val Loss: 1.6098\n",
      "Epoch [13/75], Loss: 0.8458\n",
      "Epoch [13/75], Val Loss: 1.6212\n",
      "Epoch [14/75], Loss: 0.7451\n",
      "Epoch [14/75], Val Loss: 1.5562\n",
      "Epoch [15/75], Loss: 0.7310\n",
      "Epoch [15/75], Val Loss: 1.5954\n",
      "Epoch [16/75], Loss: 0.6790\n",
      "Epoch [16/75], Val Loss: 1.5203\n",
      "Epoch [17/75], Loss: 0.5760\n",
      "Epoch [17/75], Val Loss: 1.5841\n",
      "Epoch [18/75], Loss: 0.5353\n",
      "Epoch [18/75], Val Loss: 1.5711\n",
      "Epoch [19/75], Loss: 0.4947\n",
      "Epoch [19/75], Val Loss: 1.5686\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [19/75], Loss: 0.4947\n",
      "Test Accuracy Base CNN: 59.35%\n",
      "Epoch [1/75], Loss: 3.3094\n",
      "Epoch [1/75], Val Loss: 3.2222\n",
      "Epoch [2/75], Loss: 3.1236\n",
      "Epoch [2/75], Val Loss: 3.1118\n",
      "Epoch [3/75], Loss: 2.9373\n",
      "Epoch [3/75], Val Loss: 2.9372\n",
      "Epoch [4/75], Loss: 2.6796\n",
      "Epoch [4/75], Val Loss: 2.6579\n",
      "Epoch [5/75], Loss: 2.3081\n",
      "Epoch [5/75], Val Loss: 2.3228\n",
      "Epoch [6/75], Loss: 1.9300\n",
      "Epoch [6/75], Val Loss: 1.9957\n",
      "Epoch [7/75], Loss: 1.5767\n",
      "Epoch [7/75], Val Loss: 1.8348\n",
      "Epoch [8/75], Loss: 1.4041\n",
      "Epoch [8/75], Val Loss: 1.8450\n",
      "Epoch [9/75], Loss: 1.2530\n",
      "Epoch [9/75], Val Loss: 1.8214\n",
      "Epoch [10/75], Loss: 1.1651\n",
      "Epoch [10/75], Val Loss: 1.7388\n",
      "Epoch [11/75], Loss: 1.0578\n",
      "Epoch [11/75], Val Loss: 1.7056\n",
      "Epoch [12/75], Loss: 0.9171\n",
      "Epoch [12/75], Val Loss: 1.5669\n",
      "Epoch [13/75], Loss: 0.8435\n",
      "Epoch [13/75], Val Loss: 1.5462\n",
      "Epoch [14/75], Loss: 0.7180\n",
      "Epoch [14/75], Val Loss: 1.6403\n",
      "Epoch [15/75], Loss: 0.6926\n",
      "Epoch [15/75], Val Loss: 1.5720\n",
      "Epoch [16/75], Loss: 0.6048\n",
      "Epoch [16/75], Val Loss: 1.6562\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [16/75], Loss: 0.6048\n",
      "Test Accuracy Base CNN: 56.65%\n",
      "Epoch [1/75], Loss: 3.3476\n",
      "Epoch [1/75], Val Loss: 3.2858\n",
      "Epoch [2/75], Loss: 3.2233\n",
      "Epoch [2/75], Val Loss: 3.2002\n",
      "Epoch [3/75], Loss: 3.1035\n",
      "Epoch [3/75], Val Loss: 3.1118\n",
      "Epoch [4/75], Loss: 2.9562\n",
      "Epoch [4/75], Val Loss: 2.9798\n",
      "Epoch [5/75], Loss: 2.7606\n",
      "Epoch [5/75], Val Loss: 2.7800\n",
      "Epoch [6/75], Loss: 2.4747\n",
      "Epoch [6/75], Val Loss: 2.4956\n",
      "Epoch [7/75], Loss: 2.1307\n",
      "Epoch [7/75], Val Loss: 2.1469\n",
      "Epoch [8/75], Loss: 1.7514\n",
      "Epoch [8/75], Val Loss: 1.9049\n",
      "Epoch [9/75], Loss: 1.5065\n",
      "Epoch [9/75], Val Loss: 1.7589\n",
      "Epoch [10/75], Loss: 1.3369\n",
      "Epoch [10/75], Val Loss: 1.7314\n",
      "Epoch [11/75], Loss: 1.1902\n",
      "Epoch [11/75], Val Loss: 1.6086\n",
      "Epoch [12/75], Loss: 1.0572\n",
      "Epoch [12/75], Val Loss: 1.6980\n",
      "Epoch [13/75], Loss: 1.0054\n",
      "Epoch [13/75], Val Loss: 1.6840\n",
      "Epoch [14/75], Loss: 1.0276\n",
      "Epoch [14/75], Val Loss: 1.6215\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [14/75], Loss: 1.0276\n",
      "Test Accuracy Base CNN: 55.96%\n",
      "Epoch [1/75], Loss: 3.3181\n",
      "Epoch [1/75], Val Loss: 3.2745\n",
      "Epoch [2/75], Loss: 3.1875\n",
      "Epoch [2/75], Val Loss: 3.1757\n",
      "Epoch [3/75], Loss: 3.0607\n",
      "Epoch [3/75], Val Loss: 3.0758\n",
      "Epoch [4/75], Loss: 2.8928\n",
      "Epoch [4/75], Val Loss: 2.9215\n",
      "Epoch [5/75], Loss: 2.6574\n",
      "Epoch [5/75], Val Loss: 2.6820\n",
      "Epoch [6/75], Loss: 2.3306\n",
      "Epoch [6/75], Val Loss: 2.3580\n",
      "Epoch [7/75], Loss: 1.9585\n",
      "Epoch [7/75], Val Loss: 2.0457\n",
      "Epoch [8/75], Loss: 1.6327\n",
      "Epoch [8/75], Val Loss: 1.8633\n",
      "Epoch [9/75], Loss: 1.4739\n",
      "Epoch [9/75], Val Loss: 1.8385\n",
      "Epoch [10/75], Loss: 1.2917\n",
      "Epoch [10/75], Val Loss: 1.8030\n",
      "Epoch [11/75], Loss: 1.1616\n",
      "Epoch [11/75], Val Loss: 1.6964\n",
      "Epoch [12/75], Loss: 1.1649\n",
      "Epoch [12/75], Val Loss: 1.7556\n",
      "Epoch [13/75], Loss: 1.0276\n",
      "Epoch [13/75], Val Loss: 1.6648\n",
      "Epoch [14/75], Loss: 0.9836\n",
      "Epoch [14/75], Val Loss: 1.7306\n",
      "Epoch [15/75], Loss: 0.8436\n",
      "Epoch [15/75], Val Loss: 1.6193\n",
      "Epoch [16/75], Loss: 0.7461\n",
      "Epoch [16/75], Val Loss: 1.4824\n",
      "Epoch [17/75], Loss: 0.6549\n",
      "Epoch [17/75], Val Loss: 1.5795\n",
      "Epoch [18/75], Loss: 0.5788\n",
      "Epoch [18/75], Val Loss: 1.5203\n",
      "Epoch [19/75], Loss: 0.4937\n",
      "Epoch [19/75], Val Loss: 1.5073\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [19/75], Loss: 0.4937\n",
      "Test Accuracy Base CNN: 61.23%\n",
      "Epoch [1/75], Loss: 3.3614\n",
      "Epoch [1/75], Val Loss: 3.2819\n",
      "Epoch [2/75], Loss: 3.1725\n",
      "Epoch [2/75], Val Loss: 3.1534\n",
      "Epoch [3/75], Loss: 3.0044\n",
      "Epoch [3/75], Val Loss: 3.0206\n",
      "Epoch [4/75], Loss: 2.8003\n",
      "Epoch [4/75], Val Loss: 2.8207\n",
      "Epoch [5/75], Loss: 2.5010\n",
      "Epoch [5/75], Val Loss: 2.5172\n",
      "Epoch [6/75], Loss: 2.1174\n",
      "Epoch [6/75], Val Loss: 2.1484\n",
      "Epoch [7/75], Loss: 1.7309\n",
      "Epoch [7/75], Val Loss: 1.9139\n",
      "Epoch [8/75], Loss: 1.4700\n",
      "Epoch [8/75], Val Loss: 1.8562\n",
      "Epoch [9/75], Loss: 1.3906\n",
      "Epoch [9/75], Val Loss: 1.8542\n",
      "Epoch [10/75], Loss: 1.2406\n",
      "Epoch [10/75], Val Loss: 1.7270\n",
      "Epoch [11/75], Loss: 1.1508\n",
      "Epoch [11/75], Val Loss: 1.8459\n",
      "Epoch [12/75], Loss: 1.0936\n",
      "Epoch [12/75], Val Loss: 1.6883\n",
      "Epoch [13/75], Loss: 0.9669\n",
      "Epoch [13/75], Val Loss: 1.6973\n",
      "Epoch [14/75], Loss: 0.8393\n",
      "Epoch [14/75], Val Loss: 1.5912\n",
      "Epoch [15/75], Loss: 0.8017\n",
      "Epoch [15/75], Val Loss: 1.5225\n",
      "Epoch [16/75], Loss: 0.6548\n",
      "Epoch [16/75], Val Loss: 1.6349\n",
      "Epoch [17/75], Loss: 0.6110\n",
      "Epoch [17/75], Val Loss: 1.5707\n",
      "Epoch [18/75], Loss: 0.5262\n",
      "Epoch [18/75], Val Loss: 1.5193\n",
      "Epoch [19/75], Loss: 0.4776\n",
      "Epoch [19/75], Val Loss: 1.5229\n",
      "Epoch [20/75], Loss: 0.3762\n",
      "Epoch [20/75], Val Loss: 1.5459\n",
      "Epoch [21/75], Loss: 0.3231\n",
      "Epoch [21/75], Val Loss: 1.5614\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [21/75], Loss: 0.3231\n",
      "Test Accuracy Base CNN: 60.54%\n",
      "Epoch [1/75], Loss: 3.3203\n",
      "Epoch [1/75], Val Loss: 3.2524\n",
      "Epoch [2/75], Loss: 3.1612\n",
      "Epoch [2/75], Val Loss: 3.1551\n",
      "Epoch [3/75], Loss: 3.0361\n",
      "Epoch [3/75], Val Loss: 3.0526\n",
      "Epoch [4/75], Loss: 2.8789\n",
      "Epoch [4/75], Val Loss: 2.8838\n",
      "Epoch [5/75], Loss: 2.6387\n",
      "Epoch [5/75], Val Loss: 2.6685\n",
      "Epoch [6/75], Loss: 2.3225\n",
      "Epoch [6/75], Val Loss: 2.3695\n",
      "Epoch [7/75], Loss: 1.9772\n",
      "Epoch [7/75], Val Loss: 2.0619\n",
      "Epoch [8/75], Loss: 1.6487\n",
      "Epoch [8/75], Val Loss: 1.8752\n",
      "Epoch [9/75], Loss: 1.4113\n",
      "Epoch [9/75], Val Loss: 1.7747\n",
      "Epoch [10/75], Loss: 1.2835\n",
      "Epoch [10/75], Val Loss: 1.7699\n",
      "Epoch [11/75], Loss: 1.1559\n",
      "Epoch [11/75], Val Loss: 1.7289\n",
      "Epoch [12/75], Loss: 1.0529\n",
      "Epoch [12/75], Val Loss: 1.7760\n",
      "Epoch [13/75], Loss: 1.0098\n",
      "Epoch [13/75], Val Loss: 1.6375\n",
      "Epoch [14/75], Loss: 0.8911\n",
      "Epoch [14/75], Val Loss: 1.6894\n",
      "Epoch [15/75], Loss: 0.7596\n",
      "Epoch [15/75], Val Loss: 1.5750\n",
      "Epoch [16/75], Loss: 0.6681\n",
      "Epoch [16/75], Val Loss: 1.6107\n",
      "Epoch [17/75], Loss: 0.6008\n",
      "Epoch [17/75], Val Loss: 1.5839\n",
      "Epoch [18/75], Loss: 0.5725\n",
      "Epoch [18/75], Val Loss: 1.6645\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [18/75], Loss: 0.5725\n",
      "Test Accuracy Base CNN: 57.56%\n",
      "Epoch [1/75], Loss: 3.4375\n",
      "Epoch [1/75], Val Loss: 3.3194\n",
      "Epoch [2/75], Loss: 3.2696\n",
      "Epoch [2/75], Val Loss: 3.2346\n",
      "Epoch [3/75], Loss: 3.1720\n",
      "Epoch [3/75], Val Loss: 3.1874\n",
      "Epoch [4/75], Loss: 3.0828\n",
      "Epoch [4/75], Val Loss: 3.1153\n",
      "Epoch [5/75], Loss: 2.9616\n",
      "Epoch [5/75], Val Loss: 2.9947\n",
      "Epoch [6/75], Loss: 2.7891\n",
      "Epoch [6/75], Val Loss: 2.8007\n",
      "Epoch [7/75], Loss: 2.5312\n",
      "Epoch [7/75], Val Loss: 2.5524\n",
      "Epoch [8/75], Loss: 2.2076\n",
      "Epoch [8/75], Val Loss: 2.2401\n",
      "Epoch [9/75], Loss: 1.8624\n",
      "Epoch [9/75], Val Loss: 1.9922\n",
      "Epoch [10/75], Loss: 1.5720\n",
      "Epoch [10/75], Val Loss: 1.8312\n",
      "Epoch [11/75], Loss: 1.3599\n",
      "Epoch [11/75], Val Loss: 1.7369\n",
      "Epoch [12/75], Loss: 1.2117\n",
      "Epoch [12/75], Val Loss: 1.6839\n",
      "Epoch [13/75], Loss: 1.1035\n",
      "Epoch [13/75], Val Loss: 1.6610\n",
      "Epoch [14/75], Loss: 0.9491\n",
      "Epoch [14/75], Val Loss: 1.5760\n",
      "Epoch [15/75], Loss: 0.8491\n",
      "Epoch [15/75], Val Loss: 1.6460\n",
      "Epoch [16/75], Loss: 0.7859\n",
      "Epoch [16/75], Val Loss: 1.6213\n",
      "Epoch [17/75], Loss: 0.7390\n",
      "Epoch [17/75], Val Loss: 1.6136\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [17/75], Loss: 0.7390\n",
      "Test Accuracy Base CNN: 55.97%\n",
      "Epoch [1/75], Loss: 3.3594\n",
      "Epoch [1/75], Val Loss: 3.2762\n",
      "Epoch [2/75], Loss: 3.2140\n",
      "Epoch [2/75], Val Loss: 3.1647\n",
      "Epoch [3/75], Loss: 3.0631\n",
      "Epoch [3/75], Val Loss: 3.0505\n",
      "Epoch [4/75], Loss: 2.8767\n",
      "Epoch [4/75], Val Loss: 2.8856\n",
      "Epoch [5/75], Loss: 2.6184\n",
      "Epoch [5/75], Val Loss: 2.6443\n",
      "Epoch [6/75], Loss: 2.2866\n",
      "Epoch [6/75], Val Loss: 2.3198\n",
      "Epoch [7/75], Loss: 1.9036\n",
      "Epoch [7/75], Val Loss: 1.9966\n",
      "Epoch [8/75], Loss: 1.5747\n",
      "Epoch [8/75], Val Loss: 1.8159\n",
      "Epoch [9/75], Loss: 1.3824\n",
      "Epoch [9/75], Val Loss: 1.7429\n",
      "Epoch [10/75], Loss: 1.2524\n",
      "Epoch [10/75], Val Loss: 1.7363\n",
      "Epoch [11/75], Loss: 1.1740\n",
      "Epoch [11/75], Val Loss: 1.6781\n",
      "Epoch [12/75], Loss: 1.0924\n",
      "Epoch [12/75], Val Loss: 1.6817\n",
      "Epoch [13/75], Loss: 1.0177\n",
      "Epoch [13/75], Val Loss: 1.6774\n",
      "Epoch [14/75], Loss: 0.8807\n",
      "Epoch [14/75], Val Loss: 1.6383\n",
      "Epoch [15/75], Loss: 0.7872\n",
      "Epoch [15/75], Val Loss: 1.5311\n",
      "Epoch [16/75], Loss: 0.6319\n",
      "Epoch [16/75], Val Loss: 1.5167\n",
      "Epoch [17/75], Loss: 0.6083\n",
      "Epoch [17/75], Val Loss: 1.4756\n",
      "Epoch [18/75], Loss: 0.5632\n",
      "Epoch [18/75], Val Loss: 1.5238\n",
      "Epoch [19/75], Loss: 0.5339\n",
      "Epoch [19/75], Val Loss: 1.4510\n",
      "Epoch [20/75], Loss: 0.4296\n",
      "Epoch [20/75], Val Loss: 1.4922\n",
      "Epoch [21/75], Loss: 0.3535\n",
      "Epoch [21/75], Val Loss: 1.4448\n",
      "Epoch [22/75], Loss: 0.3243\n",
      "Epoch [22/75], Val Loss: 1.5013\n",
      "Epoch [23/75], Loss: 0.2699\n",
      "Epoch [23/75], Val Loss: 1.5613\n",
      "Epoch [24/75], Loss: 0.2485\n",
      "Epoch [24/75], Val Loss: 1.5311\n",
      "Stopping early at epoch 24 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [24/75], Loss: 0.2485\n",
      "Test Accuracy Base CNN: 62.81%\n",
      "Epoch [1/75], Loss: 3.3547\n",
      "Epoch [1/75], Val Loss: 3.2710\n",
      "Epoch [2/75], Loss: 3.1862\n",
      "Epoch [2/75], Val Loss: 3.1814\n",
      "Epoch [3/75], Loss: 3.0550\n",
      "Epoch [3/75], Val Loss: 3.0998\n",
      "Epoch [4/75], Loss: 2.9048\n",
      "Epoch [4/75], Val Loss: 2.9572\n",
      "Epoch [5/75], Loss: 2.7001\n",
      "Epoch [5/75], Val Loss: 2.7530\n",
      "Epoch [6/75], Loss: 2.4212\n",
      "Epoch [6/75], Val Loss: 2.4745\n",
      "Epoch [7/75], Loss: 2.0837\n",
      "Epoch [7/75], Val Loss: 2.1770\n",
      "Epoch [8/75], Loss: 1.7475\n",
      "Epoch [8/75], Val Loss: 1.9458\n",
      "Epoch [9/75], Loss: 1.5089\n",
      "Epoch [9/75], Val Loss: 1.8399\n",
      "Epoch [10/75], Loss: 1.3167\n",
      "Epoch [10/75], Val Loss: 1.7354\n",
      "Epoch [11/75], Loss: 1.1936\n",
      "Epoch [11/75], Val Loss: 1.7172\n",
      "Epoch [12/75], Loss: 1.1719\n",
      "Epoch [12/75], Val Loss: 1.8060\n",
      "Epoch [13/75], Loss: 1.1417\n",
      "Epoch [13/75], Val Loss: 1.7697\n",
      "Epoch [14/75], Loss: 1.0306\n",
      "Epoch [14/75], Val Loss: 1.8199\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [14/75], Loss: 1.0306\n",
      "Test Accuracy Base CNN: 50.81%\n",
      "Epoch [1/75], Loss: 3.3329\n",
      "Epoch [1/75], Val Loss: 3.2567\n",
      "Epoch [2/75], Loss: 3.1906\n",
      "Epoch [2/75], Val Loss: 3.1813\n",
      "Epoch [3/75], Loss: 3.0901\n",
      "Epoch [3/75], Val Loss: 3.1045\n",
      "Epoch [4/75], Loss: 2.9554\n",
      "Epoch [4/75], Val Loss: 2.9754\n",
      "Epoch [5/75], Loss: 2.7538\n",
      "Epoch [5/75], Val Loss: 2.7639\n",
      "Epoch [6/75], Loss: 2.4704\n",
      "Epoch [6/75], Val Loss: 2.4750\n",
      "Epoch [7/75], Loss: 2.1252\n",
      "Epoch [7/75], Val Loss: 2.1308\n",
      "Epoch [8/75], Loss: 1.7640\n",
      "Epoch [8/75], Val Loss: 1.8868\n",
      "Epoch [9/75], Loss: 1.4993\n",
      "Epoch [9/75], Val Loss: 1.7983\n",
      "Epoch [10/75], Loss: 1.3690\n",
      "Epoch [10/75], Val Loss: 1.7288\n",
      "Epoch [11/75], Loss: 1.2378\n",
      "Epoch [11/75], Val Loss: 2.0448\n",
      "Epoch [12/75], Loss: 1.2310\n",
      "Epoch [12/75], Val Loss: 1.6557\n",
      "Epoch [13/75], Loss: 1.0539\n",
      "Epoch [13/75], Val Loss: 1.6145\n",
      "Epoch [14/75], Loss: 0.9768\n",
      "Epoch [14/75], Val Loss: 1.7588\n",
      "Epoch [15/75], Loss: 0.8230\n",
      "Epoch [15/75], Val Loss: 1.7058\n",
      "Epoch [16/75], Loss: 0.7326\n",
      "Epoch [16/75], Val Loss: 1.5610\n",
      "Epoch [17/75], Loss: 0.6504\n",
      "Epoch [17/75], Val Loss: 1.5177\n",
      "Epoch [18/75], Loss: 0.5744\n",
      "Epoch [18/75], Val Loss: 1.4900\n",
      "Epoch [19/75], Loss: 0.5072\n",
      "Epoch [19/75], Val Loss: 1.5260\n",
      "Epoch [20/75], Loss: 0.4541\n",
      "Epoch [20/75], Val Loss: 1.4812\n",
      "Epoch [21/75], Loss: 0.3769\n",
      "Epoch [21/75], Val Loss: 1.6010\n",
      "Epoch [22/75], Loss: 0.3068\n",
      "Epoch [22/75], Val Loss: 1.4391\n",
      "Epoch [23/75], Loss: 0.2511\n",
      "Epoch [23/75], Val Loss: 1.5662\n",
      "Epoch [24/75], Loss: 0.2170\n",
      "Epoch [24/75], Val Loss: 1.5942\n",
      "Epoch [25/75], Loss: 0.1935\n",
      "Epoch [25/75], Val Loss: 1.6226\n",
      "Stopping early at epoch 25 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [25/75], Loss: 0.1935\n",
      "Test Accuracy Base CNN: 62.09%\n",
      "Epoch [1/75], Loss: 3.3571\n",
      "Epoch [1/75], Val Loss: 3.2953\n",
      "Epoch [2/75], Loss: 3.2477\n",
      "Epoch [2/75], Val Loss: 3.2155\n",
      "Epoch [3/75], Loss: 3.1377\n",
      "Epoch [3/75], Val Loss: 3.1516\n",
      "Epoch [4/75], Loss: 3.0366\n",
      "Epoch [4/75], Val Loss: 3.0678\n",
      "Epoch [5/75], Loss: 2.8926\n",
      "Epoch [5/75], Val Loss: 2.9284\n",
      "Epoch [6/75], Loss: 2.6963\n",
      "Epoch [6/75], Val Loss: 2.7300\n",
      "Epoch [7/75], Loss: 2.4300\n",
      "Epoch [7/75], Val Loss: 2.4670\n",
      "Epoch [8/75], Loss: 2.0832\n",
      "Epoch [8/75], Val Loss: 2.1628\n",
      "Epoch [9/75], Loss: 1.7591\n",
      "Epoch [9/75], Val Loss: 1.9363\n",
      "Epoch [10/75], Loss: 1.5178\n",
      "Epoch [10/75], Val Loss: 1.8445\n",
      "Epoch [11/75], Loss: 1.4116\n",
      "Epoch [11/75], Val Loss: 1.7139\n",
      "Epoch [12/75], Loss: 1.2922\n",
      "Epoch [12/75], Val Loss: 1.7289\n",
      "Epoch [13/75], Loss: 1.1017\n",
      "Epoch [13/75], Val Loss: 1.7656\n",
      "Epoch [14/75], Loss: 0.9881\n",
      "Epoch [14/75], Val Loss: 1.6006\n",
      "Epoch [15/75], Loss: 0.8976\n",
      "Epoch [15/75], Val Loss: 1.5407\n",
      "Epoch [16/75], Loss: 0.8124\n",
      "Epoch [16/75], Val Loss: 1.5344\n",
      "Epoch [17/75], Loss: 0.7135\n",
      "Epoch [17/75], Val Loss: 1.5415\n",
      "Epoch [18/75], Loss: 0.6451\n",
      "Epoch [18/75], Val Loss: 1.4553\n",
      "Epoch [19/75], Loss: 0.5226\n",
      "Epoch [19/75], Val Loss: 1.4149\n",
      "Epoch [20/75], Loss: 0.4748\n",
      "Epoch [20/75], Val Loss: 1.4555\n",
      "Epoch [21/75], Loss: 0.4564\n",
      "Epoch [21/75], Val Loss: 1.4430\n",
      "Epoch [22/75], Loss: 0.4020\n",
      "Epoch [22/75], Val Loss: 1.5926\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [22/75], Loss: 0.4020\n",
      "Test Accuracy Base CNN: 59.21%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/75], Loss: 3.3715\n",
      "Epoch [1/75], Val Loss: 3.2736\n",
      "Epoch [2/75], Loss: 3.1614\n",
      "Epoch [2/75], Val Loss: 3.1431\n",
      "Epoch [3/75], Loss: 2.9911\n",
      "Epoch [3/75], Val Loss: 3.0062\n",
      "Epoch [4/75], Loss: 2.7640\n",
      "Epoch [4/75], Val Loss: 2.7794\n",
      "Epoch [5/75], Loss: 2.4486\n",
      "Epoch [5/75], Val Loss: 2.4445\n",
      "Epoch [6/75], Loss: 2.0577\n",
      "Epoch [6/75], Val Loss: 2.0861\n",
      "Epoch [7/75], Loss: 1.6911\n",
      "Epoch [7/75], Val Loss: 1.8394\n",
      "Epoch [8/75], Loss: 1.4148\n",
      "Epoch [8/75], Val Loss: 1.7233\n",
      "Epoch [9/75], Loss: 1.2392\n",
      "Epoch [9/75], Val Loss: 1.6402\n",
      "Epoch [10/75], Loss: 1.0929\n",
      "Epoch [10/75], Val Loss: 1.7572\n",
      "Epoch [11/75], Loss: 1.0436\n",
      "Epoch [11/75], Val Loss: 1.5832\n",
      "Epoch [12/75], Loss: 0.9163\n",
      "Epoch [12/75], Val Loss: 1.6437\n",
      "Epoch [13/75], Loss: 0.8363\n",
      "Epoch [13/75], Val Loss: 1.5322\n",
      "Epoch [14/75], Loss: 0.7720\n",
      "Epoch [14/75], Val Loss: 1.6239\n",
      "Epoch [15/75], Loss: 0.7242\n",
      "Epoch [15/75], Val Loss: 1.5340\n",
      "Epoch [16/75], Loss: 0.6844\n",
      "Epoch [16/75], Val Loss: 1.5616\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.6844\n",
      "Test Accuracy Base CNN: 57.60%\n",
      "Epoch [1/75], Loss: 3.3900\n",
      "Epoch [1/75], Val Loss: 3.2599\n",
      "Epoch [2/75], Loss: 3.1819\n",
      "Epoch [2/75], Val Loss: 3.1667\n",
      "Epoch [3/75], Loss: 3.0319\n",
      "Epoch [3/75], Val Loss: 3.0675\n",
      "Epoch [4/75], Loss: 2.8585\n",
      "Epoch [4/75], Val Loss: 2.9042\n",
      "Epoch [5/75], Loss: 2.5993\n",
      "Epoch [5/75], Val Loss: 2.6465\n",
      "Epoch [6/75], Loss: 2.2667\n",
      "Epoch [6/75], Val Loss: 2.3087\n",
      "Epoch [7/75], Loss: 1.9094\n",
      "Epoch [7/75], Val Loss: 2.0479\n",
      "Epoch [8/75], Loss: 1.6133\n",
      "Epoch [8/75], Val Loss: 1.8818\n",
      "Epoch [9/75], Loss: 1.3979\n",
      "Epoch [9/75], Val Loss: 1.8000\n",
      "Epoch [10/75], Loss: 1.2500\n",
      "Epoch [10/75], Val Loss: 1.7502\n",
      "Epoch [11/75], Loss: 1.1343\n",
      "Epoch [11/75], Val Loss: 1.8303\n",
      "Epoch [12/75], Loss: 1.0826\n",
      "Epoch [12/75], Val Loss: 1.7126\n",
      "Epoch [13/75], Loss: 0.9622\n",
      "Epoch [13/75], Val Loss: 1.6500\n",
      "Epoch [14/75], Loss: 0.8683\n",
      "Epoch [14/75], Val Loss: 1.5954\n",
      "Epoch [15/75], Loss: 0.8326\n",
      "Epoch [15/75], Val Loss: 1.6671\n",
      "Epoch [16/75], Loss: 0.7302\n",
      "Epoch [16/75], Val Loss: 1.6266\n",
      "Epoch [17/75], Loss: 0.6437\n",
      "Epoch [17/75], Val Loss: 1.5945\n",
      "Epoch [18/75], Loss: 0.6209\n",
      "Epoch [18/75], Val Loss: 1.6027\n",
      "Epoch [19/75], Loss: 0.5461\n",
      "Epoch [19/75], Val Loss: 1.6578\n",
      "Epoch [20/75], Loss: 0.5182\n",
      "Epoch [20/75], Val Loss: 1.6359\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [20/75], Loss: 0.5182\n",
      "Test Accuracy Base CNN: 58.62%\n",
      "Epoch [1/75], Loss: 3.3319\n",
      "Epoch [1/75], Val Loss: 3.2825\n",
      "Epoch [2/75], Loss: 3.2053\n",
      "Epoch [2/75], Val Loss: 3.1883\n",
      "Epoch [3/75], Loss: 3.0721\n",
      "Epoch [3/75], Val Loss: 3.0892\n",
      "Epoch [4/75], Loss: 2.9200\n",
      "Epoch [4/75], Val Loss: 2.9326\n",
      "Epoch [5/75], Loss: 2.6885\n",
      "Epoch [5/75], Val Loss: 2.6905\n",
      "Epoch [6/75], Loss: 2.3765\n",
      "Epoch [6/75], Val Loss: 2.3716\n",
      "Epoch [7/75], Loss: 1.9854\n",
      "Epoch [7/75], Val Loss: 2.0541\n",
      "Epoch [8/75], Loss: 1.6367\n",
      "Epoch [8/75], Val Loss: 1.8350\n",
      "Epoch [9/75], Loss: 1.3878\n",
      "Epoch [9/75], Val Loss: 1.7932\n",
      "Epoch [10/75], Loss: 1.2744\n",
      "Epoch [10/75], Val Loss: 1.6895\n",
      "Epoch [11/75], Loss: 1.1411\n",
      "Epoch [11/75], Val Loss: 1.6194\n",
      "Epoch [12/75], Loss: 0.9884\n",
      "Epoch [12/75], Val Loss: 1.6213\n",
      "Epoch [13/75], Loss: 0.9209\n",
      "Epoch [13/75], Val Loss: 1.6424\n",
      "Epoch [14/75], Loss: 0.8363\n",
      "Epoch [14/75], Val Loss: 1.5436\n",
      "Epoch [15/75], Loss: 0.7725\n",
      "Epoch [15/75], Val Loss: 1.6367\n",
      "Epoch [16/75], Loss: 0.7112\n",
      "Epoch [16/75], Val Loss: 1.5786\n",
      "Epoch [17/75], Loss: 0.6351\n",
      "Epoch [17/75], Val Loss: 1.5642\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [17/75], Loss: 0.6351\n",
      "Test Accuracy Base CNN: 57.60%\n",
      "Epoch [1/75], Loss: 3.3649\n",
      "Epoch [1/75], Val Loss: 3.3011\n",
      "Epoch [2/75], Loss: 3.2219\n",
      "Epoch [2/75], Val Loss: 3.2235\n",
      "Epoch [3/75], Loss: 3.1000\n",
      "Epoch [3/75], Val Loss: 3.1168\n",
      "Epoch [4/75], Loss: 2.9500\n",
      "Epoch [4/75], Val Loss: 2.9654\n",
      "Epoch [5/75], Loss: 2.7418\n",
      "Epoch [5/75], Val Loss: 2.7579\n",
      "Epoch [6/75], Loss: 2.4601\n",
      "Epoch [6/75], Val Loss: 2.4774\n",
      "Epoch [7/75], Loss: 2.1053\n",
      "Epoch [7/75], Val Loss: 2.1589\n",
      "Epoch [8/75], Loss: 1.7309\n",
      "Epoch [8/75], Val Loss: 1.8984\n",
      "Epoch [9/75], Loss: 1.4727\n",
      "Epoch [9/75], Val Loss: 1.8159\n",
      "Epoch [10/75], Loss: 1.3063\n",
      "Epoch [10/75], Val Loss: 1.8039\n",
      "Epoch [11/75], Loss: 1.2194\n",
      "Epoch [11/75], Val Loss: 1.7385\n",
      "Epoch [12/75], Loss: 1.0572\n",
      "Epoch [12/75], Val Loss: 1.6628\n",
      "Epoch [13/75], Loss: 0.9736\n",
      "Epoch [13/75], Val Loss: 1.7008\n",
      "Epoch [14/75], Loss: 0.9092\n",
      "Epoch [14/75], Val Loss: 1.6821\n",
      "Epoch [15/75], Loss: 0.8451\n",
      "Epoch [15/75], Val Loss: 1.6494\n",
      "Epoch [16/75], Loss: 0.7229\n",
      "Epoch [16/75], Val Loss: 1.7012\n",
      "Epoch [17/75], Loss: 0.7783\n",
      "Epoch [17/75], Val Loss: 1.7068\n",
      "Epoch [18/75], Loss: 0.6839\n",
      "Epoch [18/75], Val Loss: 1.6420\n",
      "Epoch [19/75], Loss: 0.6382\n",
      "Epoch [19/75], Val Loss: 1.5719\n",
      "Epoch [20/75], Loss: 0.6004\n",
      "Epoch [20/75], Val Loss: 1.5702\n",
      "Epoch [21/75], Loss: 0.5670\n",
      "Epoch [21/75], Val Loss: 1.5868\n",
      "Epoch [22/75], Loss: 0.4862\n",
      "Epoch [22/75], Val Loss: 1.5847\n",
      "Epoch [23/75], Loss: 0.4810\n",
      "Epoch [23/75], Val Loss: 1.5078\n",
      "Epoch [24/75], Loss: 0.3607\n",
      "Epoch [24/75], Val Loss: 1.5100\n",
      "Epoch [25/75], Loss: 0.3074\n",
      "Epoch [25/75], Val Loss: 1.5058\n",
      "Epoch [26/75], Loss: 0.2735\n",
      "Epoch [26/75], Val Loss: 1.4980\n",
      "Epoch [27/75], Loss: 0.2185\n",
      "Epoch [27/75], Val Loss: 1.5270\n",
      "Epoch [28/75], Loss: 0.1778\n",
      "Epoch [28/75], Val Loss: 1.5772\n",
      "Epoch [29/75], Loss: 0.1579\n",
      "Epoch [29/75], Val Loss: 1.6046\n",
      "Stopping early at epoch 29 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [29/75], Loss: 0.1579\n",
      "Test Accuracy Base CNN: 63.05%\n",
      "Epoch [1/75], Loss: 3.3731\n",
      "Epoch [1/75], Val Loss: 3.2782\n",
      "Epoch [2/75], Loss: 3.1861\n",
      "Epoch [2/75], Val Loss: 3.1701\n",
      "Epoch [3/75], Loss: 3.0374\n",
      "Epoch [3/75], Val Loss: 3.0570\n",
      "Epoch [4/75], Loss: 2.8604\n",
      "Epoch [4/75], Val Loss: 2.8797\n",
      "Epoch [5/75], Loss: 2.5908\n",
      "Epoch [5/75], Val Loss: 2.5843\n",
      "Epoch [6/75], Loss: 2.2459\n",
      "Epoch [6/75], Val Loss: 2.2335\n",
      "Epoch [7/75], Loss: 1.8559\n",
      "Epoch [7/75], Val Loss: 1.9405\n",
      "Epoch [8/75], Loss: 1.5294\n",
      "Epoch [8/75], Val Loss: 1.7694\n",
      "Epoch [9/75], Loss: 1.3331\n",
      "Epoch [9/75], Val Loss: 1.7435\n",
      "Epoch [10/75], Loss: 1.2188\n",
      "Epoch [10/75], Val Loss: 1.7471\n",
      "Epoch [11/75], Loss: 1.1087\n",
      "Epoch [11/75], Val Loss: 1.7425\n",
      "Epoch [12/75], Loss: 0.9587\n",
      "Epoch [12/75], Val Loss: 1.7557\n",
      "Epoch [13/75], Loss: 0.9457\n",
      "Epoch [13/75], Val Loss: 1.6374\n",
      "Epoch [14/75], Loss: 0.8037\n",
      "Epoch [14/75], Val Loss: 1.5693\n",
      "Epoch [15/75], Loss: 0.6667\n",
      "Epoch [15/75], Val Loss: 1.5523\n",
      "Epoch [16/75], Loss: 0.6258\n",
      "Epoch [16/75], Val Loss: 1.5120\n",
      "Epoch [17/75], Loss: 0.5142\n",
      "Epoch [17/75], Val Loss: 1.5049\n",
      "Epoch [18/75], Loss: 0.4635\n",
      "Epoch [18/75], Val Loss: 1.5167\n",
      "Epoch [19/75], Loss: 0.3907\n",
      "Epoch [19/75], Val Loss: 1.5092\n",
      "Epoch [20/75], Loss: 0.3581\n",
      "Epoch [20/75], Val Loss: 1.5615\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [20/75], Loss: 0.3581\n",
      "Test Accuracy Base CNN: 59.75%\n",
      "Epoch [1/75], Loss: 3.3051\n",
      "Epoch [1/75], Val Loss: 3.2746\n",
      "Epoch [2/75], Loss: 3.2142\n",
      "Epoch [2/75], Val Loss: 3.1954\n",
      "Epoch [3/75], Loss: 3.1087\n",
      "Epoch [3/75], Val Loss: 3.1142\n",
      "Epoch [4/75], Loss: 2.9898\n",
      "Epoch [4/75], Val Loss: 3.0102\n",
      "Epoch [5/75], Loss: 2.8323\n",
      "Epoch [5/75], Val Loss: 2.8645\n",
      "Epoch [6/75], Loss: 2.6140\n",
      "Epoch [6/75], Val Loss: 2.6592\n",
      "Epoch [7/75], Loss: 2.3274\n",
      "Epoch [7/75], Val Loss: 2.3929\n",
      "Epoch [8/75], Loss: 1.9985\n",
      "Epoch [8/75], Val Loss: 2.1085\n",
      "Epoch [9/75], Loss: 1.7068\n",
      "Epoch [9/75], Val Loss: 1.8883\n",
      "Epoch [10/75], Loss: 1.4651\n",
      "Epoch [10/75], Val Loss: 1.7773\n",
      "Epoch [11/75], Loss: 1.3054\n",
      "Epoch [11/75], Val Loss: 1.7629\n",
      "Epoch [12/75], Loss: 1.1932\n",
      "Epoch [12/75], Val Loss: 1.6130\n",
      "Epoch [13/75], Loss: 1.0232\n",
      "Epoch [13/75], Val Loss: 1.5605\n",
      "Epoch [14/75], Loss: 0.9151\n",
      "Epoch [14/75], Val Loss: 1.6059\n",
      "Epoch [15/75], Loss: 0.8178\n",
      "Epoch [15/75], Val Loss: 1.6277\n",
      "Epoch [16/75], Loss: 0.7788\n",
      "Epoch [16/75], Val Loss: 1.6052\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.7788\n",
      "Test Accuracy Base CNN: 56.31%\n",
      "Epoch [1/75], Loss: 3.3688\n",
      "Epoch [1/75], Val Loss: 3.2773\n",
      "Epoch [2/75], Loss: 3.2480\n",
      "Epoch [2/75], Val Loss: 3.2112\n",
      "Epoch [3/75], Loss: 3.1486\n",
      "Epoch [3/75], Val Loss: 3.1560\n",
      "Epoch [4/75], Loss: 3.0562\n",
      "Epoch [4/75], Val Loss: 3.0709\n",
      "Epoch [5/75], Loss: 2.9237\n",
      "Epoch [5/75], Val Loss: 2.9389\n",
      "Epoch [6/75], Loss: 2.7361\n",
      "Epoch [6/75], Val Loss: 2.7351\n",
      "Epoch [7/75], Loss: 2.4609\n",
      "Epoch [7/75], Val Loss: 2.4569\n",
      "Epoch [8/75], Loss: 2.1151\n",
      "Epoch [8/75], Val Loss: 2.1238\n",
      "Epoch [9/75], Loss: 1.7466\n",
      "Epoch [9/75], Val Loss: 1.8773\n",
      "Epoch [10/75], Loss: 1.4639\n",
      "Epoch [10/75], Val Loss: 1.7332\n",
      "Epoch [11/75], Loss: 1.2784\n",
      "Epoch [11/75], Val Loss: 1.6836\n",
      "Epoch [12/75], Loss: 1.1469\n",
      "Epoch [12/75], Val Loss: 1.6487\n",
      "Epoch [13/75], Loss: 1.0319\n",
      "Epoch [13/75], Val Loss: 1.6209\n",
      "Epoch [14/75], Loss: 1.0241\n",
      "Epoch [14/75], Val Loss: 1.6181\n",
      "Epoch [15/75], Loss: 0.9078\n",
      "Epoch [15/75], Val Loss: 1.6598\n",
      "Epoch [16/75], Loss: 0.7819\n",
      "Epoch [16/75], Val Loss: 1.6134\n",
      "Epoch [17/75], Loss: 0.7186\n",
      "Epoch [17/75], Val Loss: 1.6802\n",
      "Epoch [18/75], Loss: 0.6871\n",
      "Epoch [18/75], Val Loss: 1.4858\n",
      "Epoch [19/75], Loss: 0.5679\n",
      "Epoch [19/75], Val Loss: 1.5521\n",
      "Epoch [20/75], Loss: 0.4963\n",
      "Epoch [20/75], Val Loss: 1.5379\n",
      "Epoch [21/75], Loss: 0.4428\n",
      "Epoch [21/75], Val Loss: 1.4996\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [21/75], Loss: 0.4428\n",
      "Test Accuracy Base CNN: 61.24%\n",
      "Epoch [1/75], Loss: 3.3295\n",
      "Epoch [1/75], Val Loss: 3.2495\n",
      "Epoch [2/75], Loss: 3.1734\n",
      "Epoch [2/75], Val Loss: 3.1550\n",
      "Epoch [3/75], Loss: 3.0307\n",
      "Epoch [3/75], Val Loss: 3.0437\n",
      "Epoch [4/75], Loss: 2.8481\n",
      "Epoch [4/75], Val Loss: 2.8503\n",
      "Epoch [5/75], Loss: 2.5689\n",
      "Epoch [5/75], Val Loss: 2.5649\n",
      "Epoch [6/75], Loss: 2.2040\n",
      "Epoch [6/75], Val Loss: 2.2409\n",
      "Epoch [7/75], Loss: 1.8168\n",
      "Epoch [7/75], Val Loss: 1.9509\n",
      "Epoch [8/75], Loss: 1.5250\n",
      "Epoch [8/75], Val Loss: 1.7902\n",
      "Epoch [9/75], Loss: 1.2980\n",
      "Epoch [9/75], Val Loss: 1.6702\n",
      "Epoch [10/75], Loss: 1.1646\n",
      "Epoch [10/75], Val Loss: 1.7435\n",
      "Epoch [11/75], Loss: 1.1071\n",
      "Epoch [11/75], Val Loss: 1.6871\n",
      "Epoch [12/75], Loss: 1.0250\n",
      "Epoch [12/75], Val Loss: 1.6090\n",
      "Epoch [13/75], Loss: 0.8436\n",
      "Epoch [13/75], Val Loss: 1.6375\n",
      "Epoch [14/75], Loss: 0.7424\n",
      "Epoch [14/75], Val Loss: 1.5835\n",
      "Epoch [15/75], Loss: 0.6471\n",
      "Epoch [15/75], Val Loss: 1.6229\n",
      "Epoch [16/75], Loss: 0.6072\n",
      "Epoch [16/75], Val Loss: 1.5750\n",
      "Epoch [17/75], Loss: 0.4965\n",
      "Epoch [17/75], Val Loss: 1.5510\n",
      "Epoch [18/75], Loss: 0.4626\n",
      "Epoch [18/75], Val Loss: 1.6283\n",
      "Epoch [19/75], Loss: 0.4352\n",
      "Epoch [19/75], Val Loss: 1.5631\n",
      "Epoch [20/75], Loss: 0.3207\n",
      "Epoch [20/75], Val Loss: 1.6269\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [20/75], Loss: 0.3207\n",
      "Test Accuracy Base CNN: 58.77%\n",
      "Epoch [1/75], Loss: 3.3314\n",
      "Epoch [1/75], Val Loss: 3.2457\n",
      "Epoch [2/75], Loss: 3.1604\n",
      "Epoch [2/75], Val Loss: 3.1105\n",
      "Epoch [3/75], Loss: 2.9959\n",
      "Epoch [3/75], Val Loss: 2.9779\n",
      "Epoch [4/75], Loss: 2.7947\n",
      "Epoch [4/75], Val Loss: 2.7855\n",
      "Epoch [5/75], Loss: 2.5010\n",
      "Epoch [5/75], Val Loss: 2.4892\n",
      "Epoch [6/75], Loss: 2.1477\n",
      "Epoch [6/75], Val Loss: 2.1168\n",
      "Epoch [7/75], Loss: 1.7458\n",
      "Epoch [7/75], Val Loss: 1.8797\n",
      "Epoch [8/75], Loss: 1.5286\n",
      "Epoch [8/75], Val Loss: 1.7616\n",
      "Epoch [9/75], Loss: 1.3366\n",
      "Epoch [9/75], Val Loss: 1.7527\n",
      "Epoch [10/75], Loss: 1.2275\n",
      "Epoch [10/75], Val Loss: 1.6809\n",
      "Epoch [11/75], Loss: 1.0416\n",
      "Epoch [11/75], Val Loss: 1.5968\n",
      "Epoch [12/75], Loss: 0.9659\n",
      "Epoch [12/75], Val Loss: 1.6062\n",
      "Epoch [13/75], Loss: 0.8763\n",
      "Epoch [13/75], Val Loss: 1.5301\n",
      "Epoch [14/75], Loss: 0.7810\n",
      "Epoch [14/75], Val Loss: 1.6027\n",
      "Epoch [15/75], Loss: 0.7074\n",
      "Epoch [15/75], Val Loss: 1.5431\n",
      "Epoch [16/75], Loss: 0.6614\n",
      "Epoch [16/75], Val Loss: 1.5559\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.6614\n",
      "Test Accuracy Base CNN: 58.50%\n",
      "Epoch [1/75], Loss: 3.3189\n",
      "Epoch [1/75], Val Loss: 3.2218\n",
      "Epoch [2/75], Loss: 3.1401\n",
      "Epoch [2/75], Val Loss: 3.1080\n",
      "Epoch [3/75], Loss: 2.9613\n",
      "Epoch [3/75], Val Loss: 2.9570\n",
      "Epoch [4/75], Loss: 2.7279\n",
      "Epoch [4/75], Val Loss: 2.7194\n",
      "Epoch [5/75], Loss: 2.3890\n",
      "Epoch [5/75], Val Loss: 2.3921\n",
      "Epoch [6/75], Loss: 2.0056\n",
      "Epoch [6/75], Val Loss: 2.0307\n",
      "Epoch [7/75], Loss: 1.6566\n",
      "Epoch [7/75], Val Loss: 1.8490\n",
      "Epoch [8/75], Loss: 1.4455\n",
      "Epoch [8/75], Val Loss: 1.7779\n",
      "Epoch [9/75], Loss: 1.2908\n",
      "Epoch [9/75], Val Loss: 1.7225\n",
      "Epoch [10/75], Loss: 1.1215\n",
      "Epoch [10/75], Val Loss: 1.6585\n",
      "Epoch [11/75], Loss: 1.0269\n",
      "Epoch [11/75], Val Loss: 1.6402\n",
      "Epoch [12/75], Loss: 0.9095\n",
      "Epoch [12/75], Val Loss: 1.6364\n",
      "Epoch [13/75], Loss: 0.8476\n",
      "Epoch [13/75], Val Loss: 1.5274\n",
      "Epoch [14/75], Loss: 0.7384\n",
      "Epoch [14/75], Val Loss: 1.5484\n",
      "Epoch [15/75], Loss: 0.7001\n",
      "Epoch [15/75], Val Loss: 1.5866\n",
      "Epoch [16/75], Loss: 0.6107\n",
      "Epoch [16/75], Val Loss: 1.6349\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.6107\n",
      "Test Accuracy Base CNN: 57.99%\n",
      "Epoch [1/75], Loss: 3.3822\n",
      "Epoch [1/75], Val Loss: 3.2897\n",
      "Epoch [2/75], Loss: 3.1648\n",
      "Epoch [2/75], Val Loss: 3.1238\n",
      "Epoch [3/75], Loss: 2.9699\n",
      "Epoch [3/75], Val Loss: 2.9544\n",
      "Epoch [4/75], Loss: 2.7124\n",
      "Epoch [4/75], Val Loss: 2.7348\n",
      "Epoch [5/75], Loss: 2.3831\n",
      "Epoch [5/75], Val Loss: 2.4243\n",
      "Epoch [6/75], Loss: 1.9917\n",
      "Epoch [6/75], Val Loss: 2.0904\n",
      "Epoch [7/75], Loss: 1.6847\n",
      "Epoch [7/75], Val Loss: 1.9155\n",
      "Epoch [8/75], Loss: 1.5031\n",
      "Epoch [8/75], Val Loss: 1.8460\n",
      "Epoch [9/75], Loss: 1.3368\n",
      "Epoch [9/75], Val Loss: 1.8794\n",
      "Epoch [10/75], Loss: 1.2127\n",
      "Epoch [10/75], Val Loss: 1.7133\n",
      "Epoch [11/75], Loss: 1.1175\n",
      "Epoch [11/75], Val Loss: 1.6538\n",
      "Epoch [12/75], Loss: 1.0288\n",
      "Epoch [12/75], Val Loss: 1.5390\n",
      "Epoch [13/75], Loss: 0.8640\n",
      "Epoch [13/75], Val Loss: 1.5659\n",
      "Epoch [14/75], Loss: 0.7486\n",
      "Epoch [14/75], Val Loss: 1.4757\n",
      "Epoch [15/75], Loss: 0.6483\n",
      "Epoch [15/75], Val Loss: 1.4824\n",
      "Epoch [16/75], Loss: 0.5852\n",
      "Epoch [16/75], Val Loss: 1.5304\n",
      "Epoch [17/75], Loss: 0.5140\n",
      "Epoch [17/75], Val Loss: 1.5413\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [17/75], Loss: 0.5140\n",
      "Test Accuracy Base CNN: 58.86%\n",
      "Epoch [1/75], Loss: 3.2940\n",
      "Epoch [1/75], Val Loss: 3.2553\n",
      "Epoch [2/75], Loss: 3.1531\n",
      "Epoch [2/75], Val Loss: 3.1422\n",
      "Epoch [3/75], Loss: 2.9965\n",
      "Epoch [3/75], Val Loss: 3.0103\n",
      "Epoch [4/75], Loss: 2.7838\n",
      "Epoch [4/75], Val Loss: 2.8027\n",
      "Epoch [5/75], Loss: 2.4976\n",
      "Epoch [5/75], Val Loss: 2.5161\n",
      "Epoch [6/75], Loss: 2.1261\n",
      "Epoch [6/75], Val Loss: 2.1521\n",
      "Epoch [7/75], Loss: 1.7457\n",
      "Epoch [7/75], Val Loss: 1.8873\n",
      "Epoch [8/75], Loss: 1.4804\n",
      "Epoch [8/75], Val Loss: 1.8033\n",
      "Epoch [9/75], Loss: 1.3673\n",
      "Epoch [9/75], Val Loss: 1.7883\n",
      "Epoch [10/75], Loss: 1.2688\n",
      "Epoch [10/75], Val Loss: 1.7152\n",
      "Epoch [11/75], Loss: 1.0973\n",
      "Epoch [11/75], Val Loss: 1.6359\n",
      "Epoch [12/75], Loss: 0.9703\n",
      "Epoch [12/75], Val Loss: 1.6375\n",
      "Epoch [13/75], Loss: 0.8676\n",
      "Epoch [13/75], Val Loss: 1.5606\n",
      "Epoch [14/75], Loss: 0.7799\n",
      "Epoch [14/75], Val Loss: 1.5925\n",
      "Epoch [15/75], Loss: 0.7420\n",
      "Epoch [15/75], Val Loss: 1.6037\n",
      "Epoch [16/75], Loss: 0.6610\n",
      "Epoch [16/75], Val Loss: 1.5841\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.6610\n",
      "Test Accuracy Base CNN: 58.64%\n",
      "Epoch [1/75], Loss: 3.3542\n",
      "Epoch [1/75], Val Loss: 3.2831\n",
      "Epoch [2/75], Loss: 3.1886\n",
      "Epoch [2/75], Val Loss: 3.1732\n",
      "Epoch [3/75], Loss: 3.0567\n",
      "Epoch [3/75], Val Loss: 3.0644\n",
      "Epoch [4/75], Loss: 2.8839\n",
      "Epoch [4/75], Val Loss: 2.8938\n",
      "Epoch [5/75], Loss: 2.6431\n",
      "Epoch [5/75], Val Loss: 2.6606\n",
      "Epoch [6/75], Loss: 2.3264\n",
      "Epoch [6/75], Val Loss: 2.3464\n",
      "Epoch [7/75], Loss: 1.9750\n",
      "Epoch [7/75], Val Loss: 2.0290\n",
      "Epoch [8/75], Loss: 1.6420\n",
      "Epoch [8/75], Val Loss: 1.8666\n",
      "Epoch [9/75], Loss: 1.4387\n",
      "Epoch [9/75], Val Loss: 1.7985\n",
      "Epoch [10/75], Loss: 1.3043\n",
      "Epoch [10/75], Val Loss: 1.7328\n",
      "Epoch [11/75], Loss: 1.1157\n",
      "Epoch [11/75], Val Loss: 1.6858\n",
      "Epoch [12/75], Loss: 1.0044\n",
      "Epoch [12/75], Val Loss: 1.6520\n",
      "Epoch [13/75], Loss: 0.9106\n",
      "Epoch [13/75], Val Loss: 1.5666\n",
      "Epoch [14/75], Loss: 0.8060\n",
      "Epoch [14/75], Val Loss: 1.4787\n",
      "Epoch [15/75], Loss: 0.6859\n",
      "Epoch [15/75], Val Loss: 1.6028\n",
      "Epoch [16/75], Loss: 0.6316\n",
      "Epoch [16/75], Val Loss: 1.4737\n",
      "Epoch [17/75], Loss: 0.5621\n",
      "Epoch [17/75], Val Loss: 1.6233\n",
      "Epoch [18/75], Loss: 0.5208\n",
      "Epoch [18/75], Val Loss: 1.5528\n",
      "Epoch [19/75], Loss: 0.4316\n",
      "Epoch [19/75], Val Loss: 1.5811\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [19/75], Loss: 0.4316\n",
      "Test Accuracy Base CNN: 58.85%\n",
      "Epoch [1/75], Loss: 3.3394\n",
      "Epoch [1/75], Val Loss: 3.2549\n",
      "Epoch [2/75], Loss: 3.1603\n",
      "Epoch [2/75], Val Loss: 3.1469\n",
      "Epoch [3/75], Loss: 3.0270\n",
      "Epoch [3/75], Val Loss: 3.0421\n",
      "Epoch [4/75], Loss: 2.8604\n",
      "Epoch [4/75], Val Loss: 2.8626\n",
      "Epoch [5/75], Loss: 2.6071\n",
      "Epoch [5/75], Val Loss: 2.6086\n",
      "Epoch [6/75], Loss: 2.2736\n",
      "Epoch [6/75], Val Loss: 2.2861\n",
      "Epoch [7/75], Loss: 1.8966\n",
      "Epoch [7/75], Val Loss: 1.9835\n",
      "Epoch [8/75], Loss: 1.6007\n",
      "Epoch [8/75], Val Loss: 1.7908\n",
      "Epoch [9/75], Loss: 1.3779\n",
      "Epoch [9/75], Val Loss: 1.7575\n",
      "Epoch [10/75], Loss: 1.3247\n",
      "Epoch [10/75], Val Loss: 1.7706\n",
      "Epoch [11/75], Loss: 1.1877\n",
      "Epoch [11/75], Val Loss: 1.6694\n",
      "Epoch [12/75], Loss: 1.0969\n",
      "Epoch [12/75], Val Loss: 1.6595\n",
      "Epoch [13/75], Loss: 0.9277\n",
      "Epoch [13/75], Val Loss: 1.6127\n",
      "Epoch [14/75], Loss: 0.8571\n",
      "Epoch [14/75], Val Loss: 1.5488\n",
      "Epoch [15/75], Loss: 0.7726\n",
      "Epoch [15/75], Val Loss: 1.5424\n",
      "Epoch [16/75], Loss: 0.6837\n",
      "Epoch [16/75], Val Loss: 1.5315\n",
      "Epoch [17/75], Loss: 0.6578\n",
      "Epoch [17/75], Val Loss: 1.4790\n",
      "Epoch [18/75], Loss: 0.5746\n",
      "Epoch [18/75], Val Loss: 1.4723\n",
      "Epoch [19/75], Loss: 0.4879\n",
      "Epoch [19/75], Val Loss: 1.4973\n",
      "Epoch [20/75], Loss: 0.4607\n",
      "Epoch [20/75], Val Loss: 1.4672\n",
      "Epoch [21/75], Loss: 0.4358\n",
      "Epoch [21/75], Val Loss: 1.5122\n",
      "Epoch [22/75], Loss: 0.3547\n",
      "Epoch [22/75], Val Loss: 1.5165\n",
      "Epoch [23/75], Loss: 0.2935\n",
      "Epoch [23/75], Val Loss: 1.5583\n",
      "Stopping early at epoch 23 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [23/75], Loss: 0.2935\n",
      "Test Accuracy Base CNN: 60.40%\n",
      "Epoch [1/75], Loss: 3.3470\n",
      "Epoch [1/75], Val Loss: 3.2883\n",
      "Epoch [2/75], Loss: 3.2254\n",
      "Epoch [2/75], Val Loss: 3.2073\n",
      "Epoch [3/75], Loss: 3.1120\n",
      "Epoch [3/75], Val Loss: 3.1238\n",
      "Epoch [4/75], Loss: 2.9774\n",
      "Epoch [4/75], Val Loss: 2.9941\n",
      "Epoch [5/75], Loss: 2.7804\n",
      "Epoch [5/75], Val Loss: 2.7961\n",
      "Epoch [6/75], Loss: 2.5114\n",
      "Epoch [6/75], Val Loss: 2.5353\n",
      "Epoch [7/75], Loss: 2.1670\n",
      "Epoch [7/75], Val Loss: 2.2119\n",
      "Epoch [8/75], Loss: 1.7767\n",
      "Epoch [8/75], Val Loss: 1.9101\n",
      "Epoch [9/75], Loss: 1.4772\n",
      "Epoch [9/75], Val Loss: 1.8412\n",
      "Epoch [10/75], Loss: 1.3580\n",
      "Epoch [10/75], Val Loss: 1.7513\n",
      "Epoch [11/75], Loss: 1.1651\n",
      "Epoch [11/75], Val Loss: 1.8036\n",
      "Epoch [12/75], Loss: 1.1702\n",
      "Epoch [12/75], Val Loss: 1.8003\n",
      "Epoch [13/75], Loss: 1.1282\n",
      "Epoch [13/75], Val Loss: 1.8453\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [13/75], Loss: 1.1282\n",
      "Test Accuracy Base CNN: 50.15%\n",
      "Epoch [1/75], Loss: 3.3274\n",
      "Epoch [1/75], Val Loss: 3.2727\n",
      "Epoch [2/75], Loss: 3.1848\n",
      "Epoch [2/75], Val Loss: 3.1779\n",
      "Epoch [3/75], Loss: 3.0649\n",
      "Epoch [3/75], Val Loss: 3.0899\n",
      "Epoch [4/75], Loss: 2.9196\n",
      "Epoch [4/75], Val Loss: 2.9606\n",
      "Epoch [5/75], Loss: 2.7146\n",
      "Epoch [5/75], Val Loss: 2.7567\n",
      "Epoch [6/75], Loss: 2.4241\n",
      "Epoch [6/75], Val Loss: 2.4506\n",
      "Epoch [7/75], Loss: 2.0813\n",
      "Epoch [7/75], Val Loss: 2.1197\n",
      "Epoch [8/75], Loss: 1.7441\n",
      "Epoch [8/75], Val Loss: 1.9036\n",
      "Epoch [9/75], Loss: 1.4826\n",
      "Epoch [9/75], Val Loss: 1.7078\n",
      "Epoch [10/75], Loss: 1.2759\n",
      "Epoch [10/75], Val Loss: 1.6571\n",
      "Epoch [11/75], Loss: 1.1488\n",
      "Epoch [11/75], Val Loss: 1.5871\n",
      "Epoch [12/75], Loss: 0.9670\n",
      "Epoch [12/75], Val Loss: 1.6514\n",
      "Epoch [13/75], Loss: 0.9537\n",
      "Epoch [13/75], Val Loss: 1.6133\n",
      "Epoch [14/75], Loss: 0.8956\n",
      "Epoch [14/75], Val Loss: 1.6137\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [14/75], Loss: 0.8956\n",
      "Test Accuracy Base CNN: 55.81%\n",
      "Epoch [1/75], Loss: 3.3551\n",
      "Epoch [1/75], Val Loss: 3.2927\n",
      "Epoch [2/75], Loss: 3.2317\n",
      "Epoch [2/75], Val Loss: 3.1983\n",
      "Epoch [3/75], Loss: 3.1234\n",
      "Epoch [3/75], Val Loss: 3.1115\n",
      "Epoch [4/75], Loss: 2.9982\n",
      "Epoch [4/75], Val Loss: 3.0064\n",
      "Epoch [5/75], Loss: 2.8318\n",
      "Epoch [5/75], Val Loss: 2.8524\n",
      "Epoch [6/75], Loss: 2.5951\n",
      "Epoch [6/75], Val Loss: 2.6295\n",
      "Epoch [7/75], Loss: 2.2812\n",
      "Epoch [7/75], Val Loss: 2.3130\n",
      "Epoch [8/75], Loss: 1.9269\n",
      "Epoch [8/75], Val Loss: 2.0286\n",
      "Epoch [9/75], Loss: 1.6428\n",
      "Epoch [9/75], Val Loss: 1.8215\n",
      "Epoch [10/75], Loss: 1.4128\n",
      "Epoch [10/75], Val Loss: 1.7082\n",
      "Epoch [11/75], Loss: 1.2792\n",
      "Epoch [11/75], Val Loss: 1.7271\n",
      "Epoch [12/75], Loss: 1.1678\n",
      "Epoch [12/75], Val Loss: 1.6485\n",
      "Epoch [13/75], Loss: 1.0424\n",
      "Epoch [13/75], Val Loss: 1.6162\n",
      "Epoch [14/75], Loss: 0.9541\n",
      "Epoch [14/75], Val Loss: 1.5675\n",
      "Epoch [15/75], Loss: 0.8652\n",
      "Epoch [15/75], Val Loss: 1.6040\n",
      "Epoch [16/75], Loss: 0.8200\n",
      "Epoch [16/75], Val Loss: 1.5409\n",
      "Epoch [17/75], Loss: 0.6342\n",
      "Epoch [17/75], Val Loss: 1.5360\n",
      "Epoch [18/75], Loss: 0.6177\n",
      "Epoch [18/75], Val Loss: 1.5098\n",
      "Epoch [19/75], Loss: 0.5224\n",
      "Epoch [19/75], Val Loss: 1.5262\n",
      "Epoch [20/75], Loss: 0.4469\n",
      "Epoch [20/75], Val Loss: 1.4875\n",
      "Epoch [21/75], Loss: 0.3896\n",
      "Epoch [21/75], Val Loss: 1.4982\n",
      "Epoch [22/75], Loss: 0.3283\n",
      "Epoch [22/75], Val Loss: 1.4733\n",
      "Epoch [23/75], Loss: 0.2831\n",
      "Epoch [23/75], Val Loss: 1.5538\n",
      "Epoch [24/75], Loss: 0.2722\n",
      "Epoch [24/75], Val Loss: 1.5375\n",
      "Epoch [25/75], Loss: 0.2144\n",
      "Epoch [25/75], Val Loss: 1.6127\n",
      "Stopping early at epoch 25 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [25/75], Loss: 0.2144\n",
      "Test Accuracy Base CNN: 61.64%\n",
      "Epoch [1/75], Loss: 3.4008\n",
      "Epoch [1/75], Val Loss: 3.2884\n",
      "Epoch [2/75], Loss: 3.2132\n",
      "Epoch [2/75], Val Loss: 3.1722\n",
      "Epoch [3/75], Loss: 3.0791\n",
      "Epoch [3/75], Val Loss: 3.0834\n",
      "Epoch [4/75], Loss: 2.9357\n",
      "Epoch [4/75], Val Loss: 2.9520\n",
      "Epoch [5/75], Loss: 2.7278\n",
      "Epoch [5/75], Val Loss: 2.7478\n",
      "Epoch [6/75], Loss: 2.4277\n",
      "Epoch [6/75], Val Loss: 2.4329\n",
      "Epoch [7/75], Loss: 2.0698\n",
      "Epoch [7/75], Val Loss: 2.1096\n",
      "Epoch [8/75], Loss: 1.7482\n",
      "Epoch [8/75], Val Loss: 1.9288\n",
      "Epoch [9/75], Loss: 1.4920\n",
      "Epoch [9/75], Val Loss: 1.8022\n",
      "Epoch [10/75], Loss: 1.3744\n",
      "Epoch [10/75], Val Loss: 1.7751\n",
      "Epoch [11/75], Loss: 1.2700\n",
      "Epoch [11/75], Val Loss: 1.9377\n",
      "Epoch [12/75], Loss: 1.3275\n",
      "Epoch [12/75], Val Loss: 1.7764\n",
      "Epoch [13/75], Loss: 1.2016\n",
      "Epoch [13/75], Val Loss: 1.7943\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [13/75], Loss: 1.2016\n",
      "Test Accuracy Base CNN: 51.30%\n",
      "Epoch [1/75], Loss: 3.3496\n",
      "Epoch [1/75], Val Loss: 3.2963\n",
      "Epoch [2/75], Loss: 3.2335\n",
      "Epoch [2/75], Val Loss: 3.2035\n",
      "Epoch [3/75], Loss: 3.1087\n",
      "Epoch [3/75], Val Loss: 3.1125\n",
      "Epoch [4/75], Loss: 2.9652\n",
      "Epoch [4/75], Val Loss: 2.9807\n",
      "Epoch [5/75], Loss: 2.7510\n",
      "Epoch [5/75], Val Loss: 2.7702\n",
      "Epoch [6/75], Loss: 2.4686\n",
      "Epoch [6/75], Val Loss: 2.4882\n",
      "Epoch [7/75], Loss: 2.1079\n",
      "Epoch [7/75], Val Loss: 2.1902\n",
      "Epoch [8/75], Loss: 1.7783\n",
      "Epoch [8/75], Val Loss: 1.9344\n",
      "Epoch [9/75], Loss: 1.4950\n",
      "Epoch [9/75], Val Loss: 1.8330\n",
      "Epoch [10/75], Loss: 1.4136\n",
      "Epoch [10/75], Val Loss: 1.7732\n",
      "Epoch [11/75], Loss: 1.3594\n",
      "Epoch [11/75], Val Loss: 1.9388\n",
      "Epoch [12/75], Loss: 1.3196\n",
      "Epoch [12/75], Val Loss: 1.7762\n",
      "Epoch [13/75], Loss: 1.1382\n",
      "Epoch [13/75], Val Loss: 1.8641\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [13/75], Loss: 1.1382\n",
      "Test Accuracy Base CNN: 50.39%\n",
      "Epoch [1/75], Loss: 3.3567\n",
      "Epoch [1/75], Val Loss: 3.2696\n",
      "Epoch [2/75], Loss: 3.1961\n",
      "Epoch [2/75], Val Loss: 3.1753\n",
      "Epoch [3/75], Loss: 3.0636\n",
      "Epoch [3/75], Val Loss: 3.0859\n",
      "Epoch [4/75], Loss: 2.9153\n",
      "Epoch [4/75], Val Loss: 2.9288\n",
      "Epoch [5/75], Loss: 2.6920\n",
      "Epoch [5/75], Val Loss: 2.6839\n",
      "Epoch [6/75], Loss: 2.3735\n",
      "Epoch [6/75], Val Loss: 2.3551\n",
      "Epoch [7/75], Loss: 2.0021\n",
      "Epoch [7/75], Val Loss: 2.0576\n",
      "Epoch [8/75], Loss: 1.6962\n",
      "Epoch [8/75], Val Loss: 1.8309\n",
      "Epoch [9/75], Loss: 1.4999\n",
      "Epoch [9/75], Val Loss: 1.7300\n",
      "Epoch [10/75], Loss: 1.3316\n",
      "Epoch [10/75], Val Loss: 1.7471\n",
      "Epoch [11/75], Loss: 1.2034\n",
      "Epoch [11/75], Val Loss: 1.6341\n",
      "Epoch [12/75], Loss: 1.1319\n",
      "Epoch [12/75], Val Loss: 1.6635\n",
      "Epoch [13/75], Loss: 1.0203\n",
      "Epoch [13/75], Val Loss: 1.7122\n",
      "Epoch [14/75], Loss: 1.0449\n",
      "Epoch [14/75], Val Loss: 1.5855\n",
      "Epoch [15/75], Loss: 0.8803\n",
      "Epoch [15/75], Val Loss: 1.6843\n",
      "Epoch [16/75], Loss: 0.8493\n",
      "Epoch [16/75], Val Loss: 1.5394\n",
      "Epoch [17/75], Loss: 0.7112\n",
      "Epoch [17/75], Val Loss: 1.5535\n",
      "Epoch [18/75], Loss: 0.6254\n",
      "Epoch [18/75], Val Loss: 1.4770\n",
      "Epoch [19/75], Loss: 0.5750\n",
      "Epoch [19/75], Val Loss: 1.5226\n",
      "Epoch [20/75], Loss: 0.5081\n",
      "Epoch [20/75], Val Loss: 1.4946\n",
      "Epoch [21/75], Loss: 0.4329\n",
      "Epoch [21/75], Val Loss: 1.4782\n",
      "Stopping early at epoch 21 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [21/75], Loss: 0.4329\n",
      "Test Accuracy Base CNN: 61.00%\n",
      "Epoch [1/75], Loss: 3.4074\n",
      "Epoch [1/75], Val Loss: 3.2853\n",
      "Epoch [2/75], Loss: 3.2059\n",
      "Epoch [2/75], Val Loss: 3.1582\n",
      "Epoch [3/75], Loss: 3.0513\n",
      "Epoch [3/75], Val Loss: 3.0403\n",
      "Epoch [4/75], Loss: 2.8731\n",
      "Epoch [4/75], Val Loss: 2.8634\n",
      "Epoch [5/75], Loss: 2.6033\n",
      "Epoch [5/75], Val Loss: 2.6031\n",
      "Epoch [6/75], Loss: 2.2681\n",
      "Epoch [6/75], Val Loss: 2.2357\n",
      "Epoch [7/75], Loss: 1.8825\n",
      "Epoch [7/75], Val Loss: 1.9651\n",
      "Epoch [8/75], Loss: 1.5771\n",
      "Epoch [8/75], Val Loss: 1.7752\n",
      "Epoch [9/75], Loss: 1.3398\n",
      "Epoch [9/75], Val Loss: 1.7088\n",
      "Epoch [10/75], Loss: 1.2344\n",
      "Epoch [10/75], Val Loss: 1.7217\n",
      "Epoch [11/75], Loss: 1.1460\n",
      "Epoch [11/75], Val Loss: 1.7147\n",
      "Epoch [12/75], Loss: 1.0027\n",
      "Epoch [12/75], Val Loss: 1.6391\n",
      "Epoch [13/75], Loss: 0.8756\n",
      "Epoch [13/75], Val Loss: 1.5224\n",
      "Epoch [14/75], Loss: 0.7637\n",
      "Epoch [14/75], Val Loss: 1.5382\n",
      "Epoch [15/75], Loss: 0.6968\n",
      "Epoch [15/75], Val Loss: 1.5522\n",
      "Epoch [16/75], Loss: 0.6738\n",
      "Epoch [16/75], Val Loss: 1.4330\n",
      "Epoch [17/75], Loss: 0.6012\n",
      "Epoch [17/75], Val Loss: 1.5236\n",
      "Epoch [18/75], Loss: 0.5882\n",
      "Epoch [18/75], Val Loss: 1.4333\n",
      "Epoch [19/75], Loss: 0.5283\n",
      "Epoch [19/75], Val Loss: 1.6075\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [19/75], Loss: 0.5283\n",
      "Test Accuracy Base CNN: 58.15%\n",
      "Epoch [1/75], Loss: 3.3489\n",
      "Epoch [1/75], Val Loss: 3.2342\n",
      "Epoch [2/75], Loss: 3.1446\n",
      "Epoch [2/75], Val Loss: 3.1127\n",
      "Epoch [3/75], Loss: 2.9566\n",
      "Epoch [3/75], Val Loss: 2.9574\n",
      "Epoch [4/75], Loss: 2.7100\n",
      "Epoch [4/75], Val Loss: 2.7053\n",
      "Epoch [5/75], Loss: 2.3691\n",
      "Epoch [5/75], Val Loss: 2.3769\n",
      "Epoch [6/75], Loss: 1.9929\n",
      "Epoch [6/75], Val Loss: 2.0015\n",
      "Epoch [7/75], Loss: 1.6356\n",
      "Epoch [7/75], Val Loss: 1.8205\n",
      "Epoch [8/75], Loss: 1.3755\n",
      "Epoch [8/75], Val Loss: 1.6858\n",
      "Epoch [9/75], Loss: 1.2450\n",
      "Epoch [9/75], Val Loss: 1.7647\n",
      "Epoch [10/75], Loss: 1.1553\n",
      "Epoch [10/75], Val Loss: 1.6575\n",
      "Epoch [11/75], Loss: 1.0425\n",
      "Epoch [11/75], Val Loss: 1.6116\n",
      "Epoch [12/75], Loss: 0.8359\n",
      "Epoch [12/75], Val Loss: 1.6103\n",
      "Epoch [13/75], Loss: 0.8799\n",
      "Epoch [13/75], Val Loss: 1.5364\n",
      "Epoch [14/75], Loss: 0.7921\n",
      "Epoch [14/75], Val Loss: 1.5828\n",
      "Epoch [15/75], Loss: 0.6722\n",
      "Epoch [15/75], Val Loss: 1.5027\n",
      "Epoch [16/75], Loss: 0.5744\n",
      "Epoch [16/75], Val Loss: 1.4861\n",
      "Epoch [17/75], Loss: 0.5229\n",
      "Epoch [17/75], Val Loss: 1.4384\n",
      "Epoch [18/75], Loss: 0.4594\n",
      "Epoch [18/75], Val Loss: 1.5128\n",
      "Epoch [19/75], Loss: 0.3978\n",
      "Epoch [19/75], Val Loss: 1.4797\n",
      "Epoch [20/75], Loss: 0.3406\n",
      "Epoch [20/75], Val Loss: 1.5280\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [20/75], Loss: 0.3406\n",
      "Test Accuracy Base CNN: 61.39%\n",
      "Epoch [1/75], Loss: 3.4220\n",
      "Epoch [1/75], Val Loss: 3.2782\n",
      "Epoch [2/75], Loss: 3.1850\n",
      "Epoch [2/75], Val Loss: 3.1588\n",
      "Epoch [3/75], Loss: 3.0236\n",
      "Epoch [3/75], Val Loss: 3.0421\n",
      "Epoch [4/75], Loss: 2.8548\n",
      "Epoch [4/75], Val Loss: 2.8661\n",
      "Epoch [5/75], Loss: 2.5780\n",
      "Epoch [5/75], Val Loss: 2.5898\n",
      "Epoch [6/75], Loss: 2.2179\n",
      "Epoch [6/75], Val Loss: 2.2640\n",
      "Epoch [7/75], Loss: 1.8519\n",
      "Epoch [7/75], Val Loss: 1.9645\n",
      "Epoch [8/75], Loss: 1.5404\n",
      "Epoch [8/75], Val Loss: 1.7913\n",
      "Epoch [9/75], Loss: 1.3622\n",
      "Epoch [9/75], Val Loss: 1.7167\n",
      "Epoch [10/75], Loss: 1.2556\n",
      "Epoch [10/75], Val Loss: 1.7274\n",
      "Epoch [11/75], Loss: 1.1242\n",
      "Epoch [11/75], Val Loss: 1.7731\n",
      "Epoch [12/75], Loss: 1.0375\n",
      "Epoch [12/75], Val Loss: 1.7010\n",
      "Epoch [13/75], Loss: 0.9076\n",
      "Epoch [13/75], Val Loss: 1.5932\n",
      "Epoch [14/75], Loss: 0.7728\n",
      "Epoch [14/75], Val Loss: 1.5446\n",
      "Epoch [15/75], Loss: 0.6650\n",
      "Epoch [15/75], Val Loss: 1.4980\n",
      "Epoch [16/75], Loss: 0.5498\n",
      "Epoch [16/75], Val Loss: 1.4858\n",
      "Epoch [17/75], Loss: 0.4926\n",
      "Epoch [17/75], Val Loss: 1.5559\n",
      "Epoch [18/75], Loss: 0.4346\n",
      "Epoch [18/75], Val Loss: 1.5231\n",
      "Epoch [19/75], Loss: 0.3654\n",
      "Epoch [19/75], Val Loss: 1.5474\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [19/75], Loss: 0.3654\n",
      "Test Accuracy Base CNN: 60.67%\n",
      "Epoch [1/75], Loss: 3.3397\n",
      "Epoch [1/75], Val Loss: 3.3038\n",
      "Epoch [2/75], Loss: 3.2477\n",
      "Epoch [2/75], Val Loss: 3.2348\n",
      "Epoch [3/75], Loss: 3.1587\n",
      "Epoch [3/75], Val Loss: 3.1781\n",
      "Epoch [4/75], Loss: 3.0696\n",
      "Epoch [4/75], Val Loss: 3.1079\n",
      "Epoch [5/75], Loss: 2.9584\n",
      "Epoch [5/75], Val Loss: 3.0017\n",
      "Epoch [6/75], Loss: 2.8009\n",
      "Epoch [6/75], Val Loss: 2.8449\n",
      "Epoch [7/75], Loss: 2.5798\n",
      "Epoch [7/75], Val Loss: 2.6106\n",
      "Epoch [8/75], Loss: 2.2838\n",
      "Epoch [8/75], Val Loss: 2.3026\n",
      "Epoch [9/75], Loss: 1.9314\n",
      "Epoch [9/75], Val Loss: 2.0007\n",
      "Epoch [10/75], Loss: 1.6319\n",
      "Epoch [10/75], Val Loss: 1.7821\n",
      "Epoch [11/75], Loss: 1.4210\n",
      "Epoch [11/75], Val Loss: 1.7291\n",
      "Epoch [12/75], Loss: 1.2880\n",
      "Epoch [12/75], Val Loss: 1.6359\n",
      "Epoch [13/75], Loss: 1.1345\n",
      "Epoch [13/75], Val Loss: 1.6691\n",
      "Epoch [14/75], Loss: 1.0225\n",
      "Epoch [14/75], Val Loss: 1.5975\n",
      "Epoch [15/75], Loss: 0.9543\n",
      "Epoch [15/75], Val Loss: 1.5594\n",
      "Epoch [16/75], Loss: 0.8481\n",
      "Epoch [16/75], Val Loss: 1.5742\n",
      "Epoch [17/75], Loss: 0.7728\n",
      "Epoch [17/75], Val Loss: 1.5796\n",
      "Epoch [18/75], Loss: 0.6831\n",
      "Epoch [18/75], Val Loss: 1.5605\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [18/75], Loss: 0.6831\n",
      "Test Accuracy Base CNN: 57.39%\n",
      "Epoch [1/75], Loss: 3.3084\n",
      "Epoch [1/75], Val Loss: 3.2433\n",
      "Epoch [2/75], Loss: 3.1664\n",
      "Epoch [2/75], Val Loss: 3.1366\n",
      "Epoch [3/75], Loss: 3.0040\n",
      "Epoch [3/75], Val Loss: 3.0225\n",
      "Epoch [4/75], Loss: 2.8239\n",
      "Epoch [4/75], Val Loss: 2.8492\n",
      "Epoch [5/75], Loss: 2.5598\n",
      "Epoch [5/75], Val Loss: 2.5919\n",
      "Epoch [6/75], Loss: 2.2404\n",
      "Epoch [6/75], Val Loss: 2.2865\n",
      "Epoch [7/75], Loss: 1.8680\n",
      "Epoch [7/75], Val Loss: 2.0060\n",
      "Epoch [8/75], Loss: 1.5438\n",
      "Epoch [8/75], Val Loss: 1.7616\n",
      "Epoch [9/75], Loss: 1.3422\n",
      "Epoch [9/75], Val Loss: 1.7224\n",
      "Epoch [10/75], Loss: 1.1840\n",
      "Epoch [10/75], Val Loss: 1.5722\n",
      "Epoch [11/75], Loss: 1.0226\n",
      "Epoch [11/75], Val Loss: 1.6123\n",
      "Epoch [12/75], Loss: 0.9375\n",
      "Epoch [12/75], Val Loss: 1.6017\n",
      "Epoch [13/75], Loss: 0.8961\n",
      "Epoch [13/75], Val Loss: 1.6201\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [13/75], Loss: 0.8961\n",
      "Test Accuracy Base CNN: 56.50%\n",
      "Epoch [1/75], Loss: 3.3197\n",
      "Epoch [1/75], Val Loss: 3.2869\n",
      "Epoch [2/75], Loss: 3.1858\n",
      "Epoch [2/75], Val Loss: 3.1765\n",
      "Epoch [3/75], Loss: 3.0302\n",
      "Epoch [3/75], Val Loss: 3.0518\n",
      "Epoch [4/75], Loss: 2.8503\n",
      "Epoch [4/75], Val Loss: 2.8748\n",
      "Epoch [5/75], Loss: 2.5779\n",
      "Epoch [5/75], Val Loss: 2.6050\n",
      "Epoch [6/75], Loss: 2.2341\n",
      "Epoch [6/75], Val Loss: 2.2575\n",
      "Epoch [7/75], Loss: 1.8510\n",
      "Epoch [7/75], Val Loss: 1.9923\n",
      "Epoch [8/75], Loss: 1.5613\n",
      "Epoch [8/75], Val Loss: 1.8335\n",
      "Epoch [9/75], Loss: 1.3727\n",
      "Epoch [9/75], Val Loss: 1.8227\n",
      "Epoch [10/75], Loss: 1.2923\n",
      "Epoch [10/75], Val Loss: 1.8033\n",
      "Epoch [11/75], Loss: 1.2067\n",
      "Epoch [11/75], Val Loss: 1.8642\n",
      "Epoch [12/75], Loss: 1.0606\n",
      "Epoch [12/75], Val Loss: 1.7570\n",
      "Epoch [13/75], Loss: 1.0303\n",
      "Epoch [13/75], Val Loss: 1.6364\n",
      "Epoch [14/75], Loss: 0.8287\n",
      "Epoch [14/75], Val Loss: 1.5890\n",
      "Epoch [15/75], Loss: 0.7584\n",
      "Epoch [15/75], Val Loss: 1.5697\n",
      "Epoch [16/75], Loss: 0.6648\n",
      "Epoch [16/75], Val Loss: 1.5407\n",
      "Epoch [17/75], Loss: 0.5303\n",
      "Epoch [17/75], Val Loss: 1.4695\n",
      "Epoch [18/75], Loss: 0.4977\n",
      "Epoch [18/75], Val Loss: 1.4979\n",
      "Epoch [19/75], Loss: 0.4238\n",
      "Epoch [19/75], Val Loss: 1.5400\n",
      "Epoch [20/75], Loss: 0.3874\n",
      "Epoch [20/75], Val Loss: 1.5402\n",
      "Stopping early at epoch 20 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [20/75], Loss: 0.3874\n",
      "Test Accuracy Base CNN: 61.16%\n",
      "Epoch [1/75], Loss: 3.2889\n",
      "Epoch [1/75], Val Loss: 3.2646\n",
      "Epoch [2/75], Loss: 3.1836\n",
      "Epoch [2/75], Val Loss: 3.1727\n",
      "Epoch [3/75], Loss: 3.0523\n",
      "Epoch [3/75], Val Loss: 3.0728\n",
      "Epoch [4/75], Loss: 2.8873\n",
      "Epoch [4/75], Val Loss: 2.9193\n",
      "Epoch [5/75], Loss: 2.6552\n",
      "Epoch [5/75], Val Loss: 2.6839\n",
      "Epoch [6/75], Loss: 2.3278\n",
      "Epoch [6/75], Val Loss: 2.3535\n",
      "Epoch [7/75], Loss: 1.9556\n",
      "Epoch [7/75], Val Loss: 2.0388\n",
      "Epoch [8/75], Loss: 1.6194\n",
      "Epoch [8/75], Val Loss: 1.8459\n",
      "Epoch [9/75], Loss: 1.3859\n",
      "Epoch [9/75], Val Loss: 1.7640\n",
      "Epoch [10/75], Loss: 1.2288\n",
      "Epoch [10/75], Val Loss: 1.7164\n",
      "Epoch [11/75], Loss: 1.1250\n",
      "Epoch [11/75], Val Loss: 1.6614\n",
      "Epoch [12/75], Loss: 0.9909\n",
      "Epoch [12/75], Val Loss: 1.6071\n",
      "Epoch [13/75], Loss: 0.9412\n",
      "Epoch [13/75], Val Loss: 1.6027\n",
      "Epoch [14/75], Loss: 0.7994\n",
      "Epoch [14/75], Val Loss: 1.6774\n",
      "Epoch [15/75], Loss: 0.7713\n",
      "Epoch [15/75], Val Loss: 1.5802\n",
      "Epoch [16/75], Loss: 0.7332\n",
      "Epoch [16/75], Val Loss: 1.7216\n",
      "Epoch [17/75], Loss: 0.6675\n",
      "Epoch [17/75], Val Loss: 1.5441\n",
      "Epoch [18/75], Loss: 0.5381\n",
      "Epoch [18/75], Val Loss: 1.5292\n",
      "Epoch [19/75], Loss: 0.5120\n",
      "Epoch [19/75], Val Loss: 1.5201\n",
      "Epoch [20/75], Loss: 0.4580\n",
      "Epoch [20/75], Val Loss: 1.4643\n",
      "Epoch [21/75], Loss: 0.4114\n",
      "Epoch [21/75], Val Loss: 1.6179\n",
      "Epoch [22/75], Loss: 0.4057\n",
      "Epoch [22/75], Val Loss: 1.5796\n",
      "Epoch [23/75], Loss: 0.3295\n",
      "Epoch [23/75], Val Loss: 1.6126\n",
      "Stopping early at epoch 23 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [23/75], Loss: 0.3295\n",
      "Test Accuracy Base CNN: 59.82%\n",
      "Epoch [1/75], Loss: 3.3754\n",
      "Epoch [1/75], Val Loss: 3.2749\n",
      "Epoch [2/75], Loss: 3.1590\n",
      "Epoch [2/75], Val Loss: 3.1589\n",
      "Epoch [3/75], Loss: 3.0037\n",
      "Epoch [3/75], Val Loss: 3.0263\n",
      "Epoch [4/75], Loss: 2.7947\n",
      "Epoch [4/75], Val Loss: 2.8103\n",
      "Epoch [5/75], Loss: 2.5048\n",
      "Epoch [5/75], Val Loss: 2.4976\n",
      "Epoch [6/75], Loss: 2.1267\n",
      "Epoch [6/75], Val Loss: 2.1774\n",
      "Epoch [7/75], Loss: 1.7593\n",
      "Epoch [7/75], Val Loss: 1.9687\n",
      "Epoch [8/75], Loss: 1.5508\n",
      "Epoch [8/75], Val Loss: 1.8859\n",
      "Epoch [9/75], Loss: 1.3757\n",
      "Epoch [9/75], Val Loss: 1.7741\n",
      "Epoch [10/75], Loss: 1.2199\n",
      "Epoch [10/75], Val Loss: 1.7719\n",
      "Epoch [11/75], Loss: 1.1199\n",
      "Epoch [11/75], Val Loss: 1.7121\n",
      "Epoch [12/75], Loss: 1.0010\n",
      "Epoch [12/75], Val Loss: 1.6646\n",
      "Epoch [13/75], Loss: 0.9334\n",
      "Epoch [13/75], Val Loss: 1.6433\n",
      "Epoch [14/75], Loss: 0.8093\n",
      "Epoch [14/75], Val Loss: 1.5498\n",
      "Epoch [15/75], Loss: 0.7362\n",
      "Epoch [15/75], Val Loss: 1.5875\n",
      "Epoch [16/75], Loss: 0.7148\n",
      "Epoch [16/75], Val Loss: 1.6664\n",
      "Epoch [17/75], Loss: 0.6171\n",
      "Epoch [17/75], Val Loss: 1.5192\n",
      "Epoch [18/75], Loss: 0.5237\n",
      "Epoch [18/75], Val Loss: 1.5393\n",
      "Epoch [19/75], Loss: 0.4603\n",
      "Epoch [19/75], Val Loss: 1.4672\n",
      "Epoch [20/75], Loss: 0.3787\n",
      "Epoch [20/75], Val Loss: 1.5778\n",
      "Epoch [21/75], Loss: 0.3318\n",
      "Epoch [21/75], Val Loss: 1.4990\n",
      "Epoch [22/75], Loss: 0.2956\n",
      "Epoch [22/75], Val Loss: 1.5607\n",
      "Stopping early at epoch 22 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [22/75], Loss: 0.2956\n",
      "Test Accuracy Base CNN: 62.01%\n",
      "Epoch [1/75], Loss: 3.3907\n",
      "Epoch [1/75], Val Loss: 3.2260\n",
      "Epoch [2/75], Loss: 3.1305\n",
      "Epoch [2/75], Val Loss: 3.0702\n",
      "Epoch [3/75], Loss: 2.9255\n",
      "Epoch [3/75], Val Loss: 2.8873\n",
      "Epoch [4/75], Loss: 2.6339\n",
      "Epoch [4/75], Val Loss: 2.6151\n",
      "Epoch [5/75], Loss: 2.2480\n",
      "Epoch [5/75], Val Loss: 2.2487\n",
      "Epoch [6/75], Loss: 1.8191\n",
      "Epoch [6/75], Val Loss: 1.9142\n",
      "Epoch [7/75], Loss: 1.5138\n",
      "Epoch [7/75], Val Loss: 1.7920\n",
      "Epoch [8/75], Loss: 1.3032\n",
      "Epoch [8/75], Val Loss: 1.7651\n",
      "Epoch [9/75], Loss: 1.1490\n",
      "Epoch [9/75], Val Loss: 1.7349\n",
      "Epoch [10/75], Loss: 1.0763\n",
      "Epoch [10/75], Val Loss: 1.6418\n",
      "Epoch [11/75], Loss: 0.9136\n",
      "Epoch [11/75], Val Loss: 1.6787\n",
      "Epoch [12/75], Loss: 0.8338\n",
      "Epoch [12/75], Val Loss: 1.6275\n",
      "Epoch [13/75], Loss: 0.7421\n",
      "Epoch [13/75], Val Loss: 1.5991\n",
      "Epoch [14/75], Loss: 0.6638\n",
      "Epoch [14/75], Val Loss: 1.5899\n",
      "Epoch [15/75], Loss: 0.5952\n",
      "Epoch [15/75], Val Loss: 1.4754\n",
      "Epoch [16/75], Loss: 0.4851\n",
      "Epoch [16/75], Val Loss: 1.5280\n",
      "Epoch [17/75], Loss: 0.3842\n",
      "Epoch [17/75], Val Loss: 1.5010\n",
      "Epoch [18/75], Loss: 0.3614\n",
      "Epoch [18/75], Val Loss: 1.5783\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [18/75], Loss: 0.3614\n",
      "Test Accuracy Base CNN: 59.05%\n",
      "Epoch [1/75], Loss: 3.3595\n",
      "Epoch [1/75], Val Loss: 3.3124\n",
      "Epoch [2/75], Loss: 3.2254\n",
      "Epoch [2/75], Val Loss: 3.2116\n",
      "Epoch [3/75], Loss: 3.0967\n",
      "Epoch [3/75], Val Loss: 3.1145\n",
      "Epoch [4/75], Loss: 2.9546\n",
      "Epoch [4/75], Val Loss: 2.9868\n",
      "Epoch [5/75], Loss: 2.7703\n",
      "Epoch [5/75], Val Loss: 2.8090\n",
      "Epoch [6/75], Loss: 2.5115\n",
      "Epoch [6/75], Val Loss: 2.5686\n",
      "Epoch [7/75], Loss: 2.1877\n",
      "Epoch [7/75], Val Loss: 2.2487\n",
      "Epoch [8/75], Loss: 1.8060\n",
      "Epoch [8/75], Val Loss: 1.9458\n",
      "Epoch [9/75], Loss: 1.4920\n",
      "Epoch [9/75], Val Loss: 1.7371\n",
      "Epoch [10/75], Loss: 1.3013\n",
      "Epoch [10/75], Val Loss: 1.7409\n",
      "Epoch [11/75], Loss: 1.1767\n",
      "Epoch [11/75], Val Loss: 1.6451\n",
      "Epoch [12/75], Loss: 1.0599\n",
      "Epoch [12/75], Val Loss: 1.5897\n",
      "Epoch [13/75], Loss: 0.9612\n",
      "Epoch [13/75], Val Loss: 1.5942\n",
      "Epoch [14/75], Loss: 0.9202\n",
      "Epoch [14/75], Val Loss: 1.6580\n",
      "Epoch [15/75], Loss: 0.7403\n",
      "Epoch [15/75], Val Loss: 1.5262\n",
      "Epoch [16/75], Loss: 0.6891\n",
      "Epoch [16/75], Val Loss: 1.5778\n",
      "Epoch [17/75], Loss: 0.6497\n",
      "Epoch [17/75], Val Loss: 1.5230\n",
      "Epoch [18/75], Loss: 0.5627\n",
      "Epoch [18/75], Val Loss: 1.5305\n",
      "Epoch [19/75], Loss: 0.5380\n",
      "Epoch [19/75], Val Loss: 1.4421\n",
      "Epoch [20/75], Loss: 0.4219\n",
      "Epoch [20/75], Val Loss: 1.5225\n",
      "Epoch [21/75], Loss: 0.3817\n",
      "Epoch [21/75], Val Loss: 1.4849\n",
      "Epoch [22/75], Loss: 0.3228\n",
      "Epoch [22/75], Val Loss: 1.4392\n",
      "Epoch [23/75], Loss: 0.2792\n",
      "Epoch [23/75], Val Loss: 1.5857\n",
      "Epoch [24/75], Loss: 0.2345\n",
      "Epoch [24/75], Val Loss: 1.4870\n",
      "Epoch [25/75], Loss: 0.2191\n",
      "Epoch [25/75], Val Loss: 1.5537\n",
      "Stopping early at epoch 25 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [25/75], Loss: 0.2191\n",
      "Test Accuracy Base CNN: 62.90%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 75\n",
    "tolerance = 1e-4  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "logit_accuracy_cnn = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "    print(f\"Training with subset size: {subset_size}\")\n",
    "\n",
    "    for _ in range(30):\n",
    "        cnn_model = EMNIST_CNN(num_classes).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(cnn_model.parameters(), lr=0.005, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "        def init_weights(m):\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                    m.bias.data.fill_(0.01)\n",
    "        cnn_model.apply(init_weights)\n",
    "\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = Subset(train_set, subset_indices)\n",
    "        train_loader = DataLoader(subset, batch_size=128, shuffle=True)\n",
    "\n",
    "        best_val_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            cnn_model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = cnn_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "            # Validation loop\n",
    "            cnn_model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader_cnn:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    outputs = cnn_model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader_cnn)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        cnn_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = cnn_model(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy Base CNN: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy_cnn[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000    58.691026\n",
       " 50000    58.113462\n",
       " 10000    58.961058\n",
       " 5000     58.189744\n",
       " 1000     58.517949\n",
       " dtype: float64,\n",
       " 75000    3.065160\n",
       " 50000    3.490867\n",
       " 10000    3.076392\n",
       " 5000     3.414451\n",
       " 1000     3.270635\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_accuracy_cnn_df = pd.DataFrame(logit_accuracy_cnn)\n",
    "logit_accuracy_cnn_df.mean(), logit_accuracy_cnn_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Lipschitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with subset size: 75000\n",
      "Epoch [1/75], Loss: 3.3661\n",
      "Epoch [1/75], Val Loss: 3.2430\n",
      "Epoch [2/75], Loss: 3.1305\n",
      "Epoch [2/75], Val Loss: 3.1189\n",
      "Epoch [3/75], Loss: 2.9506\n",
      "Epoch [3/75], Val Loss: 2.8956\n",
      "Epoch [4/75], Loss: 2.6276\n",
      "Epoch [4/75], Val Loss: 2.5203\n",
      "Epoch [5/75], Loss: 2.0891\n",
      "Epoch [5/75], Val Loss: 2.0087\n",
      "Epoch [6/75], Loss: 1.5938\n",
      "Epoch [6/75], Val Loss: 1.8960\n",
      "Epoch [7/75], Loss: 1.4597\n",
      "Epoch [7/75], Val Loss: 1.8930\n",
      "Epoch [8/75], Loss: 1.3161\n",
      "Epoch [8/75], Val Loss: 1.8796\n",
      "Epoch [9/75], Loss: 1.2388\n",
      "Epoch [9/75], Val Loss: 1.8387\n",
      "Epoch [10/75], Loss: 0.9902\n",
      "Epoch [10/75], Val Loss: 1.5769\n",
      "Epoch [11/75], Loss: 0.8399\n",
      "Epoch [11/75], Val Loss: 1.6159\n",
      "Epoch [12/75], Loss: 0.7813\n",
      "Epoch [12/75], Val Loss: 1.4787\n",
      "Epoch [13/75], Loss: 0.6041\n",
      "Epoch [13/75], Val Loss: 1.4373\n",
      "Epoch [14/75], Loss: 0.4957\n",
      "Epoch [14/75], Val Loss: 1.4362\n",
      "Epoch [15/75], Loss: 0.4293\n",
      "Epoch [15/75], Val Loss: 1.5391\n",
      "Epoch [16/75], Loss: 0.3214\n",
      "Epoch [16/75], Val Loss: 1.5584\n",
      "Epoch [17/75], Loss: 0.2670\n",
      "Epoch [17/75], Val Loss: 1.5851\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [17/75], Loss: 0.2670\n",
      "Test Accuracy CNN Lipschitz: 61.75%\n",
      "Epoch [1/75], Loss: 3.2944\n",
      "Epoch [1/75], Val Loss: 3.1697\n",
      "Epoch [2/75], Loss: 3.0711\n",
      "Epoch [2/75], Val Loss: 3.0412\n",
      "Epoch [3/75], Loss: 2.8048\n",
      "Epoch [3/75], Val Loss: 2.7188\n",
      "Epoch [4/75], Loss: 2.3332\n",
      "Epoch [4/75], Val Loss: 2.2340\n",
      "Epoch [5/75], Loss: 1.7304\n",
      "Epoch [5/75], Val Loss: 1.8958\n",
      "Epoch [6/75], Loss: 1.5137\n",
      "Epoch [6/75], Val Loss: 2.1058\n",
      "Epoch [7/75], Loss: 1.5060\n",
      "Epoch [7/75], Val Loss: 2.0753\n",
      "Epoch [8/75], Loss: 1.3516\n",
      "Epoch [8/75], Val Loss: 1.7218\n",
      "Epoch [9/75], Loss: 1.0936\n",
      "Epoch [9/75], Val Loss: 1.6749\n",
      "Epoch [10/75], Loss: 0.8755\n",
      "Epoch [10/75], Val Loss: 1.5967\n",
      "Epoch [11/75], Loss: 0.7486\n",
      "Epoch [11/75], Val Loss: 1.4250\n",
      "Epoch [12/75], Loss: 0.6082\n",
      "Epoch [12/75], Val Loss: 1.4201\n",
      "Epoch [13/75], Loss: 0.4690\n",
      "Epoch [13/75], Val Loss: 1.4291\n",
      "Epoch [14/75], Loss: 0.3868\n",
      "Epoch [14/75], Val Loss: 1.4714\n",
      "Epoch [15/75], Loss: 0.2997\n",
      "Epoch [15/75], Val Loss: 1.5359\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [15/75], Loss: 0.2997\n",
      "Test Accuracy CNN Lipschitz: 62.71%\n",
      "Epoch [1/75], Loss: 3.3602\n",
      "Epoch [1/75], Val Loss: 3.2117\n",
      "Epoch [2/75], Loss: 3.0939\n",
      "Epoch [2/75], Val Loss: 3.0575\n",
      "Epoch [3/75], Loss: 2.8362\n",
      "Epoch [3/75], Val Loss: 2.7677\n",
      "Epoch [4/75], Loss: 2.4280\n",
      "Epoch [4/75], Val Loss: 2.3370\n",
      "Epoch [5/75], Loss: 1.9398\n",
      "Epoch [5/75], Val Loss: 2.0164\n",
      "Epoch [6/75], Loss: 1.6714\n",
      "Epoch [6/75], Val Loss: 2.0902\n",
      "Epoch [7/75], Loss: 1.6958\n",
      "Epoch [7/75], Val Loss: 1.9804\n",
      "Epoch [8/75], Loss: 1.4208\n",
      "Epoch [8/75], Val Loss: 1.8699\n",
      "Epoch [9/75], Loss: 1.1674\n",
      "Epoch [9/75], Val Loss: 1.6452\n",
      "Epoch [10/75], Loss: 1.0129\n",
      "Epoch [10/75], Val Loss: 1.5107\n",
      "Epoch [11/75], Loss: 0.8230\n",
      "Epoch [11/75], Val Loss: 1.4836\n",
      "Epoch [12/75], Loss: 0.7065\n",
      "Epoch [12/75], Val Loss: 1.4646\n",
      "Epoch [13/75], Loss: 0.6061\n",
      "Epoch [13/75], Val Loss: 1.4874\n",
      "Epoch [14/75], Loss: 0.4805\n",
      "Epoch [14/75], Val Loss: 1.5036\n",
      "Epoch [15/75], Loss: 0.4022\n",
      "Epoch [15/75], Val Loss: 1.5218\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [15/75], Loss: 0.4022\n",
      "Test Accuracy CNN Lipschitz: 61.11%\n",
      "Epoch [1/75], Loss: 3.3236\n",
      "Epoch [1/75], Val Loss: 3.1844\n",
      "Epoch [2/75], Loss: 3.0424\n",
      "Epoch [2/75], Val Loss: 2.9724\n",
      "Epoch [3/75], Loss: 2.7055\n",
      "Epoch [3/75], Val Loss: 2.5737\n",
      "Epoch [4/75], Loss: 2.1894\n",
      "Epoch [4/75], Val Loss: 2.0637\n",
      "Epoch [5/75], Loss: 1.7864\n",
      "Epoch [5/75], Val Loss: 1.8882\n",
      "Epoch [6/75], Loss: 2.2162\n",
      "Epoch [6/75], Val Loss: 3.1697\n",
      "Epoch [7/75], Loss: 2.6957\n",
      "Epoch [7/75], Val Loss: 2.1413\n",
      "Epoch [8/75], Loss: 1.7835\n",
      "Epoch [8/75], Val Loss: 2.0560\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [8/75], Loss: 1.7835\n",
      "Test Accuracy CNN Lipschitz: 46.38%\n",
      "Epoch [1/75], Loss: 3.3378\n",
      "Epoch [1/75], Val Loss: 3.1899\n",
      "Epoch [2/75], Loss: 3.0764\n",
      "Epoch [2/75], Val Loss: 3.0324\n",
      "Epoch [3/75], Loss: 2.7740\n",
      "Epoch [3/75], Val Loss: 2.7155\n",
      "Epoch [4/75], Loss: 2.2912\n",
      "Epoch [4/75], Val Loss: 2.2107\n",
      "Epoch [5/75], Loss: 1.7768\n",
      "Epoch [5/75], Val Loss: 2.1321\n",
      "Epoch [6/75], Loss: 1.6571\n",
      "Epoch [6/75], Val Loss: 1.9358\n",
      "Epoch [7/75], Loss: 1.4547\n",
      "Epoch [7/75], Val Loss: 1.9786\n",
      "Epoch [8/75], Loss: 1.2310\n",
      "Epoch [8/75], Val Loss: 1.8995\n",
      "Epoch [9/75], Loss: 1.1680\n",
      "Epoch [9/75], Val Loss: 1.7107\n",
      "Epoch [10/75], Loss: 0.9505\n",
      "Epoch [10/75], Val Loss: 1.4764\n",
      "Epoch [11/75], Loss: 0.7628\n",
      "Epoch [11/75], Val Loss: 1.5248\n",
      "Epoch [12/75], Loss: 0.6440\n",
      "Epoch [12/75], Val Loss: 1.4826\n",
      "Epoch [13/75], Loss: 0.4965\n",
      "Epoch [13/75], Val Loss: 1.4722\n",
      "Epoch [14/75], Loss: 0.3979\n",
      "Epoch [14/75], Val Loss: 1.5194\n",
      "Epoch [15/75], Loss: 0.3254\n",
      "Epoch [15/75], Val Loss: 1.6248\n",
      "Epoch [16/75], Loss: 0.2612\n",
      "Epoch [16/75], Val Loss: 1.5956\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [16/75], Loss: 0.2612\n",
      "Test Accuracy CNN Lipschitz: 61.69%\n",
      "Epoch [1/75], Loss: 3.2955\n",
      "Epoch [1/75], Val Loss: 3.2183\n",
      "Epoch [2/75], Loss: 3.1155\n",
      "Epoch [2/75], Val Loss: 3.0692\n",
      "Epoch [3/75], Loss: 2.8686\n",
      "Epoch [3/75], Val Loss: 2.8133\n",
      "Epoch [4/75], Loss: 2.4423\n",
      "Epoch [4/75], Val Loss: 2.3360\n",
      "Epoch [5/75], Loss: 1.9048\n",
      "Epoch [5/75], Val Loss: 1.9103\n",
      "Epoch [6/75], Loss: 1.6279\n",
      "Epoch [6/75], Val Loss: 1.8849\n",
      "Epoch [7/75], Loss: 1.3895\n",
      "Epoch [7/75], Val Loss: 1.8105\n",
      "Epoch [8/75], Loss: 1.3179\n",
      "Epoch [8/75], Val Loss: 1.6996\n",
      "Epoch [9/75], Loss: 1.1409\n",
      "Epoch [9/75], Val Loss: 1.8350\n",
      "Epoch [10/75], Loss: 1.0770\n",
      "Epoch [10/75], Val Loss: 1.5617\n",
      "Epoch [11/75], Loss: 0.8309\n",
      "Epoch [11/75], Val Loss: 1.4878\n",
      "Epoch [12/75], Loss: 0.6561\n",
      "Epoch [12/75], Val Loss: 1.5105\n",
      "Epoch [13/75], Loss: 0.5391\n",
      "Epoch [13/75], Val Loss: 1.4621\n",
      "Epoch [14/75], Loss: 0.4130\n",
      "Epoch [14/75], Val Loss: 1.5400\n",
      "Epoch [15/75], Loss: 0.3083\n",
      "Epoch [15/75], Val Loss: 1.5923\n",
      "Epoch [16/75], Loss: 0.2632\n",
      "Epoch [16/75], Val Loss: 1.5939\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [16/75], Loss: 0.2632\n",
      "Test Accuracy CNN Lipschitz: 63.28%\n",
      "Epoch [1/75], Loss: 3.2885\n",
      "Epoch [1/75], Val Loss: 3.2218\n",
      "Epoch [2/75], Loss: 3.1445\n",
      "Epoch [2/75], Val Loss: 3.0950\n",
      "Epoch [3/75], Loss: 2.9466\n",
      "Epoch [3/75], Val Loss: 2.9019\n",
      "Epoch [4/75], Loss: 2.6189\n",
      "Epoch [4/75], Val Loss: 2.4820\n",
      "Epoch [5/75], Loss: 2.1164\n",
      "Epoch [5/75], Val Loss: 2.0448\n",
      "Epoch [6/75], Loss: 1.7119\n",
      "Epoch [6/75], Val Loss: 2.1492\n",
      "Epoch [7/75], Loss: 1.6081\n",
      "Epoch [7/75], Val Loss: 2.1822\n",
      "Epoch [8/75], Loss: 1.5125\n",
      "Epoch [8/75], Val Loss: 2.1825\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [8/75], Loss: 1.5125\n",
      "Test Accuracy CNN Lipschitz: 46.06%\n",
      "Epoch [1/75], Loss: 3.3250\n",
      "Epoch [1/75], Val Loss: 3.1723\n",
      "Epoch [2/75], Loss: 3.0332\n",
      "Epoch [2/75], Val Loss: 2.9874\n",
      "Epoch [3/75], Loss: 2.6733\n",
      "Epoch [3/75], Val Loss: 2.5591\n",
      "Epoch [4/75], Loss: 2.1276\n",
      "Epoch [4/75], Val Loss: 2.1058\n",
      "Epoch [5/75], Loss: 1.7445\n",
      "Epoch [5/75], Val Loss: 1.9111\n",
      "Epoch [6/75], Loss: 1.5160\n",
      "Epoch [6/75], Val Loss: 2.0388\n",
      "Epoch [7/75], Loss: 1.4851\n",
      "Epoch [7/75], Val Loss: 1.7596\n",
      "Epoch [8/75], Loss: 1.2415\n",
      "Epoch [8/75], Val Loss: 1.6878\n",
      "Epoch [9/75], Loss: 0.9731\n",
      "Epoch [9/75], Val Loss: 1.5951\n",
      "Epoch [10/75], Loss: 0.8587\n",
      "Epoch [10/75], Val Loss: 1.3936\n",
      "Epoch [11/75], Loss: 0.7141\n",
      "Epoch [11/75], Val Loss: 1.4189\n",
      "Epoch [12/75], Loss: 0.5341\n",
      "Epoch [12/75], Val Loss: 1.4135\n",
      "Epoch [13/75], Loss: 0.4455\n",
      "Epoch [13/75], Val Loss: 1.4425\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [13/75], Loss: 0.4455\n",
      "Test Accuracy CNN Lipschitz: 62.77%\n",
      "Epoch [1/75], Loss: 3.3909\n",
      "Epoch [1/75], Val Loss: 3.2565\n",
      "Epoch [2/75], Loss: 3.1152\n",
      "Epoch [2/75], Val Loss: 3.0962\n",
      "Epoch [3/75], Loss: 2.8675\n",
      "Epoch [3/75], Val Loss: 2.8216\n",
      "Epoch [4/75], Loss: 2.4347\n",
      "Epoch [4/75], Val Loss: 2.3333\n",
      "Epoch [5/75], Loss: 1.8470\n",
      "Epoch [5/75], Val Loss: 1.9634\n",
      "Epoch [6/75], Loss: 1.5343\n",
      "Epoch [6/75], Val Loss: 2.1624\n",
      "Epoch [7/75], Loss: 1.7160\n",
      "Epoch [7/75], Val Loss: 2.7198\n",
      "Epoch [8/75], Loss: 2.1279\n",
      "Epoch [8/75], Val Loss: 2.4978\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [8/75], Loss: 2.1279\n",
      "Test Accuracy CNN Lipschitz: 44.43%\n",
      "Epoch [1/75], Loss: 3.3279\n",
      "Epoch [1/75], Val Loss: 3.2122\n",
      "Epoch [2/75], Loss: 3.1084\n",
      "Epoch [2/75], Val Loss: 3.0641\n",
      "Epoch [3/75], Loss: 2.8325\n",
      "Epoch [3/75], Val Loss: 2.7309\n",
      "Epoch [4/75], Loss: 2.3543\n",
      "Epoch [4/75], Val Loss: 2.1955\n",
      "Epoch [5/75], Loss: 1.7466\n",
      "Epoch [5/75], Val Loss: 1.9509\n",
      "Epoch [6/75], Loss: 1.5212\n",
      "Epoch [6/75], Val Loss: 1.8810\n",
      "Epoch [7/75], Loss: 1.3203\n",
      "Epoch [7/75], Val Loss: 2.1325\n",
      "Epoch [8/75], Loss: 1.5190\n",
      "Epoch [8/75], Val Loss: 2.0222\n",
      "Epoch [9/75], Loss: 1.2811\n",
      "Epoch [9/75], Val Loss: 1.7264\n",
      "Epoch [10/75], Loss: 1.0343\n",
      "Epoch [10/75], Val Loss: 1.5803\n",
      "Epoch [11/75], Loss: 0.9124\n",
      "Epoch [11/75], Val Loss: 1.5005\n",
      "Epoch [12/75], Loss: 0.7197\n",
      "Epoch [12/75], Val Loss: 1.3745\n",
      "Epoch [13/75], Loss: 0.5259\n",
      "Epoch [13/75], Val Loss: 1.4294\n",
      "Epoch [14/75], Loss: 0.4164\n",
      "Epoch [14/75], Val Loss: 1.4987\n",
      "Epoch [15/75], Loss: 0.3509\n",
      "Epoch [15/75], Val Loss: 1.5569\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [15/75], Loss: 0.3509\n",
      "Test Accuracy CNN Lipschitz: 62.23%\n",
      "Epoch [1/75], Loss: 3.3143\n",
      "Epoch [1/75], Val Loss: 3.2312\n",
      "Epoch [2/75], Loss: 3.1413\n",
      "Epoch [2/75], Val Loss: 3.1062\n",
      "Epoch [3/75], Loss: 2.9446\n",
      "Epoch [3/75], Val Loss: 2.9192\n",
      "Epoch [4/75], Loss: 2.6154\n",
      "Epoch [4/75], Val Loss: 2.5049\n",
      "Epoch [5/75], Loss: 2.1058\n",
      "Epoch [5/75], Val Loss: 2.0439\n",
      "Epoch [6/75], Loss: 1.6588\n",
      "Epoch [6/75], Val Loss: 1.9715\n",
      "Epoch [7/75], Loss: 1.6201\n",
      "Epoch [7/75], Val Loss: 1.8479\n",
      "Epoch [8/75], Loss: 1.4791\n",
      "Epoch [8/75], Val Loss: 2.3608\n",
      "Epoch [9/75], Loss: 1.4417\n",
      "Epoch [9/75], Val Loss: 1.6848\n",
      "Epoch [10/75], Loss: 1.0902\n",
      "Epoch [10/75], Val Loss: 1.6388\n",
      "Epoch [11/75], Loss: 0.9705\n",
      "Epoch [11/75], Val Loss: 1.5254\n",
      "Epoch [12/75], Loss: 0.8077\n",
      "Epoch [12/75], Val Loss: 1.4500\n",
      "Epoch [13/75], Loss: 0.6279\n",
      "Epoch [13/75], Val Loss: 1.4812\n",
      "Epoch [14/75], Loss: 0.4950\n",
      "Epoch [14/75], Val Loss: 1.5457\n",
      "Epoch [15/75], Loss: 0.4473\n",
      "Epoch [15/75], Val Loss: 1.5255\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [15/75], Loss: 0.4473\n",
      "Test Accuracy CNN Lipschitz: 61.23%\n",
      "Epoch [1/75], Loss: 3.2962\n",
      "Epoch [1/75], Val Loss: 3.2311\n",
      "Epoch [2/75], Loss: 3.1396\n",
      "Epoch [2/75], Val Loss: 3.1090\n",
      "Epoch [3/75], Loss: 2.9251\n",
      "Epoch [3/75], Val Loss: 2.9039\n",
      "Epoch [4/75], Loss: 2.5911\n",
      "Epoch [4/75], Val Loss: 2.4919\n",
      "Epoch [5/75], Loss: 2.0740\n",
      "Epoch [5/75], Val Loss: 2.0570\n",
      "Epoch [6/75], Loss: 1.6309\n",
      "Epoch [6/75], Val Loss: 2.0224\n",
      "Epoch [7/75], Loss: 1.7349\n",
      "Epoch [7/75], Val Loss: 2.4078\n",
      "Epoch [8/75], Loss: 1.6522\n",
      "Epoch [8/75], Val Loss: 2.1841\n",
      "Epoch [9/75], Loss: 1.3556\n",
      "Epoch [9/75], Val Loss: 1.7553\n",
      "Epoch [10/75], Loss: 1.1395\n",
      "Epoch [10/75], Val Loss: 1.6285\n",
      "Epoch [11/75], Loss: 0.9574\n",
      "Epoch [11/75], Val Loss: 1.5861\n",
      "Epoch [12/75], Loss: 0.7561\n",
      "Epoch [12/75], Val Loss: 1.5332\n",
      "Epoch [13/75], Loss: 0.6124\n",
      "Epoch [13/75], Val Loss: 1.5186\n",
      "Epoch [14/75], Loss: 0.5185\n",
      "Epoch [14/75], Val Loss: 1.6587\n",
      "Epoch [15/75], Loss: 0.4745\n",
      "Epoch [15/75], Val Loss: 1.6160\n",
      "Epoch [16/75], Loss: 0.3915\n",
      "Epoch [16/75], Val Loss: 1.6408\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [16/75], Loss: 0.3915\n",
      "Test Accuracy CNN Lipschitz: 60.76%\n",
      "Epoch [1/75], Loss: 3.3202\n",
      "Epoch [1/75], Val Loss: 3.2413\n",
      "Epoch [2/75], Loss: 3.1427\n",
      "Epoch [2/75], Val Loss: 3.1078\n",
      "Epoch [3/75], Loss: 2.9372\n",
      "Epoch [3/75], Val Loss: 2.9171\n",
      "Epoch [4/75], Loss: 2.6070\n",
      "Epoch [4/75], Val Loss: 2.5359\n",
      "Epoch [5/75], Loss: 2.0991\n",
      "Epoch [5/75], Val Loss: 2.0605\n",
      "Epoch [6/75], Loss: 1.6407\n",
      "Epoch [6/75], Val Loss: 1.9464\n",
      "Epoch [7/75], Loss: 1.4791\n",
      "Epoch [7/75], Val Loss: 2.3236\n",
      "Epoch [8/75], Loss: 1.5139\n",
      "Epoch [8/75], Val Loss: 1.8771\n",
      "Epoch [9/75], Loss: 1.2165\n",
      "Epoch [9/75], Val Loss: 1.6931\n",
      "Epoch [10/75], Loss: 1.0823\n",
      "Epoch [10/75], Val Loss: 1.5543\n",
      "Epoch [11/75], Loss: 0.8872\n",
      "Epoch [11/75], Val Loss: 1.5092\n",
      "Epoch [12/75], Loss: 0.7429\n",
      "Epoch [12/75], Val Loss: 1.4184\n",
      "Epoch [13/75], Loss: 0.6117\n",
      "Epoch [13/75], Val Loss: 1.3900\n",
      "Epoch [14/75], Loss: 0.4944\n",
      "Epoch [14/75], Val Loss: 1.4873\n",
      "Epoch [15/75], Loss: 0.4203\n",
      "Epoch [15/75], Val Loss: 1.4984\n",
      "Epoch [16/75], Loss: 0.3590\n",
      "Epoch [16/75], Val Loss: 1.5405\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [16/75], Loss: 0.3590\n",
      "Test Accuracy CNN Lipschitz: 62.76%\n",
      "Epoch [1/75], Loss: 3.2835\n",
      "Epoch [1/75], Val Loss: 3.2083\n",
      "Epoch [2/75], Loss: 3.0852\n",
      "Epoch [2/75], Val Loss: 3.0616\n",
      "Epoch [3/75], Loss: 2.8163\n",
      "Epoch [3/75], Val Loss: 2.7522\n",
      "Epoch [4/75], Loss: 2.3504\n",
      "Epoch [4/75], Val Loss: 2.2302\n",
      "Epoch [5/75], Loss: 1.8229\n",
      "Epoch [5/75], Val Loss: 1.9639\n",
      "Epoch [6/75], Loss: 1.6081\n",
      "Epoch [6/75], Val Loss: 1.9690\n",
      "Epoch [7/75], Loss: 1.3106\n",
      "Epoch [7/75], Val Loss: 1.7908\n",
      "Epoch [8/75], Loss: 1.0960\n",
      "Epoch [8/75], Val Loss: 1.6547\n",
      "Epoch [9/75], Loss: 0.9352\n",
      "Epoch [9/75], Val Loss: 1.6185\n",
      "Epoch [10/75], Loss: 0.7974\n",
      "Epoch [10/75], Val Loss: 1.5646\n",
      "Epoch [11/75], Loss: 0.7262\n",
      "Epoch [11/75], Val Loss: 1.5307\n",
      "Epoch [12/75], Loss: 0.5991\n",
      "Epoch [12/75], Val Loss: 1.5388\n",
      "Epoch [13/75], Loss: 0.4887\n",
      "Epoch [13/75], Val Loss: 1.5563\n",
      "Epoch [14/75], Loss: 0.4074\n",
      "Epoch [14/75], Val Loss: 1.6465\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [14/75], Loss: 0.4074\n",
      "Test Accuracy CNN Lipschitz: 58.92%\n",
      "Epoch [1/75], Loss: 3.2810\n",
      "Epoch [1/75], Val Loss: 3.2219\n",
      "Epoch [2/75], Loss: 3.1433\n",
      "Epoch [2/75], Val Loss: 3.1390\n",
      "Epoch [3/75], Loss: 2.9684\n",
      "Epoch [3/75], Val Loss: 2.9634\n",
      "Epoch [4/75], Loss: 2.6751\n",
      "Epoch [4/75], Val Loss: 2.6290\n",
      "Epoch [5/75], Loss: 2.2151\n",
      "Epoch [5/75], Val Loss: 2.1446\n",
      "Epoch [6/75], Loss: 1.7255\n",
      "Epoch [6/75], Val Loss: 2.0599\n",
      "Epoch [7/75], Loss: 1.5574\n",
      "Epoch [7/75], Val Loss: 1.9818\n",
      "Epoch [8/75], Loss: 1.3703\n",
      "Epoch [8/75], Val Loss: 2.1609\n",
      "Epoch [9/75], Loss: 1.2530\n",
      "Epoch [9/75], Val Loss: 1.7994\n",
      "Epoch [10/75], Loss: 1.0568\n",
      "Epoch [10/75], Val Loss: 1.6343\n",
      "Epoch [11/75], Loss: 0.9068\n",
      "Epoch [11/75], Val Loss: 1.7208\n",
      "Epoch [12/75], Loss: 0.8657\n",
      "Epoch [12/75], Val Loss: 1.4528\n",
      "Epoch [13/75], Loss: 0.6381\n",
      "Epoch [13/75], Val Loss: 1.4715\n",
      "Epoch [14/75], Loss: 0.4999\n",
      "Epoch [14/75], Val Loss: 1.4261\n",
      "Epoch [15/75], Loss: 0.4251\n",
      "Epoch [15/75], Val Loss: 1.4504\n",
      "Epoch [16/75], Loss: 0.3173\n",
      "Epoch [16/75], Val Loss: 1.4944\n",
      "Epoch [17/75], Loss: 0.2678\n",
      "Epoch [17/75], Val Loss: 1.4844\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [17/75], Loss: 0.2678\n",
      "Test Accuracy CNN Lipschitz: 63.58%\n",
      "Epoch [1/75], Loss: 3.3320\n",
      "Epoch [1/75], Val Loss: 3.2179\n",
      "Epoch [2/75], Loss: 3.0876\n",
      "Epoch [2/75], Val Loss: 3.0455\n",
      "Epoch [3/75], Loss: 2.8272\n",
      "Epoch [3/75], Val Loss: 2.7532\n",
      "Epoch [4/75], Loss: 2.3760\n",
      "Epoch [4/75], Val Loss: 2.2640\n",
      "Epoch [5/75], Loss: 1.8623\n",
      "Epoch [5/75], Val Loss: 2.0271\n",
      "Epoch [6/75], Loss: 1.5979\n",
      "Epoch [6/75], Val Loss: 1.8273\n",
      "Epoch [7/75], Loss: 1.4513\n",
      "Epoch [7/75], Val Loss: 2.4909\n",
      "Epoch [8/75], Loss: 1.6011\n",
      "Epoch [8/75], Val Loss: 2.0063\n",
      "Epoch [9/75], Loss: 1.3873\n",
      "Epoch [9/75], Val Loss: 1.7704\n",
      "Epoch [10/75], Loss: 1.2061\n",
      "Epoch [10/75], Val Loss: 1.6676\n",
      "Epoch [11/75], Loss: 1.0322\n",
      "Epoch [11/75], Val Loss: 1.5466\n",
      "Epoch [12/75], Loss: 0.8362\n",
      "Epoch [12/75], Val Loss: 1.4462\n",
      "Epoch [13/75], Loss: 0.7049\n",
      "Epoch [13/75], Val Loss: 1.5000\n",
      "Epoch [14/75], Loss: 0.5441\n",
      "Epoch [14/75], Val Loss: 1.5093\n",
      "Epoch [15/75], Loss: 0.4520\n",
      "Epoch [15/75], Val Loss: 1.5275\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [15/75], Loss: 0.4520\n",
      "Test Accuracy CNN Lipschitz: 61.26%\n",
      "Epoch [1/75], Loss: 3.3156\n",
      "Epoch [1/75], Val Loss: 3.1609\n",
      "Epoch [2/75], Loss: 3.0664\n",
      "Epoch [2/75], Val Loss: 2.9945\n",
      "Epoch [3/75], Loss: 2.7683\n",
      "Epoch [3/75], Val Loss: 2.6499\n",
      "Epoch [4/75], Loss: 2.2553\n",
      "Epoch [4/75], Val Loss: 2.1396\n",
      "Epoch [5/75], Loss: 1.7457\n",
      "Epoch [5/75], Val Loss: 1.8945\n",
      "Epoch [6/75], Loss: 1.4549\n",
      "Epoch [6/75], Val Loss: 1.9594\n",
      "Epoch [7/75], Loss: 1.4452\n",
      "Epoch [7/75], Val Loss: 1.8798\n",
      "Epoch [8/75], Loss: 1.1911\n",
      "Epoch [8/75], Val Loss: 1.8226\n",
      "Epoch [9/75], Loss: 1.0492\n",
      "Epoch [9/75], Val Loss: 1.7458\n",
      "Epoch [10/75], Loss: 0.9252\n",
      "Epoch [10/75], Val Loss: 1.6311\n",
      "Epoch [11/75], Loss: 0.8288\n",
      "Epoch [11/75], Val Loss: 1.4414\n",
      "Epoch [12/75], Loss: 0.7184\n",
      "Epoch [12/75], Val Loss: 1.4732\n",
      "Epoch [13/75], Loss: 0.5102\n",
      "Epoch [13/75], Val Loss: 1.4478\n",
      "Epoch [14/75], Loss: 0.4408\n",
      "Epoch [14/75], Val Loss: 1.4945\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [14/75], Loss: 0.4408\n",
      "Test Accuracy CNN Lipschitz: 60.82%\n",
      "Epoch [1/75], Loss: 3.3469\n",
      "Epoch [1/75], Val Loss: 3.2147\n",
      "Epoch [2/75], Loss: 3.1130\n",
      "Epoch [2/75], Val Loss: 3.0648\n",
      "Epoch [3/75], Loss: 2.8615\n",
      "Epoch [3/75], Val Loss: 2.7812\n",
      "Epoch [4/75], Loss: 2.4112\n",
      "Epoch [4/75], Val Loss: 2.2854\n",
      "Epoch [5/75], Loss: 1.8228\n",
      "Epoch [5/75], Val Loss: 1.9403\n",
      "Epoch [6/75], Loss: 1.6430\n",
      "Epoch [6/75], Val Loss: 2.3826\n",
      "Epoch [7/75], Loss: 1.7999\n",
      "Epoch [7/75], Val Loss: 2.4537\n",
      "Epoch [8/75], Loss: 1.7550\n",
      "Epoch [8/75], Val Loss: 1.7727\n",
      "Epoch [9/75], Loss: 1.3258\n",
      "Epoch [9/75], Val Loss: 1.7186\n",
      "Epoch [10/75], Loss: 1.1702\n",
      "Epoch [10/75], Val Loss: 1.5913\n",
      "Epoch [11/75], Loss: 0.9242\n",
      "Epoch [11/75], Val Loss: 1.4788\n",
      "Epoch [12/75], Loss: 0.7460\n",
      "Epoch [12/75], Val Loss: 1.4609\n",
      "Epoch [13/75], Loss: 0.5954\n",
      "Epoch [13/75], Val Loss: 1.5621\n",
      "Epoch [14/75], Loss: 0.5064\n",
      "Epoch [14/75], Val Loss: 1.5650\n",
      "Epoch [15/75], Loss: 0.4283\n",
      "Epoch [15/75], Val Loss: 1.5405\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [15/75], Loss: 0.4283\n",
      "Test Accuracy CNN Lipschitz: 61.50%\n",
      "Epoch [1/75], Loss: 3.2694\n",
      "Epoch [1/75], Val Loss: 3.1729\n",
      "Epoch [2/75], Loss: 3.0337\n",
      "Epoch [2/75], Val Loss: 2.9947\n",
      "Epoch [3/75], Loss: 2.7209\n",
      "Epoch [3/75], Val Loss: 2.6026\n",
      "Epoch [4/75], Loss: 2.1836\n",
      "Epoch [4/75], Val Loss: 2.0663\n",
      "Epoch [5/75], Loss: 1.6577\n",
      "Epoch [5/75], Val Loss: 1.9422\n",
      "Epoch [6/75], Loss: 1.5237\n",
      "Epoch [6/75], Val Loss: 2.2844\n",
      "Epoch [7/75], Loss: 1.5385\n",
      "Epoch [7/75], Val Loss: 2.1377\n",
      "Epoch [8/75], Loss: 1.4942\n",
      "Epoch [8/75], Val Loss: 1.9040\n",
      "Epoch [9/75], Loss: 1.1299\n",
      "Epoch [9/75], Val Loss: 1.6957\n",
      "Epoch [10/75], Loss: 0.9577\n",
      "Epoch [10/75], Val Loss: 1.4593\n",
      "Epoch [11/75], Loss: 0.7960\n",
      "Epoch [11/75], Val Loss: 1.4856\n",
      "Epoch [12/75], Loss: 0.5977\n",
      "Epoch [12/75], Val Loss: 1.4894\n",
      "Epoch [13/75], Loss: 0.5027\n",
      "Epoch [13/75], Val Loss: 1.5625\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [13/75], Loss: 0.5027\n",
      "Test Accuracy CNN Lipschitz: 60.95%\n",
      "Epoch [1/75], Loss: 3.2950\n",
      "Epoch [1/75], Val Loss: 3.2311\n",
      "Epoch [2/75], Loss: 3.1031\n",
      "Epoch [2/75], Val Loss: 3.0922\n",
      "Epoch [3/75], Loss: 2.8680\n",
      "Epoch [3/75], Val Loss: 2.8591\n",
      "Epoch [4/75], Loss: 2.4829\n",
      "Epoch [4/75], Val Loss: 2.4236\n",
      "Epoch [5/75], Loss: 1.9770\n",
      "Epoch [5/75], Val Loss: 2.0820\n",
      "Epoch [6/75], Loss: 1.6433\n",
      "Epoch [6/75], Val Loss: 2.0608\n",
      "Epoch [7/75], Loss: 1.5399\n",
      "Epoch [7/75], Val Loss: 1.9931\n",
      "Epoch [8/75], Loss: 1.5378\n",
      "Epoch [8/75], Val Loss: 2.2317\n",
      "Epoch [9/75], Loss: 1.4459\n",
      "Epoch [9/75], Val Loss: 1.6531\n",
      "Epoch [10/75], Loss: 1.0060\n",
      "Epoch [10/75], Val Loss: 1.5648\n",
      "Epoch [11/75], Loss: 0.8737\n",
      "Epoch [11/75], Val Loss: 1.4119\n",
      "Epoch [12/75], Loss: 0.6572\n",
      "Epoch [12/75], Val Loss: 1.4519\n",
      "Epoch [13/75], Loss: 0.5812\n",
      "Epoch [13/75], Val Loss: 1.4397\n",
      "Epoch [14/75], Loss: 0.5097\n",
      "Epoch [14/75], Val Loss: 1.5198\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [14/75], Loss: 0.5097\n",
      "Test Accuracy CNN Lipschitz: 60.90%\n",
      "Epoch [1/75], Loss: 3.2965\n",
      "Epoch [1/75], Val Loss: 3.1997\n",
      "Epoch [2/75], Loss: 3.0786\n",
      "Epoch [2/75], Val Loss: 3.0432\n",
      "Epoch [3/75], Loss: 2.8442\n",
      "Epoch [3/75], Val Loss: 2.7760\n",
      "Epoch [4/75], Loss: 2.4101\n",
      "Epoch [4/75], Val Loss: 2.3171\n",
      "Epoch [5/75], Loss: 1.9123\n",
      "Epoch [5/75], Val Loss: 1.9575\n",
      "Epoch [6/75], Loss: 1.5976\n",
      "Epoch [6/75], Val Loss: 2.0603\n",
      "Epoch [7/75], Loss: 1.5082\n",
      "Epoch [7/75], Val Loss: 2.1697\n",
      "Epoch [8/75], Loss: 1.5024\n",
      "Epoch [8/75], Val Loss: 2.0587\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [8/75], Loss: 1.5024\n",
      "Test Accuracy CNN Lipschitz: 46.24%\n",
      "Epoch [1/75], Loss: 3.3440\n",
      "Epoch [1/75], Val Loss: 3.2326\n",
      "Epoch [2/75], Loss: 3.0963\n",
      "Epoch [2/75], Val Loss: 3.0566\n",
      "Epoch [3/75], Loss: 2.8137\n",
      "Epoch [3/75], Val Loss: 2.7034\n",
      "Epoch [4/75], Loss: 2.3249\n",
      "Epoch [4/75], Val Loss: 2.1882\n",
      "Epoch [5/75], Loss: 1.8597\n",
      "Epoch [5/75], Val Loss: 2.1971\n",
      "Epoch [6/75], Loss: 1.8358\n",
      "Epoch [6/75], Val Loss: 2.1425\n",
      "Epoch [7/75], Loss: 1.5485\n",
      "Epoch [7/75], Val Loss: 1.8269\n",
      "Epoch [8/75], Loss: 1.2020\n",
      "Epoch [8/75], Val Loss: 1.6059\n",
      "Epoch [9/75], Loss: 1.0417\n",
      "Epoch [9/75], Val Loss: 1.6160\n",
      "Epoch [10/75], Loss: 0.9709\n",
      "Epoch [10/75], Val Loss: 1.4719\n",
      "Epoch [11/75], Loss: 0.7976\n",
      "Epoch [11/75], Val Loss: 1.4769\n",
      "Epoch [12/75], Loss: 0.6318\n",
      "Epoch [12/75], Val Loss: 1.5072\n",
      "Epoch [13/75], Loss: 0.5482\n",
      "Epoch [13/75], Val Loss: 1.4756\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [13/75], Loss: 0.5482\n",
      "Test Accuracy CNN Lipschitz: 60.59%\n",
      "Epoch [1/75], Loss: 3.2684\n",
      "Epoch [1/75], Val Loss: 3.1763\n",
      "Epoch [2/75], Loss: 3.0299\n",
      "Epoch [2/75], Val Loss: 2.9831\n",
      "Epoch [3/75], Loss: 2.7090\n",
      "Epoch [3/75], Val Loss: 2.5831\n",
      "Epoch [4/75], Loss: 2.1762\n",
      "Epoch [4/75], Val Loss: 2.1973\n",
      "Epoch [5/75], Loss: 1.8528\n",
      "Epoch [5/75], Val Loss: 1.9454\n",
      "Epoch [6/75], Loss: 1.4938\n",
      "Epoch [6/75], Val Loss: 2.1562\n",
      "Epoch [7/75], Loss: 1.4358\n",
      "Epoch [7/75], Val Loss: 1.9519\n",
      "Epoch [8/75], Loss: 1.2523\n",
      "Epoch [8/75], Val Loss: 1.8257\n",
      "Epoch [9/75], Loss: 1.0656\n",
      "Epoch [9/75], Val Loss: 1.5801\n",
      "Epoch [10/75], Loss: 0.8529\n",
      "Epoch [10/75], Val Loss: 1.4919\n",
      "Epoch [11/75], Loss: 0.6914\n",
      "Epoch [11/75], Val Loss: 1.4989\n",
      "Epoch [12/75], Loss: 0.5695\n",
      "Epoch [12/75], Val Loss: 1.5022\n",
      "Epoch [13/75], Loss: 0.4759\n",
      "Epoch [13/75], Val Loss: 1.5669\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [13/75], Loss: 0.4759\n",
      "Test Accuracy CNN Lipschitz: 60.49%\n",
      "Epoch [1/75], Loss: 3.3347\n",
      "Epoch [1/75], Val Loss: 3.1822\n",
      "Epoch [2/75], Loss: 3.0485\n",
      "Epoch [2/75], Val Loss: 2.9852\n",
      "Epoch [3/75], Loss: 2.7076\n",
      "Epoch [3/75], Val Loss: 2.5639\n",
      "Epoch [4/75], Loss: 2.1463\n",
      "Epoch [4/75], Val Loss: 2.0185\n",
      "Epoch [5/75], Loss: 1.6544\n",
      "Epoch [5/75], Val Loss: 2.0232\n",
      "Epoch [6/75], Loss: 1.5323\n",
      "Epoch [6/75], Val Loss: 1.8803\n",
      "Epoch [7/75], Loss: 1.2591\n",
      "Epoch [7/75], Val Loss: 1.9878\n",
      "Epoch [8/75], Loss: 1.1903\n",
      "Epoch [8/75], Val Loss: 1.6750\n",
      "Epoch [9/75], Loss: 1.0457\n",
      "Epoch [9/75], Val Loss: 1.5858\n",
      "Epoch [10/75], Loss: 0.9199\n",
      "Epoch [10/75], Val Loss: 1.5633\n",
      "Epoch [11/75], Loss: 0.8094\n",
      "Epoch [11/75], Val Loss: 1.4795\n",
      "Epoch [12/75], Loss: 0.6738\n",
      "Epoch [12/75], Val Loss: 1.5704\n",
      "Epoch [13/75], Loss: 0.5619\n",
      "Epoch [13/75], Val Loss: 1.5568\n",
      "Epoch [14/75], Loss: 0.4276\n",
      "Epoch [14/75], Val Loss: 1.5499\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [14/75], Loss: 0.4276\n",
      "Test Accuracy CNN Lipschitz: 60.69%\n",
      "Epoch [1/75], Loss: 3.3745\n",
      "Epoch [1/75], Val Loss: 3.2457\n",
      "Epoch [2/75], Loss: 3.1390\n",
      "Epoch [2/75], Val Loss: 3.0992\n",
      "Epoch [3/75], Loss: 2.9027\n",
      "Epoch [3/75], Val Loss: 2.8562\n",
      "Epoch [4/75], Loss: 2.5207\n",
      "Epoch [4/75], Val Loss: 2.4445\n",
      "Epoch [5/75], Loss: 2.0036\n",
      "Epoch [5/75], Val Loss: 2.0250\n",
      "Epoch [6/75], Loss: 1.6972\n",
      "Epoch [6/75], Val Loss: 1.9593\n",
      "Epoch [7/75], Loss: 1.6114\n",
      "Epoch [7/75], Val Loss: 2.2011\n",
      "Epoch [8/75], Loss: 1.4369\n",
      "Epoch [8/75], Val Loss: 2.1089\n",
      "Epoch [9/75], Loss: 1.2415\n",
      "Epoch [9/75], Val Loss: 1.9189\n",
      "Epoch [10/75], Loss: 1.0487\n",
      "Epoch [10/75], Val Loss: 1.5367\n",
      "Epoch [11/75], Loss: 0.8618\n",
      "Epoch [11/75], Val Loss: 1.5299\n",
      "Epoch [12/75], Loss: 0.7003\n",
      "Epoch [12/75], Val Loss: 1.4676\n",
      "Epoch [13/75], Loss: 0.5589\n",
      "Epoch [13/75], Val Loss: 1.5014\n",
      "Epoch [14/75], Loss: 0.4541\n",
      "Epoch [14/75], Val Loss: 1.5304\n",
      "Epoch [15/75], Loss: 0.3997\n",
      "Epoch [15/75], Val Loss: 1.5066\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [15/75], Loss: 0.3997\n",
      "Test Accuracy CNN Lipschitz: 62.30%\n",
      "Epoch [1/75], Loss: 3.3494\n",
      "Epoch [1/75], Val Loss: 3.1919\n",
      "Epoch [2/75], Loss: 3.0646\n",
      "Epoch [2/75], Val Loss: 3.0114\n",
      "Epoch [3/75], Loss: 2.7286\n",
      "Epoch [3/75], Val Loss: 2.6514\n",
      "Epoch [4/75], Loss: 2.2144\n",
      "Epoch [4/75], Val Loss: 2.1611\n",
      "Epoch [5/75], Loss: 1.7321\n",
      "Epoch [5/75], Val Loss: 2.1257\n",
      "Epoch [6/75], Loss: 1.5307\n",
      "Epoch [6/75], Val Loss: 1.9689\n",
      "Epoch [7/75], Loss: 1.3473\n",
      "Epoch [7/75], Val Loss: 2.1372\n",
      "Epoch [8/75], Loss: 1.3320\n",
      "Epoch [8/75], Val Loss: 1.9619\n",
      "Epoch [9/75], Loss: 1.0623\n",
      "Epoch [9/75], Val Loss: 1.6272\n",
      "Epoch [10/75], Loss: 0.9288\n",
      "Epoch [10/75], Val Loss: 1.4585\n",
      "Epoch [11/75], Loss: 0.7027\n",
      "Epoch [11/75], Val Loss: 1.4998\n",
      "Epoch [12/75], Loss: 0.6247\n",
      "Epoch [12/75], Val Loss: 1.5340\n",
      "Epoch [13/75], Loss: 0.5250\n",
      "Epoch [13/75], Val Loss: 1.4753\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [13/75], Loss: 0.5250\n",
      "Test Accuracy CNN Lipschitz: 62.11%\n",
      "Epoch [1/75], Loss: 3.3059\n",
      "Epoch [1/75], Val Loss: 3.1877\n",
      "Epoch [2/75], Loss: 3.0840\n",
      "Epoch [2/75], Val Loss: 3.0294\n",
      "Epoch [3/75], Loss: 2.8357\n",
      "Epoch [3/75], Val Loss: 2.7078\n",
      "Epoch [4/75], Loss: 2.3798\n",
      "Epoch [4/75], Val Loss: 2.2007\n",
      "Epoch [5/75], Loss: 1.8291\n",
      "Epoch [5/75], Val Loss: 1.9215\n",
      "Epoch [6/75], Loss: 1.7160\n",
      "Epoch [6/75], Val Loss: 2.4711\n",
      "Epoch [7/75], Loss: 1.7519\n",
      "Epoch [7/75], Val Loss: 2.0954\n",
      "Epoch [8/75], Loss: 1.3501\n",
      "Epoch [8/75], Val Loss: 1.6807\n",
      "Epoch [9/75], Loss: 1.0812\n",
      "Epoch [9/75], Val Loss: 1.7048\n",
      "Epoch [10/75], Loss: 0.9970\n",
      "Epoch [10/75], Val Loss: 1.4919\n",
      "Epoch [11/75], Loss: 0.8044\n",
      "Epoch [11/75], Val Loss: 1.4982\n",
      "Epoch [12/75], Loss: 0.6521\n",
      "Epoch [12/75], Val Loss: 1.5631\n",
      "Epoch [13/75], Loss: 0.5884\n",
      "Epoch [13/75], Val Loss: 1.5473\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [13/75], Loss: 0.5884\n",
      "Test Accuracy CNN Lipschitz: 60.00%\n",
      "Epoch [1/75], Loss: 3.3391\n",
      "Epoch [1/75], Val Loss: 3.1967\n",
      "Epoch [2/75], Loss: 3.0524\n",
      "Epoch [2/75], Val Loss: 2.9926\n",
      "Epoch [3/75], Loss: 2.7112\n",
      "Epoch [3/75], Val Loss: 2.6205\n",
      "Epoch [4/75], Loss: 2.2351\n",
      "Epoch [4/75], Val Loss: 2.1163\n",
      "Epoch [5/75], Loss: 1.7405\n",
      "Epoch [5/75], Val Loss: 2.0017\n",
      "Epoch [6/75], Loss: 1.4922\n",
      "Epoch [6/75], Val Loss: 1.7532\n",
      "Epoch [7/75], Loss: 1.3680\n",
      "Epoch [7/75], Val Loss: 1.8693\n",
      "Epoch [8/75], Loss: 1.5622\n",
      "Epoch [8/75], Val Loss: 2.1216\n",
      "Epoch [9/75], Loss: 1.4718\n",
      "Epoch [9/75], Val Loss: 1.9323\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [9/75], Loss: 1.4718\n",
      "Test Accuracy CNN Lipschitz: 49.48%\n",
      "Epoch [1/75], Loss: 3.3116\n",
      "Epoch [1/75], Val Loss: 3.1819\n",
      "Epoch [2/75], Loss: 3.0862\n",
      "Epoch [2/75], Val Loss: 3.0332\n",
      "Epoch [3/75], Loss: 2.8153\n",
      "Epoch [3/75], Val Loss: 2.7348\n",
      "Epoch [4/75], Loss: 2.3493\n",
      "Epoch [4/75], Val Loss: 2.1995\n",
      "Epoch [5/75], Loss: 1.7419\n",
      "Epoch [5/75], Val Loss: 1.9163\n",
      "Epoch [6/75], Loss: 1.6204\n",
      "Epoch [6/75], Val Loss: 2.2676\n",
      "Epoch [7/75], Loss: 1.5614\n",
      "Epoch [7/75], Val Loss: 1.8075\n",
      "Epoch [8/75], Loss: 1.3182\n",
      "Epoch [8/75], Val Loss: 1.6380\n",
      "Epoch [9/75], Loss: 1.0122\n",
      "Epoch [9/75], Val Loss: 1.5940\n",
      "Epoch [10/75], Loss: 0.9221\n",
      "Epoch [10/75], Val Loss: 1.4850\n",
      "Epoch [11/75], Loss: 0.7552\n",
      "Epoch [11/75], Val Loss: 1.4230\n",
      "Epoch [12/75], Loss: 0.5954\n",
      "Epoch [12/75], Val Loss: 1.4176\n",
      "Epoch [13/75], Loss: 0.4673\n",
      "Epoch [13/75], Val Loss: 1.4812\n",
      "Epoch [14/75], Loss: 0.3519\n",
      "Epoch [14/75], Val Loss: 1.5105\n",
      "Epoch [15/75], Loss: 0.2882\n",
      "Epoch [15/75], Val Loss: 1.5478\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [15/75], Loss: 0.2882\n",
      "Test Accuracy CNN Lipschitz: 63.36%\n",
      "Epoch [1/75], Loss: 3.3446\n",
      "Epoch [1/75], Val Loss: 3.2107\n",
      "Epoch [2/75], Loss: 3.0647\n",
      "Epoch [2/75], Val Loss: 3.0117\n",
      "Epoch [3/75], Loss: 2.7704\n",
      "Epoch [3/75], Val Loss: 2.6826\n",
      "Epoch [4/75], Loss: 2.2566\n",
      "Epoch [4/75], Val Loss: 2.1619\n",
      "Epoch [5/75], Loss: 1.7728\n",
      "Epoch [5/75], Val Loss: 2.1473\n",
      "Epoch [6/75], Loss: 1.6016\n",
      "Epoch [6/75], Val Loss: 2.2423\n",
      "Epoch [7/75], Loss: 1.5496\n",
      "Epoch [7/75], Val Loss: 2.3028\n",
      "Epoch [8/75], Loss: 1.4044\n",
      "Epoch [8/75], Val Loss: 2.0495\n",
      "Epoch [9/75], Loss: 1.2617\n",
      "Epoch [9/75], Val Loss: 1.6946\n",
      "Epoch [10/75], Loss: 1.0365\n",
      "Epoch [10/75], Val Loss: 1.5534\n",
      "Epoch [11/75], Loss: 0.8282\n",
      "Epoch [11/75], Val Loss: 1.4999\n",
      "Epoch [12/75], Loss: 0.6252\n",
      "Epoch [12/75], Val Loss: 1.5109\n",
      "Epoch [13/75], Loss: 0.5238\n",
      "Epoch [13/75], Val Loss: 1.5466\n",
      "Epoch [14/75], Loss: 0.4525\n",
      "Epoch [14/75], Val Loss: 1.5521\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 75000, Epoch [14/75], Loss: 0.4525\n",
      "Test Accuracy CNN Lipschitz: 61.64%\n",
      "Training with subset size: 50000\n",
      "Epoch [1/75], Loss: 3.3185\n",
      "Epoch [1/75], Val Loss: 3.2241\n",
      "Epoch [2/75], Loss: 3.0928\n",
      "Epoch [2/75], Val Loss: 3.0621\n",
      "Epoch [3/75], Loss: 2.8520\n",
      "Epoch [3/75], Val Loss: 2.7822\n",
      "Epoch [4/75], Loss: 2.3950\n",
      "Epoch [4/75], Val Loss: 2.2859\n",
      "Epoch [5/75], Loss: 1.8566\n",
      "Epoch [5/75], Val Loss: 1.8749\n",
      "Epoch [6/75], Loss: 1.5547\n",
      "Epoch [6/75], Val Loss: 1.8777\n",
      "Epoch [7/75], Loss: 1.4473\n",
      "Epoch [7/75], Val Loss: 2.0718\n",
      "Epoch [8/75], Loss: 1.3696\n",
      "Epoch [8/75], Val Loss: 1.9253\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [8/75], Loss: 1.3696\n",
      "Test Accuracy CNN Lipschitz: 46.26%\n",
      "Epoch [1/75], Loss: 3.3297\n",
      "Epoch [1/75], Val Loss: 3.2328\n",
      "Epoch [2/75], Loss: 3.1203\n",
      "Epoch [2/75], Val Loss: 3.1045\n",
      "Epoch [3/75], Loss: 2.8909\n",
      "Epoch [3/75], Val Loss: 2.8980\n",
      "Epoch [4/75], Loss: 2.5072\n",
      "Epoch [4/75], Val Loss: 2.4384\n",
      "Epoch [5/75], Loss: 1.9668\n",
      "Epoch [5/75], Val Loss: 2.0609\n",
      "Epoch [6/75], Loss: 1.5425\n",
      "Epoch [6/75], Val Loss: 2.1301\n",
      "Epoch [7/75], Loss: 1.5685\n",
      "Epoch [7/75], Val Loss: 2.2726\n",
      "Epoch [8/75], Loss: 1.3936\n",
      "Epoch [8/75], Val Loss: 1.7571\n",
      "Epoch [9/75], Loss: 1.0757\n",
      "Epoch [9/75], Val Loss: 1.6163\n",
      "Epoch [10/75], Loss: 0.9361\n",
      "Epoch [10/75], Val Loss: 1.5234\n",
      "Epoch [11/75], Loss: 0.7530\n",
      "Epoch [11/75], Val Loss: 1.4983\n",
      "Epoch [12/75], Loss: 0.6031\n",
      "Epoch [12/75], Val Loss: 1.4785\n",
      "Epoch [13/75], Loss: 0.4827\n",
      "Epoch [13/75], Val Loss: 1.4781\n",
      "Epoch [14/75], Loss: 0.3922\n",
      "Epoch [14/75], Val Loss: 1.4932\n",
      "Epoch [15/75], Loss: 0.3162\n",
      "Epoch [15/75], Val Loss: 1.6020\n",
      "Epoch [16/75], Loss: 0.2619\n",
      "Epoch [16/75], Val Loss: 1.6149\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [16/75], Loss: 0.2619\n",
      "Test Accuracy CNN Lipschitz: 62.07%\n",
      "Epoch [1/75], Loss: 3.3175\n",
      "Epoch [1/75], Val Loss: 3.2056\n",
      "Epoch [2/75], Loss: 3.0810\n",
      "Epoch [2/75], Val Loss: 3.0443\n",
      "Epoch [3/75], Loss: 2.8007\n",
      "Epoch [3/75], Val Loss: 2.7415\n",
      "Epoch [4/75], Loss: 2.3394\n",
      "Epoch [4/75], Val Loss: 2.2760\n",
      "Epoch [5/75], Loss: 1.8140\n",
      "Epoch [5/75], Val Loss: 1.9896\n",
      "Epoch [6/75], Loss: 1.5271\n",
      "Epoch [6/75], Val Loss: 2.0221\n",
      "Epoch [7/75], Loss: 1.4637\n",
      "Epoch [7/75], Val Loss: 1.9383\n",
      "Epoch [8/75], Loss: 1.2911\n",
      "Epoch [8/75], Val Loss: 1.8253\n",
      "Epoch [9/75], Loss: 1.2497\n",
      "Epoch [9/75], Val Loss: 1.8056\n",
      "Epoch [10/75], Loss: 1.0203\n",
      "Epoch [10/75], Val Loss: 1.6731\n",
      "Epoch [11/75], Loss: 0.8895\n",
      "Epoch [11/75], Val Loss: 1.4731\n",
      "Epoch [12/75], Loss: 0.6868\n",
      "Epoch [12/75], Val Loss: 1.3973\n",
      "Epoch [13/75], Loss: 0.5198\n",
      "Epoch [13/75], Val Loss: 1.4899\n",
      "Epoch [14/75], Loss: 0.4291\n",
      "Epoch [14/75], Val Loss: 1.5523\n",
      "Epoch [15/75], Loss: 0.3439\n",
      "Epoch [15/75], Val Loss: 1.5105\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [15/75], Loss: 0.3439\n",
      "Test Accuracy CNN Lipschitz: 62.14%\n",
      "Epoch [1/75], Loss: 3.3140\n",
      "Epoch [1/75], Val Loss: 3.2310\n",
      "Epoch [2/75], Loss: 3.1215\n",
      "Epoch [2/75], Val Loss: 3.0864\n",
      "Epoch [3/75], Loss: 2.9070\n",
      "Epoch [3/75], Val Loss: 2.8632\n",
      "Epoch [4/75], Loss: 2.5445\n",
      "Epoch [4/75], Val Loss: 2.4792\n",
      "Epoch [5/75], Loss: 2.0506\n",
      "Epoch [5/75], Val Loss: 2.0130\n",
      "Epoch [6/75], Loss: 1.6544\n",
      "Epoch [6/75], Val Loss: 2.0299\n",
      "Epoch [7/75], Loss: 1.6735\n",
      "Epoch [7/75], Val Loss: 2.2757\n",
      "Epoch [8/75], Loss: 1.7005\n",
      "Epoch [8/75], Val Loss: 2.1368\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [8/75], Loss: 1.7005\n",
      "Test Accuracy CNN Lipschitz: 46.05%\n",
      "Epoch [1/75], Loss: 3.3055\n",
      "Epoch [1/75], Val Loss: 3.1936\n",
      "Epoch [2/75], Loss: 3.0576\n",
      "Epoch [2/75], Val Loss: 3.0223\n",
      "Epoch [3/75], Loss: 2.7664\n",
      "Epoch [3/75], Val Loss: 2.6981\n",
      "Epoch [4/75], Loss: 2.2860\n",
      "Epoch [4/75], Val Loss: 2.1755\n",
      "Epoch [5/75], Loss: 1.7450\n",
      "Epoch [5/75], Val Loss: 1.9399\n",
      "Epoch [6/75], Loss: 1.4689\n",
      "Epoch [6/75], Val Loss: 1.9354\n",
      "Epoch [7/75], Loss: 1.3766\n",
      "Epoch [7/75], Val Loss: 1.7904\n",
      "Epoch [8/75], Loss: 1.2059\n",
      "Epoch [8/75], Val Loss: 1.9880\n",
      "Epoch [9/75], Loss: 1.1106\n",
      "Epoch [9/75], Val Loss: 1.7104\n",
      "Epoch [10/75], Loss: 0.9382\n",
      "Epoch [10/75], Val Loss: 1.5829\n",
      "Epoch [11/75], Loss: 0.7343\n",
      "Epoch [11/75], Val Loss: 1.4219\n",
      "Epoch [12/75], Loss: 0.5838\n",
      "Epoch [12/75], Val Loss: 1.4422\n",
      "Epoch [13/75], Loss: 0.4558\n",
      "Epoch [13/75], Val Loss: 1.4834\n",
      "Epoch [14/75], Loss: 0.3727\n",
      "Epoch [14/75], Val Loss: 1.5399\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.3727\n",
      "Test Accuracy CNN Lipschitz: 61.48%\n",
      "Epoch [1/75], Loss: 3.3367\n",
      "Epoch [1/75], Val Loss: 3.2239\n",
      "Epoch [2/75], Loss: 3.1217\n",
      "Epoch [2/75], Val Loss: 3.0970\n",
      "Epoch [3/75], Loss: 2.8994\n",
      "Epoch [3/75], Val Loss: 2.8485\n",
      "Epoch [4/75], Loss: 2.5019\n",
      "Epoch [4/75], Val Loss: 2.3448\n",
      "Epoch [5/75], Loss: 1.9346\n",
      "Epoch [5/75], Val Loss: 1.9081\n",
      "Epoch [6/75], Loss: 1.5663\n",
      "Epoch [6/75], Val Loss: 1.8478\n",
      "Epoch [7/75], Loss: 1.5462\n",
      "Epoch [7/75], Val Loss: 1.9991\n",
      "Epoch [8/75], Loss: 1.5871\n",
      "Epoch [8/75], Val Loss: 2.5870\n",
      "Epoch [9/75], Loss: 1.7727\n",
      "Epoch [9/75], Val Loss: 1.9225\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [9/75], Loss: 1.7727\n",
      "Test Accuracy CNN Lipschitz: 48.77%\n",
      "Epoch [1/75], Loss: 3.3287\n",
      "Epoch [1/75], Val Loss: 3.2301\n",
      "Epoch [2/75], Loss: 3.1289\n",
      "Epoch [2/75], Val Loss: 3.1051\n",
      "Epoch [3/75], Loss: 2.9169\n",
      "Epoch [3/75], Val Loss: 2.8767\n",
      "Epoch [4/75], Loss: 2.5686\n",
      "Epoch [4/75], Val Loss: 2.4529\n",
      "Epoch [5/75], Loss: 2.0608\n",
      "Epoch [5/75], Val Loss: 2.0139\n",
      "Epoch [6/75], Loss: 1.5891\n",
      "Epoch [6/75], Val Loss: 1.9326\n",
      "Epoch [7/75], Loss: 1.5068\n",
      "Epoch [7/75], Val Loss: 2.2259\n",
      "Epoch [8/75], Loss: 1.4800\n",
      "Epoch [8/75], Val Loss: 2.2591\n",
      "Epoch [9/75], Loss: 1.2916\n",
      "Epoch [9/75], Val Loss: 1.8095\n",
      "Epoch [10/75], Loss: 1.0964\n",
      "Epoch [10/75], Val Loss: 1.5743\n",
      "Epoch [11/75], Loss: 0.8555\n",
      "Epoch [11/75], Val Loss: 1.5162\n",
      "Epoch [12/75], Loss: 0.7374\n",
      "Epoch [12/75], Val Loss: 1.4409\n",
      "Epoch [13/75], Loss: 0.5971\n",
      "Epoch [13/75], Val Loss: 1.4363\n",
      "Epoch [14/75], Loss: 0.5090\n",
      "Epoch [14/75], Val Loss: 1.5145\n",
      "Epoch [15/75], Loss: 0.3950\n",
      "Epoch [15/75], Val Loss: 1.5196\n",
      "Epoch [16/75], Loss: 0.3238\n",
      "Epoch [16/75], Val Loss: 1.4869\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [16/75], Loss: 0.3238\n",
      "Test Accuracy CNN Lipschitz: 62.76%\n",
      "Epoch [1/75], Loss: 3.3705\n",
      "Epoch [1/75], Val Loss: 3.1870\n",
      "Epoch [2/75], Loss: 3.0729\n",
      "Epoch [2/75], Val Loss: 3.0179\n",
      "Epoch [3/75], Loss: 2.7739\n",
      "Epoch [3/75], Val Loss: 2.6766\n",
      "Epoch [4/75], Loss: 2.2555\n",
      "Epoch [4/75], Val Loss: 2.1870\n",
      "Epoch [5/75], Loss: 1.7868\n",
      "Epoch [5/75], Val Loss: 2.0554\n",
      "Epoch [6/75], Loss: 1.6329\n",
      "Epoch [6/75], Val Loss: 2.2060\n",
      "Epoch [7/75], Loss: 1.5273\n",
      "Epoch [7/75], Val Loss: 2.0809\n",
      "Epoch [8/75], Loss: 1.3324\n",
      "Epoch [8/75], Val Loss: 1.8928\n",
      "Epoch [9/75], Loss: 1.1155\n",
      "Epoch [9/75], Val Loss: 1.5929\n",
      "Epoch [10/75], Loss: 0.9383\n",
      "Epoch [10/75], Val Loss: 1.5783\n",
      "Epoch [11/75], Loss: 0.8314\n",
      "Epoch [11/75], Val Loss: 1.4751\n",
      "Epoch [12/75], Loss: 0.6361\n",
      "Epoch [12/75], Val Loss: 1.4863\n",
      "Epoch [13/75], Loss: 0.5113\n",
      "Epoch [13/75], Val Loss: 1.4838\n",
      "Epoch [14/75], Loss: 0.4085\n",
      "Epoch [14/75], Val Loss: 1.5775\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.4085\n",
      "Test Accuracy CNN Lipschitz: 60.99%\n",
      "Epoch [1/75], Loss: 3.2721\n",
      "Epoch [1/75], Val Loss: 3.1474\n",
      "Epoch [2/75], Loss: 3.0090\n",
      "Epoch [2/75], Val Loss: 2.9372\n",
      "Epoch [3/75], Loss: 2.6538\n",
      "Epoch [3/75], Val Loss: 2.5018\n",
      "Epoch [4/75], Loss: 2.1279\n",
      "Epoch [4/75], Val Loss: 2.0397\n",
      "Epoch [5/75], Loss: 1.7453\n",
      "Epoch [5/75], Val Loss: 2.1448\n",
      "Epoch [6/75], Loss: 1.6500\n",
      "Epoch [6/75], Val Loss: 2.3092\n",
      "Epoch [7/75], Loss: 1.6537\n",
      "Epoch [7/75], Val Loss: 2.0459\n",
      "Stopping early at epoch 7 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [7/75], Loss: 1.6537\n",
      "Test Accuracy CNN Lipschitz: 42.11%\n",
      "Epoch [1/75], Loss: 3.3826\n",
      "Epoch [1/75], Val Loss: 3.2389\n",
      "Epoch [2/75], Loss: 3.1680\n",
      "Epoch [2/75], Val Loss: 3.1426\n",
      "Epoch [3/75], Loss: 3.0012\n",
      "Epoch [3/75], Val Loss: 2.9802\n",
      "Epoch [4/75], Loss: 2.7462\n",
      "Epoch [4/75], Val Loss: 2.6830\n",
      "Epoch [5/75], Loss: 2.3260\n",
      "Epoch [5/75], Val Loss: 2.1921\n",
      "Epoch [6/75], Loss: 1.8259\n",
      "Epoch [6/75], Val Loss: 1.9140\n",
      "Epoch [7/75], Loss: 1.5406\n",
      "Epoch [7/75], Val Loss: 1.8991\n",
      "Epoch [8/75], Loss: 1.3730\n",
      "Epoch [8/75], Val Loss: 2.0551\n",
      "Epoch [9/75], Loss: 1.4099\n",
      "Epoch [9/75], Val Loss: 1.8929\n",
      "Epoch [10/75], Loss: 1.2144\n",
      "Epoch [10/75], Val Loss: 1.7135\n",
      "Epoch [11/75], Loss: 0.9432\n",
      "Epoch [11/75], Val Loss: 1.5701\n",
      "Epoch [12/75], Loss: 0.8196\n",
      "Epoch [12/75], Val Loss: 1.4829\n",
      "Epoch [13/75], Loss: 0.6831\n",
      "Epoch [13/75], Val Loss: 1.4662\n",
      "Epoch [14/75], Loss: 0.5937\n",
      "Epoch [14/75], Val Loss: 1.5067\n",
      "Epoch [15/75], Loss: 0.4737\n",
      "Epoch [15/75], Val Loss: 1.4761\n",
      "Epoch [16/75], Loss: 0.4001\n",
      "Epoch [16/75], Val Loss: 1.4623\n",
      "Epoch [17/75], Loss: 0.3084\n",
      "Epoch [17/75], Val Loss: 1.5543\n",
      "Epoch [18/75], Loss: 0.2775\n",
      "Epoch [18/75], Val Loss: 1.5343\n",
      "Epoch [19/75], Loss: 0.2358\n",
      "Epoch [19/75], Val Loss: 1.5811\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [19/75], Loss: 0.2358\n",
      "Test Accuracy CNN Lipschitz: 62.57%\n",
      "Epoch [1/75], Loss: 3.3552\n",
      "Epoch [1/75], Val Loss: 3.2460\n",
      "Epoch [2/75], Loss: 3.1799\n",
      "Epoch [2/75], Val Loss: 3.1480\n",
      "Epoch [3/75], Loss: 3.0228\n",
      "Epoch [3/75], Val Loss: 3.0114\n",
      "Epoch [4/75], Loss: 2.7980\n",
      "Epoch [4/75], Val Loss: 2.7655\n",
      "Epoch [5/75], Loss: 2.4116\n",
      "Epoch [5/75], Val Loss: 2.3208\n",
      "Epoch [6/75], Loss: 1.9079\n",
      "Epoch [6/75], Val Loss: 1.9566\n",
      "Epoch [7/75], Loss: 1.5431\n",
      "Epoch [7/75], Val Loss: 1.8168\n",
      "Epoch [8/75], Loss: 1.3429\n",
      "Epoch [8/75], Val Loss: 1.9560\n",
      "Epoch [9/75], Loss: 1.4409\n",
      "Epoch [9/75], Val Loss: 2.0992\n",
      "Epoch [10/75], Loss: 1.1800\n",
      "Epoch [10/75], Val Loss: 1.8241\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [10/75], Loss: 1.1800\n",
      "Test Accuracy CNN Lipschitz: 49.73%\n",
      "Epoch [1/75], Loss: 3.3262\n",
      "Epoch [1/75], Val Loss: 3.1853\n",
      "Epoch [2/75], Loss: 3.0707\n",
      "Epoch [2/75], Val Loss: 3.0411\n",
      "Epoch [3/75], Loss: 2.7974\n",
      "Epoch [3/75], Val Loss: 2.7316\n",
      "Epoch [4/75], Loss: 2.3322\n",
      "Epoch [4/75], Val Loss: 2.2142\n",
      "Epoch [5/75], Loss: 1.8399\n",
      "Epoch [5/75], Val Loss: 1.9603\n",
      "Epoch [6/75], Loss: 1.5812\n",
      "Epoch [6/75], Val Loss: 2.1178\n",
      "Epoch [7/75], Loss: 1.5601\n",
      "Epoch [7/75], Val Loss: 1.9035\n",
      "Epoch [8/75], Loss: 1.3535\n",
      "Epoch [8/75], Val Loss: 1.7632\n",
      "Epoch [9/75], Loss: 1.1556\n",
      "Epoch [9/75], Val Loss: 1.5443\n",
      "Epoch [10/75], Loss: 0.9293\n",
      "Epoch [10/75], Val Loss: 1.5391\n",
      "Epoch [11/75], Loss: 0.7965\n",
      "Epoch [11/75], Val Loss: 1.4873\n",
      "Epoch [12/75], Loss: 0.6360\n",
      "Epoch [12/75], Val Loss: 1.4285\n",
      "Epoch [13/75], Loss: 0.5123\n",
      "Epoch [13/75], Val Loss: 1.4566\n",
      "Epoch [14/75], Loss: 0.4098\n",
      "Epoch [14/75], Val Loss: 1.5493\n",
      "Epoch [15/75], Loss: 0.3136\n",
      "Epoch [15/75], Val Loss: 1.5771\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [15/75], Loss: 0.3136\n",
      "Test Accuracy CNN Lipschitz: 61.62%\n",
      "Epoch [1/75], Loss: 3.3503\n",
      "Epoch [1/75], Val Loss: 3.2156\n",
      "Epoch [2/75], Loss: 3.1226\n",
      "Epoch [2/75], Val Loss: 3.0862\n",
      "Epoch [3/75], Loss: 2.8606\n",
      "Epoch [3/75], Val Loss: 2.8063\n",
      "Epoch [4/75], Loss: 2.4437\n",
      "Epoch [4/75], Val Loss: 2.3355\n",
      "Epoch [5/75], Loss: 1.8884\n",
      "Epoch [5/75], Val Loss: 1.9812\n",
      "Epoch [6/75], Loss: 1.6250\n",
      "Epoch [6/75], Val Loss: 2.0612\n",
      "Epoch [7/75], Loss: 1.6453\n",
      "Epoch [7/75], Val Loss: 2.4253\n",
      "Epoch [8/75], Loss: 1.6270\n",
      "Epoch [8/75], Val Loss: 1.8671\n",
      "Epoch [9/75], Loss: 1.1066\n",
      "Epoch [9/75], Val Loss: 1.6847\n",
      "Epoch [10/75], Loss: 1.0717\n",
      "Epoch [10/75], Val Loss: 1.5111\n",
      "Epoch [11/75], Loss: 0.8528\n",
      "Epoch [11/75], Val Loss: 1.4191\n",
      "Epoch [12/75], Loss: 0.7045\n",
      "Epoch [12/75], Val Loss: 1.4250\n",
      "Epoch [13/75], Loss: 0.5835\n",
      "Epoch [13/75], Val Loss: 1.4607\n",
      "Epoch [14/75], Loss: 0.5063\n",
      "Epoch [14/75], Val Loss: 1.4924\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.5063\n",
      "Test Accuracy CNN Lipschitz: 61.25%\n",
      "Epoch [1/75], Loss: 3.3355\n",
      "Epoch [1/75], Val Loss: 3.2640\n",
      "Epoch [2/75], Loss: 3.1713\n",
      "Epoch [2/75], Val Loss: 3.1559\n",
      "Epoch [3/75], Loss: 2.9910\n",
      "Epoch [3/75], Val Loss: 3.0098\n",
      "Epoch [4/75], Loss: 2.7145\n",
      "Epoch [4/75], Val Loss: 2.6979\n",
      "Epoch [5/75], Loss: 2.2773\n",
      "Epoch [5/75], Val Loss: 2.2577\n",
      "Epoch [6/75], Loss: 1.8237\n",
      "Epoch [6/75], Val Loss: 2.0549\n",
      "Epoch [7/75], Loss: 1.5612\n",
      "Epoch [7/75], Val Loss: 1.9385\n",
      "Epoch [8/75], Loss: 1.4921\n",
      "Epoch [8/75], Val Loss: 1.9892\n",
      "Epoch [9/75], Loss: 1.1749\n",
      "Epoch [9/75], Val Loss: 1.6974\n",
      "Epoch [10/75], Loss: 1.0545\n",
      "Epoch [10/75], Val Loss: 1.7254\n",
      "Epoch [11/75], Loss: 0.8710\n",
      "Epoch [11/75], Val Loss: 1.5447\n",
      "Epoch [12/75], Loss: 0.7322\n",
      "Epoch [12/75], Val Loss: 1.4024\n",
      "Epoch [13/75], Loss: 0.5674\n",
      "Epoch [13/75], Val Loss: 1.4940\n",
      "Epoch [14/75], Loss: 0.5003\n",
      "Epoch [14/75], Val Loss: 1.4419\n",
      "Epoch [15/75], Loss: 0.3610\n",
      "Epoch [15/75], Val Loss: 1.4521\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [15/75], Loss: 0.3610\n",
      "Test Accuracy CNN Lipschitz: 61.82%\n",
      "Epoch [1/75], Loss: 3.3002\n",
      "Epoch [1/75], Val Loss: 3.2075\n",
      "Epoch [2/75], Loss: 3.0946\n",
      "Epoch [2/75], Val Loss: 3.0715\n",
      "Epoch [3/75], Loss: 2.8728\n",
      "Epoch [3/75], Val Loss: 2.8103\n",
      "Epoch [4/75], Loss: 2.4984\n",
      "Epoch [4/75], Val Loss: 2.3633\n",
      "Epoch [5/75], Loss: 1.9523\n",
      "Epoch [5/75], Val Loss: 1.9692\n",
      "Epoch [6/75], Loss: 1.5989\n",
      "Epoch [6/75], Val Loss: 1.9033\n",
      "Epoch [7/75], Loss: 1.4573\n",
      "Epoch [7/75], Val Loss: 1.9916\n",
      "Epoch [8/75], Loss: 1.3850\n",
      "Epoch [8/75], Val Loss: 1.6832\n",
      "Epoch [9/75], Loss: 1.1548\n",
      "Epoch [9/75], Val Loss: 1.6134\n",
      "Epoch [10/75], Loss: 0.9607\n",
      "Epoch [10/75], Val Loss: 1.5451\n",
      "Epoch [11/75], Loss: 0.7796\n",
      "Epoch [11/75], Val Loss: 1.4473\n",
      "Epoch [12/75], Loss: 0.6842\n",
      "Epoch [12/75], Val Loss: 1.3929\n",
      "Epoch [13/75], Loss: 0.5354\n",
      "Epoch [13/75], Val Loss: 1.4938\n",
      "Epoch [14/75], Loss: 0.4811\n",
      "Epoch [14/75], Val Loss: 1.4516\n",
      "Epoch [15/75], Loss: 0.4124\n",
      "Epoch [15/75], Val Loss: 1.4936\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [15/75], Loss: 0.4124\n",
      "Test Accuracy CNN Lipschitz: 62.15%\n",
      "Epoch [1/75], Loss: 3.3425\n",
      "Epoch [1/75], Val Loss: 3.2115\n",
      "Epoch [2/75], Loss: 3.0923\n",
      "Epoch [2/75], Val Loss: 3.0436\n",
      "Epoch [3/75], Loss: 2.8262\n",
      "Epoch [3/75], Val Loss: 2.7417\n",
      "Epoch [4/75], Loss: 2.3742\n",
      "Epoch [4/75], Val Loss: 2.2901\n",
      "Epoch [5/75], Loss: 1.7868\n",
      "Epoch [5/75], Val Loss: 1.9314\n",
      "Epoch [6/75], Loss: 1.5703\n",
      "Epoch [6/75], Val Loss: 2.0883\n",
      "Epoch [7/75], Loss: 1.5184\n",
      "Epoch [7/75], Val Loss: 2.4786\n",
      "Epoch [8/75], Loss: 1.6124\n",
      "Epoch [8/75], Val Loss: 1.6964\n",
      "Epoch [9/75], Loss: 1.2033\n",
      "Epoch [9/75], Val Loss: 1.6874\n",
      "Epoch [10/75], Loss: 0.9958\n",
      "Epoch [10/75], Val Loss: 1.5218\n",
      "Epoch [11/75], Loss: 0.8351\n",
      "Epoch [11/75], Val Loss: 1.4306\n",
      "Epoch [12/75], Loss: 0.6761\n",
      "Epoch [12/75], Val Loss: 1.4794\n",
      "Epoch [13/75], Loss: 0.5643\n",
      "Epoch [13/75], Val Loss: 1.5352\n",
      "Epoch [14/75], Loss: 0.5116\n",
      "Epoch [14/75], Val Loss: 1.5291\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.5116\n",
      "Test Accuracy CNN Lipschitz: 61.38%\n",
      "Epoch [1/75], Loss: 3.3542\n",
      "Epoch [1/75], Val Loss: 3.2000\n",
      "Epoch [2/75], Loss: 3.0758\n",
      "Epoch [2/75], Val Loss: 3.0272\n",
      "Epoch [3/75], Loss: 2.7660\n",
      "Epoch [3/75], Val Loss: 2.6606\n",
      "Epoch [4/75], Loss: 2.2457\n",
      "Epoch [4/75], Val Loss: 2.0956\n",
      "Epoch [5/75], Loss: 1.6975\n",
      "Epoch [5/75], Val Loss: 2.0088\n",
      "Epoch [6/75], Loss: 1.6691\n",
      "Epoch [6/75], Val Loss: 2.4735\n",
      "Epoch [7/75], Loss: 1.8299\n",
      "Epoch [7/75], Val Loss: 1.9702\n",
      "Epoch [8/75], Loss: 1.3842\n",
      "Epoch [8/75], Val Loss: 1.6928\n",
      "Epoch [9/75], Loss: 1.1276\n",
      "Epoch [9/75], Val Loss: 1.6194\n",
      "Epoch [10/75], Loss: 0.9427\n",
      "Epoch [10/75], Val Loss: 1.5100\n",
      "Epoch [11/75], Loss: 0.7773\n",
      "Epoch [11/75], Val Loss: 1.4062\n",
      "Epoch [12/75], Loss: 0.5997\n",
      "Epoch [12/75], Val Loss: 1.4757\n",
      "Epoch [13/75], Loss: 0.4904\n",
      "Epoch [13/75], Val Loss: 1.5145\n",
      "Epoch [14/75], Loss: 0.4007\n",
      "Epoch [14/75], Val Loss: 1.4930\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.4007\n",
      "Test Accuracy CNN Lipschitz: 63.63%\n",
      "Epoch [1/75], Loss: 3.3465\n",
      "Epoch [1/75], Val Loss: 3.2602\n",
      "Epoch [2/75], Loss: 3.1808\n",
      "Epoch [2/75], Val Loss: 3.1584\n",
      "Epoch [3/75], Loss: 3.0205\n",
      "Epoch [3/75], Val Loss: 2.9990\n",
      "Epoch [4/75], Loss: 2.7438\n",
      "Epoch [4/75], Val Loss: 2.6752\n",
      "Epoch [5/75], Loss: 2.2738\n",
      "Epoch [5/75], Val Loss: 2.2181\n",
      "Epoch [6/75], Loss: 1.7882\n",
      "Epoch [6/75], Val Loss: 1.9728\n",
      "Epoch [7/75], Loss: 1.6157\n",
      "Epoch [7/75], Val Loss: 1.8596\n",
      "Epoch [8/75], Loss: 1.4152\n",
      "Epoch [8/75], Val Loss: 1.7816\n",
      "Epoch [9/75], Loss: 1.2755\n",
      "Epoch [9/75], Val Loss: 1.6179\n",
      "Epoch [10/75], Loss: 1.0193\n",
      "Epoch [10/75], Val Loss: 1.7198\n",
      "Epoch [11/75], Loss: 0.9160\n",
      "Epoch [11/75], Val Loss: 1.5097\n",
      "Epoch [12/75], Loss: 0.7837\n",
      "Epoch [12/75], Val Loss: 1.4889\n",
      "Epoch [13/75], Loss: 0.6320\n",
      "Epoch [13/75], Val Loss: 1.4800\n",
      "Epoch [14/75], Loss: 0.5151\n",
      "Epoch [14/75], Val Loss: 1.4434\n",
      "Epoch [15/75], Loss: 0.4001\n",
      "Epoch [15/75], Val Loss: 1.5456\n",
      "Epoch [16/75], Loss: 0.3198\n",
      "Epoch [16/75], Val Loss: 1.5091\n",
      "Epoch [17/75], Loss: 0.2731\n",
      "Epoch [17/75], Val Loss: 1.5846\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [17/75], Loss: 0.2731\n",
      "Test Accuracy CNN Lipschitz: 62.11%\n",
      "Epoch [1/75], Loss: 3.3640\n",
      "Epoch [1/75], Val Loss: 3.1911\n",
      "Epoch [2/75], Loss: 3.0344\n",
      "Epoch [2/75], Val Loss: 2.9783\n",
      "Epoch [3/75], Loss: 2.7047\n",
      "Epoch [3/75], Val Loss: 2.6135\n",
      "Epoch [4/75], Loss: 2.1739\n",
      "Epoch [4/75], Val Loss: 2.1277\n",
      "Epoch [5/75], Loss: 1.6992\n",
      "Epoch [5/75], Val Loss: 2.0900\n",
      "Epoch [6/75], Loss: 1.5707\n",
      "Epoch [6/75], Val Loss: 2.2377\n",
      "Epoch [7/75], Loss: 1.5375\n",
      "Epoch [7/75], Val Loss: 1.9718\n",
      "Epoch [8/75], Loss: 1.2966\n",
      "Epoch [8/75], Val Loss: 1.8620\n",
      "Epoch [9/75], Loss: 1.0847\n",
      "Epoch [9/75], Val Loss: 1.6425\n",
      "Epoch [10/75], Loss: 0.8934\n",
      "Epoch [10/75], Val Loss: 1.4531\n",
      "Epoch [11/75], Loss: 0.7254\n",
      "Epoch [11/75], Val Loss: 1.4772\n",
      "Epoch [12/75], Loss: 0.5527\n",
      "Epoch [12/75], Val Loss: 1.5990\n",
      "Epoch [13/75], Loss: 0.4966\n",
      "Epoch [13/75], Val Loss: 1.4514\n",
      "Epoch [14/75], Loss: 0.3625\n",
      "Epoch [14/75], Val Loss: 1.5932\n",
      "Epoch [15/75], Loss: 0.2918\n",
      "Epoch [15/75], Val Loss: 1.6730\n",
      "Epoch [16/75], Loss: 0.2603\n",
      "Epoch [16/75], Val Loss: 1.5744\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [16/75], Loss: 0.2603\n",
      "Test Accuracy CNN Lipschitz: 63.11%\n",
      "Epoch [1/75], Loss: 3.3088\n",
      "Epoch [1/75], Val Loss: 3.2245\n",
      "Epoch [2/75], Loss: 3.1168\n",
      "Epoch [2/75], Val Loss: 3.0750\n",
      "Epoch [3/75], Loss: 2.8800\n",
      "Epoch [3/75], Val Loss: 2.8493\n",
      "Epoch [4/75], Loss: 2.5297\n",
      "Epoch [4/75], Val Loss: 2.4375\n",
      "Epoch [5/75], Loss: 1.9797\n",
      "Epoch [5/75], Val Loss: 1.9537\n",
      "Epoch [6/75], Loss: 1.5995\n",
      "Epoch [6/75], Val Loss: 1.8766\n",
      "Epoch [7/75], Loss: 1.4207\n",
      "Epoch [7/75], Val Loss: 2.0266\n",
      "Epoch [8/75], Loss: 1.3859\n",
      "Epoch [8/75], Val Loss: 1.7314\n",
      "Epoch [9/75], Loss: 1.1136\n",
      "Epoch [9/75], Val Loss: 1.8060\n",
      "Epoch [10/75], Loss: 1.0095\n",
      "Epoch [10/75], Val Loss: 1.5806\n",
      "Epoch [11/75], Loss: 0.8739\n",
      "Epoch [11/75], Val Loss: 1.4336\n",
      "Epoch [12/75], Loss: 0.6919\n",
      "Epoch [12/75], Val Loss: 1.4574\n",
      "Epoch [13/75], Loss: 0.5852\n",
      "Epoch [13/75], Val Loss: 1.4450\n",
      "Epoch [14/75], Loss: 0.4947\n",
      "Epoch [14/75], Val Loss: 1.4571\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.4947\n",
      "Test Accuracy CNN Lipschitz: 61.31%\n",
      "Epoch [1/75], Loss: 3.2927\n",
      "Epoch [1/75], Val Loss: 3.2481\n",
      "Epoch [2/75], Loss: 3.1624\n",
      "Epoch [2/75], Val Loss: 3.1518\n",
      "Epoch [3/75], Loss: 3.0013\n",
      "Epoch [3/75], Val Loss: 3.0046\n",
      "Epoch [4/75], Loss: 2.7397\n",
      "Epoch [4/75], Val Loss: 2.7243\n",
      "Epoch [5/75], Loss: 2.3598\n",
      "Epoch [5/75], Val Loss: 2.3299\n",
      "Epoch [6/75], Loss: 1.8895\n",
      "Epoch [6/75], Val Loss: 1.9509\n",
      "Epoch [7/75], Loss: 1.5568\n",
      "Epoch [7/75], Val Loss: 2.1687\n",
      "Epoch [8/75], Loss: 1.4763\n",
      "Epoch [8/75], Val Loss: 1.7162\n",
      "Epoch [9/75], Loss: 1.2704\n",
      "Epoch [9/75], Val Loss: 1.7574\n",
      "Epoch [10/75], Loss: 1.2151\n",
      "Epoch [10/75], Val Loss: 1.8398\n",
      "Epoch [11/75], Loss: 0.9832\n",
      "Epoch [11/75], Val Loss: 1.5895\n",
      "Epoch [12/75], Loss: 0.8207\n",
      "Epoch [12/75], Val Loss: 1.4647\n",
      "Epoch [13/75], Loss: 0.6746\n",
      "Epoch [13/75], Val Loss: 1.4407\n",
      "Epoch [14/75], Loss: 0.5340\n",
      "Epoch [14/75], Val Loss: 1.4244\n",
      "Epoch [15/75], Loss: 0.4459\n",
      "Epoch [15/75], Val Loss: 1.4585\n",
      "Epoch [16/75], Loss: 0.3467\n",
      "Epoch [16/75], Val Loss: 1.5392\n",
      "Epoch [17/75], Loss: 0.3128\n",
      "Epoch [17/75], Val Loss: 1.6188\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [17/75], Loss: 0.3128\n",
      "Test Accuracy CNN Lipschitz: 61.78%\n",
      "Epoch [1/75], Loss: 3.2763\n",
      "Epoch [1/75], Val Loss: 3.2285\n",
      "Epoch [2/75], Loss: 3.1116\n",
      "Epoch [2/75], Val Loss: 3.1011\n",
      "Epoch [3/75], Loss: 2.8930\n",
      "Epoch [3/75], Val Loss: 2.8634\n",
      "Epoch [4/75], Loss: 2.5170\n",
      "Epoch [4/75], Val Loss: 2.4205\n",
      "Epoch [5/75], Loss: 1.9634\n",
      "Epoch [5/75], Val Loss: 2.0905\n",
      "Epoch [6/75], Loss: 1.6165\n",
      "Epoch [6/75], Val Loss: 2.1115\n",
      "Epoch [7/75], Loss: 1.6432\n",
      "Epoch [7/75], Val Loss: 2.1621\n",
      "Epoch [8/75], Loss: 1.5430\n",
      "Epoch [8/75], Val Loss: 2.4268\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [8/75], Loss: 1.5430\n",
      "Test Accuracy CNN Lipschitz: 42.54%\n",
      "Epoch [1/75], Loss: 3.3521\n",
      "Epoch [1/75], Val Loss: 3.2327\n",
      "Epoch [2/75], Loss: 3.1094\n",
      "Epoch [2/75], Val Loss: 3.0950\n",
      "Epoch [3/75], Loss: 2.8919\n",
      "Epoch [3/75], Val Loss: 2.8663\n",
      "Epoch [4/75], Loss: 2.4982\n",
      "Epoch [4/75], Val Loss: 2.4818\n",
      "Epoch [5/75], Loss: 1.9812\n",
      "Epoch [5/75], Val Loss: 1.9609\n",
      "Epoch [6/75], Loss: 1.5475\n",
      "Epoch [6/75], Val Loss: 1.7910\n",
      "Epoch [7/75], Loss: 1.4812\n",
      "Epoch [7/75], Val Loss: 2.2292\n",
      "Epoch [8/75], Loss: 1.4506\n",
      "Epoch [8/75], Val Loss: 1.7751\n",
      "Epoch [9/75], Loss: 1.0907\n",
      "Epoch [9/75], Val Loss: 1.6257\n",
      "Epoch [10/75], Loss: 0.9656\n",
      "Epoch [10/75], Val Loss: 1.6364\n",
      "Epoch [11/75], Loss: 0.8203\n",
      "Epoch [11/75], Val Loss: 1.4921\n",
      "Epoch [12/75], Loss: 0.6629\n",
      "Epoch [12/75], Val Loss: 1.4439\n",
      "Epoch [13/75], Loss: 0.5133\n",
      "Epoch [13/75], Val Loss: 1.4927\n",
      "Epoch [14/75], Loss: 0.4207\n",
      "Epoch [14/75], Val Loss: 1.4569\n",
      "Epoch [15/75], Loss: 0.3161\n",
      "Epoch [15/75], Val Loss: 1.5084\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [15/75], Loss: 0.3161\n",
      "Test Accuracy CNN Lipschitz: 63.02%\n",
      "Epoch [1/75], Loss: 3.3141\n",
      "Epoch [1/75], Val Loss: 3.2376\n",
      "Epoch [2/75], Loss: 3.1233\n",
      "Epoch [2/75], Val Loss: 3.1168\n",
      "Epoch [3/75], Loss: 2.9230\n",
      "Epoch [3/75], Val Loss: 2.9077\n",
      "Epoch [4/75], Loss: 2.5728\n",
      "Epoch [4/75], Val Loss: 2.5225\n",
      "Epoch [5/75], Loss: 2.0779\n",
      "Epoch [5/75], Val Loss: 2.0809\n",
      "Epoch [6/75], Loss: 1.7272\n",
      "Epoch [6/75], Val Loss: 1.9701\n",
      "Epoch [7/75], Loss: 1.4851\n",
      "Epoch [7/75], Val Loss: 2.0013\n",
      "Epoch [8/75], Loss: 1.3633\n",
      "Epoch [8/75], Val Loss: 1.9542\n",
      "Epoch [9/75], Loss: 1.1977\n",
      "Epoch [9/75], Val Loss: 1.7339\n",
      "Epoch [10/75], Loss: 1.0730\n",
      "Epoch [10/75], Val Loss: 1.5604\n",
      "Epoch [11/75], Loss: 0.8488\n",
      "Epoch [11/75], Val Loss: 1.5876\n",
      "Epoch [12/75], Loss: 0.7143\n",
      "Epoch [12/75], Val Loss: 1.4772\n",
      "Epoch [13/75], Loss: 0.6309\n",
      "Epoch [13/75], Val Loss: 1.4785\n",
      "Epoch [14/75], Loss: 0.5130\n",
      "Epoch [14/75], Val Loss: 1.5123\n",
      "Epoch [15/75], Loss: 0.4272\n",
      "Epoch [15/75], Val Loss: 1.5426\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [15/75], Loss: 0.4272\n",
      "Test Accuracy CNN Lipschitz: 60.85%\n",
      "Epoch [1/75], Loss: 3.3651\n",
      "Epoch [1/75], Val Loss: 3.1816\n",
      "Epoch [2/75], Loss: 3.0666\n",
      "Epoch [2/75], Val Loss: 2.9993\n",
      "Epoch [3/75], Loss: 2.7358\n",
      "Epoch [3/75], Val Loss: 2.5694\n",
      "Epoch [4/75], Loss: 2.2333\n",
      "Epoch [4/75], Val Loss: 2.0955\n",
      "Epoch [5/75], Loss: 1.7776\n",
      "Epoch [5/75], Val Loss: 1.8416\n",
      "Epoch [6/75], Loss: 1.6623\n",
      "Epoch [6/75], Val Loss: 2.0183\n",
      "Epoch [7/75], Loss: 1.5722\n",
      "Epoch [7/75], Val Loss: 2.0653\n",
      "Epoch [8/75], Loss: 1.5350\n",
      "Epoch [8/75], Val Loss: 1.9121\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [8/75], Loss: 1.5350\n",
      "Test Accuracy CNN Lipschitz: 50.11%\n",
      "Epoch [1/75], Loss: 3.3218\n",
      "Epoch [1/75], Val Loss: 3.2018\n",
      "Epoch [2/75], Loss: 3.0966\n",
      "Epoch [2/75], Val Loss: 3.0576\n",
      "Epoch [3/75], Loss: 2.8556\n",
      "Epoch [3/75], Val Loss: 2.7775\n",
      "Epoch [4/75], Loss: 2.4532\n",
      "Epoch [4/75], Val Loss: 2.3359\n",
      "Epoch [5/75], Loss: 1.9397\n",
      "Epoch [5/75], Val Loss: 1.9749\n",
      "Epoch [6/75], Loss: 1.6327\n",
      "Epoch [6/75], Val Loss: 2.0627\n",
      "Epoch [7/75], Loss: 1.7215\n",
      "Epoch [7/75], Val Loss: 2.1683\n",
      "Epoch [8/75], Loss: 1.5420\n",
      "Epoch [8/75], Val Loss: 1.9583\n",
      "Epoch [9/75], Loss: 1.2961\n",
      "Epoch [9/75], Val Loss: 1.6592\n",
      "Epoch [10/75], Loss: 1.0960\n",
      "Epoch [10/75], Val Loss: 1.5579\n",
      "Epoch [11/75], Loss: 0.9504\n",
      "Epoch [11/75], Val Loss: 1.3386\n",
      "Epoch [12/75], Loss: 0.7518\n",
      "Epoch [12/75], Val Loss: 1.3843\n",
      "Epoch [13/75], Loss: 0.5858\n",
      "Epoch [13/75], Val Loss: 1.4782\n",
      "Epoch [14/75], Loss: 0.4933\n",
      "Epoch [14/75], Val Loss: 1.4679\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.4933\n",
      "Test Accuracy CNN Lipschitz: 62.02%\n",
      "Epoch [1/75], Loss: 3.3024\n",
      "Epoch [1/75], Val Loss: 3.1978\n",
      "Epoch [2/75], Loss: 3.0895\n",
      "Epoch [2/75], Val Loss: 3.0773\n",
      "Epoch [3/75], Loss: 2.8583\n",
      "Epoch [3/75], Val Loss: 2.8023\n",
      "Epoch [4/75], Loss: 2.4452\n",
      "Epoch [4/75], Val Loss: 2.3151\n",
      "Epoch [5/75], Loss: 1.8838\n",
      "Epoch [5/75], Val Loss: 1.9820\n",
      "Epoch [6/75], Loss: 1.5831\n",
      "Epoch [6/75], Val Loss: 1.9808\n",
      "Epoch [7/75], Loss: 1.5400\n",
      "Epoch [7/75], Val Loss: 2.4034\n",
      "Epoch [8/75], Loss: 1.6175\n",
      "Epoch [8/75], Val Loss: 2.0770\n",
      "Epoch [9/75], Loss: 1.2886\n",
      "Epoch [9/75], Val Loss: 1.8897\n",
      "Epoch [10/75], Loss: 1.1544\n",
      "Epoch [10/75], Val Loss: 1.6436\n",
      "Epoch [11/75], Loss: 0.9561\n",
      "Epoch [11/75], Val Loss: 1.4469\n",
      "Epoch [12/75], Loss: 0.7866\n",
      "Epoch [12/75], Val Loss: 1.5187\n",
      "Epoch [13/75], Loss: 0.6272\n",
      "Epoch [13/75], Val Loss: 1.4759\n",
      "Epoch [14/75], Loss: 0.5224\n",
      "Epoch [14/75], Val Loss: 1.4789\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.5224\n",
      "Test Accuracy CNN Lipschitz: 61.10%\n",
      "Epoch [1/75], Loss: 3.3338\n",
      "Epoch [1/75], Val Loss: 3.1998\n",
      "Epoch [2/75], Loss: 3.0776\n",
      "Epoch [2/75], Val Loss: 3.0375\n",
      "Epoch [3/75], Loss: 2.8155\n",
      "Epoch [3/75], Val Loss: 2.7375\n",
      "Epoch [4/75], Loss: 2.3720\n",
      "Epoch [4/75], Val Loss: 2.2559\n",
      "Epoch [5/75], Loss: 1.8292\n",
      "Epoch [5/75], Val Loss: 1.9640\n",
      "Epoch [6/75], Loss: 1.5776\n",
      "Epoch [6/75], Val Loss: 2.0923\n",
      "Epoch [7/75], Loss: 1.5530\n",
      "Epoch [7/75], Val Loss: 2.8549\n",
      "Epoch [8/75], Loss: 2.1637\n",
      "Epoch [8/75], Val Loss: 2.3367\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [8/75], Loss: 2.1637\n",
      "Test Accuracy CNN Lipschitz: 43.33%\n",
      "Epoch [1/75], Loss: 3.3645\n",
      "Epoch [1/75], Val Loss: 3.1962\n",
      "Epoch [2/75], Loss: 3.0830\n",
      "Epoch [2/75], Val Loss: 3.0555\n",
      "Epoch [3/75], Loss: 2.7999\n",
      "Epoch [3/75], Val Loss: 2.7765\n",
      "Epoch [4/75], Loss: 2.3484\n",
      "Epoch [4/75], Val Loss: 2.2590\n",
      "Epoch [5/75], Loss: 1.8558\n",
      "Epoch [5/75], Val Loss: 2.1739\n",
      "Epoch [6/75], Loss: 1.6006\n",
      "Epoch [6/75], Val Loss: 2.0361\n",
      "Epoch [7/75], Loss: 1.5972\n",
      "Epoch [7/75], Val Loss: 2.2474\n",
      "Epoch [8/75], Loss: 1.5188\n",
      "Epoch [8/75], Val Loss: 2.0607\n",
      "Epoch [9/75], Loss: 1.3286\n",
      "Epoch [9/75], Val Loss: 1.6869\n",
      "Epoch [10/75], Loss: 1.0668\n",
      "Epoch [10/75], Val Loss: 1.5806\n",
      "Epoch [11/75], Loss: 0.9176\n",
      "Epoch [11/75], Val Loss: 1.4906\n",
      "Epoch [12/75], Loss: 0.7131\n",
      "Epoch [12/75], Val Loss: 1.4723\n",
      "Epoch [13/75], Loss: 0.5837\n",
      "Epoch [13/75], Val Loss: 1.6548\n",
      "Epoch [14/75], Loss: 0.4854\n",
      "Epoch [14/75], Val Loss: 1.5916\n",
      "Epoch [15/75], Loss: 0.4108\n",
      "Epoch [15/75], Val Loss: 1.5463\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [15/75], Loss: 0.4108\n",
      "Test Accuracy CNN Lipschitz: 61.11%\n",
      "Epoch [1/75], Loss: 3.3095\n",
      "Epoch [1/75], Val Loss: 3.1988\n",
      "Epoch [2/75], Loss: 3.0539\n",
      "Epoch [2/75], Val Loss: 3.0010\n",
      "Epoch [3/75], Loss: 2.7205\n",
      "Epoch [3/75], Val Loss: 2.6318\n",
      "Epoch [4/75], Loss: 2.2085\n",
      "Epoch [4/75], Val Loss: 2.1131\n",
      "Epoch [5/75], Loss: 1.6758\n",
      "Epoch [5/75], Val Loss: 2.0324\n",
      "Epoch [6/75], Loss: 1.5096\n",
      "Epoch [6/75], Val Loss: 2.1472\n",
      "Epoch [7/75], Loss: 1.5829\n",
      "Epoch [7/75], Val Loss: 2.1727\n",
      "Epoch [8/75], Loss: 1.4096\n",
      "Epoch [8/75], Val Loss: 1.9543\n",
      "Epoch [9/75], Loss: 1.1194\n",
      "Epoch [9/75], Val Loss: 1.6321\n",
      "Epoch [10/75], Loss: 0.9294\n",
      "Epoch [10/75], Val Loss: 1.4951\n",
      "Epoch [11/75], Loss: 0.8011\n",
      "Epoch [11/75], Val Loss: 1.4167\n",
      "Epoch [12/75], Loss: 0.6037\n",
      "Epoch [12/75], Val Loss: 1.4772\n",
      "Epoch [13/75], Loss: 0.4892\n",
      "Epoch [13/75], Val Loss: 1.4530\n",
      "Epoch [14/75], Loss: 0.4029\n",
      "Epoch [14/75], Val Loss: 1.5387\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 50000, Epoch [14/75], Loss: 0.4029\n",
      "Test Accuracy CNN Lipschitz: 61.51%\n",
      "Training with subset size: 10000\n",
      "Epoch [1/75], Loss: 3.3417\n",
      "Epoch [1/75], Val Loss: 3.2374\n",
      "Epoch [2/75], Loss: 3.1306\n",
      "Epoch [2/75], Val Loss: 3.1031\n",
      "Epoch [3/75], Loss: 2.9187\n",
      "Epoch [3/75], Val Loss: 2.8706\n",
      "Epoch [4/75], Loss: 2.5363\n",
      "Epoch [4/75], Val Loss: 2.4336\n",
      "Epoch [5/75], Loss: 2.0216\n",
      "Epoch [5/75], Val Loss: 2.0508\n",
      "Epoch [6/75], Loss: 1.5804\n",
      "Epoch [6/75], Val Loss: 1.8375\n",
      "Epoch [7/75], Loss: 1.3664\n",
      "Epoch [7/75], Val Loss: 2.2579\n",
      "Epoch [8/75], Loss: 1.7341\n",
      "Epoch [8/75], Val Loss: 2.5390\n",
      "Epoch [9/75], Loss: 1.5460\n",
      "Epoch [9/75], Val Loss: 1.9635\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [9/75], Loss: 1.5460\n",
      "Test Accuracy CNN Lipschitz: 46.70%\n",
      "Epoch [1/75], Loss: 3.2899\n",
      "Epoch [1/75], Val Loss: 3.2051\n",
      "Epoch [2/75], Loss: 3.1058\n",
      "Epoch [2/75], Val Loss: 3.0606\n",
      "Epoch [3/75], Loss: 2.8709\n",
      "Epoch [3/75], Val Loss: 2.7932\n",
      "Epoch [4/75], Loss: 2.4542\n",
      "Epoch [4/75], Val Loss: 2.3030\n",
      "Epoch [5/75], Loss: 1.9406\n",
      "Epoch [5/75], Val Loss: 1.9145\n",
      "Epoch [6/75], Loss: 1.6178\n",
      "Epoch [6/75], Val Loss: 2.1767\n",
      "Epoch [7/75], Loss: 1.5374\n",
      "Epoch [7/75], Val Loss: 1.8712\n",
      "Epoch [8/75], Loss: 1.2222\n",
      "Epoch [8/75], Val Loss: 1.9053\n",
      "Epoch [9/75], Loss: 1.1133\n",
      "Epoch [9/75], Val Loss: 1.7651\n",
      "Epoch [10/75], Loss: 1.0075\n",
      "Epoch [10/75], Val Loss: 1.6032\n",
      "Epoch [11/75], Loss: 0.8021\n",
      "Epoch [11/75], Val Loss: 1.5260\n",
      "Epoch [12/75], Loss: 0.6756\n",
      "Epoch [12/75], Val Loss: 1.5447\n",
      "Epoch [13/75], Loss: 0.5728\n",
      "Epoch [13/75], Val Loss: 1.4809\n",
      "Epoch [14/75], Loss: 0.4298\n",
      "Epoch [14/75], Val Loss: 1.5959\n",
      "Epoch [15/75], Loss: 0.3817\n",
      "Epoch [15/75], Val Loss: 1.6293\n",
      "Epoch [16/75], Loss: 0.3096\n",
      "Epoch [16/75], Val Loss: 1.6237\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [16/75], Loss: 0.3096\n",
      "Test Accuracy CNN Lipschitz: 61.91%\n",
      "Epoch [1/75], Loss: 3.2912\n",
      "Epoch [1/75], Val Loss: 3.2216\n",
      "Epoch [2/75], Loss: 3.1013\n",
      "Epoch [2/75], Val Loss: 3.0912\n",
      "Epoch [3/75], Loss: 2.8734\n",
      "Epoch [3/75], Val Loss: 2.8506\n",
      "Epoch [4/75], Loss: 2.4739\n",
      "Epoch [4/75], Val Loss: 2.3917\n",
      "Epoch [5/75], Loss: 1.9635\n",
      "Epoch [5/75], Val Loss: 2.0453\n",
      "Epoch [6/75], Loss: 1.6193\n",
      "Epoch [6/75], Val Loss: 2.1022\n",
      "Epoch [7/75], Loss: 1.5849\n",
      "Epoch [7/75], Val Loss: 2.2466\n",
      "Epoch [8/75], Loss: 1.4125\n",
      "Epoch [8/75], Val Loss: 1.9171\n",
      "Epoch [9/75], Loss: 1.1391\n",
      "Epoch [9/75], Val Loss: 1.6208\n",
      "Epoch [10/75], Loss: 0.9620\n",
      "Epoch [10/75], Val Loss: 1.5709\n",
      "Epoch [11/75], Loss: 0.8347\n",
      "Epoch [11/75], Val Loss: 1.5474\n",
      "Epoch [12/75], Loss: 0.6612\n",
      "Epoch [12/75], Val Loss: 1.5197\n",
      "Epoch [13/75], Loss: 0.5365\n",
      "Epoch [13/75], Val Loss: 1.5884\n",
      "Epoch [14/75], Loss: 0.5096\n",
      "Epoch [14/75], Val Loss: 1.6750\n",
      "Epoch [15/75], Loss: 0.4270\n",
      "Epoch [15/75], Val Loss: 1.6031\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.4270\n",
      "Test Accuracy CNN Lipschitz: 59.58%\n",
      "Epoch [1/75], Loss: 3.3266\n",
      "Epoch [1/75], Val Loss: 3.2316\n",
      "Epoch [2/75], Loss: 3.1313\n",
      "Epoch [2/75], Val Loss: 3.1431\n",
      "Epoch [3/75], Loss: 2.9554\n",
      "Epoch [3/75], Val Loss: 2.9160\n",
      "Epoch [4/75], Loss: 2.6500\n",
      "Epoch [4/75], Val Loss: 2.5366\n",
      "Epoch [5/75], Loss: 2.1147\n",
      "Epoch [5/75], Val Loss: 2.0989\n",
      "Epoch [6/75], Loss: 1.7128\n",
      "Epoch [6/75], Val Loss: 1.9422\n",
      "Epoch [7/75], Loss: 1.4778\n",
      "Epoch [7/75], Val Loss: 1.9490\n",
      "Epoch [8/75], Loss: 1.3967\n",
      "Epoch [8/75], Val Loss: 1.9652\n",
      "Epoch [9/75], Loss: 1.3300\n",
      "Epoch [9/75], Val Loss: 1.7297\n",
      "Epoch [10/75], Loss: 1.1364\n",
      "Epoch [10/75], Val Loss: 1.5802\n",
      "Epoch [11/75], Loss: 0.9070\n",
      "Epoch [11/75], Val Loss: 1.5267\n",
      "Epoch [12/75], Loss: 0.7884\n",
      "Epoch [12/75], Val Loss: 1.4782\n",
      "Epoch [13/75], Loss: 0.6429\n",
      "Epoch [13/75], Val Loss: 1.4659\n",
      "Epoch [14/75], Loss: 0.5360\n",
      "Epoch [14/75], Val Loss: 1.4401\n",
      "Epoch [15/75], Loss: 0.4409\n",
      "Epoch [15/75], Val Loss: 1.5149\n",
      "Epoch [16/75], Loss: 0.3728\n",
      "Epoch [16/75], Val Loss: 1.5583\n",
      "Epoch [17/75], Loss: 0.3136\n",
      "Epoch [17/75], Val Loss: 1.6407\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [17/75], Loss: 0.3136\n",
      "Test Accuracy CNN Lipschitz: 60.94%\n",
      "Epoch [1/75], Loss: 3.2962\n",
      "Epoch [1/75], Val Loss: 3.2234\n",
      "Epoch [2/75], Loss: 3.1034\n",
      "Epoch [2/75], Val Loss: 3.0657\n",
      "Epoch [3/75], Loss: 2.8524\n",
      "Epoch [3/75], Val Loss: 2.7960\n",
      "Epoch [4/75], Loss: 2.4375\n",
      "Epoch [4/75], Val Loss: 2.3623\n",
      "Epoch [5/75], Loss: 1.9328\n",
      "Epoch [5/75], Val Loss: 1.9351\n",
      "Epoch [6/75], Loss: 1.6709\n",
      "Epoch [6/75], Val Loss: 1.9777\n",
      "Epoch [7/75], Loss: 1.5063\n",
      "Epoch [7/75], Val Loss: 1.9437\n",
      "Epoch [8/75], Loss: 1.2998\n",
      "Epoch [8/75], Val Loss: 2.0327\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [8/75], Loss: 1.2998\n",
      "Test Accuracy CNN Lipschitz: 48.31%\n",
      "Epoch [1/75], Loss: 3.3146\n",
      "Epoch [1/75], Val Loss: 3.2624\n",
      "Epoch [2/75], Loss: 3.1635\n",
      "Epoch [2/75], Val Loss: 3.1466\n",
      "Epoch [3/75], Loss: 2.9885\n",
      "Epoch [3/75], Val Loss: 2.9872\n",
      "Epoch [4/75], Loss: 2.7071\n",
      "Epoch [4/75], Val Loss: 2.6305\n",
      "Epoch [5/75], Loss: 2.2071\n",
      "Epoch [5/75], Val Loss: 2.1409\n",
      "Epoch [6/75], Loss: 1.6736\n",
      "Epoch [6/75], Val Loss: 1.8797\n",
      "Epoch [7/75], Loss: 1.4777\n",
      "Epoch [7/75], Val Loss: 1.9901\n",
      "Epoch [8/75], Loss: 1.3407\n",
      "Epoch [8/75], Val Loss: 2.4422\n",
      "Epoch [9/75], Loss: 1.4435\n",
      "Epoch [9/75], Val Loss: 2.0273\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [9/75], Loss: 1.4435\n",
      "Test Accuracy CNN Lipschitz: 46.40%\n",
      "Epoch [1/75], Loss: 3.2922\n",
      "Epoch [1/75], Val Loss: 3.2353\n",
      "Epoch [2/75], Loss: 3.1351\n",
      "Epoch [2/75], Val Loss: 3.1229\n",
      "Epoch [3/75], Loss: 2.9397\n",
      "Epoch [3/75], Val Loss: 2.9272\n",
      "Epoch [4/75], Loss: 2.6182\n",
      "Epoch [4/75], Val Loss: 2.5596\n",
      "Epoch [5/75], Loss: 2.1114\n",
      "Epoch [5/75], Val Loss: 2.0779\n",
      "Epoch [6/75], Loss: 1.6475\n",
      "Epoch [6/75], Val Loss: 2.0355\n",
      "Epoch [7/75], Loss: 1.5086\n",
      "Epoch [7/75], Val Loss: 2.3711\n",
      "Epoch [8/75], Loss: 1.7769\n",
      "Epoch [8/75], Val Loss: 2.1937\n",
      "Epoch [9/75], Loss: 1.4334\n",
      "Epoch [9/75], Val Loss: 1.8405\n",
      "Epoch [10/75], Loss: 1.2520\n",
      "Epoch [10/75], Val Loss: 1.7943\n",
      "Epoch [11/75], Loss: 1.0935\n",
      "Epoch [11/75], Val Loss: 1.6354\n",
      "Epoch [12/75], Loss: 0.8975\n",
      "Epoch [12/75], Val Loss: 1.4973\n",
      "Epoch [13/75], Loss: 0.7358\n",
      "Epoch [13/75], Val Loss: 1.5332\n",
      "Epoch [14/75], Loss: 0.5849\n",
      "Epoch [14/75], Val Loss: 1.6369\n",
      "Epoch [15/75], Loss: 0.4924\n",
      "Epoch [15/75], Val Loss: 1.5573\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.4924\n",
      "Test Accuracy CNN Lipschitz: 61.12%\n",
      "Epoch [1/75], Loss: 3.3322\n",
      "Epoch [1/75], Val Loss: 3.2549\n",
      "Epoch [2/75], Loss: 3.1796\n",
      "Epoch [2/75], Val Loss: 3.1549\n",
      "Epoch [3/75], Loss: 3.0201\n",
      "Epoch [3/75], Val Loss: 3.0222\n",
      "Epoch [4/75], Loss: 2.7781\n",
      "Epoch [4/75], Val Loss: 2.7649\n",
      "Epoch [5/75], Loss: 2.3947\n",
      "Epoch [5/75], Val Loss: 2.3301\n",
      "Epoch [6/75], Loss: 1.9219\n",
      "Epoch [6/75], Val Loss: 1.9195\n",
      "Epoch [7/75], Loss: 1.6297\n",
      "Epoch [7/75], Val Loss: 2.0300\n",
      "Epoch [8/75], Loss: 1.6735\n",
      "Epoch [8/75], Val Loss: 2.0090\n",
      "Epoch [9/75], Loss: 1.2808\n",
      "Epoch [9/75], Val Loss: 1.7133\n",
      "Epoch [10/75], Loss: 1.1397\n",
      "Epoch [10/75], Val Loss: 1.5943\n",
      "Epoch [11/75], Loss: 0.9474\n",
      "Epoch [11/75], Val Loss: 1.5137\n",
      "Epoch [12/75], Loss: 0.8255\n",
      "Epoch [12/75], Val Loss: 1.4563\n",
      "Epoch [13/75], Loss: 0.6858\n",
      "Epoch [13/75], Val Loss: 1.4008\n",
      "Epoch [14/75], Loss: 0.5594\n",
      "Epoch [14/75], Val Loss: 1.4972\n",
      "Epoch [15/75], Loss: 0.4744\n",
      "Epoch [15/75], Val Loss: 1.4667\n",
      "Epoch [16/75], Loss: 0.3690\n",
      "Epoch [16/75], Val Loss: 1.4788\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [16/75], Loss: 0.3690\n",
      "Test Accuracy CNN Lipschitz: 62.32%\n",
      "Epoch [1/75], Loss: 3.3198\n",
      "Epoch [1/75], Val Loss: 3.2231\n",
      "Epoch [2/75], Loss: 3.0985\n",
      "Epoch [2/75], Val Loss: 3.0810\n",
      "Epoch [3/75], Loss: 2.8391\n",
      "Epoch [3/75], Val Loss: 2.7695\n",
      "Epoch [4/75], Loss: 2.3908\n",
      "Epoch [4/75], Val Loss: 2.2602\n",
      "Epoch [5/75], Loss: 1.8635\n",
      "Epoch [5/75], Val Loss: 1.9935\n",
      "Epoch [6/75], Loss: 1.8088\n",
      "Epoch [6/75], Val Loss: 2.1027\n",
      "Epoch [7/75], Loss: 1.5993\n",
      "Epoch [7/75], Val Loss: 2.1934\n",
      "Epoch [8/75], Loss: 1.4305\n",
      "Epoch [8/75], Val Loss: 1.7516\n",
      "Epoch [9/75], Loss: 1.1469\n",
      "Epoch [9/75], Val Loss: 1.6913\n",
      "Epoch [10/75], Loss: 1.0353\n",
      "Epoch [10/75], Val Loss: 1.5439\n",
      "Epoch [11/75], Loss: 0.8154\n",
      "Epoch [11/75], Val Loss: 1.5326\n",
      "Epoch [12/75], Loss: 0.6652\n",
      "Epoch [12/75], Val Loss: 1.4652\n",
      "Epoch [13/75], Loss: 0.5699\n",
      "Epoch [13/75], Val Loss: 1.5652\n",
      "Epoch [14/75], Loss: 0.4541\n",
      "Epoch [14/75], Val Loss: 1.5380\n",
      "Epoch [15/75], Loss: 0.3733\n",
      "Epoch [15/75], Val Loss: 1.5677\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.3733\n",
      "Test Accuracy CNN Lipschitz: 62.11%\n",
      "Epoch [1/75], Loss: 3.3324\n",
      "Epoch [1/75], Val Loss: 3.2318\n",
      "Epoch [2/75], Loss: 3.1147\n",
      "Epoch [2/75], Val Loss: 3.0828\n",
      "Epoch [3/75], Loss: 2.8735\n",
      "Epoch [3/75], Val Loss: 2.8542\n",
      "Epoch [4/75], Loss: 2.4963\n",
      "Epoch [4/75], Val Loss: 2.4467\n",
      "Epoch [5/75], Loss: 2.0299\n",
      "Epoch [5/75], Val Loss: 2.0628\n",
      "Epoch [6/75], Loss: 1.6429\n",
      "Epoch [6/75], Val Loss: 1.9161\n",
      "Epoch [7/75], Loss: 1.4910\n",
      "Epoch [7/75], Val Loss: 1.9688\n",
      "Epoch [8/75], Loss: 1.4273\n",
      "Epoch [8/75], Val Loss: 2.0545\n",
      "Epoch [9/75], Loss: 1.4205\n",
      "Epoch [9/75], Val Loss: 1.6041\n",
      "Epoch [10/75], Loss: 1.0690\n",
      "Epoch [10/75], Val Loss: 1.6270\n",
      "Epoch [11/75], Loss: 0.8841\n",
      "Epoch [11/75], Val Loss: 1.4858\n",
      "Epoch [12/75], Loss: 0.7814\n",
      "Epoch [12/75], Val Loss: 1.4620\n",
      "Epoch [13/75], Loss: 0.5916\n",
      "Epoch [13/75], Val Loss: 1.4533\n",
      "Epoch [14/75], Loss: 0.4926\n",
      "Epoch [14/75], Val Loss: 1.4339\n",
      "Epoch [15/75], Loss: 0.3834\n",
      "Epoch [15/75], Val Loss: 1.5312\n",
      "Epoch [16/75], Loss: 0.2956\n",
      "Epoch [16/75], Val Loss: 1.5233\n",
      "Epoch [17/75], Loss: 0.2274\n",
      "Epoch [17/75], Val Loss: 1.6077\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [17/75], Loss: 0.2274\n",
      "Test Accuracy CNN Lipschitz: 62.80%\n",
      "Epoch [1/75], Loss: 3.3162\n",
      "Epoch [1/75], Val Loss: 3.2037\n",
      "Epoch [2/75], Loss: 3.0549\n",
      "Epoch [2/75], Val Loss: 3.0350\n",
      "Epoch [3/75], Loss: 2.7607\n",
      "Epoch [3/75], Val Loss: 2.6860\n",
      "Epoch [4/75], Loss: 2.3059\n",
      "Epoch [4/75], Val Loss: 2.1955\n",
      "Epoch [5/75], Loss: 1.7584\n",
      "Epoch [5/75], Val Loss: 2.0055\n",
      "Epoch [6/75], Loss: 1.5862\n",
      "Epoch [6/75], Val Loss: 2.0879\n",
      "Epoch [7/75], Loss: 1.5964\n",
      "Epoch [7/75], Val Loss: 2.1117\n",
      "Epoch [8/75], Loss: 1.3512\n",
      "Epoch [8/75], Val Loss: 1.9104\n",
      "Epoch [9/75], Loss: 1.1940\n",
      "Epoch [9/75], Val Loss: 1.8030\n",
      "Epoch [10/75], Loss: 1.0431\n",
      "Epoch [10/75], Val Loss: 1.5829\n",
      "Epoch [11/75], Loss: 0.8803\n",
      "Epoch [11/75], Val Loss: 1.5111\n",
      "Epoch [12/75], Loss: 0.7238\n",
      "Epoch [12/75], Val Loss: 1.4784\n",
      "Epoch [13/75], Loss: 0.5918\n",
      "Epoch [13/75], Val Loss: 1.5015\n",
      "Epoch [14/75], Loss: 0.4656\n",
      "Epoch [14/75], Val Loss: 1.5562\n",
      "Epoch [15/75], Loss: 0.4096\n",
      "Epoch [15/75], Val Loss: 1.5682\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.4096\n",
      "Test Accuracy CNN Lipschitz: 60.54%\n",
      "Epoch [1/75], Loss: 3.3244\n",
      "Epoch [1/75], Val Loss: 3.2191\n",
      "Epoch [2/75], Loss: 3.1124\n",
      "Epoch [2/75], Val Loss: 3.0681\n",
      "Epoch [3/75], Loss: 2.8738\n",
      "Epoch [3/75], Val Loss: 2.8172\n",
      "Epoch [4/75], Loss: 2.4738\n",
      "Epoch [4/75], Val Loss: 2.3871\n",
      "Epoch [5/75], Loss: 1.9736\n",
      "Epoch [5/75], Val Loss: 1.9673\n",
      "Epoch [6/75], Loss: 1.6059\n",
      "Epoch [6/75], Val Loss: 1.9920\n",
      "Epoch [7/75], Loss: 1.5479\n",
      "Epoch [7/75], Val Loss: 1.7796\n",
      "Epoch [8/75], Loss: 1.2748\n",
      "Epoch [8/75], Val Loss: 1.7196\n",
      "Epoch [9/75], Loss: 1.0482\n",
      "Epoch [9/75], Val Loss: 1.7465\n",
      "Epoch [10/75], Loss: 0.9533\n",
      "Epoch [10/75], Val Loss: 1.5955\n",
      "Epoch [11/75], Loss: 0.7820\n",
      "Epoch [11/75], Val Loss: 1.5012\n",
      "Epoch [12/75], Loss: 0.6588\n",
      "Epoch [12/75], Val Loss: 1.4978\n",
      "Epoch [13/75], Loss: 0.5600\n",
      "Epoch [13/75], Val Loss: 1.5465\n",
      "Epoch [14/75], Loss: 0.4514\n",
      "Epoch [14/75], Val Loss: 1.4623\n",
      "Epoch [15/75], Loss: 0.3663\n",
      "Epoch [15/75], Val Loss: 1.6058\n",
      "Epoch [16/75], Loss: 0.3183\n",
      "Epoch [16/75], Val Loss: 1.5942\n",
      "Epoch [17/75], Loss: 0.2386\n",
      "Epoch [17/75], Val Loss: 1.6411\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [17/75], Loss: 0.2386\n",
      "Test Accuracy CNN Lipschitz: 61.95%\n",
      "Epoch [1/75], Loss: 3.3168\n",
      "Epoch [1/75], Val Loss: 3.2048\n",
      "Epoch [2/75], Loss: 3.0585\n",
      "Epoch [2/75], Val Loss: 3.0096\n",
      "Epoch [3/75], Loss: 2.7387\n",
      "Epoch [3/75], Val Loss: 2.6440\n",
      "Epoch [4/75], Loss: 2.2236\n",
      "Epoch [4/75], Val Loss: 2.1287\n",
      "Epoch [5/75], Loss: 1.6854\n",
      "Epoch [5/75], Val Loss: 2.0555\n",
      "Epoch [6/75], Loss: 1.7805\n",
      "Epoch [6/75], Val Loss: 2.7087\n",
      "Epoch [7/75], Loss: 1.9936\n",
      "Epoch [7/75], Val Loss: 2.0337\n",
      "Epoch [8/75], Loss: 1.6396\n",
      "Epoch [8/75], Val Loss: 1.8179\n",
      "Epoch [9/75], Loss: 1.3676\n",
      "Epoch [9/75], Val Loss: 1.7998\n",
      "Epoch [10/75], Loss: 1.2730\n",
      "Epoch [10/75], Val Loss: 1.5526\n",
      "Epoch [11/75], Loss: 0.9191\n",
      "Epoch [11/75], Val Loss: 1.4774\n",
      "Epoch [12/75], Loss: 0.7412\n",
      "Epoch [12/75], Val Loss: 1.5432\n",
      "Epoch [13/75], Loss: 0.6282\n",
      "Epoch [13/75], Val Loss: 1.5730\n",
      "Epoch [14/75], Loss: 0.5353\n",
      "Epoch [14/75], Val Loss: 1.5790\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [14/75], Loss: 0.5353\n",
      "Test Accuracy CNN Lipschitz: 60.54%\n",
      "Epoch [1/75], Loss: 3.3640\n",
      "Epoch [1/75], Val Loss: 3.2055\n",
      "Epoch [2/75], Loss: 3.0991\n",
      "Epoch [2/75], Val Loss: 3.0370\n",
      "Epoch [3/75], Loss: 2.8115\n",
      "Epoch [3/75], Val Loss: 2.7218\n",
      "Epoch [4/75], Loss: 2.3398\n",
      "Epoch [4/75], Val Loss: 2.1992\n",
      "Epoch [5/75], Loss: 1.7705\n",
      "Epoch [5/75], Val Loss: 1.8806\n",
      "Epoch [6/75], Loss: 1.4810\n",
      "Epoch [6/75], Val Loss: 1.9666\n",
      "Epoch [7/75], Loss: 1.3333\n",
      "Epoch [7/75], Val Loss: 2.0962\n",
      "Epoch [8/75], Loss: 1.3360\n",
      "Epoch [8/75], Val Loss: 1.9810\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [8/75], Loss: 1.3360\n",
      "Test Accuracy CNN Lipschitz: 48.43%\n",
      "Epoch [1/75], Loss: 3.2991\n",
      "Epoch [1/75], Val Loss: 3.2288\n",
      "Epoch [2/75], Loss: 3.1183\n",
      "Epoch [2/75], Val Loss: 3.0890\n",
      "Epoch [3/75], Loss: 2.9071\n",
      "Epoch [3/75], Val Loss: 2.8601\n",
      "Epoch [4/75], Loss: 2.5457\n",
      "Epoch [4/75], Val Loss: 2.4490\n",
      "Epoch [5/75], Loss: 2.0004\n",
      "Epoch [5/75], Val Loss: 1.9554\n",
      "Epoch [6/75], Loss: 1.5682\n",
      "Epoch [6/75], Val Loss: 1.9188\n",
      "Epoch [7/75], Loss: 1.3705\n",
      "Epoch [7/75], Val Loss: 1.8327\n",
      "Epoch [8/75], Loss: 1.2287\n",
      "Epoch [8/75], Val Loss: 1.7622\n",
      "Epoch [9/75], Loss: 1.1016\n",
      "Epoch [9/75], Val Loss: 1.6752\n",
      "Epoch [10/75], Loss: 0.8939\n",
      "Epoch [10/75], Val Loss: 1.4628\n",
      "Epoch [11/75], Loss: 0.7338\n",
      "Epoch [11/75], Val Loss: 1.4427\n",
      "Epoch [12/75], Loss: 0.6573\n",
      "Epoch [12/75], Val Loss: 1.4278\n",
      "Epoch [13/75], Loss: 0.5153\n",
      "Epoch [13/75], Val Loss: 1.3586\n",
      "Epoch [14/75], Loss: 0.3947\n",
      "Epoch [14/75], Val Loss: 1.4558\n",
      "Epoch [15/75], Loss: 0.3339\n",
      "Epoch [15/75], Val Loss: 1.3939\n",
      "Epoch [16/75], Loss: 0.2586\n",
      "Epoch [16/75], Val Loss: 1.4854\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [16/75], Loss: 0.2586\n",
      "Test Accuracy CNN Lipschitz: 63.43%\n",
      "Epoch [1/75], Loss: 3.2563\n",
      "Epoch [1/75], Val Loss: 3.1555\n",
      "Epoch [2/75], Loss: 3.0175\n",
      "Epoch [2/75], Val Loss: 2.9379\n",
      "Epoch [3/75], Loss: 2.6677\n",
      "Epoch [3/75], Val Loss: 2.5530\n",
      "Epoch [4/75], Loss: 2.1661\n",
      "Epoch [4/75], Val Loss: 2.0576\n",
      "Epoch [5/75], Loss: 1.7273\n",
      "Epoch [5/75], Val Loss: 1.8511\n",
      "Epoch [6/75], Loss: 1.5763\n",
      "Epoch [6/75], Val Loss: 2.1669\n",
      "Epoch [7/75], Loss: 1.3984\n",
      "Epoch [7/75], Val Loss: 1.8471\n",
      "Epoch [8/75], Loss: 1.3191\n",
      "Epoch [8/75], Val Loss: 1.7330\n",
      "Epoch [9/75], Loss: 1.1305\n",
      "Epoch [9/75], Val Loss: 1.6205\n",
      "Epoch [10/75], Loss: 0.9007\n",
      "Epoch [10/75], Val Loss: 1.5882\n",
      "Epoch [11/75], Loss: 0.8468\n",
      "Epoch [11/75], Val Loss: 1.5043\n",
      "Epoch [12/75], Loss: 0.6386\n",
      "Epoch [12/75], Val Loss: 1.5369\n",
      "Epoch [13/75], Loss: 0.5604\n",
      "Epoch [13/75], Val Loss: 1.5759\n",
      "Epoch [14/75], Loss: 0.5218\n",
      "Epoch [14/75], Val Loss: 1.5788\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [14/75], Loss: 0.5218\n",
      "Test Accuracy CNN Lipschitz: 60.10%\n",
      "Epoch [1/75], Loss: 3.3499\n",
      "Epoch [1/75], Val Loss: 3.2411\n",
      "Epoch [2/75], Loss: 3.1412\n",
      "Epoch [2/75], Val Loss: 3.1293\n",
      "Epoch [3/75], Loss: 2.9270\n",
      "Epoch [3/75], Val Loss: 2.8889\n",
      "Epoch [4/75], Loss: 2.5712\n",
      "Epoch [4/75], Val Loss: 2.4538\n",
      "Epoch [5/75], Loss: 2.0147\n",
      "Epoch [5/75], Val Loss: 2.0959\n",
      "Epoch [6/75], Loss: 1.6338\n",
      "Epoch [6/75], Val Loss: 1.9457\n",
      "Epoch [7/75], Loss: 1.5246\n",
      "Epoch [7/75], Val Loss: 1.9090\n",
      "Epoch [8/75], Loss: 1.3369\n",
      "Epoch [8/75], Val Loss: 2.0064\n",
      "Epoch [9/75], Loss: 1.1748\n",
      "Epoch [9/75], Val Loss: 1.7044\n",
      "Epoch [10/75], Loss: 1.0425\n",
      "Epoch [10/75], Val Loss: 1.5823\n",
      "Epoch [11/75], Loss: 0.8336\n",
      "Epoch [11/75], Val Loss: 1.4534\n",
      "Epoch [12/75], Loss: 0.6793\n",
      "Epoch [12/75], Val Loss: 1.4456\n",
      "Epoch [13/75], Loss: 0.5413\n",
      "Epoch [13/75], Val Loss: 1.4767\n",
      "Epoch [14/75], Loss: 0.4359\n",
      "Epoch [14/75], Val Loss: 1.5273\n",
      "Epoch [15/75], Loss: 0.3518\n",
      "Epoch [15/75], Val Loss: 1.5827\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.3518\n",
      "Test Accuracy CNN Lipschitz: 60.34%\n",
      "Epoch [1/75], Loss: 3.3177\n",
      "Epoch [1/75], Val Loss: 3.2172\n",
      "Epoch [2/75], Loss: 3.0620\n",
      "Epoch [2/75], Val Loss: 3.0164\n",
      "Epoch [3/75], Loss: 2.7750\n",
      "Epoch [3/75], Val Loss: 2.6828\n",
      "Epoch [4/75], Loss: 2.3000\n",
      "Epoch [4/75], Val Loss: 2.2184\n",
      "Epoch [5/75], Loss: 1.7648\n",
      "Epoch [5/75], Val Loss: 1.9269\n",
      "Epoch [6/75], Loss: 1.5362\n",
      "Epoch [6/75], Val Loss: 2.0449\n",
      "Epoch [7/75], Loss: 1.6426\n",
      "Epoch [7/75], Val Loss: 2.5056\n",
      "Epoch [8/75], Loss: 1.4877\n",
      "Epoch [8/75], Val Loss: 1.8766\n",
      "Epoch [9/75], Loss: 1.2122\n",
      "Epoch [9/75], Val Loss: 1.6479\n",
      "Epoch [10/75], Loss: 1.0409\n",
      "Epoch [10/75], Val Loss: 1.5586\n",
      "Epoch [11/75], Loss: 0.8568\n",
      "Epoch [11/75], Val Loss: 1.4355\n",
      "Epoch [12/75], Loss: 0.6715\n",
      "Epoch [12/75], Val Loss: 1.4530\n",
      "Epoch [13/75], Loss: 0.5238\n",
      "Epoch [13/75], Val Loss: 1.5102\n",
      "Epoch [14/75], Loss: 0.4458\n",
      "Epoch [14/75], Val Loss: 1.5452\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [14/75], Loss: 0.4458\n",
      "Test Accuracy CNN Lipschitz: 60.20%\n",
      "Epoch [1/75], Loss: 3.2996\n",
      "Epoch [1/75], Val Loss: 3.1845\n",
      "Epoch [2/75], Loss: 3.0124\n",
      "Epoch [2/75], Val Loss: 2.9823\n",
      "Epoch [3/75], Loss: 2.6626\n",
      "Epoch [3/75], Val Loss: 2.5526\n",
      "Epoch [4/75], Loss: 2.1238\n",
      "Epoch [4/75], Val Loss: 2.0378\n",
      "Epoch [5/75], Loss: 1.6655\n",
      "Epoch [5/75], Val Loss: 1.9277\n",
      "Epoch [6/75], Loss: 1.5885\n",
      "Epoch [6/75], Val Loss: 2.8965\n",
      "Epoch [7/75], Loss: 2.2612\n",
      "Epoch [7/75], Val Loss: 2.7524\n",
      "Epoch [8/75], Loss: 1.7928\n",
      "Epoch [8/75], Val Loss: 1.9182\n",
      "Epoch [9/75], Loss: 1.4563\n",
      "Epoch [9/75], Val Loss: 1.8221\n",
      "Epoch [10/75], Loss: 1.2708\n",
      "Epoch [10/75], Val Loss: 1.4848\n",
      "Epoch [11/75], Loss: 0.8473\n",
      "Epoch [11/75], Val Loss: 1.4707\n",
      "Epoch [12/75], Loss: 0.6809\n",
      "Epoch [12/75], Val Loss: 1.5805\n",
      "Epoch [13/75], Loss: 0.5722\n",
      "Epoch [13/75], Val Loss: 1.6694\n",
      "Epoch [14/75], Loss: 0.4616\n",
      "Epoch [14/75], Val Loss: 1.7152\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [14/75], Loss: 0.4616\n",
      "Test Accuracy CNN Lipschitz: 60.73%\n",
      "Epoch [1/75], Loss: 3.3285\n",
      "Epoch [1/75], Val Loss: 3.2381\n",
      "Epoch [2/75], Loss: 3.1288\n",
      "Epoch [2/75], Val Loss: 3.1043\n",
      "Epoch [3/75], Loss: 2.9179\n",
      "Epoch [3/75], Val Loss: 2.8863\n",
      "Epoch [4/75], Loss: 2.5685\n",
      "Epoch [4/75], Val Loss: 2.5016\n",
      "Epoch [5/75], Loss: 2.0615\n",
      "Epoch [5/75], Val Loss: 2.0458\n",
      "Epoch [6/75], Loss: 1.6104\n",
      "Epoch [6/75], Val Loss: 1.9282\n",
      "Epoch [7/75], Loss: 1.4438\n",
      "Epoch [7/75], Val Loss: 1.9462\n",
      "Epoch [8/75], Loss: 1.3971\n",
      "Epoch [8/75], Val Loss: 1.9326\n",
      "Epoch [9/75], Loss: 1.1957\n",
      "Epoch [9/75], Val Loss: 1.8143\n",
      "Epoch [10/75], Loss: 1.0224\n",
      "Epoch [10/75], Val Loss: 1.8253\n",
      "Epoch [11/75], Loss: 0.9878\n",
      "Epoch [11/75], Val Loss: 1.4526\n",
      "Epoch [12/75], Loss: 0.7697\n",
      "Epoch [12/75], Val Loss: 1.4184\n",
      "Epoch [13/75], Loss: 0.6390\n",
      "Epoch [13/75], Val Loss: 1.4813\n",
      "Epoch [14/75], Loss: 0.5351\n",
      "Epoch [14/75], Val Loss: 1.4612\n",
      "Epoch [15/75], Loss: 0.4176\n",
      "Epoch [15/75], Val Loss: 1.5261\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.4176\n",
      "Test Accuracy CNN Lipschitz: 60.79%\n",
      "Epoch [1/75], Loss: 3.3792\n",
      "Epoch [1/75], Val Loss: 3.2441\n",
      "Epoch [2/75], Loss: 3.1410\n",
      "Epoch [2/75], Val Loss: 3.1351\n",
      "Epoch [3/75], Loss: 2.9592\n",
      "Epoch [3/75], Val Loss: 2.9428\n",
      "Epoch [4/75], Loss: 2.6376\n",
      "Epoch [4/75], Val Loss: 2.5666\n",
      "Epoch [5/75], Loss: 2.1605\n",
      "Epoch [5/75], Val Loss: 2.0670\n",
      "Epoch [6/75], Loss: 1.6768\n",
      "Epoch [6/75], Val Loss: 1.8946\n",
      "Epoch [7/75], Loss: 1.5728\n",
      "Epoch [7/75], Val Loss: 2.1565\n",
      "Epoch [8/75], Loss: 1.8263\n",
      "Epoch [8/75], Val Loss: 2.1570\n",
      "Epoch [9/75], Loss: 1.6595\n",
      "Epoch [9/75], Val Loss: 1.8034\n",
      "Epoch [10/75], Loss: 1.2529\n",
      "Epoch [10/75], Val Loss: 1.8197\n",
      "Epoch [11/75], Loss: 1.1735\n",
      "Epoch [11/75], Val Loss: 1.5748\n",
      "Epoch [12/75], Loss: 0.8724\n",
      "Epoch [12/75], Val Loss: 1.4243\n",
      "Epoch [13/75], Loss: 0.6626\n",
      "Epoch [13/75], Val Loss: 1.5155\n",
      "Epoch [14/75], Loss: 0.5907\n",
      "Epoch [14/75], Val Loss: 1.5987\n",
      "Epoch [15/75], Loss: 0.5390\n",
      "Epoch [15/75], Val Loss: 1.5468\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.5390\n",
      "Test Accuracy CNN Lipschitz: 61.39%\n",
      "Epoch [1/75], Loss: 3.3187\n",
      "Epoch [1/75], Val Loss: 3.1623\n",
      "Epoch [2/75], Loss: 3.0215\n",
      "Epoch [2/75], Val Loss: 2.9326\n",
      "Epoch [3/75], Loss: 2.6906\n",
      "Epoch [3/75], Val Loss: 2.5632\n",
      "Epoch [4/75], Loss: 2.1530\n",
      "Epoch [4/75], Val Loss: 2.0717\n",
      "Epoch [5/75], Loss: 1.7399\n",
      "Epoch [5/75], Val Loss: 1.9038\n",
      "Epoch [6/75], Loss: 1.6499\n",
      "Epoch [6/75], Val Loss: 2.0138\n",
      "Epoch [7/75], Loss: 1.3643\n",
      "Epoch [7/75], Val Loss: 1.7917\n",
      "Epoch [8/75], Loss: 1.1052\n",
      "Epoch [8/75], Val Loss: 1.6074\n",
      "Epoch [9/75], Loss: 0.9071\n",
      "Epoch [9/75], Val Loss: 1.4717\n",
      "Epoch [10/75], Loss: 0.7447\n",
      "Epoch [10/75], Val Loss: 1.4823\n",
      "Epoch [11/75], Loss: 0.6386\n",
      "Epoch [11/75], Val Loss: 1.5416\n",
      "Epoch [12/75], Loss: 0.5468\n",
      "Epoch [12/75], Val Loss: 1.5339\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [12/75], Loss: 0.5468\n",
      "Test Accuracy CNN Lipschitz: 59.62%\n",
      "Epoch [1/75], Loss: 3.3252\n",
      "Epoch [1/75], Val Loss: 3.2282\n",
      "Epoch [2/75], Loss: 3.1189\n",
      "Epoch [2/75], Val Loss: 3.0885\n",
      "Epoch [3/75], Loss: 2.9028\n",
      "Epoch [3/75], Val Loss: 2.8473\n",
      "Epoch [4/75], Loss: 2.5173\n",
      "Epoch [4/75], Val Loss: 2.3999\n",
      "Epoch [5/75], Loss: 1.9964\n",
      "Epoch [5/75], Val Loss: 1.9133\n",
      "Epoch [6/75], Loss: 1.5744\n",
      "Epoch [6/75], Val Loss: 2.0279\n",
      "Epoch [7/75], Loss: 1.6011\n",
      "Epoch [7/75], Val Loss: 1.8953\n",
      "Epoch [8/75], Loss: 1.5054\n",
      "Epoch [8/75], Val Loss: 1.8391\n",
      "Epoch [9/75], Loss: 1.3422\n",
      "Epoch [9/75], Val Loss: 1.7104\n",
      "Epoch [10/75], Loss: 1.1495\n",
      "Epoch [10/75], Val Loss: 1.6497\n",
      "Epoch [11/75], Loss: 0.9527\n",
      "Epoch [11/75], Val Loss: 1.5577\n",
      "Epoch [12/75], Loss: 0.7854\n",
      "Epoch [12/75], Val Loss: 1.4795\n",
      "Epoch [13/75], Loss: 0.6782\n",
      "Epoch [13/75], Val Loss: 1.5138\n",
      "Epoch [14/75], Loss: 0.4983\n",
      "Epoch [14/75], Val Loss: 1.4993\n",
      "Epoch [15/75], Loss: 0.4413\n",
      "Epoch [15/75], Val Loss: 1.5666\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.4413\n",
      "Test Accuracy CNN Lipschitz: 60.04%\n",
      "Epoch [1/75], Loss: 3.3094\n",
      "Epoch [1/75], Val Loss: 3.2325\n",
      "Epoch [2/75], Loss: 3.0548\n",
      "Epoch [2/75], Val Loss: 3.0124\n",
      "Epoch [3/75], Loss: 2.7369\n",
      "Epoch [3/75], Val Loss: 2.6424\n",
      "Epoch [4/75], Loss: 2.1947\n",
      "Epoch [4/75], Val Loss: 2.1415\n",
      "Epoch [5/75], Loss: 1.6773\n",
      "Epoch [5/75], Val Loss: 2.1083\n",
      "Epoch [6/75], Loss: 1.5562\n",
      "Epoch [6/75], Val Loss: 1.9550\n",
      "Epoch [7/75], Loss: 1.6408\n",
      "Epoch [7/75], Val Loss: 2.1585\n",
      "Epoch [8/75], Loss: 1.3983\n",
      "Epoch [8/75], Val Loss: 1.9402\n",
      "Epoch [9/75], Loss: 1.1633\n",
      "Epoch [9/75], Val Loss: 1.6810\n",
      "Epoch [10/75], Loss: 0.9865\n",
      "Epoch [10/75], Val Loss: 1.5704\n",
      "Epoch [11/75], Loss: 0.8056\n",
      "Epoch [11/75], Val Loss: 1.4822\n",
      "Epoch [12/75], Loss: 0.6169\n",
      "Epoch [12/75], Val Loss: 1.4652\n",
      "Epoch [13/75], Loss: 0.4857\n",
      "Epoch [13/75], Val Loss: 1.5156\n",
      "Epoch [14/75], Loss: 0.4261\n",
      "Epoch [14/75], Val Loss: 1.5464\n",
      "Epoch [15/75], Loss: 0.3285\n",
      "Epoch [15/75], Val Loss: 1.6718\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.3285\n",
      "Test Accuracy CNN Lipschitz: 60.03%\n",
      "Epoch [1/75], Loss: 3.3792\n",
      "Epoch [1/75], Val Loss: 3.2675\n",
      "Epoch [2/75], Loss: 3.1699\n",
      "Epoch [2/75], Val Loss: 3.1601\n",
      "Epoch [3/75], Loss: 3.0002\n",
      "Epoch [3/75], Val Loss: 2.9943\n",
      "Epoch [4/75], Loss: 2.7169\n",
      "Epoch [4/75], Val Loss: 2.6511\n",
      "Epoch [5/75], Loss: 2.2511\n",
      "Epoch [5/75], Val Loss: 2.2320\n",
      "Epoch [6/75], Loss: 1.7626\n",
      "Epoch [6/75], Val Loss: 1.9979\n",
      "Epoch [7/75], Loss: 1.5663\n",
      "Epoch [7/75], Val Loss: 1.9616\n",
      "Epoch [8/75], Loss: 1.4129\n",
      "Epoch [8/75], Val Loss: 2.0280\n",
      "Epoch [9/75], Loss: 1.1789\n",
      "Epoch [9/75], Val Loss: 1.9437\n",
      "Epoch [10/75], Loss: 1.0475\n",
      "Epoch [10/75], Val Loss: 1.5365\n",
      "Epoch [11/75], Loss: 0.8793\n",
      "Epoch [11/75], Val Loss: 1.5250\n",
      "Epoch [12/75], Loss: 0.6951\n",
      "Epoch [12/75], Val Loss: 1.4965\n",
      "Epoch [13/75], Loss: 0.5991\n",
      "Epoch [13/75], Val Loss: 1.4289\n",
      "Epoch [14/75], Loss: 0.4842\n",
      "Epoch [14/75], Val Loss: 1.4820\n",
      "Epoch [15/75], Loss: 0.4199\n",
      "Epoch [15/75], Val Loss: 1.5258\n",
      "Epoch [16/75], Loss: 0.3781\n",
      "Epoch [16/75], Val Loss: 1.5369\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [16/75], Loss: 0.3781\n",
      "Test Accuracy CNN Lipschitz: 61.33%\n",
      "Epoch [1/75], Loss: 3.3157\n",
      "Epoch [1/75], Val Loss: 3.2222\n",
      "Epoch [2/75], Loss: 3.1161\n",
      "Epoch [2/75], Val Loss: 3.0923\n",
      "Epoch [3/75], Loss: 2.9133\n",
      "Epoch [3/75], Val Loss: 2.8641\n",
      "Epoch [4/75], Loss: 2.5766\n",
      "Epoch [4/75], Val Loss: 2.4684\n",
      "Epoch [5/75], Loss: 2.0920\n",
      "Epoch [5/75], Val Loss: 2.0044\n",
      "Epoch [6/75], Loss: 1.6970\n",
      "Epoch [6/75], Val Loss: 1.8994\n",
      "Epoch [7/75], Loss: 1.5939\n",
      "Epoch [7/75], Val Loss: 1.8803\n",
      "Epoch [8/75], Loss: 1.6285\n",
      "Epoch [8/75], Val Loss: 2.1158\n",
      "Epoch [9/75], Loss: 1.5050\n",
      "Epoch [9/75], Val Loss: 1.7953\n",
      "Epoch [10/75], Loss: 1.1733\n",
      "Epoch [10/75], Val Loss: 1.5891\n",
      "Epoch [11/75], Loss: 0.9631\n",
      "Epoch [11/75], Val Loss: 1.4945\n",
      "Epoch [12/75], Loss: 0.8065\n",
      "Epoch [12/75], Val Loss: 1.4409\n",
      "Epoch [13/75], Loss: 0.6323\n",
      "Epoch [13/75], Val Loss: 1.5165\n",
      "Epoch [14/75], Loss: 0.5164\n",
      "Epoch [14/75], Val Loss: 1.5161\n",
      "Epoch [15/75], Loss: 0.4312\n",
      "Epoch [15/75], Val Loss: 1.5379\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [15/75], Loss: 0.4312\n",
      "Test Accuracy CNN Lipschitz: 61.75%\n",
      "Epoch [1/75], Loss: 3.3587\n",
      "Epoch [1/75], Val Loss: 3.1985\n",
      "Epoch [2/75], Loss: 3.0597\n",
      "Epoch [2/75], Val Loss: 3.0030\n",
      "Epoch [3/75], Loss: 2.7428\n",
      "Epoch [3/75], Val Loss: 2.6737\n",
      "Epoch [4/75], Loss: 2.2743\n",
      "Epoch [4/75], Val Loss: 2.2137\n",
      "Epoch [5/75], Loss: 1.7465\n",
      "Epoch [5/75], Val Loss: 1.9091\n",
      "Epoch [6/75], Loss: 1.5070\n",
      "Epoch [6/75], Val Loss: 2.0987\n",
      "Epoch [7/75], Loss: 1.4916\n",
      "Epoch [7/75], Val Loss: 1.9276\n",
      "Epoch [8/75], Loss: 1.4278\n",
      "Epoch [8/75], Val Loss: 2.2439\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [8/75], Loss: 1.4278\n",
      "Test Accuracy CNN Lipschitz: 49.69%\n",
      "Epoch [1/75], Loss: 3.3394\n",
      "Epoch [1/75], Val Loss: 3.2220\n",
      "Epoch [2/75], Loss: 3.0795\n",
      "Epoch [2/75], Val Loss: 3.0385\n",
      "Epoch [3/75], Loss: 2.7989\n",
      "Epoch [3/75], Val Loss: 2.7568\n",
      "Epoch [4/75], Loss: 2.3833\n",
      "Epoch [4/75], Val Loss: 2.3082\n",
      "Epoch [5/75], Loss: 1.9183\n",
      "Epoch [5/75], Val Loss: 2.1029\n",
      "Epoch [6/75], Loss: 1.6055\n",
      "Epoch [6/75], Val Loss: 2.0915\n",
      "Epoch [7/75], Loss: 1.5532\n",
      "Epoch [7/75], Val Loss: 2.1518\n",
      "Epoch [8/75], Loss: 1.3161\n",
      "Epoch [8/75], Val Loss: 1.8130\n",
      "Epoch [9/75], Loss: 1.0885\n",
      "Epoch [9/75], Val Loss: 1.6884\n",
      "Epoch [10/75], Loss: 0.9264\n",
      "Epoch [10/75], Val Loss: 1.5975\n",
      "Epoch [11/75], Loss: 0.8282\n",
      "Epoch [11/75], Val Loss: 1.5798\n",
      "Epoch [12/75], Loss: 0.7096\n",
      "Epoch [12/75], Val Loss: 1.5469\n",
      "Epoch [13/75], Loss: 0.6046\n",
      "Epoch [13/75], Val Loss: 1.5719\n",
      "Epoch [14/75], Loss: 0.5136\n",
      "Epoch [14/75], Val Loss: 1.5621\n",
      "Epoch [15/75], Loss: 0.4134\n",
      "Epoch [15/75], Val Loss: 1.5427\n",
      "Epoch [16/75], Loss: 0.3335\n",
      "Epoch [16/75], Val Loss: 1.5800\n",
      "Epoch [17/75], Loss: 0.2660\n",
      "Epoch [17/75], Val Loss: 1.6645\n",
      "Epoch [18/75], Loss: 0.2502\n",
      "Epoch [18/75], Val Loss: 1.6923\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [18/75], Loss: 0.2502\n",
      "Test Accuracy CNN Lipschitz: 60.14%\n",
      "Epoch [1/75], Loss: 3.3147\n",
      "Epoch [1/75], Val Loss: 3.1757\n",
      "Epoch [2/75], Loss: 3.0450\n",
      "Epoch [2/75], Val Loss: 3.0048\n",
      "Epoch [3/75], Loss: 2.7600\n",
      "Epoch [3/75], Val Loss: 2.6256\n",
      "Epoch [4/75], Loss: 2.2101\n",
      "Epoch [4/75], Val Loss: 2.0663\n",
      "Epoch [5/75], Loss: 1.7081\n",
      "Epoch [5/75], Val Loss: 2.0067\n",
      "Epoch [6/75], Loss: 1.6623\n",
      "Epoch [6/75], Val Loss: 2.0195\n",
      "Epoch [7/75], Loss: 1.5498\n",
      "Epoch [7/75], Val Loss: 2.2398\n",
      "Epoch [8/75], Loss: 1.5280\n",
      "Epoch [8/75], Val Loss: 1.7277\n",
      "Epoch [9/75], Loss: 1.2073\n",
      "Epoch [9/75], Val Loss: 1.6359\n",
      "Epoch [10/75], Loss: 0.9760\n",
      "Epoch [10/75], Val Loss: 1.5141\n",
      "Epoch [11/75], Loss: 0.8116\n",
      "Epoch [11/75], Val Loss: 1.4611\n",
      "Epoch [12/75], Loss: 0.6568\n",
      "Epoch [12/75], Val Loss: 1.4654\n",
      "Epoch [13/75], Loss: 0.5430\n",
      "Epoch [13/75], Val Loss: 1.4873\n",
      "Epoch [14/75], Loss: 0.4513\n",
      "Epoch [14/75], Val Loss: 1.5773\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [14/75], Loss: 0.4513\n",
      "Test Accuracy CNN Lipschitz: 60.98%\n",
      "Epoch [1/75], Loss: 3.3906\n",
      "Epoch [1/75], Val Loss: 3.2566\n",
      "Epoch [2/75], Loss: 3.1883\n",
      "Epoch [2/75], Val Loss: 3.1443\n",
      "Epoch [3/75], Loss: 2.9930\n",
      "Epoch [3/75], Val Loss: 2.9507\n",
      "Epoch [4/75], Loss: 2.6909\n",
      "Epoch [4/75], Val Loss: 2.6067\n",
      "Epoch [5/75], Loss: 2.2362\n",
      "Epoch [5/75], Val Loss: 2.1187\n",
      "Epoch [6/75], Loss: 1.7584\n",
      "Epoch [6/75], Val Loss: 1.8616\n",
      "Epoch [7/75], Loss: 1.4673\n",
      "Epoch [7/75], Val Loss: 1.8222\n",
      "Epoch [8/75], Loss: 1.4784\n",
      "Epoch [8/75], Val Loss: 2.2400\n",
      "Epoch [9/75], Loss: 1.6351\n",
      "Epoch [9/75], Val Loss: 2.3953\n",
      "Epoch [10/75], Loss: 1.5272\n",
      "Epoch [10/75], Val Loss: 1.8734\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Subset 10000, Epoch [10/75], Loss: 1.5272\n",
      "Test Accuracy CNN Lipschitz: 48.76%\n",
      "Training with subset size: 5000\n",
      "Epoch [1/75], Loss: 3.3399\n",
      "Epoch [1/75], Val Loss: 3.1725\n",
      "Epoch [2/75], Loss: 3.0561\n",
      "Epoch [2/75], Val Loss: 2.9862\n",
      "Epoch [3/75], Loss: 2.7441\n",
      "Epoch [3/75], Val Loss: 2.6666\n",
      "Epoch [4/75], Loss: 2.2774\n",
      "Epoch [4/75], Val Loss: 2.1424\n",
      "Epoch [5/75], Loss: 1.7653\n",
      "Epoch [5/75], Val Loss: 1.9793\n",
      "Epoch [6/75], Loss: 1.5194\n",
      "Epoch [6/75], Val Loss: 1.9127\n",
      "Epoch [7/75], Loss: 1.2996\n",
      "Epoch [7/75], Val Loss: 1.7243\n",
      "Epoch [8/75], Loss: 1.1426\n",
      "Epoch [8/75], Val Loss: 1.7016\n",
      "Epoch [9/75], Loss: 1.0375\n",
      "Epoch [9/75], Val Loss: 1.5686\n",
      "Epoch [10/75], Loss: 0.8863\n",
      "Epoch [10/75], Val Loss: 1.4991\n",
      "Epoch [11/75], Loss: 0.7309\n",
      "Epoch [11/75], Val Loss: 1.5203\n",
      "Epoch [12/75], Loss: 0.5971\n",
      "Epoch [12/75], Val Loss: 1.3966\n",
      "Epoch [13/75], Loss: 0.4760\n",
      "Epoch [13/75], Val Loss: 1.4457\n",
      "Epoch [14/75], Loss: 0.3834\n",
      "Epoch [14/75], Val Loss: 1.4913\n",
      "Epoch [15/75], Loss: 0.2735\n",
      "Epoch [15/75], Val Loss: 1.4441\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.2735\n",
      "Test Accuracy CNN Lipschitz: 63.62%\n",
      "Epoch [1/75], Loss: 3.3187\n",
      "Epoch [1/75], Val Loss: 3.2069\n",
      "Epoch [2/75], Loss: 3.0963\n",
      "Epoch [2/75], Val Loss: 3.0386\n",
      "Epoch [3/75], Loss: 2.8116\n",
      "Epoch [3/75], Val Loss: 2.7324\n",
      "Epoch [4/75], Loss: 2.3538\n",
      "Epoch [4/75], Val Loss: 2.2287\n",
      "Epoch [5/75], Loss: 1.8165\n",
      "Epoch [5/75], Val Loss: 1.9583\n",
      "Epoch [6/75], Loss: 1.5857\n",
      "Epoch [6/75], Val Loss: 2.1394\n",
      "Epoch [7/75], Loss: 1.6457\n",
      "Epoch [7/75], Val Loss: 2.1348\n",
      "Epoch [8/75], Loss: 1.4673\n",
      "Epoch [8/75], Val Loss: 1.7078\n",
      "Epoch [9/75], Loss: 1.1619\n",
      "Epoch [9/75], Val Loss: 1.6216\n",
      "Epoch [10/75], Loss: 0.9529\n",
      "Epoch [10/75], Val Loss: 1.5739\n",
      "Epoch [11/75], Loss: 0.7632\n",
      "Epoch [11/75], Val Loss: 1.4549\n",
      "Epoch [12/75], Loss: 0.6078\n",
      "Epoch [12/75], Val Loss: 1.4599\n",
      "Epoch [13/75], Loss: 0.4871\n",
      "Epoch [13/75], Val Loss: 1.5509\n",
      "Epoch [14/75], Loss: 0.3803\n",
      "Epoch [14/75], Val Loss: 1.5791\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [14/75], Loss: 0.3803\n",
      "Test Accuracy CNN Lipschitz: 60.76%\n",
      "Epoch [1/75], Loss: 3.2811\n",
      "Epoch [1/75], Val Loss: 3.1902\n",
      "Epoch [2/75], Loss: 3.0706\n",
      "Epoch [2/75], Val Loss: 3.0449\n",
      "Epoch [3/75], Loss: 2.8208\n",
      "Epoch [3/75], Val Loss: 2.7650\n",
      "Epoch [4/75], Loss: 2.4182\n",
      "Epoch [4/75], Val Loss: 2.2934\n",
      "Epoch [5/75], Loss: 1.9037\n",
      "Epoch [5/75], Val Loss: 1.9045\n",
      "Epoch [6/75], Loss: 1.5807\n",
      "Epoch [6/75], Val Loss: 2.0447\n",
      "Epoch [7/75], Loss: 1.5222\n",
      "Epoch [7/75], Val Loss: 2.0291\n",
      "Epoch [8/75], Loss: 1.3249\n",
      "Epoch [8/75], Val Loss: 1.9349\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [8/75], Loss: 1.3249\n",
      "Test Accuracy CNN Lipschitz: 47.24%\n",
      "Epoch [1/75], Loss: 3.3675\n",
      "Epoch [1/75], Val Loss: 3.1974\n",
      "Epoch [2/75], Loss: 3.0407\n",
      "Epoch [2/75], Val Loss: 2.9571\n",
      "Epoch [3/75], Loss: 2.6575\n",
      "Epoch [3/75], Val Loss: 2.5338\n",
      "Epoch [4/75], Loss: 2.0827\n",
      "Epoch [4/75], Val Loss: 2.0945\n",
      "Epoch [5/75], Loss: 1.6791\n",
      "Epoch [5/75], Val Loss: 2.1512\n",
      "Epoch [6/75], Loss: 1.6882\n",
      "Epoch [6/75], Val Loss: 2.6102\n",
      "Epoch [7/75], Loss: 1.6645\n",
      "Epoch [7/75], Val Loss: 2.0479\n",
      "Epoch [8/75], Loss: 1.2951\n",
      "Epoch [8/75], Val Loss: 1.6915\n",
      "Epoch [9/75], Loss: 1.1754\n",
      "Epoch [9/75], Val Loss: 1.5363\n",
      "Epoch [10/75], Loss: 0.9011\n",
      "Epoch [10/75], Val Loss: 1.5051\n",
      "Epoch [11/75], Loss: 0.7403\n",
      "Epoch [11/75], Val Loss: 1.4971\n",
      "Epoch [12/75], Loss: 0.5968\n",
      "Epoch [12/75], Val Loss: 1.5628\n",
      "Epoch [13/75], Loss: 0.4709\n",
      "Epoch [13/75], Val Loss: 1.5173\n",
      "Epoch [14/75], Loss: 0.3820\n",
      "Epoch [14/75], Val Loss: 1.6188\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [14/75], Loss: 0.3820\n",
      "Test Accuracy CNN Lipschitz: 59.64%\n",
      "Epoch [1/75], Loss: 3.3030\n",
      "Epoch [1/75], Val Loss: 3.2357\n",
      "Epoch [2/75], Loss: 3.1183\n",
      "Epoch [2/75], Val Loss: 3.0950\n",
      "Epoch [3/75], Loss: 2.9187\n",
      "Epoch [3/75], Val Loss: 2.8765\n",
      "Epoch [4/75], Loss: 2.5496\n",
      "Epoch [4/75], Val Loss: 2.4697\n",
      "Epoch [5/75], Loss: 2.0209\n",
      "Epoch [5/75], Val Loss: 2.0680\n",
      "Epoch [6/75], Loss: 1.5981\n",
      "Epoch [6/75], Val Loss: 1.9013\n",
      "Epoch [7/75], Loss: 1.4250\n",
      "Epoch [7/75], Val Loss: 2.3719\n",
      "Epoch [8/75], Loss: 2.0354\n",
      "Epoch [8/75], Val Loss: 2.6412\n",
      "Epoch [9/75], Loss: 1.6402\n",
      "Epoch [9/75], Val Loss: 1.5999\n",
      "Epoch [10/75], Loss: 1.1970\n",
      "Epoch [10/75], Val Loss: 1.7106\n",
      "Epoch [11/75], Loss: 1.0215\n",
      "Epoch [11/75], Val Loss: 1.4995\n",
      "Epoch [12/75], Loss: 0.7657\n",
      "Epoch [12/75], Val Loss: 1.4541\n",
      "Epoch [13/75], Loss: 0.6221\n",
      "Epoch [13/75], Val Loss: 1.5009\n",
      "Epoch [14/75], Loss: 0.5101\n",
      "Epoch [14/75], Val Loss: 1.5634\n",
      "Epoch [15/75], Loss: 0.4282\n",
      "Epoch [15/75], Val Loss: 1.5547\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.4282\n",
      "Test Accuracy CNN Lipschitz: 61.52%\n",
      "Epoch [1/75], Loss: 3.3521\n",
      "Epoch [1/75], Val Loss: 3.2643\n",
      "Epoch [2/75], Loss: 3.1961\n",
      "Epoch [2/75], Val Loss: 3.1742\n",
      "Epoch [3/75], Loss: 3.0602\n",
      "Epoch [3/75], Val Loss: 3.0758\n",
      "Epoch [4/75], Loss: 2.8837\n",
      "Epoch [4/75], Val Loss: 2.8664\n",
      "Epoch [5/75], Loss: 2.5534\n",
      "Epoch [5/75], Val Loss: 2.4800\n",
      "Epoch [6/75], Loss: 2.1459\n",
      "Epoch [6/75], Val Loss: 2.0689\n",
      "Epoch [7/75], Loss: 1.6926\n",
      "Epoch [7/75], Val Loss: 1.8486\n",
      "Epoch [8/75], Loss: 1.4024\n",
      "Epoch [8/75], Val Loss: 1.6875\n",
      "Epoch [9/75], Loss: 1.1869\n",
      "Epoch [9/75], Val Loss: 1.6399\n",
      "Epoch [10/75], Loss: 1.1222\n",
      "Epoch [10/75], Val Loss: 1.8314\n",
      "Epoch [11/75], Loss: 1.1180\n",
      "Epoch [11/75], Val Loss: 1.8488\n",
      "Epoch [12/75], Loss: 0.9644\n",
      "Epoch [12/75], Val Loss: 1.5784\n",
      "Epoch [13/75], Loss: 0.8352\n",
      "Epoch [13/75], Val Loss: 1.3972\n",
      "Epoch [14/75], Loss: 0.6326\n",
      "Epoch [14/75], Val Loss: 1.3991\n",
      "Epoch [15/75], Loss: 0.5443\n",
      "Epoch [15/75], Val Loss: 1.3918\n",
      "Epoch [16/75], Loss: 0.4087\n",
      "Epoch [16/75], Val Loss: 1.3560\n",
      "Epoch [17/75], Loss: 0.3306\n",
      "Epoch [17/75], Val Loss: 1.4525\n",
      "Epoch [18/75], Loss: 0.2508\n",
      "Epoch [18/75], Val Loss: 1.4483\n",
      "Epoch [19/75], Loss: 0.1959\n",
      "Epoch [19/75], Val Loss: 1.4736\n",
      "Stopping early at epoch 19 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [19/75], Loss: 0.1959\n",
      "Test Accuracy CNN Lipschitz: 64.14%\n",
      "Epoch [1/75], Loss: 3.2809\n",
      "Epoch [1/75], Val Loss: 3.1602\n",
      "Epoch [2/75], Loss: 2.9973\n",
      "Epoch [2/75], Val Loss: 2.9516\n",
      "Epoch [3/75], Loss: 2.6158\n",
      "Epoch [3/75], Val Loss: 2.5158\n",
      "Epoch [4/75], Loss: 2.0396\n",
      "Epoch [4/75], Val Loss: 2.0000\n",
      "Epoch [5/75], Loss: 1.6188\n",
      "Epoch [5/75], Val Loss: 1.9640\n",
      "Epoch [6/75], Loss: 1.4941\n",
      "Epoch [6/75], Val Loss: 2.2096\n",
      "Epoch [7/75], Loss: 1.4137\n",
      "Epoch [7/75], Val Loss: 1.8504\n",
      "Epoch [8/75], Loss: 1.0671\n",
      "Epoch [8/75], Val Loss: 1.5918\n",
      "Epoch [9/75], Loss: 0.9151\n",
      "Epoch [9/75], Val Loss: 1.5112\n",
      "Epoch [10/75], Loss: 0.7149\n",
      "Epoch [10/75], Val Loss: 1.4444\n",
      "Epoch [11/75], Loss: 0.5898\n",
      "Epoch [11/75], Val Loss: 1.4567\n",
      "Epoch [12/75], Loss: 0.4568\n",
      "Epoch [12/75], Val Loss: 1.4872\n",
      "Epoch [13/75], Loss: 0.3747\n",
      "Epoch [13/75], Val Loss: 1.5076\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [13/75], Loss: 0.3747\n",
      "Test Accuracy CNN Lipschitz: 62.22%\n",
      "Epoch [1/75], Loss: 3.3121\n",
      "Epoch [1/75], Val Loss: 3.1651\n",
      "Epoch [2/75], Loss: 3.0247\n",
      "Epoch [2/75], Val Loss: 2.9787\n",
      "Epoch [3/75], Loss: 2.6792\n",
      "Epoch [3/75], Val Loss: 2.5807\n",
      "Epoch [4/75], Loss: 2.1543\n",
      "Epoch [4/75], Val Loss: 2.0605\n",
      "Epoch [5/75], Loss: 1.6582\n",
      "Epoch [5/75], Val Loss: 1.9294\n",
      "Epoch [6/75], Loss: 1.5926\n",
      "Epoch [6/75], Val Loss: 2.2597\n",
      "Epoch [7/75], Loss: 1.6027\n",
      "Epoch [7/75], Val Loss: 1.9937\n",
      "Epoch [8/75], Loss: 1.3632\n",
      "Epoch [8/75], Val Loss: 1.7721\n",
      "Epoch [9/75], Loss: 1.1361\n",
      "Epoch [9/75], Val Loss: 1.6466\n",
      "Epoch [10/75], Loss: 1.0267\n",
      "Epoch [10/75], Val Loss: 1.5139\n",
      "Epoch [11/75], Loss: 0.8230\n",
      "Epoch [11/75], Val Loss: 1.4975\n",
      "Epoch [12/75], Loss: 0.6852\n",
      "Epoch [12/75], Val Loss: 1.4399\n",
      "Epoch [13/75], Loss: 0.5891\n",
      "Epoch [13/75], Val Loss: 1.4911\n",
      "Epoch [14/75], Loss: 0.4792\n",
      "Epoch [14/75], Val Loss: 1.5420\n",
      "Epoch [15/75], Loss: 0.3636\n",
      "Epoch [15/75], Val Loss: 1.5862\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.3636\n",
      "Test Accuracy CNN Lipschitz: 59.38%\n",
      "Epoch [1/75], Loss: 3.2616\n",
      "Epoch [1/75], Val Loss: 3.2012\n",
      "Epoch [2/75], Loss: 3.0758\n",
      "Epoch [2/75], Val Loss: 3.0533\n",
      "Epoch [3/75], Loss: 2.8042\n",
      "Epoch [3/75], Val Loss: 2.7490\n",
      "Epoch [4/75], Loss: 2.3852\n",
      "Epoch [4/75], Val Loss: 2.2550\n",
      "Epoch [5/75], Loss: 1.8385\n",
      "Epoch [5/75], Val Loss: 1.9994\n",
      "Epoch [6/75], Loss: 1.5835\n",
      "Epoch [6/75], Val Loss: 1.8987\n",
      "Epoch [7/75], Loss: 1.5147\n",
      "Epoch [7/75], Val Loss: 1.9718\n",
      "Epoch [8/75], Loss: 1.3900\n",
      "Epoch [8/75], Val Loss: 2.0972\n",
      "Epoch [9/75], Loss: 1.2543\n",
      "Epoch [9/75], Val Loss: 1.5294\n",
      "Epoch [10/75], Loss: 1.0215\n",
      "Epoch [10/75], Val Loss: 1.5724\n",
      "Epoch [11/75], Loss: 0.8242\n",
      "Epoch [11/75], Val Loss: 1.5125\n",
      "Epoch [12/75], Loss: 0.6991\n",
      "Epoch [12/75], Val Loss: 1.4778\n",
      "Epoch [13/75], Loss: 0.5244\n",
      "Epoch [13/75], Val Loss: 1.4454\n",
      "Epoch [14/75], Loss: 0.4250\n",
      "Epoch [14/75], Val Loss: 1.4916\n",
      "Epoch [15/75], Loss: 0.3370\n",
      "Epoch [15/75], Val Loss: 1.6214\n",
      "Epoch [16/75], Loss: 0.2743\n",
      "Epoch [16/75], Val Loss: 1.6075\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [16/75], Loss: 0.2743\n",
      "Test Accuracy CNN Lipschitz: 62.87%\n",
      "Epoch [1/75], Loss: 3.3498\n",
      "Epoch [1/75], Val Loss: 3.2449\n",
      "Epoch [2/75], Loss: 3.1488\n",
      "Epoch [2/75], Val Loss: 3.1130\n",
      "Epoch [3/75], Loss: 2.9534\n",
      "Epoch [3/75], Val Loss: 2.9228\n",
      "Epoch [4/75], Loss: 2.6293\n",
      "Epoch [4/75], Val Loss: 2.5460\n",
      "Epoch [5/75], Loss: 2.1290\n",
      "Epoch [5/75], Val Loss: 2.0652\n",
      "Epoch [6/75], Loss: 1.6539\n",
      "Epoch [6/75], Val Loss: 1.9297\n",
      "Epoch [7/75], Loss: 1.4617\n",
      "Epoch [7/75], Val Loss: 1.8753\n",
      "Epoch [8/75], Loss: 1.3301\n",
      "Epoch [8/75], Val Loss: 2.1480\n",
      "Epoch [9/75], Loss: 1.2090\n",
      "Epoch [9/75], Val Loss: 1.8825\n",
      "Epoch [10/75], Loss: 1.1152\n",
      "Epoch [10/75], Val Loss: 1.6946\n",
      "Epoch [11/75], Loss: 0.8721\n",
      "Epoch [11/75], Val Loss: 1.5953\n",
      "Epoch [12/75], Loss: 0.7538\n",
      "Epoch [12/75], Val Loss: 1.5120\n",
      "Epoch [13/75], Loss: 0.5975\n",
      "Epoch [13/75], Val Loss: 1.5478\n",
      "Epoch [14/75], Loss: 0.5281\n",
      "Epoch [14/75], Val Loss: 1.4942\n",
      "Epoch [15/75], Loss: 0.4270\n",
      "Epoch [15/75], Val Loss: 1.5943\n",
      "Epoch [16/75], Loss: 0.3205\n",
      "Epoch [16/75], Val Loss: 1.5881\n",
      "Epoch [17/75], Loss: 0.2766\n",
      "Epoch [17/75], Val Loss: 1.6159\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [17/75], Loss: 0.2766\n",
      "Test Accuracy CNN Lipschitz: 62.11%\n",
      "Epoch [1/75], Loss: 3.2993\n",
      "Epoch [1/75], Val Loss: 3.1746\n",
      "Epoch [2/75], Loss: 3.0124\n",
      "Epoch [2/75], Val Loss: 2.9522\n",
      "Epoch [3/75], Loss: 2.6629\n",
      "Epoch [3/75], Val Loss: 2.5698\n",
      "Epoch [4/75], Loss: 2.1408\n",
      "Epoch [4/75], Val Loss: 2.0950\n",
      "Epoch [5/75], Loss: 1.7188\n",
      "Epoch [5/75], Val Loss: 1.9708\n",
      "Epoch [6/75], Loss: 1.6584\n",
      "Epoch [6/75], Val Loss: 2.1992\n",
      "Epoch [7/75], Loss: 1.6197\n",
      "Epoch [7/75], Val Loss: 1.7709\n",
      "Epoch [8/75], Loss: 1.2605\n",
      "Epoch [8/75], Val Loss: 1.7798\n",
      "Epoch [9/75], Loss: 1.0560\n",
      "Epoch [9/75], Val Loss: 1.5672\n",
      "Epoch [10/75], Loss: 0.9026\n",
      "Epoch [10/75], Val Loss: 1.5710\n",
      "Epoch [11/75], Loss: 0.7393\n",
      "Epoch [11/75], Val Loss: 1.4754\n",
      "Epoch [12/75], Loss: 0.6307\n",
      "Epoch [12/75], Val Loss: 1.4640\n",
      "Epoch [13/75], Loss: 0.5063\n",
      "Epoch [13/75], Val Loss: 1.5701\n",
      "Epoch [14/75], Loss: 0.4087\n",
      "Epoch [14/75], Val Loss: 1.5338\n",
      "Epoch [15/75], Loss: 0.3459\n",
      "Epoch [15/75], Val Loss: 1.5368\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.3459\n",
      "Test Accuracy CNN Lipschitz: 62.43%\n",
      "Epoch [1/75], Loss: 3.2745\n",
      "Epoch [1/75], Val Loss: 3.2154\n",
      "Epoch [2/75], Loss: 3.0819\n",
      "Epoch [2/75], Val Loss: 3.0762\n",
      "Epoch [3/75], Loss: 2.8579\n",
      "Epoch [3/75], Val Loss: 2.8395\n",
      "Epoch [4/75], Loss: 2.4764\n",
      "Epoch [4/75], Val Loss: 2.3917\n",
      "Epoch [5/75], Loss: 1.9014\n",
      "Epoch [5/75], Val Loss: 1.8970\n",
      "Epoch [6/75], Loss: 1.5083\n",
      "Epoch [6/75], Val Loss: 2.0702\n",
      "Epoch [7/75], Loss: 1.4699\n",
      "Epoch [7/75], Val Loss: 1.9190\n",
      "Epoch [8/75], Loss: 1.2112\n",
      "Epoch [8/75], Val Loss: 1.7677\n",
      "Epoch [9/75], Loss: 1.0295\n",
      "Epoch [9/75], Val Loss: 1.7962\n",
      "Epoch [10/75], Loss: 0.9123\n",
      "Epoch [10/75], Val Loss: 1.6430\n",
      "Epoch [11/75], Loss: 0.7678\n",
      "Epoch [11/75], Val Loss: 1.5716\n",
      "Epoch [12/75], Loss: 0.6297\n",
      "Epoch [12/75], Val Loss: 1.5304\n",
      "Epoch [13/75], Loss: 0.4982\n",
      "Epoch [13/75], Val Loss: 1.5482\n",
      "Epoch [14/75], Loss: 0.4070\n",
      "Epoch [14/75], Val Loss: 1.4372\n",
      "Epoch [15/75], Loss: 0.3317\n",
      "Epoch [15/75], Val Loss: 1.5766\n",
      "Epoch [16/75], Loss: 0.2838\n",
      "Epoch [16/75], Val Loss: 1.5770\n",
      "Epoch [17/75], Loss: 0.2036\n",
      "Epoch [17/75], Val Loss: 1.6198\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [17/75], Loss: 0.2036\n",
      "Test Accuracy CNN Lipschitz: 63.08%\n",
      "Epoch [1/75], Loss: 3.2895\n",
      "Epoch [1/75], Val Loss: 3.1938\n",
      "Epoch [2/75], Loss: 3.0637\n",
      "Epoch [2/75], Val Loss: 3.0297\n",
      "Epoch [3/75], Loss: 2.7803\n",
      "Epoch [3/75], Val Loss: 2.6962\n",
      "Epoch [4/75], Loss: 2.2996\n",
      "Epoch [4/75], Val Loss: 2.1574\n",
      "Epoch [5/75], Loss: 1.7554\n",
      "Epoch [5/75], Val Loss: 2.0438\n",
      "Epoch [6/75], Loss: 1.6640\n",
      "Epoch [6/75], Val Loss: 2.2576\n",
      "Epoch [7/75], Loss: 1.8478\n",
      "Epoch [7/75], Val Loss: 2.6203\n",
      "Epoch [8/75], Loss: 1.7456\n",
      "Epoch [8/75], Val Loss: 2.1140\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [8/75], Loss: 1.7456\n",
      "Test Accuracy CNN Lipschitz: 44.82%\n",
      "Epoch [1/75], Loss: 3.3360\n",
      "Epoch [1/75], Val Loss: 3.2272\n",
      "Epoch [2/75], Loss: 3.1088\n",
      "Epoch [2/75], Val Loss: 3.0703\n",
      "Epoch [3/75], Loss: 2.8648\n",
      "Epoch [3/75], Val Loss: 2.8071\n",
      "Epoch [4/75], Loss: 2.4536\n",
      "Epoch [4/75], Val Loss: 2.3241\n",
      "Epoch [5/75], Loss: 1.9153\n",
      "Epoch [5/75], Val Loss: 1.9666\n",
      "Epoch [6/75], Loss: 1.5821\n",
      "Epoch [6/75], Val Loss: 1.9728\n",
      "Epoch [7/75], Loss: 1.4378\n",
      "Epoch [7/75], Val Loss: 2.0755\n",
      "Epoch [8/75], Loss: 1.4754\n",
      "Epoch [8/75], Val Loss: 2.0031\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [8/75], Loss: 1.4754\n",
      "Test Accuracy CNN Lipschitz: 47.12%\n",
      "Epoch [1/75], Loss: 3.3694\n",
      "Epoch [1/75], Val Loss: 3.2519\n",
      "Epoch [2/75], Loss: 3.1405\n",
      "Epoch [2/75], Val Loss: 3.1273\n",
      "Epoch [3/75], Loss: 2.9461\n",
      "Epoch [3/75], Val Loss: 2.9229\n",
      "Epoch [4/75], Loss: 2.5985\n",
      "Epoch [4/75], Val Loss: 2.4988\n",
      "Epoch [5/75], Loss: 2.0876\n",
      "Epoch [5/75], Val Loss: 2.0873\n",
      "Epoch [6/75], Loss: 1.6959\n",
      "Epoch [6/75], Val Loss: 2.0023\n",
      "Epoch [7/75], Loss: 1.5074\n",
      "Epoch [7/75], Val Loss: 1.9866\n",
      "Epoch [8/75], Loss: 1.3732\n",
      "Epoch [8/75], Val Loss: 1.6930\n",
      "Epoch [9/75], Loss: 1.0654\n",
      "Epoch [9/75], Val Loss: 1.8493\n",
      "Epoch [10/75], Loss: 1.0590\n",
      "Epoch [10/75], Val Loss: 1.5880\n",
      "Epoch [11/75], Loss: 0.9190\n",
      "Epoch [11/75], Val Loss: 1.5746\n",
      "Epoch [12/75], Loss: 0.7479\n",
      "Epoch [12/75], Val Loss: 1.5117\n",
      "Epoch [13/75], Loss: 0.6012\n",
      "Epoch [13/75], Val Loss: 1.5330\n",
      "Epoch [14/75], Loss: 0.5092\n",
      "Epoch [14/75], Val Loss: 1.5354\n",
      "Epoch [15/75], Loss: 0.4187\n",
      "Epoch [15/75], Val Loss: 1.5976\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.4187\n",
      "Test Accuracy CNN Lipschitz: 59.22%\n",
      "Epoch [1/75], Loss: 3.3094\n",
      "Epoch [1/75], Val Loss: 3.2105\n",
      "Epoch [2/75], Loss: 3.1147\n",
      "Epoch [2/75], Val Loss: 3.0821\n",
      "Epoch [3/75], Loss: 2.8892\n",
      "Epoch [3/75], Val Loss: 2.8362\n",
      "Epoch [4/75], Loss: 2.5286\n",
      "Epoch [4/75], Val Loss: 2.4016\n",
      "Epoch [5/75], Loss: 1.9415\n",
      "Epoch [5/75], Val Loss: 2.0202\n",
      "Epoch [6/75], Loss: 1.6043\n",
      "Epoch [6/75], Val Loss: 2.0276\n",
      "Epoch [7/75], Loss: 1.4679\n",
      "Epoch [7/75], Val Loss: 1.8994\n",
      "Epoch [8/75], Loss: 1.3750\n",
      "Epoch [8/75], Val Loss: 2.0084\n",
      "Epoch [9/75], Loss: 1.1827\n",
      "Epoch [9/75], Val Loss: 1.7674\n",
      "Epoch [10/75], Loss: 0.9788\n",
      "Epoch [10/75], Val Loss: 1.6415\n",
      "Epoch [11/75], Loss: 0.8435\n",
      "Epoch [11/75], Val Loss: 1.4853\n",
      "Epoch [12/75], Loss: 0.6878\n",
      "Epoch [12/75], Val Loss: 1.5266\n",
      "Epoch [13/75], Loss: 0.5500\n",
      "Epoch [13/75], Val Loss: 1.5269\n",
      "Epoch [14/75], Loss: 0.4583\n",
      "Epoch [14/75], Val Loss: 1.6035\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [14/75], Loss: 0.4583\n",
      "Test Accuracy CNN Lipschitz: 60.80%\n",
      "Epoch [1/75], Loss: 3.3363\n",
      "Epoch [1/75], Val Loss: 3.1557\n",
      "Epoch [2/75], Loss: 3.0319\n",
      "Epoch [2/75], Val Loss: 2.9667\n",
      "Epoch [3/75], Loss: 2.7284\n",
      "Epoch [3/75], Val Loss: 2.5786\n",
      "Epoch [4/75], Loss: 2.2196\n",
      "Epoch [4/75], Val Loss: 2.1183\n",
      "Epoch [5/75], Loss: 1.7855\n",
      "Epoch [5/75], Val Loss: 2.1051\n",
      "Epoch [6/75], Loss: 1.6461\n",
      "Epoch [6/75], Val Loss: 2.0710\n",
      "Epoch [7/75], Loss: 1.5434\n",
      "Epoch [7/75], Val Loss: 1.8099\n",
      "Epoch [8/75], Loss: 1.2380\n",
      "Epoch [8/75], Val Loss: 1.6015\n",
      "Epoch [9/75], Loss: 1.0034\n",
      "Epoch [9/75], Val Loss: 1.5574\n",
      "Epoch [10/75], Loss: 0.8930\n",
      "Epoch [10/75], Val Loss: 1.4325\n",
      "Epoch [11/75], Loss: 0.7630\n",
      "Epoch [11/75], Val Loss: 1.4531\n",
      "Epoch [12/75], Loss: 0.6060\n",
      "Epoch [12/75], Val Loss: 1.4152\n",
      "Epoch [13/75], Loss: 0.4949\n",
      "Epoch [13/75], Val Loss: 1.4639\n",
      "Epoch [14/75], Loss: 0.4142\n",
      "Epoch [14/75], Val Loss: 1.4644\n",
      "Epoch [15/75], Loss: 0.3318\n",
      "Epoch [15/75], Val Loss: 1.5402\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.3318\n",
      "Test Accuracy CNN Lipschitz: 62.89%\n",
      "Epoch [1/75], Loss: 3.3412\n",
      "Epoch [1/75], Val Loss: 3.2445\n",
      "Epoch [2/75], Loss: 3.1713\n",
      "Epoch [2/75], Val Loss: 3.1327\n",
      "Epoch [3/75], Loss: 2.9856\n",
      "Epoch [3/75], Val Loss: 2.9522\n",
      "Epoch [4/75], Loss: 2.6700\n",
      "Epoch [4/75], Val Loss: 2.5808\n",
      "Epoch [5/75], Loss: 2.1795\n",
      "Epoch [5/75], Val Loss: 2.1142\n",
      "Epoch [6/75], Loss: 1.7111\n",
      "Epoch [6/75], Val Loss: 1.8059\n",
      "Epoch [7/75], Loss: 1.5080\n",
      "Epoch [7/75], Val Loss: 1.9993\n",
      "Epoch [8/75], Loss: 1.4307\n",
      "Epoch [8/75], Val Loss: 1.8350\n",
      "Epoch [9/75], Loss: 1.1716\n",
      "Epoch [9/75], Val Loss: 1.7711\n",
      "Epoch [10/75], Loss: 1.0569\n",
      "Epoch [10/75], Val Loss: 1.6530\n",
      "Epoch [11/75], Loss: 0.8745\n",
      "Epoch [11/75], Val Loss: 1.5130\n",
      "Epoch [12/75], Loss: 0.7213\n",
      "Epoch [12/75], Val Loss: 1.5036\n",
      "Epoch [13/75], Loss: 0.6233\n",
      "Epoch [13/75], Val Loss: 1.4784\n",
      "Epoch [14/75], Loss: 0.5088\n",
      "Epoch [14/75], Val Loss: 1.4984\n",
      "Epoch [15/75], Loss: 0.3821\n",
      "Epoch [15/75], Val Loss: 1.4425\n",
      "Epoch [16/75], Loss: 0.2946\n",
      "Epoch [16/75], Val Loss: 1.5292\n",
      "Epoch [17/75], Loss: 0.2500\n",
      "Epoch [17/75], Val Loss: 1.6062\n",
      "Epoch [18/75], Loss: 0.2103\n",
      "Epoch [18/75], Val Loss: 1.6379\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [18/75], Loss: 0.2103\n",
      "Test Accuracy CNN Lipschitz: 63.27%\n",
      "Epoch [1/75], Loss: 3.3863\n",
      "Epoch [1/75], Val Loss: 3.1918\n",
      "Epoch [2/75], Loss: 3.0697\n",
      "Epoch [2/75], Val Loss: 3.0222\n",
      "Epoch [3/75], Loss: 2.7762\n",
      "Epoch [3/75], Val Loss: 2.6765\n",
      "Epoch [4/75], Loss: 2.2523\n",
      "Epoch [4/75], Val Loss: 2.1608\n",
      "Epoch [5/75], Loss: 1.7631\n",
      "Epoch [5/75], Val Loss: 2.0376\n",
      "Epoch [6/75], Loss: 1.6322\n",
      "Epoch [6/75], Val Loss: 2.0587\n",
      "Epoch [7/75], Loss: 1.4267\n",
      "Epoch [7/75], Val Loss: 1.8426\n",
      "Epoch [8/75], Loss: 1.2101\n",
      "Epoch [8/75], Val Loss: 1.9020\n",
      "Epoch [9/75], Loss: 1.1207\n",
      "Epoch [9/75], Val Loss: 1.6676\n",
      "Epoch [10/75], Loss: 0.9526\n",
      "Epoch [10/75], Val Loss: 1.5030\n",
      "Epoch [11/75], Loss: 0.7424\n",
      "Epoch [11/75], Val Loss: 1.4370\n",
      "Epoch [12/75], Loss: 0.6607\n",
      "Epoch [12/75], Val Loss: 1.4531\n",
      "Epoch [13/75], Loss: 0.5027\n",
      "Epoch [13/75], Val Loss: 1.5052\n",
      "Epoch [14/75], Loss: 0.4267\n",
      "Epoch [14/75], Val Loss: 1.5929\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [14/75], Loss: 0.4267\n",
      "Test Accuracy CNN Lipschitz: 60.93%\n",
      "Epoch [1/75], Loss: 3.3075\n",
      "Epoch [1/75], Val Loss: 3.1971\n",
      "Epoch [2/75], Loss: 3.0702\n",
      "Epoch [2/75], Val Loss: 3.0413\n",
      "Epoch [3/75], Loss: 2.8072\n",
      "Epoch [3/75], Val Loss: 2.7498\n",
      "Epoch [4/75], Loss: 2.3539\n",
      "Epoch [4/75], Val Loss: 2.2316\n",
      "Epoch [5/75], Loss: 1.7933\n",
      "Epoch [5/75], Val Loss: 1.8971\n",
      "Epoch [6/75], Loss: 1.5065\n",
      "Epoch [6/75], Val Loss: 2.0622\n",
      "Epoch [7/75], Loss: 1.3859\n",
      "Epoch [7/75], Val Loss: 1.9102\n",
      "Epoch [8/75], Loss: 1.1074\n",
      "Epoch [8/75], Val Loss: 1.6509\n",
      "Epoch [9/75], Loss: 0.8818\n",
      "Epoch [9/75], Val Loss: 1.6351\n",
      "Epoch [10/75], Loss: 0.7316\n",
      "Epoch [10/75], Val Loss: 1.5559\n",
      "Epoch [11/75], Loss: 0.6304\n",
      "Epoch [11/75], Val Loss: 1.5429\n",
      "Epoch [12/75], Loss: 0.5058\n",
      "Epoch [12/75], Val Loss: 1.5065\n",
      "Epoch [13/75], Loss: 0.4361\n",
      "Epoch [13/75], Val Loss: 1.5107\n",
      "Epoch [14/75], Loss: 0.3419\n",
      "Epoch [14/75], Val Loss: 1.6888\n",
      "Epoch [15/75], Loss: 0.2983\n",
      "Epoch [15/75], Val Loss: 1.5931\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.2983\n",
      "Test Accuracy CNN Lipschitz: 61.34%\n",
      "Epoch [1/75], Loss: 3.3205\n",
      "Epoch [1/75], Val Loss: 3.1893\n",
      "Epoch [2/75], Loss: 3.0722\n",
      "Epoch [2/75], Val Loss: 3.0263\n",
      "Epoch [3/75], Loss: 2.7620\n",
      "Epoch [3/75], Val Loss: 2.6534\n",
      "Epoch [4/75], Loss: 2.2139\n",
      "Epoch [4/75], Val Loss: 2.1065\n",
      "Epoch [5/75], Loss: 1.7069\n",
      "Epoch [5/75], Val Loss: 1.9904\n",
      "Epoch [6/75], Loss: 1.5163\n",
      "Epoch [6/75], Val Loss: 2.0388\n",
      "Epoch [7/75], Loss: 1.4487\n",
      "Epoch [7/75], Val Loss: 2.1027\n",
      "Epoch [8/75], Loss: 1.2180\n",
      "Epoch [8/75], Val Loss: 1.8260\n",
      "Epoch [9/75], Loss: 0.9948\n",
      "Epoch [9/75], Val Loss: 1.6451\n",
      "Epoch [10/75], Loss: 0.8536\n",
      "Epoch [10/75], Val Loss: 1.5242\n",
      "Epoch [11/75], Loss: 0.7300\n",
      "Epoch [11/75], Val Loss: 1.5214\n",
      "Epoch [12/75], Loss: 0.5817\n",
      "Epoch [12/75], Val Loss: 1.4439\n",
      "Epoch [13/75], Loss: 0.4649\n",
      "Epoch [13/75], Val Loss: 1.6379\n",
      "Epoch [14/75], Loss: 0.3744\n",
      "Epoch [14/75], Val Loss: 1.5929\n",
      "Epoch [15/75], Loss: 0.3033\n",
      "Epoch [15/75], Val Loss: 1.6602\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.3033\n",
      "Test Accuracy CNN Lipschitz: 61.08%\n",
      "Epoch [1/75], Loss: 3.3253\n",
      "Epoch [1/75], Val Loss: 3.2385\n",
      "Epoch [2/75], Loss: 3.1632\n",
      "Epoch [2/75], Val Loss: 3.1498\n",
      "Epoch [3/75], Loss: 2.9883\n",
      "Epoch [3/75], Val Loss: 2.9801\n",
      "Epoch [4/75], Loss: 2.7098\n",
      "Epoch [4/75], Val Loss: 2.6403\n",
      "Epoch [5/75], Loss: 2.2568\n",
      "Epoch [5/75], Val Loss: 2.1513\n",
      "Epoch [6/75], Loss: 1.7156\n",
      "Epoch [6/75], Val Loss: 1.8524\n",
      "Epoch [7/75], Loss: 1.4507\n",
      "Epoch [7/75], Val Loss: 1.8614\n",
      "Epoch [8/75], Loss: 1.3684\n",
      "Epoch [8/75], Val Loss: 1.9962\n",
      "Epoch [9/75], Loss: 1.2764\n",
      "Epoch [9/75], Val Loss: 1.8052\n",
      "Epoch [10/75], Loss: 1.1145\n",
      "Epoch [10/75], Val Loss: 1.6502\n",
      "Epoch [11/75], Loss: 0.9115\n",
      "Epoch [11/75], Val Loss: 1.5933\n",
      "Epoch [12/75], Loss: 0.7952\n",
      "Epoch [12/75], Val Loss: 1.4639\n",
      "Epoch [13/75], Loss: 0.6428\n",
      "Epoch [13/75], Val Loss: 1.4999\n",
      "Epoch [14/75], Loss: 0.5499\n",
      "Epoch [14/75], Val Loss: 1.4703\n",
      "Epoch [15/75], Loss: 0.4222\n",
      "Epoch [15/75], Val Loss: 1.4862\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.4222\n",
      "Test Accuracy CNN Lipschitz: 62.03%\n",
      "Epoch [1/75], Loss: 3.3347\n",
      "Epoch [1/75], Val Loss: 3.2277\n",
      "Epoch [2/75], Loss: 3.1060\n",
      "Epoch [2/75], Val Loss: 3.0737\n",
      "Epoch [3/75], Loss: 2.8793\n",
      "Epoch [3/75], Val Loss: 2.8277\n",
      "Epoch [4/75], Loss: 2.4833\n",
      "Epoch [4/75], Val Loss: 2.3667\n",
      "Epoch [5/75], Loss: 1.9284\n",
      "Epoch [5/75], Val Loss: 1.9591\n",
      "Epoch [6/75], Loss: 1.5090\n",
      "Epoch [6/75], Val Loss: 1.9276\n",
      "Epoch [7/75], Loss: 1.6318\n",
      "Epoch [7/75], Val Loss: 2.2658\n",
      "Epoch [8/75], Loss: 1.6757\n",
      "Epoch [8/75], Val Loss: 1.8932\n",
      "Epoch [9/75], Loss: 1.2457\n",
      "Epoch [9/75], Val Loss: 1.7845\n",
      "Epoch [10/75], Loss: 0.9962\n",
      "Epoch [10/75], Val Loss: 1.5327\n",
      "Epoch [11/75], Loss: 0.8439\n",
      "Epoch [11/75], Val Loss: 1.4484\n",
      "Epoch [12/75], Loss: 0.7138\n",
      "Epoch [12/75], Val Loss: 1.4777\n",
      "Epoch [13/75], Loss: 0.5749\n",
      "Epoch [13/75], Val Loss: 1.4981\n",
      "Epoch [14/75], Loss: 0.4534\n",
      "Epoch [14/75], Val Loss: 1.5602\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [14/75], Loss: 0.4534\n",
      "Test Accuracy CNN Lipschitz: 61.22%\n",
      "Epoch [1/75], Loss: 3.4260\n",
      "Epoch [1/75], Val Loss: 3.2324\n",
      "Epoch [2/75], Loss: 3.1338\n",
      "Epoch [2/75], Val Loss: 3.0743\n",
      "Epoch [3/75], Loss: 2.8467\n",
      "Epoch [3/75], Val Loss: 2.7582\n",
      "Epoch [4/75], Loss: 2.4037\n",
      "Epoch [4/75], Val Loss: 2.2968\n",
      "Epoch [5/75], Loss: 1.8667\n",
      "Epoch [5/75], Val Loss: 1.8755\n",
      "Epoch [6/75], Loss: 1.5750\n",
      "Epoch [6/75], Val Loss: 1.9659\n",
      "Epoch [7/75], Loss: 1.3829\n",
      "Epoch [7/75], Val Loss: 1.9492\n",
      "Epoch [8/75], Loss: 1.3265\n",
      "Epoch [8/75], Val Loss: 1.7093\n",
      "Epoch [9/75], Loss: 1.1373\n",
      "Epoch [9/75], Val Loss: 1.7621\n",
      "Epoch [10/75], Loss: 0.9399\n",
      "Epoch [10/75], Val Loss: 1.5074\n",
      "Epoch [11/75], Loss: 0.7888\n",
      "Epoch [11/75], Val Loss: 1.4773\n",
      "Epoch [12/75], Loss: 0.6522\n",
      "Epoch [12/75], Val Loss: 1.4313\n",
      "Epoch [13/75], Loss: 0.4840\n",
      "Epoch [13/75], Val Loss: 1.5021\n",
      "Epoch [14/75], Loss: 0.3679\n",
      "Epoch [14/75], Val Loss: 1.4966\n",
      "Epoch [15/75], Loss: 0.2995\n",
      "Epoch [15/75], Val Loss: 1.5418\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [15/75], Loss: 0.2995\n",
      "Test Accuracy CNN Lipschitz: 63.01%\n",
      "Epoch [1/75], Loss: 3.3466\n",
      "Epoch [1/75], Val Loss: 3.2563\n",
      "Epoch [2/75], Loss: 3.1610\n",
      "Epoch [2/75], Val Loss: 3.1487\n",
      "Epoch [3/75], Loss: 2.9859\n",
      "Epoch [3/75], Val Loss: 3.0018\n",
      "Epoch [4/75], Loss: 2.7264\n",
      "Epoch [4/75], Val Loss: 2.7057\n",
      "Epoch [5/75], Loss: 2.2902\n",
      "Epoch [5/75], Val Loss: 2.2690\n",
      "Epoch [6/75], Loss: 1.8332\n",
      "Epoch [6/75], Val Loss: 2.0271\n",
      "Epoch [7/75], Loss: 1.5340\n",
      "Epoch [7/75], Val Loss: 2.0370\n",
      "Epoch [8/75], Loss: 1.4410\n",
      "Epoch [8/75], Val Loss: 2.1724\n",
      "Epoch [9/75], Loss: 1.5427\n",
      "Epoch [9/75], Val Loss: 2.2203\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [9/75], Loss: 1.5427\n",
      "Test Accuracy CNN Lipschitz: 46.85%\n",
      "Epoch [1/75], Loss: 3.3355\n",
      "Epoch [1/75], Val Loss: 3.2457\n",
      "Epoch [2/75], Loss: 3.1745\n",
      "Epoch [2/75], Val Loss: 3.1670\n",
      "Epoch [3/75], Loss: 3.0305\n",
      "Epoch [3/75], Val Loss: 3.0311\n",
      "Epoch [4/75], Loss: 2.7969\n",
      "Epoch [4/75], Val Loss: 2.7393\n",
      "Epoch [5/75], Loss: 2.3914\n",
      "Epoch [5/75], Val Loss: 2.2657\n",
      "Epoch [6/75], Loss: 1.8419\n",
      "Epoch [6/75], Val Loss: 1.8544\n",
      "Epoch [7/75], Loss: 1.5082\n",
      "Epoch [7/75], Val Loss: 2.1887\n",
      "Epoch [8/75], Loss: 1.6299\n",
      "Epoch [8/75], Val Loss: 2.0953\n",
      "Epoch [9/75], Loss: 1.4385\n",
      "Epoch [9/75], Val Loss: 1.8917\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [9/75], Loss: 1.4385\n",
      "Test Accuracy CNN Lipschitz: 48.81%\n",
      "Epoch [1/75], Loss: 3.3249\n",
      "Epoch [1/75], Val Loss: 3.2211\n",
      "Epoch [2/75], Loss: 3.0987\n",
      "Epoch [2/75], Val Loss: 3.0814\n",
      "Epoch [3/75], Loss: 2.8727\n",
      "Epoch [3/75], Val Loss: 2.8307\n",
      "Epoch [4/75], Loss: 2.4581\n",
      "Epoch [4/75], Val Loss: 2.4015\n",
      "Epoch [5/75], Loss: 1.9147\n",
      "Epoch [5/75], Val Loss: 1.9783\n",
      "Epoch [6/75], Loss: 1.5684\n",
      "Epoch [6/75], Val Loss: 1.9889\n",
      "Epoch [7/75], Loss: 1.3499\n",
      "Epoch [7/75], Val Loss: 2.0099\n",
      "Epoch [8/75], Loss: 1.3200\n",
      "Epoch [8/75], Val Loss: 2.2285\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [8/75], Loss: 1.3200\n",
      "Test Accuracy CNN Lipschitz: 48.02%\n",
      "Epoch [1/75], Loss: 3.3156\n",
      "Epoch [1/75], Val Loss: 3.2344\n",
      "Epoch [2/75], Loss: 3.1524\n",
      "Epoch [2/75], Val Loss: 3.1066\n",
      "Epoch [3/75], Loss: 2.9361\n",
      "Epoch [3/75], Val Loss: 2.8604\n",
      "Epoch [4/75], Loss: 2.5662\n",
      "Epoch [4/75], Val Loss: 2.4339\n",
      "Epoch [5/75], Loss: 2.0520\n",
      "Epoch [5/75], Val Loss: 1.9982\n",
      "Epoch [6/75], Loss: 1.6313\n",
      "Epoch [6/75], Val Loss: 1.9138\n",
      "Epoch [7/75], Loss: 1.4741\n",
      "Epoch [7/75], Val Loss: 1.8708\n",
      "Epoch [8/75], Loss: 1.5414\n",
      "Epoch [8/75], Val Loss: 2.1053\n",
      "Epoch [9/75], Loss: 1.3642\n",
      "Epoch [9/75], Val Loss: 1.7712\n",
      "Epoch [10/75], Loss: 1.1340\n",
      "Epoch [10/75], Val Loss: 1.6765\n",
      "Epoch [11/75], Loss: 0.9639\n",
      "Epoch [11/75], Val Loss: 1.5849\n",
      "Epoch [12/75], Loss: 0.8089\n",
      "Epoch [12/75], Val Loss: 1.4750\n",
      "Epoch [13/75], Loss: 0.6519\n",
      "Epoch [13/75], Val Loss: 1.4687\n",
      "Epoch [14/75], Loss: 0.5556\n",
      "Epoch [14/75], Val Loss: 1.4833\n",
      "Epoch [15/75], Loss: 0.4257\n",
      "Epoch [15/75], Val Loss: 1.5355\n",
      "Epoch [16/75], Loss: 0.3728\n",
      "Epoch [16/75], Val Loss: 1.5974\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [16/75], Loss: 0.3728\n",
      "Test Accuracy CNN Lipschitz: 60.85%\n",
      "Epoch [1/75], Loss: 3.2997\n",
      "Epoch [1/75], Val Loss: 3.2429\n",
      "Epoch [2/75], Loss: 3.1490\n",
      "Epoch [2/75], Val Loss: 3.1308\n",
      "Epoch [3/75], Loss: 2.9672\n",
      "Epoch [3/75], Val Loss: 2.9531\n",
      "Epoch [4/75], Loss: 2.6785\n",
      "Epoch [4/75], Val Loss: 2.6147\n",
      "Epoch [5/75], Loss: 2.2315\n",
      "Epoch [5/75], Val Loss: 2.1804\n",
      "Epoch [6/75], Loss: 1.8043\n",
      "Epoch [6/75], Val Loss: 1.9844\n",
      "Epoch [7/75], Loss: 1.5636\n",
      "Epoch [7/75], Val Loss: 2.0457\n",
      "Epoch [8/75], Loss: 1.4177\n",
      "Epoch [8/75], Val Loss: 1.9607\n",
      "Epoch [9/75], Loss: 1.2702\n",
      "Epoch [9/75], Val Loss: 1.8945\n",
      "Epoch [10/75], Loss: 1.1198\n",
      "Epoch [10/75], Val Loss: 1.5654\n",
      "Epoch [11/75], Loss: 0.8484\n",
      "Epoch [11/75], Val Loss: 1.7670\n",
      "Epoch [12/75], Loss: 0.8291\n",
      "Epoch [12/75], Val Loss: 1.5343\n",
      "Epoch [13/75], Loss: 0.6825\n",
      "Epoch [13/75], Val Loss: 1.5217\n",
      "Epoch [14/75], Loss: 0.5681\n",
      "Epoch [14/75], Val Loss: 1.5513\n",
      "Epoch [15/75], Loss: 0.4876\n",
      "Epoch [15/75], Val Loss: 1.5744\n",
      "Epoch [16/75], Loss: 0.3829\n",
      "Epoch [16/75], Val Loss: 1.5903\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [16/75], Loss: 0.3829\n",
      "Test Accuracy CNN Lipschitz: 59.89%\n",
      "Epoch [1/75], Loss: 3.3474\n",
      "Epoch [1/75], Val Loss: 3.2382\n",
      "Epoch [2/75], Loss: 3.1186\n",
      "Epoch [2/75], Val Loss: 3.1073\n",
      "Epoch [3/75], Loss: 2.9189\n",
      "Epoch [3/75], Val Loss: 2.8626\n",
      "Epoch [4/75], Loss: 2.5637\n",
      "Epoch [4/75], Val Loss: 2.4399\n",
      "Epoch [5/75], Loss: 2.0811\n",
      "Epoch [5/75], Val Loss: 2.0738\n",
      "Epoch [6/75], Loss: 1.7017\n",
      "Epoch [6/75], Val Loss: 2.1666\n",
      "Epoch [7/75], Loss: 1.6813\n",
      "Epoch [7/75], Val Loss: 1.9455\n",
      "Epoch [8/75], Loss: 1.4544\n",
      "Epoch [8/75], Val Loss: 1.7929\n",
      "Epoch [9/75], Loss: 1.2521\n",
      "Epoch [9/75], Val Loss: 1.7175\n",
      "Epoch [10/75], Loss: 1.0591\n",
      "Epoch [10/75], Val Loss: 1.5593\n",
      "Epoch [11/75], Loss: 0.8775\n",
      "Epoch [11/75], Val Loss: 1.4983\n",
      "Epoch [12/75], Loss: 0.7649\n",
      "Epoch [12/75], Val Loss: 1.4809\n",
      "Epoch [13/75], Loss: 0.6135\n",
      "Epoch [13/75], Val Loss: 1.4445\n",
      "Epoch [14/75], Loss: 0.5417\n",
      "Epoch [14/75], Val Loss: 1.4744\n",
      "Epoch [15/75], Loss: 0.4660\n",
      "Epoch [15/75], Val Loss: 1.5871\n",
      "Epoch [16/75], Loss: 0.3961\n",
      "Epoch [16/75], Val Loss: 1.4889\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 5000, Epoch [16/75], Loss: 0.3961\n",
      "Test Accuracy CNN Lipschitz: 62.35%\n",
      "Training with subset size: 1000\n",
      "Epoch [1/75], Loss: 3.3448\n",
      "Epoch [1/75], Val Loss: 3.2210\n",
      "Epoch [2/75], Loss: 3.1122\n",
      "Epoch [2/75], Val Loss: 3.0709\n",
      "Epoch [3/75], Loss: 2.8642\n",
      "Epoch [3/75], Val Loss: 2.8189\n",
      "Epoch [4/75], Loss: 2.4844\n",
      "Epoch [4/75], Val Loss: 2.3528\n",
      "Epoch [5/75], Loss: 1.9483\n",
      "Epoch [5/75], Val Loss: 2.0308\n",
      "Epoch [6/75], Loss: 1.6731\n",
      "Epoch [6/75], Val Loss: 2.1740\n",
      "Epoch [7/75], Loss: 1.6536\n",
      "Epoch [7/75], Val Loss: 2.2165\n",
      "Epoch [8/75], Loss: 1.5460\n",
      "Epoch [8/75], Val Loss: 1.7442\n",
      "Epoch [9/75], Loss: 1.1470\n",
      "Epoch [9/75], Val Loss: 1.6052\n",
      "Epoch [10/75], Loss: 0.9713\n",
      "Epoch [10/75], Val Loss: 1.5139\n",
      "Epoch [11/75], Loss: 0.8062\n",
      "Epoch [11/75], Val Loss: 1.3884\n",
      "Epoch [12/75], Loss: 0.6467\n",
      "Epoch [12/75], Val Loss: 1.4720\n",
      "Epoch [13/75], Loss: 0.5519\n",
      "Epoch [13/75], Val Loss: 1.4725\n",
      "Epoch [14/75], Loss: 0.4550\n",
      "Epoch [14/75], Val Loss: 1.4900\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [14/75], Loss: 0.4550\n",
      "Test Accuracy CNN Lipschitz: 61.66%\n",
      "Epoch [1/75], Loss: 3.3353\n",
      "Epoch [1/75], Val Loss: 3.2491\n",
      "Epoch [2/75], Loss: 3.1258\n",
      "Epoch [2/75], Val Loss: 3.1297\n",
      "Epoch [3/75], Loss: 2.8991\n",
      "Epoch [3/75], Val Loss: 2.8735\n",
      "Epoch [4/75], Loss: 2.5025\n",
      "Epoch [4/75], Val Loss: 2.4297\n",
      "Epoch [5/75], Loss: 1.9582\n",
      "Epoch [5/75], Val Loss: 2.0261\n",
      "Epoch [6/75], Loss: 1.5697\n",
      "Epoch [6/75], Val Loss: 2.0466\n",
      "Epoch [7/75], Loss: 1.5672\n",
      "Epoch [7/75], Val Loss: 2.0133\n",
      "Epoch [8/75], Loss: 1.4346\n",
      "Epoch [8/75], Val Loss: 2.2130\n",
      "Epoch [9/75], Loss: 1.1719\n",
      "Epoch [9/75], Val Loss: 1.7524\n",
      "Epoch [10/75], Loss: 0.9637\n",
      "Epoch [10/75], Val Loss: 1.6464\n",
      "Epoch [11/75], Loss: 0.8259\n",
      "Epoch [11/75], Val Loss: 1.5131\n",
      "Epoch [12/75], Loss: 0.6891\n",
      "Epoch [12/75], Val Loss: 1.4866\n",
      "Epoch [13/75], Loss: 0.5402\n",
      "Epoch [13/75], Val Loss: 1.5218\n",
      "Epoch [14/75], Loss: 0.4422\n",
      "Epoch [14/75], Val Loss: 1.5474\n",
      "Epoch [15/75], Loss: 0.3447\n",
      "Epoch [15/75], Val Loss: 1.5949\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [15/75], Loss: 0.3447\n",
      "Test Accuracy CNN Lipschitz: 60.07%\n",
      "Epoch [1/75], Loss: 3.3151\n",
      "Epoch [1/75], Val Loss: 3.2124\n",
      "Epoch [2/75], Loss: 3.0921\n",
      "Epoch [2/75], Val Loss: 3.0633\n",
      "Epoch [3/75], Loss: 2.8300\n",
      "Epoch [3/75], Val Loss: 2.7619\n",
      "Epoch [4/75], Loss: 2.3852\n",
      "Epoch [4/75], Val Loss: 2.2923\n",
      "Epoch [5/75], Loss: 1.8830\n",
      "Epoch [5/75], Val Loss: 1.9914\n",
      "Epoch [6/75], Loss: 1.5519\n",
      "Epoch [6/75], Val Loss: 1.8991\n",
      "Epoch [7/75], Loss: 1.4751\n",
      "Epoch [7/75], Val Loss: 2.0218\n",
      "Epoch [8/75], Loss: 1.3132\n",
      "Epoch [8/75], Val Loss: 1.8519\n",
      "Epoch [9/75], Loss: 1.1331\n",
      "Epoch [9/75], Val Loss: 1.6978\n",
      "Epoch [10/75], Loss: 0.9650\n",
      "Epoch [10/75], Val Loss: 1.4977\n",
      "Epoch [11/75], Loss: 0.7816\n",
      "Epoch [11/75], Val Loss: 1.5202\n",
      "Epoch [12/75], Loss: 0.6680\n",
      "Epoch [12/75], Val Loss: 1.4525\n",
      "Epoch [13/75], Loss: 0.5393\n",
      "Epoch [13/75], Val Loss: 1.4309\n",
      "Epoch [14/75], Loss: 0.4355\n",
      "Epoch [14/75], Val Loss: 1.4714\n",
      "Epoch [15/75], Loss: 0.3633\n",
      "Epoch [15/75], Val Loss: 1.5272\n",
      "Epoch [16/75], Loss: 0.3342\n",
      "Epoch [16/75], Val Loss: 1.5132\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.3342\n",
      "Test Accuracy CNN Lipschitz: 63.26%\n",
      "Epoch [1/75], Loss: 3.3264\n",
      "Epoch [1/75], Val Loss: 3.2211\n",
      "Epoch [2/75], Loss: 3.1123\n",
      "Epoch [2/75], Val Loss: 3.0799\n",
      "Epoch [3/75], Loss: 2.8893\n",
      "Epoch [3/75], Val Loss: 2.8076\n",
      "Epoch [4/75], Loss: 2.4808\n",
      "Epoch [4/75], Val Loss: 2.3478\n",
      "Epoch [5/75], Loss: 1.9403\n",
      "Epoch [5/75], Val Loss: 1.9771\n",
      "Epoch [6/75], Loss: 1.5672\n",
      "Epoch [6/75], Val Loss: 1.8575\n",
      "Epoch [7/75], Loss: 1.4794\n",
      "Epoch [7/75], Val Loss: 1.9030\n",
      "Epoch [8/75], Loss: 1.4010\n",
      "Epoch [8/75], Val Loss: 1.7696\n",
      "Epoch [9/75], Loss: 1.1831\n",
      "Epoch [9/75], Val Loss: 1.7824\n",
      "Epoch [10/75], Loss: 1.0062\n",
      "Epoch [10/75], Val Loss: 1.5432\n",
      "Epoch [11/75], Loss: 0.8610\n",
      "Epoch [11/75], Val Loss: 1.5301\n",
      "Epoch [12/75], Loss: 0.7199\n",
      "Epoch [12/75], Val Loss: 1.4634\n",
      "Epoch [13/75], Loss: 0.6073\n",
      "Epoch [13/75], Val Loss: 1.4349\n",
      "Epoch [14/75], Loss: 0.5261\n",
      "Epoch [14/75], Val Loss: 1.4300\n",
      "Epoch [15/75], Loss: 0.4030\n",
      "Epoch [15/75], Val Loss: 1.5952\n",
      "Epoch [16/75], Loss: 0.3505\n",
      "Epoch [16/75], Val Loss: 1.5067\n",
      "Epoch [17/75], Loss: 0.2681\n",
      "Epoch [17/75], Val Loss: 1.5196\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [17/75], Loss: 0.2681\n",
      "Test Accuracy CNN Lipschitz: 62.71%\n",
      "Epoch [1/75], Loss: 3.3096\n",
      "Epoch [1/75], Val Loss: 3.2057\n",
      "Epoch [2/75], Loss: 3.0995\n",
      "Epoch [2/75], Val Loss: 3.0794\n",
      "Epoch [3/75], Loss: 2.8603\n",
      "Epoch [3/75], Val Loss: 2.8207\n",
      "Epoch [4/75], Loss: 2.4276\n",
      "Epoch [4/75], Val Loss: 2.2804\n",
      "Epoch [5/75], Loss: 1.8843\n",
      "Epoch [5/75], Val Loss: 1.9649\n",
      "Epoch [6/75], Loss: 1.4885\n",
      "Epoch [6/75], Val Loss: 2.1464\n",
      "Epoch [7/75], Loss: 1.5251\n",
      "Epoch [7/75], Val Loss: 2.3650\n",
      "Epoch [8/75], Loss: 1.4237\n",
      "Epoch [8/75], Val Loss: 1.6213\n",
      "Epoch [9/75], Loss: 1.0574\n",
      "Epoch [9/75], Val Loss: 1.6383\n",
      "Epoch [10/75], Loss: 0.8924\n",
      "Epoch [10/75], Val Loss: 1.4869\n",
      "Epoch [11/75], Loss: 0.7058\n",
      "Epoch [11/75], Val Loss: 1.4928\n",
      "Epoch [12/75], Loss: 0.5949\n",
      "Epoch [12/75], Val Loss: 1.4424\n",
      "Epoch [13/75], Loss: 0.4577\n",
      "Epoch [13/75], Val Loss: 1.4443\n",
      "Epoch [14/75], Loss: 0.3898\n",
      "Epoch [14/75], Val Loss: 1.5246\n",
      "Epoch [15/75], Loss: 0.3507\n",
      "Epoch [15/75], Val Loss: 1.6702\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [15/75], Loss: 0.3507\n",
      "Test Accuracy CNN Lipschitz: 59.64%\n",
      "Epoch [1/75], Loss: 3.3129\n",
      "Epoch [1/75], Val Loss: 3.1788\n",
      "Epoch [2/75], Loss: 3.0588\n",
      "Epoch [2/75], Val Loss: 3.0094\n",
      "Epoch [3/75], Loss: 2.7496\n",
      "Epoch [3/75], Val Loss: 2.6646\n",
      "Epoch [4/75], Loss: 2.2367\n",
      "Epoch [4/75], Val Loss: 2.1560\n",
      "Epoch [5/75], Loss: 1.7695\n",
      "Epoch [5/75], Val Loss: 1.9158\n",
      "Epoch [6/75], Loss: 1.5132\n",
      "Epoch [6/75], Val Loss: 1.8845\n",
      "Epoch [7/75], Loss: 1.4185\n",
      "Epoch [7/75], Val Loss: 2.0261\n",
      "Epoch [8/75], Loss: 1.1379\n",
      "Epoch [8/75], Val Loss: 1.8868\n",
      "Epoch [9/75], Loss: 0.9978\n",
      "Epoch [9/75], Val Loss: 1.5137\n",
      "Epoch [10/75], Loss: 0.8485\n",
      "Epoch [10/75], Val Loss: 1.4392\n",
      "Epoch [11/75], Loss: 0.6658\n",
      "Epoch [11/75], Val Loss: 1.5166\n",
      "Epoch [12/75], Loss: 0.5452\n",
      "Epoch [12/75], Val Loss: 1.4152\n",
      "Epoch [13/75], Loss: 0.4560\n",
      "Epoch [13/75], Val Loss: 1.4392\n",
      "Epoch [14/75], Loss: 0.3443\n",
      "Epoch [14/75], Val Loss: 1.5455\n",
      "Epoch [15/75], Loss: 0.3269\n",
      "Epoch [15/75], Val Loss: 1.4372\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [15/75], Loss: 0.3269\n",
      "Test Accuracy CNN Lipschitz: 63.69%\n",
      "Epoch [1/75], Loss: 3.3085\n",
      "Epoch [1/75], Val Loss: 3.2036\n",
      "Epoch [2/75], Loss: 3.0832\n",
      "Epoch [2/75], Val Loss: 3.0482\n",
      "Epoch [3/75], Loss: 2.8406\n",
      "Epoch [3/75], Val Loss: 2.7817\n",
      "Epoch [4/75], Loss: 2.4217\n",
      "Epoch [4/75], Val Loss: 2.2910\n",
      "Epoch [5/75], Loss: 1.8717\n",
      "Epoch [5/75], Val Loss: 1.9205\n",
      "Epoch [6/75], Loss: 1.6044\n",
      "Epoch [6/75], Val Loss: 2.0776\n",
      "Epoch [7/75], Loss: 1.7535\n",
      "Epoch [7/75], Val Loss: 2.1392\n",
      "Epoch [8/75], Loss: 1.5450\n",
      "Epoch [8/75], Val Loss: 2.0170\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [8/75], Loss: 1.5450\n",
      "Test Accuracy CNN Lipschitz: 49.09%\n",
      "Epoch [1/75], Loss: 3.3034\n",
      "Epoch [1/75], Val Loss: 3.1943\n",
      "Epoch [2/75], Loss: 3.0799\n",
      "Epoch [2/75], Val Loss: 3.0274\n",
      "Epoch [3/75], Loss: 2.8021\n",
      "Epoch [3/75], Val Loss: 2.6963\n",
      "Epoch [4/75], Loss: 2.3385\n",
      "Epoch [4/75], Val Loss: 2.2185\n",
      "Epoch [5/75], Loss: 1.8579\n",
      "Epoch [5/75], Val Loss: 2.0438\n",
      "Epoch [6/75], Loss: 1.6539\n",
      "Epoch [6/75], Val Loss: 1.9128\n",
      "Epoch [7/75], Loss: 1.4889\n",
      "Epoch [7/75], Val Loss: 2.0131\n",
      "Epoch [8/75], Loss: 1.3114\n",
      "Epoch [8/75], Val Loss: 1.8501\n",
      "Epoch [9/75], Loss: 1.1412\n",
      "Epoch [9/75], Val Loss: 1.6156\n",
      "Epoch [10/75], Loss: 0.9884\n",
      "Epoch [10/75], Val Loss: 1.6607\n",
      "Epoch [11/75], Loss: 0.7943\n",
      "Epoch [11/75], Val Loss: 1.4770\n",
      "Epoch [12/75], Loss: 0.6887\n",
      "Epoch [12/75], Val Loss: 1.4402\n",
      "Epoch [13/75], Loss: 0.5785\n",
      "Epoch [13/75], Val Loss: 1.4645\n",
      "Epoch [14/75], Loss: 0.4550\n",
      "Epoch [14/75], Val Loss: 1.5650\n",
      "Epoch [15/75], Loss: 0.3968\n",
      "Epoch [15/75], Val Loss: 1.5542\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [15/75], Loss: 0.3968\n",
      "Test Accuracy CNN Lipschitz: 61.40%\n",
      "Epoch [1/75], Loss: 3.3602\n",
      "Epoch [1/75], Val Loss: 3.2151\n",
      "Epoch [2/75], Loss: 3.0888\n",
      "Epoch [2/75], Val Loss: 3.0628\n",
      "Epoch [3/75], Loss: 2.8476\n",
      "Epoch [3/75], Val Loss: 2.7886\n",
      "Epoch [4/75], Loss: 2.4471\n",
      "Epoch [4/75], Val Loss: 2.3492\n",
      "Epoch [5/75], Loss: 1.8982\n",
      "Epoch [5/75], Val Loss: 1.9900\n",
      "Epoch [6/75], Loss: 1.5635\n",
      "Epoch [6/75], Val Loss: 1.8202\n",
      "Epoch [7/75], Loss: 1.3154\n",
      "Epoch [7/75], Val Loss: 1.9575\n",
      "Epoch [8/75], Loss: 1.2505\n",
      "Epoch [8/75], Val Loss: 1.8546\n",
      "Epoch [9/75], Loss: 1.2606\n",
      "Epoch [9/75], Val Loss: 1.6131\n",
      "Epoch [10/75], Loss: 0.8871\n",
      "Epoch [10/75], Val Loss: 1.6143\n",
      "Epoch [11/75], Loss: 0.8391\n",
      "Epoch [11/75], Val Loss: 1.4415\n",
      "Epoch [12/75], Loss: 0.6435\n",
      "Epoch [12/75], Val Loss: 1.5458\n",
      "Epoch [13/75], Loss: 0.5059\n",
      "Epoch [13/75], Val Loss: 1.3741\n",
      "Epoch [14/75], Loss: 0.4503\n",
      "Epoch [14/75], Val Loss: 1.4593\n",
      "Epoch [15/75], Loss: 0.3735\n",
      "Epoch [15/75], Val Loss: 1.5010\n",
      "Epoch [16/75], Loss: 0.2782\n",
      "Epoch [16/75], Val Loss: 1.5581\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.2782\n",
      "Test Accuracy CNN Lipschitz: 61.57%\n",
      "Epoch [1/75], Loss: 3.3156\n",
      "Epoch [1/75], Val Loss: 3.2244\n",
      "Epoch [2/75], Loss: 3.1117\n",
      "Epoch [2/75], Val Loss: 3.0781\n",
      "Epoch [3/75], Loss: 2.8719\n",
      "Epoch [3/75], Val Loss: 2.8533\n",
      "Epoch [4/75], Loss: 2.4742\n",
      "Epoch [4/75], Val Loss: 2.3974\n",
      "Epoch [5/75], Loss: 1.9356\n",
      "Epoch [5/75], Val Loss: 1.9524\n",
      "Epoch [6/75], Loss: 1.5394\n",
      "Epoch [6/75], Val Loss: 1.8948\n",
      "Epoch [7/75], Loss: 1.4214\n",
      "Epoch [7/75], Val Loss: 2.0923\n",
      "Epoch [8/75], Loss: 1.5410\n",
      "Epoch [8/75], Val Loss: 1.8631\n",
      "Epoch [9/75], Loss: 1.2665\n",
      "Epoch [9/75], Val Loss: 1.6672\n",
      "Epoch [10/75], Loss: 0.9987\n",
      "Epoch [10/75], Val Loss: 1.6075\n",
      "Epoch [11/75], Loss: 0.9137\n",
      "Epoch [11/75], Val Loss: 1.3971\n",
      "Epoch [12/75], Loss: 0.7126\n",
      "Epoch [12/75], Val Loss: 1.4784\n",
      "Epoch [13/75], Loss: 0.5935\n",
      "Epoch [13/75], Val Loss: 1.4972\n",
      "Epoch [14/75], Loss: 0.4895\n",
      "Epoch [14/75], Val Loss: 1.4632\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [14/75], Loss: 0.4895\n",
      "Test Accuracy CNN Lipschitz: 60.53%\n",
      "Epoch [1/75], Loss: 3.3193\n",
      "Epoch [1/75], Val Loss: 3.2239\n",
      "Epoch [2/75], Loss: 3.1113\n",
      "Epoch [2/75], Val Loss: 3.0662\n",
      "Epoch [3/75], Loss: 2.8729\n",
      "Epoch [3/75], Val Loss: 2.8234\n",
      "Epoch [4/75], Loss: 2.4763\n",
      "Epoch [4/75], Val Loss: 2.3820\n",
      "Epoch [5/75], Loss: 1.9407\n",
      "Epoch [5/75], Val Loss: 1.9634\n",
      "Epoch [6/75], Loss: 1.5583\n",
      "Epoch [6/75], Val Loss: 1.8719\n",
      "Epoch [7/75], Loss: 1.4077\n",
      "Epoch [7/75], Val Loss: 1.9954\n",
      "Epoch [8/75], Loss: 1.3383\n",
      "Epoch [8/75], Val Loss: 1.9266\n",
      "Epoch [9/75], Loss: 1.1363\n",
      "Epoch [9/75], Val Loss: 1.8110\n",
      "Epoch [10/75], Loss: 1.1109\n",
      "Epoch [10/75], Val Loss: 1.7545\n",
      "Epoch [11/75], Loss: 0.9369\n",
      "Epoch [11/75], Val Loss: 1.5350\n",
      "Epoch [12/75], Loss: 0.7585\n",
      "Epoch [12/75], Val Loss: 1.4163\n",
      "Epoch [13/75], Loss: 0.5972\n",
      "Epoch [13/75], Val Loss: 1.3847\n",
      "Epoch [14/75], Loss: 0.4723\n",
      "Epoch [14/75], Val Loss: 1.4967\n",
      "Epoch [15/75], Loss: 0.3966\n",
      "Epoch [15/75], Val Loss: 1.4008\n",
      "Epoch [16/75], Loss: 0.3084\n",
      "Epoch [16/75], Val Loss: 1.5154\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.3084\n",
      "Test Accuracy CNN Lipschitz: 62.56%\n",
      "Epoch [1/75], Loss: 3.4337\n",
      "Epoch [1/75], Val Loss: 3.2495\n",
      "Epoch [2/75], Loss: 3.1619\n",
      "Epoch [2/75], Val Loss: 3.1317\n",
      "Epoch [3/75], Loss: 2.9764\n",
      "Epoch [3/75], Val Loss: 2.9534\n",
      "Epoch [4/75], Loss: 2.6915\n",
      "Epoch [4/75], Val Loss: 2.6443\n",
      "Epoch [5/75], Loss: 2.2423\n",
      "Epoch [5/75], Val Loss: 2.1712\n",
      "Epoch [6/75], Loss: 1.7913\n",
      "Epoch [6/75], Val Loss: 2.0087\n",
      "Epoch [7/75], Loss: 1.5874\n",
      "Epoch [7/75], Val Loss: 2.4258\n",
      "Epoch [8/75], Loss: 1.9725\n",
      "Epoch [8/75], Val Loss: 2.1012\n",
      "Epoch [9/75], Loss: 1.4648\n",
      "Epoch [9/75], Val Loss: 1.8692\n",
      "Epoch [10/75], Loss: 1.2458\n",
      "Epoch [10/75], Val Loss: 1.6686\n",
      "Epoch [11/75], Loss: 1.0764\n",
      "Epoch [11/75], Val Loss: 1.5724\n",
      "Epoch [12/75], Loss: 0.8652\n",
      "Epoch [12/75], Val Loss: 1.4889\n",
      "Epoch [13/75], Loss: 0.7681\n",
      "Epoch [13/75], Val Loss: 1.4599\n",
      "Epoch [14/75], Loss: 0.6455\n",
      "Epoch [14/75], Val Loss: 1.5545\n",
      "Epoch [15/75], Loss: 0.5378\n",
      "Epoch [15/75], Val Loss: 1.5325\n",
      "Epoch [16/75], Loss: 0.4476\n",
      "Epoch [16/75], Val Loss: 1.5268\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.4476\n",
      "Test Accuracy CNN Lipschitz: 61.06%\n",
      "Epoch [1/75], Loss: 3.3054\n",
      "Epoch [1/75], Val Loss: 3.2094\n",
      "Epoch [2/75], Loss: 3.0933\n",
      "Epoch [2/75], Val Loss: 3.0582\n",
      "Epoch [3/75], Loss: 2.8361\n",
      "Epoch [3/75], Val Loss: 2.7763\n",
      "Epoch [4/75], Loss: 2.4168\n",
      "Epoch [4/75], Val Loss: 2.2653\n",
      "Epoch [5/75], Loss: 1.9047\n",
      "Epoch [5/75], Val Loss: 1.9705\n",
      "Epoch [6/75], Loss: 1.6439\n",
      "Epoch [6/75], Val Loss: 2.0331\n",
      "Epoch [7/75], Loss: 1.4857\n",
      "Epoch [7/75], Val Loss: 1.8838\n",
      "Epoch [8/75], Loss: 1.2748\n",
      "Epoch [8/75], Val Loss: 1.8886\n",
      "Epoch [9/75], Loss: 1.1127\n",
      "Epoch [9/75], Val Loss: 1.6612\n",
      "Epoch [10/75], Loss: 0.9618\n",
      "Epoch [10/75], Val Loss: 1.5809\n",
      "Epoch [11/75], Loss: 0.7889\n",
      "Epoch [11/75], Val Loss: 1.4763\n",
      "Epoch [12/75], Loss: 0.6969\n",
      "Epoch [12/75], Val Loss: 1.4402\n",
      "Epoch [13/75], Loss: 0.5551\n",
      "Epoch [13/75], Val Loss: 1.5108\n",
      "Epoch [14/75], Loss: 0.4670\n",
      "Epoch [14/75], Val Loss: 1.4771\n",
      "Epoch [15/75], Loss: 0.3689\n",
      "Epoch [15/75], Val Loss: 1.5342\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [15/75], Loss: 0.3689\n",
      "Test Accuracy CNN Lipschitz: 61.29%\n",
      "Epoch [1/75], Loss: 3.3232\n",
      "Epoch [1/75], Val Loss: 3.2397\n",
      "Epoch [2/75], Loss: 3.1531\n",
      "Epoch [2/75], Val Loss: 3.1413\n",
      "Epoch [3/75], Loss: 2.9829\n",
      "Epoch [3/75], Val Loss: 2.9347\n",
      "Epoch [4/75], Loss: 2.6544\n",
      "Epoch [4/75], Val Loss: 2.5458\n",
      "Epoch [5/75], Loss: 2.1794\n",
      "Epoch [5/75], Val Loss: 2.1030\n",
      "Epoch [6/75], Loss: 1.7489\n",
      "Epoch [6/75], Val Loss: 2.1885\n",
      "Epoch [7/75], Loss: 1.5368\n",
      "Epoch [7/75], Val Loss: 2.1742\n",
      "Epoch [8/75], Loss: 1.5566\n",
      "Epoch [8/75], Val Loss: 1.9768\n",
      "Epoch [9/75], Loss: 1.3671\n",
      "Epoch [9/75], Val Loss: 1.7178\n",
      "Epoch [10/75], Loss: 1.1785\n",
      "Epoch [10/75], Val Loss: 1.6529\n",
      "Epoch [11/75], Loss: 0.9743\n",
      "Epoch [11/75], Val Loss: 1.5713\n",
      "Epoch [12/75], Loss: 0.8031\n",
      "Epoch [12/75], Val Loss: 1.5105\n",
      "Epoch [13/75], Loss: 0.6665\n",
      "Epoch [13/75], Val Loss: 1.4535\n",
      "Epoch [14/75], Loss: 0.5199\n",
      "Epoch [14/75], Val Loss: 1.5038\n",
      "Epoch [15/75], Loss: 0.4153\n",
      "Epoch [15/75], Val Loss: 1.5580\n",
      "Epoch [16/75], Loss: 0.3733\n",
      "Epoch [16/75], Val Loss: 1.6719\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.3733\n",
      "Test Accuracy CNN Lipschitz: 60.50%\n",
      "Epoch [1/75], Loss: 3.3643\n",
      "Epoch [1/75], Val Loss: 3.2422\n",
      "Epoch [2/75], Loss: 3.1615\n",
      "Epoch [2/75], Val Loss: 3.1350\n",
      "Epoch [3/75], Loss: 2.9814\n",
      "Epoch [3/75], Val Loss: 2.9515\n",
      "Epoch [4/75], Loss: 2.6824\n",
      "Epoch [4/75], Val Loss: 2.6230\n",
      "Epoch [5/75], Loss: 2.2137\n",
      "Epoch [5/75], Val Loss: 2.1614\n",
      "Epoch [6/75], Loss: 1.7373\n",
      "Epoch [6/75], Val Loss: 1.9257\n",
      "Epoch [7/75], Loss: 1.4386\n",
      "Epoch [7/75], Val Loss: 1.9949\n",
      "Epoch [8/75], Loss: 1.5212\n",
      "Epoch [8/75], Val Loss: 2.1955\n",
      "Epoch [9/75], Loss: 1.3982\n",
      "Epoch [9/75], Val Loss: 1.8676\n",
      "Epoch [10/75], Loss: 1.2130\n",
      "Epoch [10/75], Val Loss: 1.9464\n",
      "Epoch [11/75], Loss: 1.0330\n",
      "Epoch [11/75], Val Loss: 1.6360\n",
      "Epoch [12/75], Loss: 0.9154\n",
      "Epoch [12/75], Val Loss: 1.5463\n",
      "Epoch [13/75], Loss: 0.7018\n",
      "Epoch [13/75], Val Loss: 1.5293\n",
      "Epoch [14/75], Loss: 0.5878\n",
      "Epoch [14/75], Val Loss: 1.4767\n",
      "Epoch [15/75], Loss: 0.4645\n",
      "Epoch [15/75], Val Loss: 1.5056\n",
      "Epoch [16/75], Loss: 0.4002\n",
      "Epoch [16/75], Val Loss: 1.5888\n",
      "Epoch [17/75], Loss: 0.3130\n",
      "Epoch [17/75], Val Loss: 1.6370\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [17/75], Loss: 0.3130\n",
      "Test Accuracy CNN Lipschitz: 60.36%\n",
      "Epoch [1/75], Loss: 3.3648\n",
      "Epoch [1/75], Val Loss: 3.2513\n",
      "Epoch [2/75], Loss: 3.1468\n",
      "Epoch [2/75], Val Loss: 3.1020\n",
      "Epoch [3/75], Loss: 2.9491\n",
      "Epoch [3/75], Val Loss: 2.8778\n",
      "Epoch [4/75], Loss: 2.5626\n",
      "Epoch [4/75], Val Loss: 2.4746\n",
      "Epoch [5/75], Loss: 2.0515\n",
      "Epoch [5/75], Val Loss: 2.0469\n",
      "Epoch [6/75], Loss: 1.7191\n",
      "Epoch [6/75], Val Loss: 2.1345\n",
      "Epoch [7/75], Loss: 1.6443\n",
      "Epoch [7/75], Val Loss: 2.0838\n",
      "Epoch [8/75], Loss: 1.5125\n",
      "Epoch [8/75], Val Loss: 2.0299\n",
      "Epoch [9/75], Loss: 1.2642\n",
      "Epoch [9/75], Val Loss: 1.6497\n",
      "Epoch [10/75], Loss: 1.0457\n",
      "Epoch [10/75], Val Loss: 1.5886\n",
      "Epoch [11/75], Loss: 0.8512\n",
      "Epoch [11/75], Val Loss: 1.4528\n",
      "Epoch [12/75], Loss: 0.7080\n",
      "Epoch [12/75], Val Loss: 1.4966\n",
      "Epoch [13/75], Loss: 0.5784\n",
      "Epoch [13/75], Val Loss: 1.4593\n",
      "Epoch [14/75], Loss: 0.5043\n",
      "Epoch [14/75], Val Loss: 1.5476\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [14/75], Loss: 0.5043\n",
      "Test Accuracy CNN Lipschitz: 58.90%\n",
      "Epoch [1/75], Loss: 3.3081\n",
      "Epoch [1/75], Val Loss: 3.2042\n",
      "Epoch [2/75], Loss: 3.0969\n",
      "Epoch [2/75], Val Loss: 3.0765\n",
      "Epoch [3/75], Loss: 2.8484\n",
      "Epoch [3/75], Val Loss: 2.8030\n",
      "Epoch [4/75], Loss: 2.4576\n",
      "Epoch [4/75], Val Loss: 2.3763\n",
      "Epoch [5/75], Loss: 1.9624\n",
      "Epoch [5/75], Val Loss: 2.1182\n",
      "Epoch [6/75], Loss: 1.6603\n",
      "Epoch [6/75], Val Loss: 2.0528\n",
      "Epoch [7/75], Loss: 1.6516\n",
      "Epoch [7/75], Val Loss: 2.0784\n",
      "Epoch [8/75], Loss: 1.5388\n",
      "Epoch [8/75], Val Loss: 1.9712\n",
      "Epoch [9/75], Loss: 1.3160\n",
      "Epoch [9/75], Val Loss: 1.6747\n",
      "Epoch [10/75], Loss: 1.1106\n",
      "Epoch [10/75], Val Loss: 1.4702\n",
      "Epoch [11/75], Loss: 0.9195\n",
      "Epoch [11/75], Val Loss: 1.5188\n",
      "Epoch [12/75], Loss: 0.7715\n",
      "Epoch [12/75], Val Loss: 1.4085\n",
      "Epoch [13/75], Loss: 0.6531\n",
      "Epoch [13/75], Val Loss: 1.4709\n",
      "Epoch [14/75], Loss: 0.5346\n",
      "Epoch [14/75], Val Loss: 1.4996\n",
      "Epoch [15/75], Loss: 0.4512\n",
      "Epoch [15/75], Val Loss: 1.4963\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [15/75], Loss: 0.4512\n",
      "Test Accuracy CNN Lipschitz: 60.87%\n",
      "Epoch [1/75], Loss: 3.3457\n",
      "Epoch [1/75], Val Loss: 3.2193\n",
      "Epoch [2/75], Loss: 3.0813\n",
      "Epoch [2/75], Val Loss: 3.0633\n",
      "Epoch [3/75], Loss: 2.7825\n",
      "Epoch [3/75], Val Loss: 2.7065\n",
      "Epoch [4/75], Loss: 2.3000\n",
      "Epoch [4/75], Val Loss: 2.2290\n",
      "Epoch [5/75], Loss: 1.7319\n",
      "Epoch [5/75], Val Loss: 2.0068\n",
      "Epoch [6/75], Loss: 1.4544\n",
      "Epoch [6/75], Val Loss: 2.0591\n",
      "Epoch [7/75], Loss: 1.4076\n",
      "Epoch [7/75], Val Loss: 1.9293\n",
      "Epoch [8/75], Loss: 1.1991\n",
      "Epoch [8/75], Val Loss: 2.0234\n",
      "Epoch [9/75], Loss: 1.1633\n",
      "Epoch [9/75], Val Loss: 1.7044\n",
      "Epoch [10/75], Loss: 0.9498\n",
      "Epoch [10/75], Val Loss: 1.5234\n",
      "Epoch [11/75], Loss: 0.7709\n",
      "Epoch [11/75], Val Loss: 1.4404\n",
      "Epoch [12/75], Loss: 0.6146\n",
      "Epoch [12/75], Val Loss: 1.4071\n",
      "Epoch [13/75], Loss: 0.5241\n",
      "Epoch [13/75], Val Loss: 1.4274\n",
      "Epoch [14/75], Loss: 0.3794\n",
      "Epoch [14/75], Val Loss: 1.4507\n",
      "Epoch [15/75], Loss: 0.3245\n",
      "Epoch [15/75], Val Loss: 1.4795\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [15/75], Loss: 0.3245\n",
      "Test Accuracy CNN Lipschitz: 63.55%\n",
      "Epoch [1/75], Loss: 3.3177\n",
      "Epoch [1/75], Val Loss: 3.2461\n",
      "Epoch [2/75], Loss: 3.1454\n",
      "Epoch [2/75], Val Loss: 3.1392\n",
      "Epoch [3/75], Loss: 2.9780\n",
      "Epoch [3/75], Val Loss: 2.9672\n",
      "Epoch [4/75], Loss: 2.6887\n",
      "Epoch [4/75], Val Loss: 2.6457\n",
      "Epoch [5/75], Loss: 2.2261\n",
      "Epoch [5/75], Val Loss: 2.1806\n",
      "Epoch [6/75], Loss: 1.7031\n",
      "Epoch [6/75], Val Loss: 1.8416\n",
      "Epoch [7/75], Loss: 1.4268\n",
      "Epoch [7/75], Val Loss: 1.7465\n",
      "Epoch [8/75], Loss: 1.2720\n",
      "Epoch [8/75], Val Loss: 1.7753\n",
      "Epoch [9/75], Loss: 1.2083\n",
      "Epoch [9/75], Val Loss: 1.8062\n",
      "Epoch [10/75], Loss: 1.0279\n",
      "Epoch [10/75], Val Loss: 1.6852\n",
      "Epoch [11/75], Loss: 0.9234\n",
      "Epoch [11/75], Val Loss: 1.5420\n",
      "Epoch [12/75], Loss: 0.7303\n",
      "Epoch [12/75], Val Loss: 1.4515\n",
      "Epoch [13/75], Loss: 0.6201\n",
      "Epoch [13/75], Val Loss: 1.4348\n",
      "Epoch [14/75], Loss: 0.5225\n",
      "Epoch [14/75], Val Loss: 1.3961\n",
      "Epoch [15/75], Loss: 0.4053\n",
      "Epoch [15/75], Val Loss: 1.4040\n",
      "Epoch [16/75], Loss: 0.3066\n",
      "Epoch [16/75], Val Loss: 1.4956\n",
      "Epoch [17/75], Loss: 0.2397\n",
      "Epoch [17/75], Val Loss: 1.6019\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [17/75], Loss: 0.2397\n",
      "Test Accuracy CNN Lipschitz: 61.12%\n",
      "Epoch [1/75], Loss: 3.3067\n",
      "Epoch [1/75], Val Loss: 3.1952\n",
      "Epoch [2/75], Loss: 3.0712\n",
      "Epoch [2/75], Val Loss: 3.0179\n",
      "Epoch [3/75], Loss: 2.7762\n",
      "Epoch [3/75], Val Loss: 2.6819\n",
      "Epoch [4/75], Loss: 2.2881\n",
      "Epoch [4/75], Val Loss: 2.1701\n",
      "Epoch [5/75], Loss: 1.7758\n",
      "Epoch [5/75], Val Loss: 1.9035\n",
      "Epoch [6/75], Loss: 1.5381\n",
      "Epoch [6/75], Val Loss: 1.9432\n",
      "Epoch [7/75], Loss: 1.4708\n",
      "Epoch [7/75], Val Loss: 1.9050\n",
      "Epoch [8/75], Loss: 1.3747\n",
      "Epoch [8/75], Val Loss: 1.9119\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [8/75], Loss: 1.3747\n",
      "Test Accuracy CNN Lipschitz: 47.21%\n",
      "Epoch [1/75], Loss: 3.2994\n",
      "Epoch [1/75], Val Loss: 3.2152\n",
      "Epoch [2/75], Loss: 3.1008\n",
      "Epoch [2/75], Val Loss: 3.0504\n",
      "Epoch [3/75], Loss: 2.8306\n",
      "Epoch [3/75], Val Loss: 2.7498\n",
      "Epoch [4/75], Loss: 2.3746\n",
      "Epoch [4/75], Val Loss: 2.2798\n",
      "Epoch [5/75], Loss: 1.8852\n",
      "Epoch [5/75], Val Loss: 1.8971\n",
      "Epoch [6/75], Loss: 1.5634\n",
      "Epoch [6/75], Val Loss: 2.0747\n",
      "Epoch [7/75], Loss: 1.5356\n",
      "Epoch [7/75], Val Loss: 1.9453\n",
      "Epoch [8/75], Loss: 1.5636\n",
      "Epoch [8/75], Val Loss: 2.2773\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [8/75], Loss: 1.5636\n",
      "Test Accuracy CNN Lipschitz: 47.36%\n",
      "Epoch [1/75], Loss: 3.3630\n",
      "Epoch [1/75], Val Loss: 3.2498\n",
      "Epoch [2/75], Loss: 3.1605\n",
      "Epoch [2/75], Val Loss: 3.1636\n",
      "Epoch [3/75], Loss: 3.0093\n",
      "Epoch [3/75], Val Loss: 3.0183\n",
      "Epoch [4/75], Loss: 2.7491\n",
      "Epoch [4/75], Val Loss: 2.7186\n",
      "Epoch [5/75], Loss: 2.3222\n",
      "Epoch [5/75], Val Loss: 2.3024\n",
      "Epoch [6/75], Loss: 1.7966\n",
      "Epoch [6/75], Val Loss: 1.9112\n",
      "Epoch [7/75], Loss: 1.4867\n",
      "Epoch [7/75], Val Loss: 1.9846\n",
      "Epoch [8/75], Loss: 1.4373\n",
      "Epoch [8/75], Val Loss: 1.9439\n",
      "Epoch [9/75], Loss: 1.1917\n",
      "Epoch [9/75], Val Loss: 1.7679\n",
      "Epoch [10/75], Loss: 0.9602\n",
      "Epoch [10/75], Val Loss: 1.7203\n",
      "Epoch [11/75], Loss: 0.7987\n",
      "Epoch [11/75], Val Loss: 1.5201\n",
      "Epoch [12/75], Loss: 0.6846\n",
      "Epoch [12/75], Val Loss: 1.5417\n",
      "Epoch [13/75], Loss: 0.5784\n",
      "Epoch [13/75], Val Loss: 1.5163\n",
      "Epoch [14/75], Loss: 0.4865\n",
      "Epoch [14/75], Val Loss: 1.5174\n",
      "Epoch [15/75], Loss: 0.3719\n",
      "Epoch [15/75], Val Loss: 1.4864\n",
      "Epoch [16/75], Loss: 0.2625\n",
      "Epoch [16/75], Val Loss: 1.5549\n",
      "Epoch [17/75], Loss: 0.2190\n",
      "Epoch [17/75], Val Loss: 1.5672\n",
      "Epoch [18/75], Loss: 0.1496\n",
      "Epoch [18/75], Val Loss: 1.6581\n",
      "Stopping early at epoch 18 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [18/75], Loss: 0.1496\n",
      "Test Accuracy CNN Lipschitz: 62.71%\n",
      "Epoch [1/75], Loss: 3.3508\n",
      "Epoch [1/75], Val Loss: 3.2441\n",
      "Epoch [2/75], Loss: 3.1310\n",
      "Epoch [2/75], Val Loss: 3.1145\n",
      "Epoch [3/75], Loss: 2.9176\n",
      "Epoch [3/75], Val Loss: 2.8756\n",
      "Epoch [4/75], Loss: 2.5371\n",
      "Epoch [4/75], Val Loss: 2.4089\n",
      "Epoch [5/75], Loss: 2.0170\n",
      "Epoch [5/75], Val Loss: 2.0268\n",
      "Epoch [6/75], Loss: 1.6786\n",
      "Epoch [6/75], Val Loss: 1.8779\n",
      "Epoch [7/75], Loss: 1.3990\n",
      "Epoch [7/75], Val Loss: 1.8977\n",
      "Epoch [8/75], Loss: 1.2751\n",
      "Epoch [8/75], Val Loss: 1.8864\n",
      "Epoch [9/75], Loss: 1.1697\n",
      "Epoch [9/75], Val Loss: 1.8944\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [9/75], Loss: 1.1697\n",
      "Test Accuracy CNN Lipschitz: 47.20%\n",
      "Epoch [1/75], Loss: 3.3387\n",
      "Epoch [1/75], Val Loss: 3.2425\n",
      "Epoch [2/75], Loss: 3.1273\n",
      "Epoch [2/75], Val Loss: 3.1282\n",
      "Epoch [3/75], Loss: 2.9542\n",
      "Epoch [3/75], Val Loss: 2.9259\n",
      "Epoch [4/75], Loss: 2.6493\n",
      "Epoch [4/75], Val Loss: 2.5781\n",
      "Epoch [5/75], Loss: 2.1639\n",
      "Epoch [5/75], Val Loss: 2.1041\n",
      "Epoch [6/75], Loss: 1.6607\n",
      "Epoch [6/75], Val Loss: 1.7849\n",
      "Epoch [7/75], Loss: 1.4130\n",
      "Epoch [7/75], Val Loss: 1.8809\n",
      "Epoch [8/75], Loss: 1.2581\n",
      "Epoch [8/75], Val Loss: 1.8791\n",
      "Epoch [9/75], Loss: 1.1624\n",
      "Epoch [9/75], Val Loss: 2.1461\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [9/75], Loss: 1.1624\n",
      "Test Accuracy CNN Lipschitz: 45.25%\n",
      "Epoch [1/75], Loss: 3.3365\n",
      "Epoch [1/75], Val Loss: 3.2130\n",
      "Epoch [2/75], Loss: 3.1115\n",
      "Epoch [2/75], Val Loss: 3.0812\n",
      "Epoch [3/75], Loss: 2.8899\n",
      "Epoch [3/75], Val Loss: 2.8346\n",
      "Epoch [4/75], Loss: 2.4872\n",
      "Epoch [4/75], Val Loss: 2.3774\n",
      "Epoch [5/75], Loss: 1.9690\n",
      "Epoch [5/75], Val Loss: 1.9577\n",
      "Epoch [6/75], Loss: 1.5934\n",
      "Epoch [6/75], Val Loss: 2.0123\n",
      "Epoch [7/75], Loss: 1.5484\n",
      "Epoch [7/75], Val Loss: 1.9305\n",
      "Epoch [8/75], Loss: 1.3494\n",
      "Epoch [8/75], Val Loss: 1.9972\n",
      "Epoch [9/75], Loss: 1.2034\n",
      "Epoch [9/75], Val Loss: 1.8381\n",
      "Epoch [10/75], Loss: 1.0032\n",
      "Epoch [10/75], Val Loss: 1.6216\n",
      "Epoch [11/75], Loss: 0.8699\n",
      "Epoch [11/75], Val Loss: 1.5676\n",
      "Epoch [12/75], Loss: 0.7026\n",
      "Epoch [12/75], Val Loss: 1.4852\n",
      "Epoch [13/75], Loss: 0.5657\n",
      "Epoch [13/75], Val Loss: 1.4509\n",
      "Epoch [14/75], Loss: 0.4843\n",
      "Epoch [14/75], Val Loss: 1.5038\n",
      "Epoch [15/75], Loss: 0.3838\n",
      "Epoch [15/75], Val Loss: 1.5285\n",
      "Epoch [16/75], Loss: 0.2968\n",
      "Epoch [16/75], Val Loss: 1.5414\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [16/75], Loss: 0.2968\n",
      "Test Accuracy CNN Lipschitz: 62.53%\n",
      "Epoch [1/75], Loss: 3.3466\n",
      "Epoch [1/75], Val Loss: 3.2159\n",
      "Epoch [2/75], Loss: 3.1184\n",
      "Epoch [2/75], Val Loss: 3.0856\n",
      "Epoch [3/75], Loss: 2.8871\n",
      "Epoch [3/75], Val Loss: 2.8546\n",
      "Epoch [4/75], Loss: 2.4913\n",
      "Epoch [4/75], Val Loss: 2.4055\n",
      "Epoch [5/75], Loss: 1.9315\n",
      "Epoch [5/75], Val Loss: 1.9487\n",
      "Epoch [6/75], Loss: 1.5786\n",
      "Epoch [6/75], Val Loss: 1.9561\n",
      "Epoch [7/75], Loss: 1.4661\n",
      "Epoch [7/75], Val Loss: 1.9062\n",
      "Epoch [8/75], Loss: 1.3538\n",
      "Epoch [8/75], Val Loss: 2.1342\n",
      "Epoch [9/75], Loss: 1.4244\n",
      "Epoch [9/75], Val Loss: 2.1554\n",
      "Epoch [10/75], Loss: 1.1914\n",
      "Epoch [10/75], Val Loss: 1.6158\n",
      "Epoch [11/75], Loss: 0.9476\n",
      "Epoch [11/75], Val Loss: 1.5792\n",
      "Epoch [12/75], Loss: 0.8130\n",
      "Epoch [12/75], Val Loss: 1.4860\n",
      "Epoch [13/75], Loss: 0.6101\n",
      "Epoch [13/75], Val Loss: 1.5082\n",
      "Epoch [14/75], Loss: 0.5139\n",
      "Epoch [14/75], Val Loss: 1.5235\n",
      "Epoch [15/75], Loss: 0.4053\n",
      "Epoch [15/75], Val Loss: 1.5054\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [15/75], Loss: 0.4053\n",
      "Test Accuracy CNN Lipschitz: 61.84%\n",
      "Epoch [1/75], Loss: 3.3430\n",
      "Epoch [1/75], Val Loss: 3.2371\n",
      "Epoch [2/75], Loss: 3.1503\n",
      "Epoch [2/75], Val Loss: 3.1349\n",
      "Epoch [3/75], Loss: 2.9782\n",
      "Epoch [3/75], Val Loss: 2.9722\n",
      "Epoch [4/75], Loss: 2.6755\n",
      "Epoch [4/75], Val Loss: 2.6416\n",
      "Epoch [5/75], Loss: 2.2353\n",
      "Epoch [5/75], Val Loss: 2.1907\n",
      "Epoch [6/75], Loss: 1.7844\n",
      "Epoch [6/75], Val Loss: 1.9620\n",
      "Epoch [7/75], Loss: 1.5398\n",
      "Epoch [7/75], Val Loss: 2.2255\n",
      "Epoch [8/75], Loss: 1.5890\n",
      "Epoch [8/75], Val Loss: 1.9746\n",
      "Epoch [9/75], Loss: 1.2171\n",
      "Epoch [9/75], Val Loss: 1.6546\n",
      "Epoch [10/75], Loss: 0.9784\n",
      "Epoch [10/75], Val Loss: 1.6882\n",
      "Epoch [11/75], Loss: 0.8664\n",
      "Epoch [11/75], Val Loss: 1.5000\n",
      "Epoch [12/75], Loss: 0.7394\n",
      "Epoch [12/75], Val Loss: 1.5866\n",
      "Epoch [13/75], Loss: 0.6492\n",
      "Epoch [13/75], Val Loss: 1.5328\n",
      "Epoch [14/75], Loss: 0.5400\n",
      "Epoch [14/75], Val Loss: 1.6385\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [14/75], Loss: 0.5400\n",
      "Test Accuracy CNN Lipschitz: 57.98%\n",
      "Epoch [1/75], Loss: 3.3313\n",
      "Epoch [1/75], Val Loss: 3.2322\n",
      "Epoch [2/75], Loss: 3.1058\n",
      "Epoch [2/75], Val Loss: 3.0761\n",
      "Epoch [3/75], Loss: 2.8534\n",
      "Epoch [3/75], Val Loss: 2.7829\n",
      "Epoch [4/75], Loss: 2.4063\n",
      "Epoch [4/75], Val Loss: 2.3173\n",
      "Epoch [5/75], Loss: 1.8276\n",
      "Epoch [5/75], Val Loss: 1.9216\n",
      "Epoch [6/75], Loss: 1.5021\n",
      "Epoch [6/75], Val Loss: 1.8355\n",
      "Epoch [7/75], Loss: 1.4767\n",
      "Epoch [7/75], Val Loss: 2.0335\n",
      "Epoch [8/75], Loss: 1.6431\n",
      "Epoch [8/75], Val Loss: 2.1163\n",
      "Epoch [9/75], Loss: 1.3389\n",
      "Epoch [9/75], Val Loss: 1.9553\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [9/75], Loss: 1.3389\n",
      "Test Accuracy CNN Lipschitz: 45.80%\n",
      "Epoch [1/75], Loss: 3.3349\n",
      "Epoch [1/75], Val Loss: 3.2230\n",
      "Epoch [2/75], Loss: 3.1110\n",
      "Epoch [2/75], Val Loss: 3.0850\n",
      "Epoch [3/75], Loss: 2.8941\n",
      "Epoch [3/75], Val Loss: 2.8266\n",
      "Epoch [4/75], Loss: 2.4817\n",
      "Epoch [4/75], Val Loss: 2.3508\n",
      "Epoch [5/75], Loss: 1.9256\n",
      "Epoch [5/75], Val Loss: 1.9426\n",
      "Epoch [6/75], Loss: 1.6677\n",
      "Epoch [6/75], Val Loss: 2.3219\n",
      "Epoch [7/75], Loss: 1.9372\n",
      "Epoch [7/75], Val Loss: 2.2421\n",
      "Epoch [8/75], Loss: 1.6441\n",
      "Epoch [8/75], Val Loss: 1.7426\n",
      "Epoch [9/75], Loss: 1.2141\n",
      "Epoch [9/75], Val Loss: 1.7073\n",
      "Epoch [10/75], Loss: 1.0678\n",
      "Epoch [10/75], Val Loss: 1.5686\n",
      "Epoch [11/75], Loss: 0.9144\n",
      "Epoch [11/75], Val Loss: 1.4849\n",
      "Epoch [12/75], Loss: 0.7318\n",
      "Epoch [12/75], Val Loss: 1.5092\n",
      "Epoch [13/75], Loss: 0.6080\n",
      "Epoch [13/75], Val Loss: 1.5067\n",
      "Epoch [14/75], Loss: 0.5204\n",
      "Epoch [14/75], Val Loss: 1.5667\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [14/75], Loss: 0.5204\n",
      "Test Accuracy CNN Lipschitz: 60.11%\n",
      "Epoch [1/75], Loss: 3.3495\n",
      "Epoch [1/75], Val Loss: 3.2289\n",
      "Epoch [2/75], Loss: 3.1331\n",
      "Epoch [2/75], Val Loss: 3.0851\n",
      "Epoch [3/75], Loss: 2.8911\n",
      "Epoch [3/75], Val Loss: 2.8636\n",
      "Epoch [4/75], Loss: 2.5201\n",
      "Epoch [4/75], Val Loss: 2.4441\n",
      "Epoch [5/75], Loss: 1.9904\n",
      "Epoch [5/75], Val Loss: 2.0099\n",
      "Epoch [6/75], Loss: 1.5815\n",
      "Epoch [6/75], Val Loss: 2.0667\n",
      "Epoch [7/75], Loss: 1.4383\n",
      "Epoch [7/75], Val Loss: 1.9562\n",
      "Epoch [8/75], Loss: 1.3608\n",
      "Epoch [8/75], Val Loss: 1.8549\n",
      "Epoch [9/75], Loss: 1.1954\n",
      "Epoch [9/75], Val Loss: 1.8236\n",
      "Epoch [10/75], Loss: 1.0081\n",
      "Epoch [10/75], Val Loss: 1.6976\n",
      "Epoch [11/75], Loss: 0.8777\n",
      "Epoch [11/75], Val Loss: 1.5483\n",
      "Epoch [12/75], Loss: 0.7282\n",
      "Epoch [12/75], Val Loss: 1.4800\n",
      "Epoch [13/75], Loss: 0.6183\n",
      "Epoch [13/75], Val Loss: 1.4715\n",
      "Epoch [14/75], Loss: 0.4627\n",
      "Epoch [14/75], Val Loss: 1.4578\n",
      "Epoch [15/75], Loss: 0.3771\n",
      "Epoch [15/75], Val Loss: 1.4933\n",
      "Epoch [16/75], Loss: 0.3075\n",
      "Epoch [16/75], Val Loss: 1.5792\n",
      "Epoch [17/75], Loss: 0.2335\n",
      "Epoch [17/75], Val Loss: 1.5867\n",
      "Stopping early at epoch 17 (No improvement in 3 epochs)\n",
      "Subset 1000, Epoch [17/75], Loss: 0.2335\n",
      "Test Accuracy CNN Lipschitz: 61.86%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 75\n",
    "tolerance = 1e-4  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "logit_accuracy_cnn_lipschitz = {}\n",
    "\n",
    "for subset_size in num_subsets:\n",
    "    accuracy_list = []\n",
    "    print(f\"Training with subset size: {subset_size}\")\n",
    "\n",
    "    for _ in range(30):\n",
    "        cnn_model_lipschitz = EMNIST_CNN(num_classes).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_sdg = optim.SGD(cnn_model_lipschitz.parameters(), lr=0.005, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "        def init_weights(m):\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                    m.bias.data.fill_(0.01)\n",
    "        cnn_model_lipschitz.apply(init_weights)\n",
    "\n",
    "        subset_indices = torch.randperm(train_data.shape[0])[:subset_size]\n",
    "        subset = Subset(train_set, subset_indices)\n",
    "        train_loader = DataLoader(subset, batch_size=128, shuffle=True)\n",
    "\n",
    "        best_val_loss = float('inf')  # Track the best loss\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            cnn_model_lipschitz.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = cnn_model_lipschitz(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass\n",
    "                optimizer_sdg.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "\n",
    "                L = max(param.grad.abs().max().item() for param in cnn_model_lipschitz.parameters() if param.grad is not None)\n",
    "                adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "                # Update optimizer's learning rate\n",
    "                for param_group in optimizer_sdg.param_groups:\n",
    "                    param_group['lr'] = adaptive_lr\n",
    "\n",
    "                optimizer_sdg.step()\n",
    "\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "            current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "           # Validation loop\n",
    "            cnn_model_lipschitz.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader_cnn:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    outputs = cnn_model_lipschitz(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader_cnn)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                    break\n",
    "\n",
    "        print(f\"Subset {subset_size}, Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        cnn_model_lipschitz.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = cnn_model_lipschitz(images)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Test Accuracy CNN Lipschitz: {accuracy:.2f}%\")\n",
    "\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    logit_accuracy_cnn_lipschitz[subset_size] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000    59.066506\n",
       " 50000    57.689103\n",
       " 10000    58.432372\n",
       " 5000     58.784295\n",
       " 1000     58.456250\n",
       " dtype: float64,\n",
       " 75000    5.838888\n",
       " 50000    7.303048\n",
       " 10000    5.382737\n",
       " 5000     6.069793\n",
       " 1000     5.998636\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_accuracy_cnn_lipschitz_df = pd.DataFrame(logit_accuracy_cnn_lipschitz)\n",
    "logit_accuracy_cnn_lipschitz_df.mean(), logit_accuracy_cnn_lipschitz_df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans(Dataset):\n",
    "    def __init__(self, centroids, labels):\n",
    "        \"\"\"\n",
    "        centroids: torch.Tensor or numpy array of shape (N, D) where N is the number of samples, D is the feature size.\n",
    "        labels: torch.Tensor or numpy array of shape (N,) containing class labels.\n",
    "        \"\"\"\n",
    "        self.centroids = centroids.clone().detach().float()\n",
    "        self.labels = labels.clone().detach().long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centroids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.centroids[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 175.4606\n",
      "Epoch [2/100], Loss: 158.9786\n",
      "Epoch [3/100], Loss: 156.4019\n",
      "Epoch [4/100], Loss: 155.2115\n",
      "Epoch [5/100], Loss: 154.2610\n",
      "Epoch [6/100], Loss: 153.4348\n",
      "Epoch [7/100], Loss: 153.0185\n",
      "Epoch [8/100], Loss: 152.6335\n",
      "Epoch [9/100], Loss: 152.0356\n",
      "Epoch [10/100], Loss: 151.7344\n",
      "Epoch [11/100], Loss: 151.5714\n",
      "Epoch [12/100], Loss: 150.6712\n",
      "Epoch [13/100], Loss: 151.0839\n",
      "Epoch [14/100], Loss: 150.6006\n",
      "Epoch [15/100], Loss: 150.1626\n",
      "Epoch [16/100], Loss: 149.9008\n",
      "Epoch [17/100], Loss: 150.2816\n",
      "Epoch [18/100], Loss: 149.7011\n",
      "Epoch [19/100], Loss: 149.4352\n",
      "Epoch [20/100], Loss: 149.3675\n",
      "Epoch [21/100], Loss: 149.4079\n",
      "Epoch [22/100], Loss: 149.1935\n",
      "Epoch [23/100], Loss: 148.8121\n",
      "Epoch [24/100], Loss: 148.7812\n",
      "Epoch [25/100], Loss: 148.8039\n",
      "Epoch [26/100], Loss: 148.6196\n",
      "Epoch [27/100], Loss: 148.8564\n",
      "Epoch [28/100], Loss: 148.4779\n",
      "Epoch [29/100], Loss: 148.5490\n",
      "Epoch [30/100], Loss: 148.5787\n",
      "Epoch [31/100], Loss: 148.5956\n",
      "Stopping early at epoch 31 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 69.46%\n",
      "Epoch [1/100], Loss: 121.2048\n",
      "Epoch [2/100], Loss: 106.6236\n",
      "Epoch [3/100], Loss: 104.1628\n",
      "Epoch [4/100], Loss: 102.9018\n",
      "Epoch [5/100], Loss: 102.2278\n",
      "Epoch [6/100], Loss: 101.6111\n",
      "Epoch [7/100], Loss: 100.9835\n",
      "Epoch [8/100], Loss: 100.3985\n",
      "Epoch [9/100], Loss: 100.0811\n",
      "Epoch [10/100], Loss: 99.9867\n",
      "Epoch [11/100], Loss: 99.6988\n",
      "Epoch [12/100], Loss: 99.1106\n",
      "Epoch [13/100], Loss: 98.8430\n",
      "Epoch [14/100], Loss: 98.7612\n",
      "Epoch [15/100], Loss: 98.4477\n",
      "Epoch [16/100], Loss: 98.4176\n",
      "Epoch [17/100], Loss: 98.1846\n",
      "Epoch [18/100], Loss: 98.2690\n",
      "Epoch [19/100], Loss: 97.8946\n",
      "Epoch [20/100], Loss: 97.8834\n",
      "Epoch [21/100], Loss: 97.7701\n",
      "Epoch [22/100], Loss: 97.7608\n",
      "Epoch [23/100], Loss: 97.3312\n",
      "Epoch [24/100], Loss: 97.3840\n",
      "Epoch [25/100], Loss: 97.2651\n",
      "Epoch [26/100], Loss: 97.0385\n",
      "Epoch [27/100], Loss: 96.7432\n",
      "Epoch [28/100], Loss: 96.6729\n",
      "Epoch [29/100], Loss: 96.6914\n",
      "Epoch [30/100], Loss: 96.7511\n",
      "Epoch [31/100], Loss: 96.6749\n",
      "Epoch [32/100], Loss: 96.2098\n",
      "Epoch [33/100], Loss: 96.6210\n",
      "Epoch [34/100], Loss: 96.3072\n",
      "Epoch [35/100], Loss: 96.1842\n",
      "Epoch [36/100], Loss: 95.8686\n",
      "Epoch [37/100], Loss: 96.1538\n",
      "Epoch [38/100], Loss: 95.9489\n",
      "Epoch [39/100], Loss: 95.6969\n",
      "Epoch [40/100], Loss: 95.7312\n",
      "Epoch [41/100], Loss: 95.7618\n",
      "Epoch [42/100], Loss: 95.4176\n",
      "Epoch [43/100], Loss: 95.8159\n",
      "Epoch [44/100], Loss: 95.3388\n",
      "Epoch [45/100], Loss: 95.3501\n",
      "Epoch [46/100], Loss: 95.4767\n",
      "Epoch [47/100], Loss: 95.2194\n",
      "Epoch [48/100], Loss: 95.2748\n",
      "Epoch [49/100], Loss: 95.1065\n",
      "Epoch [50/100], Loss: 95.2739\n",
      "Epoch [51/100], Loss: 95.3620\n",
      "Epoch [52/100], Loss: 95.2172\n",
      "Epoch [53/100], Loss: 95.0857\n",
      "Epoch [54/100], Loss: 94.8820\n",
      "Epoch [55/100], Loss: 94.9243\n",
      "Epoch [56/100], Loss: 94.7713\n",
      "Epoch [57/100], Loss: 94.8409\n",
      "Epoch [58/100], Loss: 94.8502\n",
      "Epoch [59/100], Loss: 94.8105\n",
      "Epoch [60/100], Loss: 94.8421\n",
      "Epoch [61/100], Loss: 94.9186\n",
      "Epoch [62/100], Loss: 94.4927\n",
      "Epoch [63/100], Loss: 94.1438\n",
      "Epoch [64/100], Loss: 94.3762\n",
      "Epoch [65/100], Loss: 94.6065\n",
      "Epoch [66/100], Loss: 94.4247\n",
      "Epoch [67/100], Loss: 94.5378\n",
      "Epoch [68/100], Loss: 94.2698\n",
      "Epoch [69/100], Loss: 94.3839\n",
      "Epoch [70/100], Loss: 94.5795\n",
      "Epoch [71/100], Loss: 94.4615\n",
      "Epoch [72/100], Loss: 94.1959\n",
      "Epoch [73/100], Loss: 94.2446\n",
      "Epoch [74/100], Loss: 94.0644\n",
      "Epoch [75/100], Loss: 94.3262\n",
      "Epoch [76/100], Loss: 94.1385\n",
      "Epoch [77/100], Loss: 93.9647\n",
      "Epoch [78/100], Loss: 94.3753\n",
      "Epoch [79/100], Loss: 93.9312\n",
      "Epoch [80/100], Loss: 94.0483\n",
      "Epoch [81/100], Loss: 94.0347\n",
      "Epoch [82/100], Loss: 94.0885\n",
      "Epoch [83/100], Loss: 94.0421\n",
      "Epoch [84/100], Loss: 93.6677\n",
      "Epoch [85/100], Loss: 93.8822\n",
      "Epoch [86/100], Loss: 93.5978\n",
      "Epoch [87/100], Loss: 94.1506\n",
      "Epoch [88/100], Loss: 93.9510\n",
      "Epoch [89/100], Loss: 93.8951\n",
      "Epoch [90/100], Loss: 93.6110\n",
      "Epoch [91/100], Loss: 93.8096\n",
      "Epoch [92/100], Loss: 93.7718\n",
      "Epoch [93/100], Loss: 93.8740\n",
      "Epoch [94/100], Loss: 93.7752\n",
      "Epoch [95/100], Loss: 93.5658\n",
      "Epoch [96/100], Loss: 93.6774\n",
      "Epoch [97/100], Loss: 93.7526\n",
      "Epoch [98/100], Loss: 93.6827\n",
      "Epoch [99/100], Loss: 93.6310\n",
      "Epoch [100/100], Loss: 93.2773\n",
      "Test Accuracy Base Logit K Means: 67.89%\n",
      "Epoch [1/100], Loss: 30.0688\n",
      "Epoch [2/100], Loss: 20.5317\n",
      "Epoch [3/100], Loss: 19.1188\n",
      "Epoch [4/100], Loss: 18.3951\n",
      "Epoch [5/100], Loss: 17.7621\n",
      "Epoch [6/100], Loss: 17.4458\n",
      "Epoch [7/100], Loss: 16.9153\n",
      "Epoch [8/100], Loss: 16.6629\n",
      "Epoch [9/100], Loss: 16.4184\n",
      "Epoch [10/100], Loss: 16.1260\n",
      "Epoch [11/100], Loss: 15.9602\n",
      "Epoch [12/100], Loss: 15.8520\n",
      "Epoch [13/100], Loss: 15.7394\n",
      "Epoch [14/100], Loss: 15.5360\n",
      "Epoch [15/100], Loss: 15.4526\n",
      "Epoch [16/100], Loss: 15.1932\n",
      "Epoch [17/100], Loss: 15.0943\n",
      "Epoch [18/100], Loss: 15.0637\n",
      "Epoch [19/100], Loss: 15.0001\n",
      "Epoch [20/100], Loss: 14.8108\n",
      "Epoch [21/100], Loss: 14.7738\n",
      "Epoch [22/100], Loss: 14.5938\n",
      "Epoch [23/100], Loss: 14.4896\n",
      "Epoch [24/100], Loss: 14.4009\n",
      "Epoch [25/100], Loss: 14.3562\n",
      "Epoch [26/100], Loss: 14.3883\n",
      "Epoch [27/100], Loss: 14.4467\n",
      "Epoch [28/100], Loss: 14.2249\n",
      "Epoch [29/100], Loss: 14.1994\n",
      "Epoch [30/100], Loss: 14.0161\n",
      "Epoch [31/100], Loss: 13.9255\n",
      "Epoch [32/100], Loss: 13.8594\n",
      "Epoch [33/100], Loss: 13.8539\n",
      "Epoch [34/100], Loss: 13.7900\n",
      "Epoch [35/100], Loss: 13.7441\n",
      "Epoch [36/100], Loss: 13.8687\n",
      "Epoch [37/100], Loss: 13.7082\n",
      "Epoch [38/100], Loss: 13.6007\n",
      "Epoch [39/100], Loss: 13.7608\n",
      "Epoch [40/100], Loss: 13.5828\n",
      "Epoch [41/100], Loss: 13.4703\n",
      "Epoch [42/100], Loss: 13.4823\n",
      "Epoch [43/100], Loss: 13.3959\n",
      "Epoch [44/100], Loss: 13.4484\n",
      "Epoch [45/100], Loss: 13.2834\n",
      "Epoch [46/100], Loss: 13.3060\n",
      "Epoch [47/100], Loss: 13.1002\n",
      "Epoch [48/100], Loss: 13.1051\n",
      "Epoch [49/100], Loss: 13.0906\n",
      "Epoch [50/100], Loss: 13.1251\n",
      "Epoch [51/100], Loss: 13.0585\n",
      "Epoch [52/100], Loss: 12.9194\n",
      "Epoch [53/100], Loss: 12.8705\n",
      "Epoch [54/100], Loss: 13.0454\n",
      "Epoch [55/100], Loss: 13.0810\n",
      "Epoch [56/100], Loss: 12.8709\n",
      "Epoch [57/100], Loss: 12.8426\n",
      "Epoch [58/100], Loss: 12.8494\n",
      "Epoch [59/100], Loss: 12.6686\n",
      "Epoch [60/100], Loss: 12.6691\n",
      "Epoch [61/100], Loss: 12.7027\n",
      "Epoch [62/100], Loss: 12.8174\n",
      "Stopping early at epoch 62 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 65.20%\n",
      "Epoch [1/100], Loss: 17.8023\n",
      "Epoch [2/100], Loss: 10.2566\n",
      "Epoch [3/100], Loss: 9.0319\n",
      "Epoch [4/100], Loss: 8.4150\n",
      "Epoch [5/100], Loss: 8.0643\n",
      "Epoch [6/100], Loss: 7.8879\n",
      "Epoch [7/100], Loss: 7.5755\n",
      "Epoch [8/100], Loss: 7.3623\n",
      "Epoch [9/100], Loss: 7.0680\n",
      "Epoch [10/100], Loss: 6.9351\n",
      "Epoch [11/100], Loss: 6.9512\n",
      "Epoch [12/100], Loss: 6.8182\n",
      "Epoch [13/100], Loss: 6.7616\n",
      "Epoch [14/100], Loss: 6.6275\n",
      "Epoch [15/100], Loss: 6.6037\n",
      "Epoch [16/100], Loss: 6.5506\n",
      "Epoch [17/100], Loss: 6.5004\n",
      "Epoch [18/100], Loss: 6.5729\n",
      "Epoch [19/100], Loss: 6.3034\n",
      "Epoch [20/100], Loss: 6.0198\n",
      "Epoch [21/100], Loss: 6.1392\n",
      "Epoch [22/100], Loss: 5.9880\n",
      "Epoch [23/100], Loss: 5.8978\n",
      "Epoch [24/100], Loss: 5.9818\n",
      "Epoch [25/100], Loss: 5.7413\n",
      "Epoch [26/100], Loss: 5.8904\n",
      "Epoch [27/100], Loss: 5.9573\n",
      "Epoch [28/100], Loss: 5.8030\n",
      "Epoch [29/100], Loss: 5.8547\n",
      "Epoch [30/100], Loss: 5.5166\n",
      "Epoch [31/100], Loss: 5.5470\n",
      "Epoch [32/100], Loss: 5.6194\n",
      "Epoch [33/100], Loss: 5.5425\n",
      "Epoch [34/100], Loss: 5.4652\n",
      "Epoch [35/100], Loss: 5.3477\n",
      "Epoch [36/100], Loss: 5.2538\n",
      "Epoch [37/100], Loss: 5.2199\n",
      "Epoch [38/100], Loss: 5.2649\n",
      "Epoch [39/100], Loss: 5.2093\n",
      "Epoch [40/100], Loss: 5.2406\n",
      "Epoch [41/100], Loss: 5.3391\n",
      "Epoch [42/100], Loss: 5.2608\n",
      "Epoch [43/100], Loss: 5.0957\n",
      "Epoch [44/100], Loss: 5.1053\n",
      "Epoch [45/100], Loss: 5.2286\n",
      "Epoch [46/100], Loss: 5.0763\n",
      "Epoch [47/100], Loss: 4.9739\n",
      "Epoch [48/100], Loss: 4.9965\n",
      "Epoch [49/100], Loss: 5.0230\n",
      "Epoch [50/100], Loss: 5.0926\n",
      "Stopping early at epoch 50 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 64.96%\n",
      "Epoch [1/100], Loss: 5.8706\n",
      "Epoch [2/100], Loss: 3.5665\n",
      "Epoch [3/100], Loss: 2.3533\n",
      "Epoch [4/100], Loss: 1.8903\n",
      "Epoch [5/100], Loss: 1.6331\n",
      "Epoch [6/100], Loss: 1.4730\n",
      "Epoch [7/100], Loss: 1.3507\n",
      "Epoch [8/100], Loss: 1.2657\n",
      "Epoch [9/100], Loss: 1.1959\n",
      "Epoch [10/100], Loss: 1.1393\n",
      "Epoch [11/100], Loss: 1.0945\n",
      "Epoch [12/100], Loss: 1.0490\n",
      "Epoch [13/100], Loss: 0.9977\n",
      "Epoch [14/100], Loss: 0.9687\n",
      "Epoch [15/100], Loss: 0.9301\n",
      "Epoch [16/100], Loss: 0.8998\n",
      "Epoch [17/100], Loss: 0.8754\n",
      "Epoch [18/100], Loss: 0.8485\n",
      "Epoch [19/100], Loss: 0.8273\n",
      "Epoch [20/100], Loss: 0.8107\n",
      "Epoch [21/100], Loss: 0.7905\n",
      "Epoch [22/100], Loss: 0.7786\n",
      "Epoch [23/100], Loss: 0.7496\n",
      "Epoch [24/100], Loss: 0.7386\n",
      "Epoch [25/100], Loss: 0.7199\n",
      "Epoch [26/100], Loss: 0.7056\n",
      "Epoch [27/100], Loss: 0.6913\n",
      "Epoch [28/100], Loss: 0.6850\n",
      "Epoch [29/100], Loss: 0.6664\n",
      "Epoch [30/100], Loss: 0.6575\n",
      "Epoch [31/100], Loss: 0.6438\n",
      "Epoch [32/100], Loss: 0.6322\n",
      "Epoch [33/100], Loss: 0.6197\n",
      "Epoch [34/100], Loss: 0.6167\n",
      "Epoch [35/100], Loss: 0.6024\n",
      "Epoch [36/100], Loss: 0.5947\n",
      "Epoch [37/100], Loss: 0.5852\n",
      "Epoch [38/100], Loss: 0.5820\n",
      "Epoch [39/100], Loss: 0.5690\n",
      "Epoch [40/100], Loss: 0.5607\n",
      "Epoch [41/100], Loss: 0.5545\n",
      "Epoch [42/100], Loss: 0.5414\n",
      "Epoch [43/100], Loss: 0.5364\n",
      "Epoch [44/100], Loss: 0.5282\n",
      "Epoch [45/100], Loss: 0.5271\n",
      "Epoch [46/100], Loss: 0.5136\n",
      "Epoch [47/100], Loss: 0.5071\n",
      "Epoch [48/100], Loss: 0.5036\n",
      "Epoch [49/100], Loss: 0.4987\n",
      "Epoch [50/100], Loss: 0.4886\n",
      "Epoch [51/100], Loss: 0.4827\n",
      "Epoch [52/100], Loss: 0.4797\n",
      "Epoch [53/100], Loss: 0.4776\n",
      "Epoch [54/100], Loss: 0.4705\n",
      "Epoch [55/100], Loss: 0.4634\n",
      "Epoch [56/100], Loss: 0.4622\n",
      "Epoch [57/100], Loss: 0.4539\n",
      "Epoch [58/100], Loss: 0.4481\n",
      "Epoch [59/100], Loss: 0.4503\n",
      "Epoch [60/100], Loss: 0.4388\n",
      "Epoch [61/100], Loss: 0.4327\n",
      "Epoch [62/100], Loss: 0.4286\n",
      "Epoch [63/100], Loss: 0.4252\n",
      "Epoch [64/100], Loss: 0.4204\n",
      "Epoch [65/100], Loss: 0.4143\n",
      "Epoch [66/100], Loss: 0.4139\n",
      "Epoch [67/100], Loss: 0.4087\n",
      "Epoch [68/100], Loss: 0.4029\n",
      "Epoch [69/100], Loss: 0.4034\n",
      "Epoch [70/100], Loss: 0.3977\n",
      "Epoch [71/100], Loss: 0.3970\n",
      "Epoch [72/100], Loss: 0.3909\n",
      "Epoch [73/100], Loss: 0.3902\n",
      "Epoch [74/100], Loss: 0.3816\n",
      "Epoch [75/100], Loss: 0.3795\n",
      "Epoch [76/100], Loss: 0.3742\n",
      "Epoch [77/100], Loss: 0.3756\n",
      "Epoch [78/100], Loss: 0.3733\n",
      "Epoch [79/100], Loss: 0.3662\n",
      "Epoch [80/100], Loss: 0.3670\n",
      "Epoch [81/100], Loss: 0.3625\n",
      "Epoch [82/100], Loss: 0.3590\n",
      "Epoch [83/100], Loss: 0.3559\n",
      "Epoch [84/100], Loss: 0.3523\n",
      "Epoch [85/100], Loss: 0.3486\n",
      "Epoch [86/100], Loss: 0.3497\n",
      "Epoch [87/100], Loss: 0.3428\n",
      "Epoch [88/100], Loss: 0.3411\n",
      "Epoch [89/100], Loss: 0.3426\n",
      "Epoch [90/100], Loss: 0.3371\n",
      "Epoch [91/100], Loss: 0.3334\n",
      "Epoch [92/100], Loss: 0.3287\n",
      "Epoch [93/100], Loss: 0.3260\n",
      "Epoch [94/100], Loss: 0.3274\n",
      "Epoch [95/100], Loss: 0.3262\n",
      "Epoch [96/100], Loss: 0.3244\n",
      "Epoch [97/100], Loss: 0.3206\n",
      "Epoch [98/100], Loss: 0.3182\n",
      "Epoch [99/100], Loss: 0.3163\n",
      "Epoch [100/100], Loss: 0.3112\n",
      "Test Accuracy Base Logit K Means: 63.06%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "base_logit_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    logit_model = LogisticRegression(input_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(logit_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "    previous_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logit_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = logit_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if previous_loss - current_loss < tolerance:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                break\n",
    "        else:\n",
    "            epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "        previous_loss = current_loss\n",
    "\n",
    "    logit_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            outputs = logit_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Base Logit K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    base_logit_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.461538</td>\n",
       "      <td>67.889423</td>\n",
       "      <td>65.197115</td>\n",
       "      <td>64.961538</td>\n",
       "      <td>63.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000     1000 \n",
       "0  69.461538  67.889423  65.197115  64.961538  63.0625"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_logit_kmeans_df = pd.DataFrame(base_logit_kmeans)\n",
    "base_logit_kmeans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 174.7797\n",
      "Epoch [2/100], Loss: 158.9948\n",
      "Epoch [3/100], Loss: 156.4363\n",
      "Epoch [4/100], Loss: 155.3280\n",
      "Epoch [5/100], Loss: 154.2844\n",
      "Epoch [6/100], Loss: 153.4449\n",
      "Epoch [7/100], Loss: 152.8901\n",
      "Epoch [8/100], Loss: 152.3302\n",
      "Epoch [9/100], Loss: 152.0780\n",
      "Epoch [10/100], Loss: 151.5151\n",
      "Epoch [11/100], Loss: 151.4044\n",
      "Epoch [12/100], Loss: 151.1122\n",
      "Epoch [13/100], Loss: 150.9331\n",
      "Epoch [14/100], Loss: 150.2972\n",
      "Epoch [15/100], Loss: 150.3928\n",
      "Epoch [16/100], Loss: 150.3375\n",
      "Epoch [17/100], Loss: 150.0230\n",
      "Epoch [18/100], Loss: 149.4260\n",
      "Epoch [19/100], Loss: 149.8679\n",
      "Epoch [20/100], Loss: 149.2008\n",
      "Epoch [21/100], Loss: 149.7504\n",
      "Epoch [22/100], Loss: 149.3839\n",
      "Epoch [23/100], Loss: 149.1636\n",
      "Epoch [24/100], Loss: 149.1011\n",
      "Epoch [25/100], Loss: 149.1429\n",
      "Epoch [26/100], Loss: 148.6327\n",
      "Epoch [27/100], Loss: 148.9050\n",
      "Epoch [28/100], Loss: 148.3092\n",
      "Epoch [29/100], Loss: 148.3335\n",
      "Epoch [30/100], Loss: 148.4233\n",
      "Epoch [31/100], Loss: 148.7612\n",
      "Stopping early at epoch 31 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 69.41%\n",
      "Epoch [1/100], Loss: 120.9242\n",
      "Epoch [2/100], Loss: 106.2355\n",
      "Epoch [3/100], Loss: 104.4279\n",
      "Epoch [4/100], Loss: 102.9723\n",
      "Epoch [5/100], Loss: 102.3947\n",
      "Epoch [6/100], Loss: 101.7374\n",
      "Epoch [7/100], Loss: 100.8850\n",
      "Epoch [8/100], Loss: 100.6141\n",
      "Epoch [9/100], Loss: 100.3017\n",
      "Epoch [10/100], Loss: 100.2366\n",
      "Epoch [11/100], Loss: 99.5111\n",
      "Epoch [12/100], Loss: 99.2689\n",
      "Epoch [13/100], Loss: 98.7988\n",
      "Epoch [14/100], Loss: 98.8263\n",
      "Epoch [15/100], Loss: 98.5736\n",
      "Epoch [16/100], Loss: 98.8779\n",
      "Epoch [17/100], Loss: 98.0306\n",
      "Epoch [18/100], Loss: 97.9374\n",
      "Epoch [19/100], Loss: 97.8743\n",
      "Epoch [20/100], Loss: 97.7601\n",
      "Epoch [21/100], Loss: 97.4635\n",
      "Epoch [22/100], Loss: 97.2938\n",
      "Epoch [23/100], Loss: 97.4197\n",
      "Epoch [24/100], Loss: 97.1681\n",
      "Epoch [25/100], Loss: 97.3970\n",
      "Epoch [26/100], Loss: 97.0500\n",
      "Epoch [27/100], Loss: 96.8637\n",
      "Epoch [28/100], Loss: 96.8633\n",
      "Epoch [29/100], Loss: 96.6718\n",
      "Epoch [30/100], Loss: 96.4185\n",
      "Epoch [31/100], Loss: 96.2934\n",
      "Epoch [32/100], Loss: 96.3742\n",
      "Epoch [33/100], Loss: 96.4072\n",
      "Epoch [34/100], Loss: 96.1355\n",
      "Epoch [35/100], Loss: 95.9351\n",
      "Epoch [36/100], Loss: 96.0182\n",
      "Epoch [37/100], Loss: 95.8465\n",
      "Epoch [38/100], Loss: 96.2839\n",
      "Epoch [39/100], Loss: 95.9689\n",
      "Epoch [40/100], Loss: 95.9976\n",
      "Epoch [41/100], Loss: 95.7066\n",
      "Epoch [42/100], Loss: 95.4776\n",
      "Epoch [43/100], Loss: 95.3630\n",
      "Epoch [44/100], Loss: 95.2750\n",
      "Epoch [45/100], Loss: 95.6250\n",
      "Epoch [46/100], Loss: 95.4836\n",
      "Epoch [47/100], Loss: 95.1569\n",
      "Epoch [48/100], Loss: 95.2703\n",
      "Epoch [49/100], Loss: 95.1601\n",
      "Epoch [50/100], Loss: 95.1672\n",
      "Epoch [51/100], Loss: 95.0050\n",
      "Epoch [52/100], Loss: 95.1294\n",
      "Epoch [53/100], Loss: 95.0838\n",
      "Epoch [54/100], Loss: 94.9724\n",
      "Epoch [55/100], Loss: 95.1881\n",
      "Epoch [56/100], Loss: 94.6699\n",
      "Epoch [57/100], Loss: 95.2084\n",
      "Epoch [58/100], Loss: 94.8855\n",
      "Epoch [59/100], Loss: 94.8306\n",
      "Epoch [60/100], Loss: 94.7683\n",
      "Epoch [61/100], Loss: 94.8180\n",
      "Epoch [62/100], Loss: 94.5767\n",
      "Epoch [63/100], Loss: 94.7403\n",
      "Epoch [64/100], Loss: 94.2910\n",
      "Epoch [65/100], Loss: 94.5698\n",
      "Epoch [66/100], Loss: 94.4308\n",
      "Epoch [67/100], Loss: 94.2636\n",
      "Epoch [68/100], Loss: 94.2237\n",
      "Epoch [69/100], Loss: 94.4411\n",
      "Epoch [70/100], Loss: 94.5186\n",
      "Epoch [71/100], Loss: 94.2757\n",
      "Epoch [72/100], Loss: 94.0821\n",
      "Epoch [73/100], Loss: 94.5829\n",
      "Epoch [74/100], Loss: 94.3947\n",
      "Epoch [75/100], Loss: 93.7149\n",
      "Epoch [76/100], Loss: 94.2138\n",
      "Epoch [77/100], Loss: 93.7268\n",
      "Epoch [78/100], Loss: 93.8639\n",
      "Epoch [79/100], Loss: 93.9922\n",
      "Epoch [80/100], Loss: 94.1043\n",
      "Stopping early at epoch 80 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 68.55%\n",
      "Epoch [1/100], Loss: 29.7088\n",
      "Epoch [2/100], Loss: 20.3822\n",
      "Epoch [3/100], Loss: 19.2759\n",
      "Epoch [4/100], Loss: 18.3297\n",
      "Epoch [5/100], Loss: 17.7561\n",
      "Epoch [6/100], Loss: 17.2910\n",
      "Epoch [7/100], Loss: 16.9710\n",
      "Epoch [8/100], Loss: 16.5439\n",
      "Epoch [9/100], Loss: 16.2671\n",
      "Epoch [10/100], Loss: 16.2236\n",
      "Epoch [11/100], Loss: 16.0861\n",
      "Epoch [12/100], Loss: 15.9566\n",
      "Epoch [13/100], Loss: 15.6121\n",
      "Epoch [14/100], Loss: 15.5416\n",
      "Epoch [15/100], Loss: 15.4860\n",
      "Epoch [16/100], Loss: 15.5370\n",
      "Epoch [17/100], Loss: 15.2141\n",
      "Epoch [18/100], Loss: 15.0947\n",
      "Epoch [19/100], Loss: 15.0145\n",
      "Epoch [20/100], Loss: 14.8687\n",
      "Epoch [21/100], Loss: 14.8067\n",
      "Epoch [22/100], Loss: 14.7883\n",
      "Epoch [23/100], Loss: 14.6593\n",
      "Epoch [24/100], Loss: 14.6173\n",
      "Epoch [25/100], Loss: 14.3860\n",
      "Epoch [26/100], Loss: 14.2519\n",
      "Epoch [27/100], Loss: 14.2191\n",
      "Epoch [28/100], Loss: 14.2961\n",
      "Epoch [29/100], Loss: 14.1807\n",
      "Epoch [30/100], Loss: 14.0992\n",
      "Epoch [31/100], Loss: 14.0277\n",
      "Epoch [32/100], Loss: 14.0625\n",
      "Epoch [33/100], Loss: 14.0814\n",
      "Epoch [34/100], Loss: 13.9281\n",
      "Epoch [35/100], Loss: 13.6698\n",
      "Epoch [36/100], Loss: 13.6125\n",
      "Epoch [37/100], Loss: 13.6950\n",
      "Epoch [38/100], Loss: 13.5739\n",
      "Epoch [39/100], Loss: 13.5530\n",
      "Epoch [40/100], Loss: 13.5972\n",
      "Epoch [41/100], Loss: 13.4718\n",
      "Epoch [42/100], Loss: 13.2820\n",
      "Epoch [43/100], Loss: 13.4044\n",
      "Epoch [44/100], Loss: 13.3259\n",
      "Epoch [45/100], Loss: 13.4228\n",
      "Epoch [46/100], Loss: 13.2145\n",
      "Epoch [47/100], Loss: 13.1019\n",
      "Epoch [48/100], Loss: 13.1362\n",
      "Epoch [49/100], Loss: 13.1498\n",
      "Epoch [50/100], Loss: 13.3211\n",
      "Stopping early at epoch 50 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 65.92%\n",
      "Epoch [1/100], Loss: 17.7899\n",
      "Epoch [2/100], Loss: 10.2299\n",
      "Epoch [3/100], Loss: 9.1737\n",
      "Epoch [4/100], Loss: 8.5578\n",
      "Epoch [5/100], Loss: 8.0596\n",
      "Epoch [6/100], Loss: 7.7611\n",
      "Epoch [7/100], Loss: 7.4068\n",
      "Epoch [8/100], Loss: 7.4857\n",
      "Epoch [9/100], Loss: 7.1122\n",
      "Epoch [10/100], Loss: 6.8915\n",
      "Epoch [11/100], Loss: 6.8507\n",
      "Epoch [12/100], Loss: 6.7225\n",
      "Epoch [13/100], Loss: 6.7583\n",
      "Epoch [14/100], Loss: 6.5604\n",
      "Epoch [15/100], Loss: 6.5745\n",
      "Epoch [16/100], Loss: 6.3471\n",
      "Epoch [17/100], Loss: 6.3090\n",
      "Epoch [18/100], Loss: 6.3066\n",
      "Epoch [19/100], Loss: 6.3097\n",
      "Epoch [20/100], Loss: 6.1035\n",
      "Epoch [21/100], Loss: 6.0533\n",
      "Epoch [22/100], Loss: 6.1723\n",
      "Epoch [23/100], Loss: 6.1760\n",
      "Epoch [24/100], Loss: 5.9566\n",
      "Epoch [25/100], Loss: 5.8514\n",
      "Epoch [26/100], Loss: 5.8690\n",
      "Epoch [27/100], Loss: 5.6536\n",
      "Epoch [28/100], Loss: 5.8039\n",
      "Epoch [29/100], Loss: 6.0965\n",
      "Epoch [30/100], Loss: 5.6405\n",
      "Epoch [31/100], Loss: 5.4986\n",
      "Epoch [32/100], Loss: 5.4972\n",
      "Epoch [33/100], Loss: 5.5038\n",
      "Epoch [34/100], Loss: 5.4047\n",
      "Epoch [35/100], Loss: 5.3679\n",
      "Epoch [36/100], Loss: 5.6453\n",
      "Epoch [37/100], Loss: 5.5095\n",
      "Epoch [38/100], Loss: 5.4495\n",
      "Epoch [39/100], Loss: 5.2357\n",
      "Epoch [40/100], Loss: 5.2275\n",
      "Epoch [41/100], Loss: 5.2317\n",
      "Epoch [42/100], Loss: 5.1360\n",
      "Epoch [43/100], Loss: 5.3674\n",
      "Epoch [44/100], Loss: 5.4289\n",
      "Epoch [45/100], Loss: 5.2755\n",
      "Epoch [46/100], Loss: 5.1038\n",
      "Epoch [47/100], Loss: 4.9509\n",
      "Epoch [48/100], Loss: 4.9996\n",
      "Epoch [49/100], Loss: 4.9637\n",
      "Epoch [50/100], Loss: 4.9051\n",
      "Epoch [51/100], Loss: 4.9365\n",
      "Epoch [52/100], Loss: 4.9449\n",
      "Epoch [53/100], Loss: 5.0772\n",
      "Stopping early at epoch 53 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 64.72%\n",
      "Epoch [1/100], Loss: 5.8950\n",
      "Epoch [2/100], Loss: 3.5436\n",
      "Epoch [3/100], Loss: 2.3627\n",
      "Epoch [4/100], Loss: 1.8847\n",
      "Epoch [5/100], Loss: 1.6468\n",
      "Epoch [6/100], Loss: 1.4703\n",
      "Epoch [7/100], Loss: 1.3425\n",
      "Epoch [8/100], Loss: 1.2547\n",
      "Epoch [9/100], Loss: 1.1883\n",
      "Epoch [10/100], Loss: 1.1261\n",
      "Epoch [11/100], Loss: 1.0817\n",
      "Epoch [12/100], Loss: 1.0382\n",
      "Epoch [13/100], Loss: 0.9974\n",
      "Epoch [14/100], Loss: 0.9612\n",
      "Epoch [15/100], Loss: 0.9269\n",
      "Epoch [16/100], Loss: 0.9059\n",
      "Epoch [17/100], Loss: 0.8828\n",
      "Epoch [18/100], Loss: 0.8541\n",
      "Epoch [19/100], Loss: 0.8337\n",
      "Epoch [20/100], Loss: 0.8114\n",
      "Epoch [21/100], Loss: 0.7914\n",
      "Epoch [22/100], Loss: 0.7680\n",
      "Epoch [23/100], Loss: 0.7561\n",
      "Epoch [24/100], Loss: 0.7422\n",
      "Epoch [25/100], Loss: 0.7246\n",
      "Epoch [26/100], Loss: 0.7068\n",
      "Epoch [27/100], Loss: 0.7008\n",
      "Epoch [28/100], Loss: 0.6832\n",
      "Epoch [29/100], Loss: 0.6682\n",
      "Epoch [30/100], Loss: 0.6600\n",
      "Epoch [31/100], Loss: 0.6435\n",
      "Epoch [32/100], Loss: 0.6342\n",
      "Epoch [33/100], Loss: 0.6253\n",
      "Epoch [34/100], Loss: 0.6140\n",
      "Epoch [35/100], Loss: 0.6084\n",
      "Epoch [36/100], Loss: 0.5946\n",
      "Epoch [37/100], Loss: 0.5830\n",
      "Epoch [38/100], Loss: 0.5745\n",
      "Epoch [39/100], Loss: 0.5660\n",
      "Epoch [40/100], Loss: 0.5592\n",
      "Epoch [41/100], Loss: 0.5545\n",
      "Epoch [42/100], Loss: 0.5464\n",
      "Epoch [43/100], Loss: 0.5336\n",
      "Epoch [44/100], Loss: 0.5319\n",
      "Epoch [45/100], Loss: 0.5251\n",
      "Epoch [46/100], Loss: 0.5158\n",
      "Epoch [47/100], Loss: 0.5090\n",
      "Epoch [48/100], Loss: 0.5009\n",
      "Epoch [49/100], Loss: 0.5002\n",
      "Epoch [50/100], Loss: 0.4871\n",
      "Epoch [51/100], Loss: 0.4911\n",
      "Epoch [52/100], Loss: 0.4801\n",
      "Epoch [53/100], Loss: 0.4756\n",
      "Epoch [54/100], Loss: 0.4688\n",
      "Epoch [55/100], Loss: 0.4656\n",
      "Epoch [56/100], Loss: 0.4625\n",
      "Epoch [57/100], Loss: 0.4496\n",
      "Epoch [58/100], Loss: 0.4476\n",
      "Epoch [59/100], Loss: 0.4445\n",
      "Epoch [60/100], Loss: 0.4406\n",
      "Epoch [61/100], Loss: 0.4329\n",
      "Epoch [62/100], Loss: 0.4293\n",
      "Epoch [63/100], Loss: 0.4257\n",
      "Epoch [64/100], Loss: 0.4200\n",
      "Epoch [65/100], Loss: 0.4171\n",
      "Epoch [66/100], Loss: 0.4153\n",
      "Epoch [67/100], Loss: 0.4121\n",
      "Epoch [68/100], Loss: 0.4064\n",
      "Epoch [69/100], Loss: 0.4016\n",
      "Epoch [70/100], Loss: 0.3967\n",
      "Epoch [71/100], Loss: 0.3964\n",
      "Epoch [72/100], Loss: 0.3933\n",
      "Epoch [73/100], Loss: 0.3836\n",
      "Epoch [74/100], Loss: 0.3812\n",
      "Epoch [75/100], Loss: 0.3797\n",
      "Epoch [76/100], Loss: 0.3733\n",
      "Epoch [77/100], Loss: 0.3723\n",
      "Epoch [78/100], Loss: 0.3688\n",
      "Epoch [79/100], Loss: 0.3674\n",
      "Epoch [80/100], Loss: 0.3661\n",
      "Epoch [81/100], Loss: 0.3600\n",
      "Epoch [82/100], Loss: 0.3598\n",
      "Epoch [83/100], Loss: 0.3534\n",
      "Epoch [84/100], Loss: 0.3498\n",
      "Epoch [85/100], Loss: 0.3506\n",
      "Epoch [86/100], Loss: 0.3450\n",
      "Epoch [87/100], Loss: 0.3430\n",
      "Epoch [88/100], Loss: 0.3434\n",
      "Epoch [89/100], Loss: 0.3409\n",
      "Epoch [90/100], Loss: 0.3350\n",
      "Epoch [91/100], Loss: 0.3342\n",
      "Epoch [92/100], Loss: 0.3327\n",
      "Epoch [93/100], Loss: 0.3304\n",
      "Epoch [94/100], Loss: 0.3270\n",
      "Epoch [95/100], Loss: 0.3242\n",
      "Epoch [96/100], Loss: 0.3234\n",
      "Epoch [97/100], Loss: 0.3205\n",
      "Epoch [98/100], Loss: 0.3157\n",
      "Epoch [99/100], Loss: 0.3153\n",
      "Epoch [100/100], Loss: 0.3142\n",
      "Test Accuracy Lipschitz Logit K Means: 63.19%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "lipschitz_logit_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    logit_model_lipschitz = LogisticRegression(input_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(logit_model_lipschitz.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "    previous_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logit_model_lipschitz.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = logit_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            L = max(param.grad.abs().max().item() for param in logit_model_lipschitz.parameters() if param.grad is not None)\n",
    "            adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "            # Update optimizer's learning rate\n",
    "            for param_group in optimizer_sdg.param_groups:\n",
    "                param_group['lr'] = adaptive_lr\n",
    "\n",
    "\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if previous_loss - current_loss < tolerance:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                break\n",
    "        else:\n",
    "            epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "        previous_loss = current_loss\n",
    "\n",
    "    logit_model_lipschitz.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            outputs = logit_model_lipschitz(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Lipschitz Logit K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    lipschitz_logit_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.408654</td>\n",
       "      <td>68.548077</td>\n",
       "      <td>65.918269</td>\n",
       "      <td>64.716346</td>\n",
       "      <td>63.1875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000     1000 \n",
       "0  69.408654  68.548077  65.918269  64.716346  63.1875"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lipschitz_logit_kmeans_df = pd.DataFrame(lipschitz_logit_kmeans)\n",
    "lipschitz_logit_kmeans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 94.5194\n",
      "Epoch [1/50], Val Loss: 0.2846\n",
      "Epoch [2/50], Loss: 44.7997\n",
      "Epoch [2/50], Val Loss: 0.2217\n",
      "Epoch [3/50], Loss: 36.9589\n",
      "Epoch [3/50], Val Loss: 0.1807\n",
      "Epoch [4/50], Loss: 32.0297\n",
      "Epoch [4/50], Val Loss: 0.1678\n",
      "Epoch [5/50], Loss: 28.7951\n",
      "Epoch [5/50], Val Loss: 0.1569\n",
      "Epoch [6/50], Loss: 25.9148\n",
      "Epoch [6/50], Val Loss: 0.1677\n",
      "Epoch [7/50], Loss: 23.0563\n",
      "Epoch [7/50], Val Loss: 0.1464\n",
      "Epoch [8/50], Loss: 21.1001\n",
      "Epoch [8/50], Val Loss: 0.1412\n",
      "Epoch [9/50], Loss: 18.7614\n",
      "Epoch [9/50], Val Loss: 0.1422\n",
      "Epoch [10/50], Loss: 17.3641\n",
      "Epoch [10/50], Val Loss: 0.1389\n",
      "Epoch [11/50], Loss: 15.4320\n",
      "Epoch [11/50], Val Loss: 0.1325\n",
      "Epoch [12/50], Loss: 14.2311\n",
      "Epoch [12/50], Val Loss: 0.1361\n",
      "Epoch [13/50], Loss: 13.0377\n",
      "Epoch [13/50], Val Loss: 0.1347\n",
      "Epoch [14/50], Loss: 12.0162\n",
      "Epoch [14/50], Val Loss: 0.1372\n",
      "Stopping early at epoch 14 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 92.61%\n",
      "Epoch [1/50], Loss: 78.8384\n",
      "Epoch [1/50], Val Loss: 0.3505\n",
      "Epoch [2/50], Loss: 35.6237\n",
      "Epoch [2/50], Val Loss: 0.2643\n",
      "Epoch [3/50], Loss: 29.1912\n",
      "Epoch [3/50], Val Loss: 0.2298\n",
      "Epoch [4/50], Loss: 24.9857\n",
      "Epoch [4/50], Val Loss: 0.2113\n",
      "Epoch [5/50], Loss: 22.2271\n",
      "Epoch [5/50], Val Loss: 0.1987\n",
      "Epoch [6/50], Loss: 19.7962\n",
      "Epoch [6/50], Val Loss: 0.1962\n",
      "Epoch [7/50], Loss: 18.0930\n",
      "Epoch [7/50], Val Loss: 0.1918\n",
      "Epoch [8/50], Loss: 16.2062\n",
      "Epoch [8/50], Val Loss: 0.1915\n",
      "Epoch [9/50], Loss: 14.4370\n",
      "Epoch [9/50], Val Loss: 0.1884\n",
      "Epoch [10/50], Loss: 13.1155\n",
      "Epoch [10/50], Val Loss: 0.2031\n",
      "Epoch [11/50], Loss: 11.8399\n",
      "Epoch [11/50], Val Loss: 0.1967\n",
      "Epoch [12/50], Loss: 10.8224\n",
      "Epoch [12/50], Val Loss: 0.1954\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 92.01%\n",
      "Epoch [1/50], Loss: 34.8576\n",
      "Epoch [1/50], Val Loss: 0.9705\n",
      "Epoch [2/50], Loss: 13.5302\n",
      "Epoch [2/50], Val Loss: 0.7309\n",
      "Epoch [3/50], Loss: 9.0522\n",
      "Epoch [3/50], Val Loss: 0.5429\n",
      "Epoch [4/50], Loss: 6.9517\n",
      "Epoch [4/50], Val Loss: 0.4767\n",
      "Epoch [5/50], Loss: 5.5631\n",
      "Epoch [5/50], Val Loss: 0.4429\n",
      "Epoch [6/50], Loss: 4.8080\n",
      "Epoch [6/50], Val Loss: 0.4965\n",
      "Epoch [7/50], Loss: 4.1766\n",
      "Epoch [7/50], Val Loss: 0.4482\n",
      "Epoch [8/50], Loss: 3.3512\n",
      "Epoch [8/50], Val Loss: 0.4881\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 86.12%\n",
      "Epoch [1/50], Loss: 22.8670\n",
      "Epoch [1/50], Val Loss: 1.7310\n",
      "Epoch [2/50], Loss: 10.4631\n",
      "Epoch [2/50], Val Loss: 1.1706\n",
      "Epoch [3/50], Loss: 6.8546\n",
      "Epoch [3/50], Val Loss: 0.8604\n",
      "Epoch [4/50], Loss: 5.3420\n",
      "Epoch [4/50], Val Loss: 0.7471\n",
      "Epoch [5/50], Loss: 4.0402\n",
      "Epoch [5/50], Val Loss: 0.7278\n",
      "Epoch [6/50], Loss: 3.2088\n",
      "Epoch [6/50], Val Loss: 0.6474\n",
      "Epoch [7/50], Loss: 2.4712\n",
      "Epoch [7/50], Val Loss: 0.7096\n",
      "Epoch [8/50], Loss: 2.2840\n",
      "Epoch [8/50], Val Loss: 0.6558\n",
      "Epoch [9/50], Loss: 1.6999\n",
      "Epoch [9/50], Val Loss: 0.6444\n",
      "Epoch [10/50], Loss: 1.2476\n",
      "Epoch [10/50], Val Loss: 0.7589\n",
      "Epoch [11/50], Loss: 1.5028\n",
      "Epoch [11/50], Val Loss: 0.7283\n",
      "Epoch [12/50], Loss: 1.7875\n",
      "Epoch [12/50], Val Loss: 0.6422\n",
      "Epoch [13/50], Loss: 0.9213\n",
      "Epoch [13/50], Val Loss: 0.6809\n",
      "Epoch [14/50], Loss: 0.7393\n",
      "Epoch [14/50], Val Loss: 0.6699\n",
      "Epoch [15/50], Loss: 0.6189\n",
      "Epoch [15/50], Val Loss: 0.7913\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 83.13%\n",
      "Epoch [1/50], Loss: 6.5613\n",
      "Epoch [1/50], Val Loss: 3.1654\n",
      "Epoch [2/50], Loss: 6.0576\n",
      "Epoch [2/50], Val Loss: 2.8341\n",
      "Epoch [3/50], Loss: 4.9463\n",
      "Epoch [3/50], Val Loss: 1.9671\n",
      "Epoch [4/50], Loss: 3.0551\n",
      "Epoch [4/50], Val Loss: 1.8394\n",
      "Epoch [5/50], Loss: 2.4933\n",
      "Epoch [5/50], Val Loss: 2.0541\n",
      "Epoch [6/50], Loss: 2.1651\n",
      "Epoch [6/50], Val Loss: 1.7257\n",
      "Epoch [7/50], Loss: 1.8843\n",
      "Epoch [7/50], Val Loss: 1.4265\n",
      "Epoch [8/50], Loss: 1.3555\n",
      "Epoch [8/50], Val Loss: 1.3664\n",
      "Epoch [9/50], Loss: 0.9983\n",
      "Epoch [9/50], Val Loss: 1.3859\n",
      "Epoch [10/50], Loss: 0.7834\n",
      "Epoch [10/50], Val Loss: 1.4936\n",
      "Epoch [11/50], Loss: 0.7114\n",
      "Epoch [11/50], Val Loss: 1.5427\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 66.23%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "cnn_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    cnn_model = EMNIST_CNN(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(cnn_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "    def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                m.bias.data.fill_(0.01)\n",
    "    cnn_model.apply(init_weights)\n",
    "\n",
    "    best_val_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = cnn_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        cnn_model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader_cnn:\n",
    "                images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "                outputs = cnn_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_cnn)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    cnn_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "            outputs = cnn_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy CNN K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    cnn_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.610577</td>\n",
       "      <td>92.009615</td>\n",
       "      <td>86.120192</td>\n",
       "      <td>83.129808</td>\n",
       "      <td>66.225962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000       1000 \n",
       "0  92.610577  92.009615  86.120192  83.129808  66.225962"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_kmeans_df = pd.DataFrame(cnn_kmeans)\n",
    "cnn_kmeans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Lipschitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 99.0845\n",
      "Epoch [1/50], Val Loss: 0.2954\n",
      "Epoch [2/50], Loss: 45.3697\n",
      "Epoch [2/50], Val Loss: 0.2222\n",
      "Epoch [3/50], Loss: 37.5560\n",
      "Epoch [3/50], Val Loss: 0.1874\n",
      "Epoch [4/50], Loss: 32.9338\n",
      "Epoch [4/50], Val Loss: 0.1738\n",
      "Epoch [5/50], Loss: 29.2415\n",
      "Epoch [5/50], Val Loss: 0.1615\n",
      "Epoch [6/50], Loss: 26.3568\n",
      "Epoch [6/50], Val Loss: 0.1512\n",
      "Epoch [7/50], Loss: 24.1001\n",
      "Epoch [7/50], Val Loss: 0.1534\n",
      "Epoch [8/50], Loss: 21.8716\n",
      "Epoch [8/50], Val Loss: 0.1401\n",
      "Epoch [9/50], Loss: 20.1402\n",
      "Epoch [9/50], Val Loss: 0.1351\n",
      "Epoch [10/50], Loss: 18.3980\n",
      "Epoch [10/50], Val Loss: 0.1399\n",
      "Epoch [11/50], Loss: 16.6092\n",
      "Epoch [11/50], Val Loss: 0.1396\n",
      "Epoch [12/50], Loss: 15.2523\n",
      "Epoch [12/50], Val Loss: 0.1343\n",
      "Epoch [13/50], Loss: 13.6351\n",
      "Epoch [13/50], Val Loss: 0.1301\n",
      "Epoch [14/50], Loss: 12.9812\n",
      "Epoch [14/50], Val Loss: 0.1333\n",
      "Epoch [15/50], Loss: 11.8987\n",
      "Epoch [15/50], Val Loss: 0.1358\n",
      "Epoch [16/50], Loss: 10.9193\n",
      "Epoch [16/50], Val Loss: 0.1405\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 92.48%\n",
      "Epoch [1/50], Loss: 83.4219\n",
      "Epoch [1/50], Val Loss: 0.3732\n",
      "Epoch [2/50], Loss: 36.5396\n",
      "Epoch [2/50], Val Loss: 0.2834\n",
      "Epoch [3/50], Loss: 28.9895\n",
      "Epoch [3/50], Val Loss: 0.2489\n",
      "Epoch [4/50], Loss: 25.2347\n",
      "Epoch [4/50], Val Loss: 0.2105\n",
      "Epoch [5/50], Loss: 22.4048\n",
      "Epoch [5/50], Val Loss: 0.2209\n",
      "Epoch [6/50], Loss: 20.1296\n",
      "Epoch [6/50], Val Loss: 0.1959\n",
      "Epoch [7/50], Loss: 18.0182\n",
      "Epoch [7/50], Val Loss: 0.1929\n",
      "Epoch [8/50], Loss: 16.5464\n",
      "Epoch [8/50], Val Loss: 0.1974\n",
      "Epoch [9/50], Loss: 14.3980\n",
      "Epoch [9/50], Val Loss: 0.1942\n",
      "Epoch [10/50], Loss: 13.0829\n",
      "Epoch [10/50], Val Loss: 0.1899\n",
      "Epoch [11/50], Loss: 12.0602\n",
      "Epoch [11/50], Val Loss: 0.2065\n",
      "Epoch [12/50], Loss: 10.4938\n",
      "Epoch [12/50], Val Loss: 0.1984\n",
      "Epoch [13/50], Loss: 9.9832\n",
      "Epoch [13/50], Val Loss: 0.2187\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 91.23%\n",
      "Epoch [1/50], Loss: 36.0117\n",
      "Epoch [1/50], Val Loss: 1.0175\n",
      "Epoch [2/50], Loss: 14.5665\n",
      "Epoch [2/50], Val Loss: 0.6806\n",
      "Epoch [3/50], Loss: 9.8354\n",
      "Epoch [3/50], Val Loss: 0.5532\n",
      "Epoch [4/50], Loss: 7.2486\n",
      "Epoch [4/50], Val Loss: 0.4739\n",
      "Epoch [5/50], Loss: 5.6743\n",
      "Epoch [5/50], Val Loss: 0.4197\n",
      "Epoch [6/50], Loss: 4.4558\n",
      "Epoch [6/50], Val Loss: 0.4304\n",
      "Epoch [7/50], Loss: 3.8596\n",
      "Epoch [7/50], Val Loss: 0.3961\n",
      "Epoch [8/50], Loss: 3.3227\n",
      "Epoch [8/50], Val Loss: 0.4375\n",
      "Epoch [9/50], Loss: 2.8865\n",
      "Epoch [9/50], Val Loss: 0.4311\n",
      "Epoch [10/50], Loss: 2.6032\n",
      "Epoch [10/50], Val Loss: 0.4898\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 86.19%\n",
      "Epoch [1/50], Loss: 23.5910\n",
      "Epoch [1/50], Val Loss: 1.6075\n",
      "Epoch [2/50], Loss: 10.7505\n",
      "Epoch [2/50], Val Loss: 1.1170\n",
      "Epoch [3/50], Loss: 7.0084\n",
      "Epoch [3/50], Val Loss: 0.9519\n",
      "Epoch [4/50], Loss: 5.5982\n",
      "Epoch [4/50], Val Loss: 0.7515\n",
      "Epoch [5/50], Loss: 4.3762\n",
      "Epoch [5/50], Val Loss: 0.7059\n",
      "Epoch [6/50], Loss: 3.4438\n",
      "Epoch [6/50], Val Loss: 0.6523\n",
      "Epoch [7/50], Loss: 2.7197\n",
      "Epoch [7/50], Val Loss: 0.7467\n",
      "Epoch [8/50], Loss: 2.6797\n",
      "Epoch [8/50], Val Loss: 0.6930\n",
      "Epoch [9/50], Loss: 2.0006\n",
      "Epoch [9/50], Val Loss: 0.6375\n",
      "Epoch [10/50], Loss: 1.9482\n",
      "Epoch [10/50], Val Loss: 0.7452\n",
      "Epoch [11/50], Loss: 2.0218\n",
      "Epoch [11/50], Val Loss: 0.6863\n",
      "Epoch [12/50], Loss: 1.2104\n",
      "Epoch [12/50], Val Loss: 0.7405\n",
      "Stopping early at epoch 12 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 82.30%\n",
      "Epoch [1/50], Loss: 6.5562\n",
      "Epoch [1/50], Val Loss: 3.1466\n",
      "Epoch [2/50], Loss: 6.0220\n",
      "Epoch [2/50], Val Loss: 2.7708\n",
      "Epoch [3/50], Loss: 4.7302\n",
      "Epoch [3/50], Val Loss: 1.9356\n",
      "Epoch [4/50], Loss: 3.0510\n",
      "Epoch [4/50], Val Loss: 2.3672\n",
      "Epoch [5/50], Loss: 3.4902\n",
      "Epoch [5/50], Val Loss: 2.3603\n",
      "Epoch [6/50], Loss: 2.7312\n",
      "Epoch [6/50], Val Loss: 1.5064\n",
      "Epoch [7/50], Loss: 1.8762\n",
      "Epoch [7/50], Val Loss: 1.3192\n",
      "Epoch [8/50], Loss: 1.2337\n",
      "Epoch [8/50], Val Loss: 1.7087\n",
      "Epoch [9/50], Loss: 0.9843\n",
      "Epoch [9/50], Val Loss: 1.5345\n",
      "Epoch [10/50], Loss: 0.8076\n",
      "Epoch [10/50], Val Loss: 1.5200\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 65.16%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "cnn_lipschitz_kmeans = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    cnn_model_lipschitz = EMNIST_CNN(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(cnn_model_lipschitz.parameters(), lr=0.005, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "    def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                m.bias.data.fill_(0.01)\n",
    "    cnn_model_lipschitz.apply(init_weights)\n",
    "\n",
    "    best_val_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model_lipschitz.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            L = max(param.grad.abs().max().item() for param in cnn_model_lipschitz.parameters() if param.grad is not None)\n",
    "            adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "            # Update optimizer's learning rate\n",
    "            for param_group in optimizer_sdg.param_groups:\n",
    "                param_group['lr'] = adaptive_lr\n",
    "                \n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        cnn_model_lipschitz.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader_cnn:\n",
    "                images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "                outputs = cnn_model_lipschitz(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_cnn)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    cnn_model_lipschitz.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Lipschitz CNN K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    cnn_lipschitz_kmeans[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.480769</td>\n",
       "      <td>91.230769</td>\n",
       "      <td>86.192308</td>\n",
       "      <td>82.302885</td>\n",
       "      <td>65.163462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000       1000 \n",
       "0  92.480769  91.230769  86.192308  82.302885  65.163462"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_lipschitz_kmeans_df = pd.DataFrame(cnn_lipschitz_kmeans)\n",
    "cnn_lipschitz_kmeans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 174.9038\n",
      "Epoch [2/100], Loss: 158.5837\n",
      "Epoch [3/100], Loss: 156.6188\n",
      "Epoch [4/100], Loss: 155.2408\n",
      "Epoch [5/100], Loss: 154.1766\n",
      "Epoch [6/100], Loss: 153.3897\n",
      "Epoch [7/100], Loss: 152.9343\n",
      "Epoch [8/100], Loss: 152.2684\n",
      "Epoch [9/100], Loss: 151.7720\n",
      "Epoch [10/100], Loss: 151.3398\n",
      "Epoch [11/100], Loss: 151.4978\n",
      "Epoch [12/100], Loss: 150.7178\n",
      "Epoch [13/100], Loss: 150.9036\n",
      "Epoch [14/100], Loss: 150.8395\n",
      "Epoch [15/100], Loss: 150.1416\n",
      "Epoch [16/100], Loss: 149.7245\n",
      "Epoch [17/100], Loss: 150.3522\n",
      "Epoch [18/100], Loss: 149.7974\n",
      "Epoch [19/100], Loss: 149.3703\n",
      "Epoch [20/100], Loss: 149.4271\n",
      "Epoch [21/100], Loss: 149.0711\n",
      "Epoch [22/100], Loss: 148.9810\n",
      "Epoch [23/100], Loss: 148.9032\n",
      "Epoch [24/100], Loss: 148.5919\n",
      "Epoch [25/100], Loss: 148.9894\n",
      "Epoch [26/100], Loss: 148.5118\n",
      "Epoch [27/100], Loss: 148.0841\n",
      "Epoch [28/100], Loss: 148.3945\n",
      "Epoch [29/100], Loss: 148.4000\n",
      "Epoch [30/100], Loss: 148.2652\n",
      "Epoch [31/100], Loss: 148.2148\n",
      "Epoch [32/100], Loss: 148.0385\n",
      "Epoch [33/100], Loss: 148.0330\n",
      "Epoch [34/100], Loss: 147.7681\n",
      "Epoch [35/100], Loss: 147.9898\n",
      "Epoch [36/100], Loss: 147.9973\n",
      "Epoch [37/100], Loss: 147.9655\n",
      "Epoch [38/100], Loss: 147.5687\n",
      "Epoch [39/100], Loss: 148.0634\n",
      "Epoch [40/100], Loss: 147.9298\n",
      "Epoch [41/100], Loss: 147.2302\n",
      "Epoch [42/100], Loss: 147.3659\n",
      "Epoch [43/100], Loss: 147.1660\n",
      "Epoch [44/100], Loss: 147.1083\n",
      "Epoch [45/100], Loss: 147.1138\n",
      "Epoch [46/100], Loss: 147.2823\n",
      "Epoch [47/100], Loss: 147.2486\n",
      "Epoch [48/100], Loss: 146.9936\n",
      "Epoch [49/100], Loss: 147.0457\n",
      "Epoch [50/100], Loss: 146.9130\n",
      "Epoch [51/100], Loss: 146.9753\n",
      "Epoch [52/100], Loss: 146.8406\n",
      "Epoch [53/100], Loss: 146.5515\n",
      "Epoch [54/100], Loss: 146.7627\n",
      "Epoch [55/100], Loss: 146.7513\n",
      "Epoch [56/100], Loss: 146.8656\n",
      "Epoch [57/100], Loss: 146.2744\n",
      "Epoch [58/100], Loss: 146.6849\n",
      "Epoch [59/100], Loss: 146.3815\n",
      "Epoch [60/100], Loss: 146.9031\n",
      "Epoch [61/100], Loss: 146.8424\n",
      "Epoch [62/100], Loss: 146.5523\n",
      "Epoch [63/100], Loss: 146.8293\n",
      "Epoch [64/100], Loss: 146.2725\n",
      "Epoch [65/100], Loss: 146.2832\n",
      "Epoch [66/100], Loss: 146.4593\n",
      "Epoch [67/100], Loss: 146.5209\n",
      "Stopping early at epoch 67 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 69.21%\n",
      "Epoch [1/100], Loss: 120.8990\n",
      "Epoch [2/100], Loss: 106.6844\n",
      "Epoch [3/100], Loss: 104.5239\n",
      "Epoch [4/100], Loss: 102.9559\n",
      "Epoch [5/100], Loss: 102.5706\n",
      "Epoch [6/100], Loss: 101.6481\n",
      "Epoch [7/100], Loss: 101.0998\n",
      "Epoch [8/100], Loss: 100.7840\n",
      "Epoch [9/100], Loss: 100.3689\n",
      "Epoch [10/100], Loss: 100.2488\n",
      "Epoch [11/100], Loss: 100.0303\n",
      "Epoch [12/100], Loss: 99.5009\n",
      "Epoch [13/100], Loss: 99.1561\n",
      "Epoch [14/100], Loss: 99.0193\n",
      "Epoch [15/100], Loss: 98.8522\n",
      "Epoch [16/100], Loss: 98.6089\n",
      "Epoch [17/100], Loss: 98.3207\n",
      "Epoch [18/100], Loss: 98.2073\n",
      "Epoch [19/100], Loss: 98.1473\n",
      "Epoch [20/100], Loss: 98.0328\n",
      "Epoch [21/100], Loss: 97.7724\n",
      "Epoch [22/100], Loss: 97.5507\n",
      "Epoch [23/100], Loss: 97.3380\n",
      "Epoch [24/100], Loss: 97.1284\n",
      "Epoch [25/100], Loss: 97.3307\n",
      "Epoch [26/100], Loss: 97.2387\n",
      "Epoch [27/100], Loss: 96.7963\n",
      "Epoch [28/100], Loss: 96.8000\n",
      "Epoch [29/100], Loss: 96.8423\n",
      "Epoch [30/100], Loss: 96.5181\n",
      "Epoch [31/100], Loss: 96.4152\n",
      "Epoch [32/100], Loss: 96.5236\n",
      "Epoch [33/100], Loss: 96.2724\n",
      "Epoch [34/100], Loss: 96.1925\n",
      "Epoch [35/100], Loss: 96.3500\n",
      "Epoch [36/100], Loss: 95.9254\n",
      "Epoch [37/100], Loss: 95.8942\n",
      "Epoch [38/100], Loss: 96.0630\n",
      "Epoch [39/100], Loss: 96.1209\n",
      "Epoch [40/100], Loss: 95.7884\n",
      "Epoch [41/100], Loss: 96.0905\n",
      "Epoch [42/100], Loss: 95.9403\n",
      "Epoch [43/100], Loss: 95.4983\n",
      "Epoch [44/100], Loss: 95.6244\n",
      "Epoch [45/100], Loss: 95.4685\n",
      "Epoch [46/100], Loss: 95.4135\n",
      "Epoch [47/100], Loss: 95.6025\n",
      "Epoch [48/100], Loss: 95.4333\n",
      "Epoch [49/100], Loss: 95.0268\n",
      "Epoch [50/100], Loss: 95.1214\n",
      "Epoch [51/100], Loss: 95.0520\n",
      "Epoch [52/100], Loss: 95.1505\n",
      "Epoch [53/100], Loss: 95.0083\n",
      "Epoch [54/100], Loss: 94.9539\n",
      "Epoch [55/100], Loss: 95.1746\n",
      "Epoch [56/100], Loss: 95.0879\n",
      "Epoch [57/100], Loss: 94.8130\n",
      "Epoch [58/100], Loss: 95.0772\n",
      "Epoch [59/100], Loss: 94.9730\n",
      "Epoch [60/100], Loss: 94.5036\n",
      "Epoch [61/100], Loss: 94.7527\n",
      "Epoch [62/100], Loss: 94.7576\n",
      "Epoch [63/100], Loss: 94.7066\n",
      "Epoch [64/100], Loss: 94.4627\n",
      "Epoch [65/100], Loss: 94.7394\n",
      "Epoch [66/100], Loss: 94.7682\n",
      "Epoch [67/100], Loss: 94.3883\n",
      "Epoch [68/100], Loss: 94.5116\n",
      "Epoch [69/100], Loss: 94.5649\n",
      "Epoch [70/100], Loss: 94.4813\n",
      "Epoch [71/100], Loss: 94.4596\n",
      "Epoch [72/100], Loss: 94.3875\n",
      "Epoch [73/100], Loss: 94.4612\n",
      "Epoch [74/100], Loss: 94.4203\n",
      "Epoch [75/100], Loss: 94.2874\n",
      "Epoch [76/100], Loss: 94.3039\n",
      "Epoch [77/100], Loss: 94.1068\n",
      "Epoch [78/100], Loss: 94.4664\n",
      "Epoch [79/100], Loss: 94.4219\n",
      "Epoch [80/100], Loss: 93.8059\n",
      "Epoch [81/100], Loss: 94.1793\n",
      "Epoch [82/100], Loss: 94.0532\n",
      "Epoch [83/100], Loss: 94.0156\n",
      "Epoch [84/100], Loss: 93.8599\n",
      "Epoch [85/100], Loss: 94.1369\n",
      "Epoch [86/100], Loss: 93.8196\n",
      "Epoch [87/100], Loss: 93.8509\n",
      "Epoch [88/100], Loss: 93.9633\n",
      "Epoch [89/100], Loss: 93.6183\n",
      "Epoch [90/100], Loss: 94.0369\n",
      "Epoch [91/100], Loss: 93.7293\n",
      "Epoch [92/100], Loss: 93.8607\n",
      "Epoch [93/100], Loss: 93.6698\n",
      "Epoch [94/100], Loss: 93.6021\n",
      "Epoch [95/100], Loss: 93.7318\n",
      "Epoch [96/100], Loss: 94.0519\n",
      "Epoch [97/100], Loss: 93.7536\n",
      "Epoch [98/100], Loss: 94.1886\n",
      "Epoch [99/100], Loss: 93.6905\n",
      "Epoch [100/100], Loss: 93.6557\n",
      "Test Accuracy Base Logit K Means: 68.43%\n",
      "Epoch [1/100], Loss: 30.2291\n",
      "Epoch [2/100], Loss: 20.5994\n",
      "Epoch [3/100], Loss: 18.9491\n",
      "Epoch [4/100], Loss: 18.1979\n",
      "Epoch [5/100], Loss: 17.7677\n",
      "Epoch [6/100], Loss: 17.2763\n",
      "Epoch [7/100], Loss: 16.8764\n",
      "Epoch [8/100], Loss: 16.6834\n",
      "Epoch [9/100], Loss: 16.4931\n",
      "Epoch [10/100], Loss: 16.2115\n",
      "Epoch [11/100], Loss: 16.3233\n",
      "Epoch [12/100], Loss: 15.9606\n",
      "Epoch [13/100], Loss: 15.8063\n",
      "Epoch [14/100], Loss: 15.7633\n",
      "Epoch [15/100], Loss: 15.6970\n",
      "Epoch [16/100], Loss: 15.4702\n",
      "Epoch [17/100], Loss: 15.2625\n",
      "Epoch [18/100], Loss: 15.2718\n",
      "Epoch [19/100], Loss: 15.2863\n",
      "Epoch [20/100], Loss: 15.1817\n",
      "Epoch [21/100], Loss: 14.8124\n",
      "Epoch [22/100], Loss: 14.8066\n",
      "Epoch [23/100], Loss: 14.6198\n",
      "Epoch [24/100], Loss: 14.7108\n",
      "Epoch [25/100], Loss: 14.5334\n",
      "Epoch [26/100], Loss: 14.4391\n",
      "Epoch [27/100], Loss: 14.3014\n",
      "Epoch [28/100], Loss: 14.3113\n",
      "Epoch [29/100], Loss: 14.2745\n",
      "Epoch [30/100], Loss: 14.2091\n",
      "Epoch [31/100], Loss: 14.1291\n",
      "Epoch [32/100], Loss: 14.0858\n",
      "Epoch [33/100], Loss: 14.0123\n",
      "Epoch [34/100], Loss: 14.0846\n",
      "Epoch [35/100], Loss: 13.9954\n",
      "Epoch [36/100], Loss: 13.8202\n",
      "Epoch [37/100], Loss: 13.6721\n",
      "Epoch [38/100], Loss: 13.7699\n",
      "Epoch [39/100], Loss: 13.7353\n",
      "Epoch [40/100], Loss: 13.6520\n",
      "Epoch [41/100], Loss: 13.6501\n",
      "Epoch [42/100], Loss: 13.6868\n",
      "Epoch [43/100], Loss: 13.5666\n",
      "Epoch [44/100], Loss: 13.5825\n",
      "Epoch [45/100], Loss: 13.3869\n",
      "Epoch [46/100], Loss: 13.3050\n",
      "Epoch [47/100], Loss: 13.3461\n",
      "Epoch [48/100], Loss: 13.1878\n",
      "Epoch [49/100], Loss: 13.2695\n",
      "Epoch [50/100], Loss: 13.1651\n",
      "Epoch [51/100], Loss: 13.3299\n",
      "Epoch [52/100], Loss: 13.1628\n",
      "Epoch [53/100], Loss: 12.9577\n",
      "Epoch [54/100], Loss: 13.1290\n",
      "Epoch [55/100], Loss: 13.0808\n",
      "Epoch [56/100], Loss: 13.0499\n",
      "Epoch [57/100], Loss: 12.8111\n",
      "Epoch [58/100], Loss: 12.9700\n",
      "Epoch [59/100], Loss: 12.8807\n",
      "Epoch [60/100], Loss: 12.8616\n",
      "Epoch [61/100], Loss: 12.8065\n",
      "Epoch [62/100], Loss: 12.6935\n",
      "Epoch [63/100], Loss: 12.7437\n",
      "Epoch [64/100], Loss: 12.6219\n",
      "Epoch [65/100], Loss: 12.5862\n",
      "Epoch [66/100], Loss: 12.6382\n",
      "Epoch [67/100], Loss: 12.6642\n",
      "Epoch [68/100], Loss: 12.5451\n",
      "Epoch [69/100], Loss: 12.5668\n",
      "Epoch [70/100], Loss: 12.5909\n",
      "Epoch [71/100], Loss: 12.5322\n",
      "Epoch [72/100], Loss: 12.5106\n",
      "Epoch [73/100], Loss: 12.4391\n",
      "Epoch [74/100], Loss: 12.2834\n",
      "Epoch [75/100], Loss: 12.4430\n",
      "Epoch [76/100], Loss: 12.2983\n",
      "Epoch [77/100], Loss: 12.4050\n",
      "Epoch [78/100], Loss: 12.3761\n",
      "Epoch [79/100], Loss: 12.2275\n",
      "Epoch [80/100], Loss: 12.2767\n",
      "Epoch [81/100], Loss: 12.2380\n",
      "Epoch [82/100], Loss: 12.1863\n",
      "Epoch [83/100], Loss: 12.2303\n",
      "Epoch [84/100], Loss: 12.2134\n",
      "Epoch [85/100], Loss: 12.3589\n",
      "Epoch [86/100], Loss: 12.3352\n",
      "Epoch [87/100], Loss: 12.1556\n",
      "Epoch [88/100], Loss: 12.0191\n",
      "Epoch [89/100], Loss: 12.1443\n",
      "Epoch [90/100], Loss: 12.0793\n",
      "Epoch [91/100], Loss: 12.1917\n",
      "Epoch [92/100], Loss: 11.9896\n",
      "Epoch [93/100], Loss: 11.9436\n",
      "Epoch [94/100], Loss: 11.9434\n",
      "Epoch [95/100], Loss: 11.8834\n",
      "Epoch [96/100], Loss: 11.9485\n",
      "Epoch [97/100], Loss: 11.9009\n",
      "Epoch [98/100], Loss: 11.7608\n",
      "Epoch [99/100], Loss: 11.8179\n",
      "Epoch [100/100], Loss: 11.9314\n",
      "Test Accuracy Base Logit K Means: 64.03%\n",
      "Epoch [1/100], Loss: 17.8016\n",
      "Epoch [2/100], Loss: 10.3073\n",
      "Epoch [3/100], Loss: 9.1423\n",
      "Epoch [4/100], Loss: 8.7570\n",
      "Epoch [5/100], Loss: 8.6933\n",
      "Epoch [6/100], Loss: 8.1531\n",
      "Epoch [7/100], Loss: 7.4551\n",
      "Epoch [8/100], Loss: 7.4221\n",
      "Epoch [9/100], Loss: 7.2996\n",
      "Epoch [10/100], Loss: 7.4707\n",
      "Epoch [11/100], Loss: 6.9177\n",
      "Epoch [12/100], Loss: 6.7474\n",
      "Epoch [13/100], Loss: 6.6855\n",
      "Epoch [14/100], Loss: 6.7692\n",
      "Epoch [15/100], Loss: 6.5982\n",
      "Epoch [16/100], Loss: 6.5677\n",
      "Epoch [17/100], Loss: 6.2668\n",
      "Epoch [18/100], Loss: 6.4216\n",
      "Epoch [19/100], Loss: 6.3312\n",
      "Epoch [20/100], Loss: 6.1062\n",
      "Epoch [21/100], Loss: 6.1332\n",
      "Epoch [22/100], Loss: 6.2292\n",
      "Epoch [23/100], Loss: 6.0220\n",
      "Epoch [24/100], Loss: 5.9088\n",
      "Epoch [25/100], Loss: 5.8362\n",
      "Epoch [26/100], Loss: 5.8509\n",
      "Epoch [27/100], Loss: 5.7399\n",
      "Epoch [28/100], Loss: 5.8692\n",
      "Epoch [29/100], Loss: 5.8768\n",
      "Epoch [30/100], Loss: 5.7268\n",
      "Epoch [31/100], Loss: 5.6826\n",
      "Epoch [32/100], Loss: 5.6840\n",
      "Epoch [33/100], Loss: 5.7176\n",
      "Epoch [34/100], Loss: 5.7088\n",
      "Epoch [35/100], Loss: 5.5612\n",
      "Epoch [36/100], Loss: 5.3994\n",
      "Epoch [37/100], Loss: 5.3014\n",
      "Epoch [38/100], Loss: 5.4657\n",
      "Epoch [39/100], Loss: 5.3549\n",
      "Epoch [40/100], Loss: 5.3409\n",
      "Epoch [41/100], Loss: 5.4943\n",
      "Epoch [42/100], Loss: 5.3095\n",
      "Epoch [43/100], Loss: 5.1821\n",
      "Epoch [44/100], Loss: 5.2130\n",
      "Epoch [45/100], Loss: 5.0945\n",
      "Epoch [46/100], Loss: 5.1325\n",
      "Epoch [47/100], Loss: 5.5264\n",
      "Epoch [48/100], Loss: 5.1262\n",
      "Epoch [49/100], Loss: 5.0361\n",
      "Epoch [50/100], Loss: 4.9883\n",
      "Epoch [51/100], Loss: 5.0101\n",
      "Epoch [52/100], Loss: 4.9223\n",
      "Epoch [53/100], Loss: 4.8228\n",
      "Epoch [54/100], Loss: 4.9069\n",
      "Epoch [55/100], Loss: 5.0137\n",
      "Epoch [56/100], Loss: 4.8052\n",
      "Epoch [57/100], Loss: 4.9445\n",
      "Epoch [58/100], Loss: 5.3780\n",
      "Epoch [59/100], Loss: 4.8849\n",
      "Epoch [60/100], Loss: 4.8483\n",
      "Epoch [61/100], Loss: 4.8027\n",
      "Epoch [62/100], Loss: 4.7343\n",
      "Epoch [63/100], Loss: 4.7786\n",
      "Epoch [64/100], Loss: 4.7037\n",
      "Epoch [65/100], Loss: 4.7320\n",
      "Epoch [66/100], Loss: 4.7702\n",
      "Epoch [67/100], Loss: 4.6807\n",
      "Epoch [68/100], Loss: 4.6931\n",
      "Epoch [69/100], Loss: 4.6800\n",
      "Epoch [70/100], Loss: 4.7267\n",
      "Epoch [71/100], Loss: 4.8056\n",
      "Epoch [72/100], Loss: 4.6670\n",
      "Epoch [73/100], Loss: 4.5711\n",
      "Epoch [74/100], Loss: 4.4555\n",
      "Epoch [75/100], Loss: 4.7466\n",
      "Epoch [76/100], Loss: 4.7165\n",
      "Epoch [77/100], Loss: 4.4931\n",
      "Epoch [78/100], Loss: 4.4841\n",
      "Epoch [79/100], Loss: 4.4901\n",
      "Epoch [80/100], Loss: 4.5916\n",
      "Epoch [81/100], Loss: 4.6324\n",
      "Stopping early at epoch 81 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Base Logit K Means: 63.98%\n",
      "Epoch [1/100], Loss: 5.9017\n",
      "Epoch [2/100], Loss: 3.5789\n",
      "Epoch [3/100], Loss: 2.3565\n",
      "Epoch [4/100], Loss: 1.8888\n",
      "Epoch [5/100], Loss: 1.6429\n",
      "Epoch [6/100], Loss: 1.4818\n",
      "Epoch [7/100], Loss: 1.3578\n",
      "Epoch [8/100], Loss: 1.2706\n",
      "Epoch [9/100], Loss: 1.1971\n",
      "Epoch [10/100], Loss: 1.1330\n",
      "Epoch [11/100], Loss: 1.0752\n",
      "Epoch [12/100], Loss: 1.0374\n",
      "Epoch [13/100], Loss: 0.9877\n",
      "Epoch [14/100], Loss: 0.9651\n",
      "Epoch [15/100], Loss: 0.9347\n",
      "Epoch [16/100], Loss: 0.9010\n",
      "Epoch [17/100], Loss: 0.8644\n",
      "Epoch [18/100], Loss: 0.8450\n",
      "Epoch [19/100], Loss: 0.8192\n",
      "Epoch [20/100], Loss: 0.7952\n",
      "Epoch [21/100], Loss: 0.7787\n",
      "Epoch [22/100], Loss: 0.7640\n",
      "Epoch [23/100], Loss: 0.7423\n",
      "Epoch [24/100], Loss: 0.7295\n",
      "Epoch [25/100], Loss: 0.7081\n",
      "Epoch [26/100], Loss: 0.6957\n",
      "Epoch [27/100], Loss: 0.6851\n",
      "Epoch [28/100], Loss: 0.6714\n",
      "Epoch [29/100], Loss: 0.6540\n",
      "Epoch [30/100], Loss: 0.6398\n",
      "Epoch [31/100], Loss: 0.6328\n",
      "Epoch [32/100], Loss: 0.6183\n",
      "Epoch [33/100], Loss: 0.6094\n",
      "Epoch [34/100], Loss: 0.5981\n",
      "Epoch [35/100], Loss: 0.5906\n",
      "Epoch [36/100], Loss: 0.5776\n",
      "Epoch [37/100], Loss: 0.5748\n",
      "Epoch [38/100], Loss: 0.5602\n",
      "Epoch [39/100], Loss: 0.5545\n",
      "Epoch [40/100], Loss: 0.5411\n",
      "Epoch [41/100], Loss: 0.5339\n",
      "Epoch [42/100], Loss: 0.5260\n",
      "Epoch [43/100], Loss: 0.5177\n",
      "Epoch [44/100], Loss: 0.5125\n",
      "Epoch [45/100], Loss: 0.5049\n",
      "Epoch [46/100], Loss: 0.4950\n",
      "Epoch [47/100], Loss: 0.4911\n",
      "Epoch [48/100], Loss: 0.4830\n",
      "Epoch [49/100], Loss: 0.4757\n",
      "Epoch [50/100], Loss: 0.4728\n",
      "Epoch [51/100], Loss: 0.4663\n",
      "Epoch [52/100], Loss: 0.4583\n",
      "Epoch [53/100], Loss: 0.4513\n",
      "Epoch [54/100], Loss: 0.4510\n",
      "Epoch [55/100], Loss: 0.4433\n",
      "Epoch [56/100], Loss: 0.4378\n",
      "Epoch [57/100], Loss: 0.4339\n",
      "Epoch [58/100], Loss: 0.4329\n",
      "Epoch [59/100], Loss: 0.4232\n",
      "Epoch [60/100], Loss: 0.4187\n",
      "Epoch [61/100], Loss: 0.4154\n",
      "Epoch [62/100], Loss: 0.4108\n",
      "Epoch [63/100], Loss: 0.4037\n",
      "Epoch [64/100], Loss: 0.4011\n",
      "Epoch [65/100], Loss: 0.3951\n",
      "Epoch [66/100], Loss: 0.3914\n",
      "Epoch [67/100], Loss: 0.3871\n",
      "Epoch [68/100], Loss: 0.3844\n",
      "Epoch [69/100], Loss: 0.3791\n",
      "Epoch [70/100], Loss: 0.3759\n",
      "Epoch [71/100], Loss: 0.3713\n",
      "Epoch [72/100], Loss: 0.3689\n",
      "Epoch [73/100], Loss: 0.3667\n",
      "Epoch [74/100], Loss: 0.3622\n",
      "Epoch [75/100], Loss: 0.3568\n",
      "Epoch [76/100], Loss: 0.3550\n",
      "Epoch [77/100], Loss: 0.3543\n",
      "Epoch [78/100], Loss: 0.3457\n",
      "Epoch [79/100], Loss: 0.3428\n",
      "Epoch [80/100], Loss: 0.3417\n",
      "Epoch [81/100], Loss: 0.3376\n",
      "Epoch [82/100], Loss: 0.3333\n",
      "Epoch [83/100], Loss: 0.3325\n",
      "Epoch [84/100], Loss: 0.3291\n",
      "Epoch [85/100], Loss: 0.3264\n",
      "Epoch [86/100], Loss: 0.3229\n",
      "Epoch [87/100], Loss: 0.3199\n",
      "Epoch [88/100], Loss: 0.3163\n",
      "Epoch [89/100], Loss: 0.3134\n",
      "Epoch [90/100], Loss: 0.3109\n",
      "Epoch [91/100], Loss: 0.3116\n",
      "Epoch [92/100], Loss: 0.3082\n",
      "Epoch [93/100], Loss: 0.3046\n",
      "Epoch [94/100], Loss: 0.3022\n",
      "Epoch [95/100], Loss: 0.3010\n",
      "Epoch [96/100], Loss: 0.2984\n",
      "Epoch [97/100], Loss: 0.2946\n",
      "Epoch [98/100], Loss: 0.2937\n",
      "Epoch [99/100], Loss: 0.2908\n",
      "Epoch [100/100], Loss: 0.2889\n",
      "Test Accuracy Base Logit K Means: 62.68%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "base_logit_kmeans_plus = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    logit_model = LogisticRegression(input_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(logit_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "    previous_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logit_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = logit_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if previous_loss - current_loss < tolerance:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                break\n",
    "        else:\n",
    "            epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "        previous_loss = current_loss\n",
    "\n",
    "    logit_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            outputs = logit_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Base Logit K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    base_logit_kmeans_plus[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.206731</td>\n",
       "      <td>68.427885</td>\n",
       "      <td>64.028846</td>\n",
       "      <td>63.975962</td>\n",
       "      <td>62.682692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000       1000 \n",
       "0  69.206731  68.427885  64.028846  63.975962  62.682692"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_logit_kmeans_plus_df = pd.DataFrame(base_logit_kmeans_plus)\n",
    "base_logit_kmeans_plus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 174.9405\n",
      "Epoch [2/100], Loss: 159.1250\n",
      "Epoch [3/100], Loss: 156.4808\n",
      "Epoch [4/100], Loss: 154.8477\n",
      "Epoch [5/100], Loss: 153.9503\n",
      "Epoch [6/100], Loss: 153.3894\n",
      "Epoch [7/100], Loss: 152.9291\n",
      "Epoch [8/100], Loss: 152.2461\n",
      "Epoch [9/100], Loss: 151.7452\n",
      "Epoch [10/100], Loss: 151.9274\n",
      "Epoch [11/100], Loss: 151.0914\n",
      "Epoch [12/100], Loss: 151.0563\n",
      "Epoch [13/100], Loss: 150.9092\n",
      "Epoch [14/100], Loss: 150.3412\n",
      "Epoch [15/100], Loss: 150.1272\n",
      "Epoch [16/100], Loss: 150.0380\n",
      "Epoch [17/100], Loss: 149.6670\n",
      "Epoch [18/100], Loss: 149.4642\n",
      "Epoch [19/100], Loss: 149.5227\n",
      "Epoch [20/100], Loss: 149.3764\n",
      "Epoch [21/100], Loss: 149.3691\n",
      "Epoch [22/100], Loss: 149.1877\n",
      "Epoch [23/100], Loss: 148.8774\n",
      "Epoch [24/100], Loss: 148.4891\n",
      "Epoch [25/100], Loss: 148.5754\n",
      "Epoch [26/100], Loss: 148.4985\n",
      "Epoch [27/100], Loss: 148.2859\n",
      "Epoch [28/100], Loss: 148.5809\n",
      "Epoch [29/100], Loss: 148.2279\n",
      "Epoch [30/100], Loss: 147.9092\n",
      "Epoch [31/100], Loss: 148.0905\n",
      "Epoch [32/100], Loss: 147.9455\n",
      "Epoch [33/100], Loss: 148.0844\n",
      "Epoch [34/100], Loss: 147.8019\n",
      "Epoch [35/100], Loss: 148.0716\n",
      "Epoch [36/100], Loss: 147.5828\n",
      "Epoch [37/100], Loss: 147.7961\n",
      "Epoch [38/100], Loss: 147.6227\n",
      "Epoch [39/100], Loss: 147.6509\n",
      "Epoch [40/100], Loss: 147.4855\n",
      "Epoch [41/100], Loss: 147.3830\n",
      "Epoch [42/100], Loss: 147.2884\n",
      "Epoch [43/100], Loss: 147.2878\n",
      "Epoch [44/100], Loss: 147.0250\n",
      "Epoch [45/100], Loss: 147.0257\n",
      "Epoch [46/100], Loss: 147.3934\n",
      "Epoch [47/100], Loss: 147.0344\n",
      "Epoch [48/100], Loss: 146.9226\n",
      "Epoch [49/100], Loss: 147.1524\n",
      "Epoch [50/100], Loss: 146.4305\n",
      "Epoch [51/100], Loss: 146.8398\n",
      "Epoch [52/100], Loss: 146.7030\n",
      "Epoch [53/100], Loss: 146.8918\n",
      "Epoch [54/100], Loss: 146.7543\n",
      "Epoch [55/100], Loss: 146.6081\n",
      "Epoch [56/100], Loss: 146.7860\n",
      "Epoch [57/100], Loss: 146.6392\n",
      "Epoch [58/100], Loss: 146.5767\n",
      "Epoch [59/100], Loss: 146.2732\n",
      "Epoch [60/100], Loss: 146.5110\n",
      "Epoch [61/100], Loss: 146.4605\n",
      "Epoch [62/100], Loss: 146.6328\n",
      "Epoch [63/100], Loss: 146.6047\n",
      "Epoch [64/100], Loss: 146.3802\n",
      "Epoch [65/100], Loss: 146.2324\n",
      "Epoch [66/100], Loss: 146.4378\n",
      "Epoch [67/100], Loss: 146.2899\n",
      "Epoch [68/100], Loss: 146.3065\n",
      "Epoch [69/100], Loss: 146.0450\n",
      "Epoch [70/100], Loss: 146.3828\n",
      "Epoch [71/100], Loss: 146.4483\n",
      "Epoch [72/100], Loss: 146.5170\n",
      "Stopping early at epoch 72 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 68.88%\n",
      "Epoch [1/100], Loss: 121.1539\n",
      "Epoch [2/100], Loss: 106.3630\n",
      "Epoch [3/100], Loss: 104.2895\n",
      "Epoch [4/100], Loss: 103.2809\n",
      "Epoch [5/100], Loss: 102.2511\n",
      "Epoch [6/100], Loss: 101.6895\n",
      "Epoch [7/100], Loss: 101.4350\n",
      "Epoch [8/100], Loss: 100.9163\n",
      "Epoch [9/100], Loss: 100.3193\n",
      "Epoch [10/100], Loss: 100.0680\n",
      "Epoch [11/100], Loss: 99.6303\n",
      "Epoch [12/100], Loss: 99.1730\n",
      "Epoch [13/100], Loss: 98.7304\n",
      "Epoch [14/100], Loss: 99.0177\n",
      "Epoch [15/100], Loss: 98.6315\n",
      "Epoch [16/100], Loss: 98.6896\n",
      "Epoch [17/100], Loss: 98.3841\n",
      "Epoch [18/100], Loss: 98.2535\n",
      "Epoch [19/100], Loss: 98.2456\n",
      "Epoch [20/100], Loss: 97.8216\n",
      "Epoch [21/100], Loss: 97.6362\n",
      "Epoch [22/100], Loss: 97.4598\n",
      "Epoch [23/100], Loss: 97.7911\n",
      "Epoch [24/100], Loss: 97.2635\n",
      "Epoch [25/100], Loss: 96.9533\n",
      "Epoch [26/100], Loss: 97.1062\n",
      "Epoch [27/100], Loss: 97.0876\n",
      "Epoch [28/100], Loss: 96.8318\n",
      "Epoch [29/100], Loss: 96.7694\n",
      "Epoch [30/100], Loss: 96.9733\n",
      "Epoch [31/100], Loss: 96.6383\n",
      "Epoch [32/100], Loss: 96.4912\n",
      "Epoch [33/100], Loss: 96.0464\n",
      "Epoch [34/100], Loss: 96.5777\n",
      "Epoch [35/100], Loss: 96.1743\n",
      "Epoch [36/100], Loss: 95.9662\n",
      "Epoch [37/100], Loss: 95.9814\n",
      "Epoch [38/100], Loss: 96.0926\n",
      "Epoch [39/100], Loss: 95.8907\n",
      "Epoch [40/100], Loss: 96.0201\n",
      "Epoch [41/100], Loss: 95.9185\n",
      "Epoch [42/100], Loss: 95.5442\n",
      "Epoch [43/100], Loss: 95.7431\n",
      "Epoch [44/100], Loss: 95.6599\n",
      "Epoch [45/100], Loss: 95.7392\n",
      "Epoch [46/100], Loss: 95.1334\n",
      "Epoch [47/100], Loss: 95.5591\n",
      "Epoch [48/100], Loss: 95.1859\n",
      "Epoch [49/100], Loss: 95.2932\n",
      "Epoch [50/100], Loss: 95.2463\n",
      "Epoch [51/100], Loss: 95.1271\n",
      "Epoch [52/100], Loss: 95.3291\n",
      "Epoch [53/100], Loss: 95.0214\n",
      "Epoch [54/100], Loss: 94.7562\n",
      "Epoch [55/100], Loss: 94.9805\n",
      "Epoch [56/100], Loss: 94.9510\n",
      "Epoch [57/100], Loss: 94.7638\n",
      "Epoch [58/100], Loss: 95.0958\n",
      "Epoch [59/100], Loss: 94.9061\n",
      "Epoch [60/100], Loss: 94.7788\n",
      "Epoch [61/100], Loss: 94.9253\n",
      "Epoch [62/100], Loss: 94.8411\n",
      "Epoch [63/100], Loss: 94.8071\n",
      "Epoch [64/100], Loss: 94.6044\n",
      "Epoch [65/100], Loss: 94.4600\n",
      "Epoch [66/100], Loss: 94.7599\n",
      "Epoch [67/100], Loss: 94.7031\n",
      "Epoch [68/100], Loss: 94.4970\n",
      "Epoch [69/100], Loss: 94.4466\n",
      "Epoch [70/100], Loss: 94.3382\n",
      "Epoch [71/100], Loss: 94.4818\n",
      "Epoch [72/100], Loss: 94.4329\n",
      "Epoch [73/100], Loss: 94.4995\n",
      "Epoch [74/100], Loss: 94.1070\n",
      "Epoch [75/100], Loss: 94.1747\n",
      "Epoch [76/100], Loss: 94.2005\n",
      "Epoch [77/100], Loss: 94.3905\n",
      "Stopping early at epoch 77 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 68.66%\n",
      "Epoch [1/100], Loss: 30.2349\n",
      "Epoch [2/100], Loss: 20.5571\n",
      "Epoch [3/100], Loss: 19.0565\n",
      "Epoch [4/100], Loss: 18.2890\n",
      "Epoch [5/100], Loss: 17.8786\n",
      "Epoch [6/100], Loss: 17.5182\n",
      "Epoch [7/100], Loss: 17.0793\n",
      "Epoch [8/100], Loss: 16.7228\n",
      "Epoch [9/100], Loss: 16.6709\n",
      "Epoch [10/100], Loss: 16.3961\n",
      "Epoch [11/100], Loss: 16.1241\n",
      "Epoch [12/100], Loss: 15.7716\n",
      "Epoch [13/100], Loss: 15.7977\n",
      "Epoch [14/100], Loss: 15.7831\n",
      "Epoch [15/100], Loss: 15.5932\n",
      "Epoch [16/100], Loss: 15.2909\n",
      "Epoch [17/100], Loss: 15.1808\n",
      "Epoch [18/100], Loss: 15.2619\n",
      "Epoch [19/100], Loss: 15.0469\n",
      "Epoch [20/100], Loss: 14.9687\n",
      "Epoch [21/100], Loss: 14.9143\n",
      "Epoch [22/100], Loss: 14.9129\n",
      "Epoch [23/100], Loss: 14.5578\n",
      "Epoch [24/100], Loss: 14.5888\n",
      "Epoch [25/100], Loss: 14.5893\n",
      "Epoch [26/100], Loss: 14.4676\n",
      "Epoch [27/100], Loss: 14.4364\n",
      "Epoch [28/100], Loss: 14.2486\n",
      "Epoch [29/100], Loss: 14.2309\n",
      "Epoch [30/100], Loss: 14.1328\n",
      "Epoch [31/100], Loss: 14.0442\n",
      "Epoch [32/100], Loss: 14.0439\n",
      "Epoch [33/100], Loss: 14.0445\n",
      "Epoch [34/100], Loss: 14.0950\n",
      "Stopping early at epoch 34 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 66.22%\n",
      "Epoch [1/100], Loss: 17.8250\n",
      "Epoch [2/100], Loss: 10.4112\n",
      "Epoch [3/100], Loss: 9.1402\n",
      "Epoch [4/100], Loss: 8.5687\n",
      "Epoch [5/100], Loss: 8.1242\n",
      "Epoch [6/100], Loss: 8.1063\n",
      "Epoch [7/100], Loss: 7.6545\n",
      "Epoch [8/100], Loss: 7.5286\n",
      "Epoch [9/100], Loss: 7.2415\n",
      "Epoch [10/100], Loss: 7.2629\n",
      "Epoch [11/100], Loss: 7.1187\n",
      "Epoch [12/100], Loss: 6.9197\n",
      "Epoch [13/100], Loss: 6.7011\n",
      "Epoch [14/100], Loss: 6.5376\n",
      "Epoch [15/100], Loss: 6.4615\n",
      "Epoch [16/100], Loss: 6.4079\n",
      "Epoch [17/100], Loss: 6.4733\n",
      "Epoch [18/100], Loss: 6.4516\n",
      "Epoch [19/100], Loss: 6.1479\n",
      "Epoch [20/100], Loss: 6.1819\n",
      "Epoch [21/100], Loss: 6.1267\n",
      "Epoch [22/100], Loss: 5.9909\n",
      "Epoch [23/100], Loss: 5.9680\n",
      "Epoch [24/100], Loss: 6.0543\n",
      "Epoch [25/100], Loss: 5.9809\n",
      "Epoch [26/100], Loss: 5.7465\n",
      "Epoch [27/100], Loss: 5.6935\n",
      "Epoch [28/100], Loss: 5.7399\n",
      "Epoch [29/100], Loss: 5.9134\n",
      "Epoch [30/100], Loss: 5.8291\n",
      "Epoch [31/100], Loss: 5.6603\n",
      "Epoch [32/100], Loss: 5.4810\n",
      "Epoch [33/100], Loss: 5.4228\n",
      "Epoch [34/100], Loss: 5.3861\n",
      "Epoch [35/100], Loss: 5.4681\n",
      "Epoch [36/100], Loss: 5.4742\n",
      "Epoch [37/100], Loss: 5.6129\n",
      "Stopping early at epoch 37 (Loss improvement < 0.001 for 3 epochs)\n",
      "Test Accuracy Lipschitz Logit K Means: 65.29%\n",
      "Epoch [1/100], Loss: 5.8936\n",
      "Epoch [2/100], Loss: 3.5857\n",
      "Epoch [3/100], Loss: 2.3701\n",
      "Epoch [4/100], Loss: 1.9147\n",
      "Epoch [5/100], Loss: 1.6647\n",
      "Epoch [6/100], Loss: 1.4740\n",
      "Epoch [7/100], Loss: 1.3449\n",
      "Epoch [8/100], Loss: 1.2671\n",
      "Epoch [9/100], Loss: 1.1868\n",
      "Epoch [10/100], Loss: 1.1323\n",
      "Epoch [11/100], Loss: 1.0805\n",
      "Epoch [12/100], Loss: 1.0350\n",
      "Epoch [13/100], Loss: 0.9944\n",
      "Epoch [14/100], Loss: 0.9561\n",
      "Epoch [15/100], Loss: 0.9252\n",
      "Epoch [16/100], Loss: 0.8903\n",
      "Epoch [17/100], Loss: 0.8775\n",
      "Epoch [18/100], Loss: 0.8519\n",
      "Epoch [19/100], Loss: 0.8290\n",
      "Epoch [20/100], Loss: 0.8047\n",
      "Epoch [21/100], Loss: 0.7739\n",
      "Epoch [22/100], Loss: 0.7664\n",
      "Epoch [23/100], Loss: 0.7421\n",
      "Epoch [24/100], Loss: 0.7284\n",
      "Epoch [25/100], Loss: 0.7046\n",
      "Epoch [26/100], Loss: 0.6896\n",
      "Epoch [27/100], Loss: 0.6855\n",
      "Epoch [28/100], Loss: 0.6626\n",
      "Epoch [29/100], Loss: 0.6511\n",
      "Epoch [30/100], Loss: 0.6468\n",
      "Epoch [31/100], Loss: 0.6320\n",
      "Epoch [32/100], Loss: 0.6166\n",
      "Epoch [33/100], Loss: 0.6085\n",
      "Epoch [34/100], Loss: 0.5996\n",
      "Epoch [35/100], Loss: 0.5894\n",
      "Epoch [36/100], Loss: 0.5782\n",
      "Epoch [37/100], Loss: 0.5768\n",
      "Epoch [38/100], Loss: 0.5626\n",
      "Epoch [39/100], Loss: 0.5517\n",
      "Epoch [40/100], Loss: 0.5449\n",
      "Epoch [41/100], Loss: 0.5321\n",
      "Epoch [42/100], Loss: 0.5252\n",
      "Epoch [43/100], Loss: 0.5173\n",
      "Epoch [44/100], Loss: 0.5152\n",
      "Epoch [45/100], Loss: 0.5021\n",
      "Epoch [46/100], Loss: 0.5052\n",
      "Epoch [47/100], Loss: 0.4908\n",
      "Epoch [48/100], Loss: 0.4869\n",
      "Epoch [49/100], Loss: 0.4783\n",
      "Epoch [50/100], Loss: 0.4697\n",
      "Epoch [51/100], Loss: 0.4646\n",
      "Epoch [52/100], Loss: 0.4580\n",
      "Epoch [53/100], Loss: 0.4525\n",
      "Epoch [54/100], Loss: 0.4520\n",
      "Epoch [55/100], Loss: 0.4413\n",
      "Epoch [56/100], Loss: 0.4401\n",
      "Epoch [57/100], Loss: 0.4379\n",
      "Epoch [58/100], Loss: 0.4296\n",
      "Epoch [59/100], Loss: 0.4261\n",
      "Epoch [60/100], Loss: 0.4220\n",
      "Epoch [61/100], Loss: 0.4128\n",
      "Epoch [62/100], Loss: 0.4069\n",
      "Epoch [63/100], Loss: 0.4021\n",
      "Epoch [64/100], Loss: 0.3980\n",
      "Epoch [65/100], Loss: 0.3929\n",
      "Epoch [66/100], Loss: 0.3894\n",
      "Epoch [67/100], Loss: 0.3865\n",
      "Epoch [68/100], Loss: 0.3830\n",
      "Epoch [69/100], Loss: 0.3834\n",
      "Epoch [70/100], Loss: 0.3775\n",
      "Epoch [71/100], Loss: 0.3717\n",
      "Epoch [72/100], Loss: 0.3675\n",
      "Epoch [73/100], Loss: 0.3620\n",
      "Epoch [74/100], Loss: 0.3617\n",
      "Epoch [75/100], Loss: 0.3578\n",
      "Epoch [76/100], Loss: 0.3544\n",
      "Epoch [77/100], Loss: 0.3528\n",
      "Epoch [78/100], Loss: 0.3445\n",
      "Epoch [79/100], Loss: 0.3464\n",
      "Epoch [80/100], Loss: 0.3430\n",
      "Epoch [81/100], Loss: 0.3371\n",
      "Epoch [82/100], Loss: 0.3342\n",
      "Epoch [83/100], Loss: 0.3323\n",
      "Epoch [84/100], Loss: 0.3280\n",
      "Epoch [85/100], Loss: 0.3249\n",
      "Epoch [86/100], Loss: 0.3252\n",
      "Epoch [87/100], Loss: 0.3192\n",
      "Epoch [88/100], Loss: 0.3183\n",
      "Epoch [89/100], Loss: 0.3139\n",
      "Epoch [90/100], Loss: 0.3127\n",
      "Epoch [91/100], Loss: 0.3081\n",
      "Epoch [92/100], Loss: 0.3071\n",
      "Epoch [93/100], Loss: 0.3069\n",
      "Epoch [94/100], Loss: 0.3027\n",
      "Epoch [95/100], Loss: 0.3006\n",
      "Epoch [96/100], Loss: 0.2977\n",
      "Epoch [97/100], Loss: 0.2960\n",
      "Epoch [98/100], Loss: 0.2940\n",
      "Epoch [99/100], Loss: 0.2914\n",
      "Epoch [100/100], Loss: 0.2874\n",
      "Test Accuracy Lipschitz Logit K Means: 62.70%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "lipschitz_logit_kmeans_plus = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    logit_model_lipschitz = LogisticRegression(input_dim, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(logit_model_lipschitz.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "    previous_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logit_model_lipschitz.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images = images.view(-1, 28*28).to(device, non_blocking=True)  # Flatten images\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = logit_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            L = max(param.grad.abs().max().item() for param in logit_model_lipschitz.parameters() if param.grad is not None)\n",
    "            adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "            # Update optimizer's learning rate\n",
    "            for param_group in optimizer_sdg.param_groups:\n",
    "                param_group['lr'] = adaptive_lr\n",
    "\n",
    "\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if previous_loss - current_loss < tolerance:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (Loss improvement < {tolerance} for {patience} epochs)\")\n",
    "                break\n",
    "        else:\n",
    "            epochs_without_improvement = 0  # Reset counter if improvement occurs\n",
    "        previous_loss = current_loss\n",
    "\n",
    "    logit_model_lipschitz.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 28*28)\n",
    "            outputs = logit_model_lipschitz(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Lipschitz Logit K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    lipschitz_logit_kmeans_plus[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.879808</td>\n",
       "      <td>68.663462</td>\n",
       "      <td>66.216346</td>\n",
       "      <td>65.293269</td>\n",
       "      <td>62.701923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000       1000 \n",
       "0  68.879808  68.663462  66.216346  65.293269  62.701923"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lipschitz_logit_kmeans_plus_df = pd.DataFrame(lipschitz_logit_kmeans_plus)\n",
    "lipschitz_logit_kmeans_plus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 98.8394\n",
      "Epoch [1/50], Val Loss: 0.3164\n",
      "Epoch [2/50], Loss: 45.7895\n",
      "Epoch [2/50], Val Loss: 0.2282\n",
      "Epoch [3/50], Loss: 37.1084\n",
      "Epoch [3/50], Val Loss: 0.2005\n",
      "Epoch [4/50], Loss: 32.3625\n",
      "Epoch [4/50], Val Loss: 0.1843\n",
      "Epoch [5/50], Loss: 29.0090\n",
      "Epoch [5/50], Val Loss: 0.1614\n",
      "Epoch [6/50], Loss: 25.9818\n",
      "Epoch [6/50], Val Loss: 0.1706\n",
      "Epoch [7/50], Loss: 23.6929\n",
      "Epoch [7/50], Val Loss: 0.1518\n",
      "Epoch [8/50], Loss: 21.1231\n",
      "Epoch [8/50], Val Loss: 0.1482\n",
      "Epoch [9/50], Loss: 19.3193\n",
      "Epoch [9/50], Val Loss: 0.1493\n",
      "Epoch [10/50], Loss: 17.2926\n",
      "Epoch [10/50], Val Loss: 0.1447\n",
      "Epoch [11/50], Loss: 16.0399\n",
      "Epoch [11/50], Val Loss: 0.1499\n",
      "Epoch [12/50], Loss: 14.5679\n",
      "Epoch [12/50], Val Loss: 0.1418\n",
      "Epoch [13/50], Loss: 12.9405\n",
      "Epoch [13/50], Val Loss: 0.1339\n",
      "Epoch [14/50], Loss: 11.7933\n",
      "Epoch [14/50], Val Loss: 0.1485\n",
      "Epoch [15/50], Loss: 11.2073\n",
      "Epoch [15/50], Val Loss: 0.1450\n",
      "Epoch [16/50], Loss: 10.1059\n",
      "Epoch [16/50], Val Loss: 0.1372\n",
      "Stopping early at epoch 16 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 92.44%\n",
      "Epoch [1/50], Loss: 79.6455\n",
      "Epoch [1/50], Val Loss: 0.3353\n",
      "Epoch [2/50], Loss: 35.3983\n",
      "Epoch [2/50], Val Loss: 0.2615\n",
      "Epoch [3/50], Loss: 28.9746\n",
      "Epoch [3/50], Val Loss: 0.2189\n",
      "Epoch [4/50], Loss: 25.0847\n",
      "Epoch [4/50], Val Loss: 0.2248\n",
      "Epoch [5/50], Loss: 22.3897\n",
      "Epoch [5/50], Val Loss: 0.2017\n",
      "Epoch [6/50], Loss: 19.9190\n",
      "Epoch [6/50], Val Loss: 0.2095\n",
      "Epoch [7/50], Loss: 18.1137\n",
      "Epoch [7/50], Val Loss: 0.1873\n",
      "Epoch [8/50], Loss: 16.2575\n",
      "Epoch [8/50], Val Loss: 0.1859\n",
      "Epoch [9/50], Loss: 14.6779\n",
      "Epoch [9/50], Val Loss: 0.1910\n",
      "Epoch [10/50], Loss: 13.2997\n",
      "Epoch [10/50], Val Loss: 0.1976\n",
      "Epoch [11/50], Loss: 12.3116\n",
      "Epoch [11/50], Val Loss: 0.1926\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 91.96%\n",
      "Epoch [1/50], Loss: 34.6392\n",
      "Epoch [1/50], Val Loss: 0.9076\n",
      "Epoch [2/50], Loss: 13.7607\n",
      "Epoch [2/50], Val Loss: 0.6791\n",
      "Epoch [3/50], Loss: 9.4296\n",
      "Epoch [3/50], Val Loss: 0.5000\n",
      "Epoch [4/50], Loss: 6.9871\n",
      "Epoch [4/50], Val Loss: 0.5336\n",
      "Epoch [5/50], Loss: 5.9858\n",
      "Epoch [5/50], Val Loss: 0.4566\n",
      "Epoch [6/50], Loss: 4.8837\n",
      "Epoch [6/50], Val Loss: 0.4708\n",
      "Epoch [7/50], Loss: 4.0746\n",
      "Epoch [7/50], Val Loss: 0.4697\n",
      "Epoch [8/50], Loss: 3.3256\n",
      "Epoch [8/50], Val Loss: 0.4775\n",
      "Stopping early at epoch 8 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 86.44%\n",
      "Epoch [1/50], Loss: 23.8466\n",
      "Epoch [1/50], Val Loss: 1.5893\n",
      "Epoch [2/50], Loss: 10.2531\n",
      "Epoch [2/50], Val Loss: 1.0127\n",
      "Epoch [3/50], Loss: 6.3196\n",
      "Epoch [3/50], Val Loss: 0.8121\n",
      "Epoch [4/50], Loss: 4.8543\n",
      "Epoch [4/50], Val Loss: 0.7624\n",
      "Epoch [5/50], Loss: 3.7286\n",
      "Epoch [5/50], Val Loss: 0.7091\n",
      "Epoch [6/50], Loss: 2.7810\n",
      "Epoch [6/50], Val Loss: 0.7495\n",
      "Epoch [7/50], Loss: 3.0251\n",
      "Epoch [7/50], Val Loss: 0.6113\n",
      "Epoch [8/50], Loss: 2.0885\n",
      "Epoch [8/50], Val Loss: 0.6813\n",
      "Epoch [9/50], Loss: 1.7407\n",
      "Epoch [9/50], Val Loss: 0.6613\n",
      "Epoch [10/50], Loss: 2.2892\n",
      "Epoch [10/50], Val Loss: 0.6647\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 82.15%\n",
      "Epoch [1/50], Loss: 6.4542\n",
      "Epoch [1/50], Val Loss: 3.0250\n",
      "Epoch [2/50], Loss: 5.4195\n",
      "Epoch [2/50], Val Loss: 2.1392\n",
      "Epoch [3/50], Loss: 3.1603\n",
      "Epoch [3/50], Val Loss: 2.2225\n",
      "Epoch [4/50], Loss: 2.9674\n",
      "Epoch [4/50], Val Loss: 1.9394\n",
      "Epoch [5/50], Loss: 1.9878\n",
      "Epoch [5/50], Val Loss: 1.4359\n",
      "Epoch [6/50], Loss: 1.5160\n",
      "Epoch [6/50], Val Loss: 1.3640\n",
      "Epoch [7/50], Loss: 1.1500\n",
      "Epoch [7/50], Val Loss: 1.3946\n",
      "Epoch [8/50], Loss: 0.8608\n",
      "Epoch [8/50], Val Loss: 1.4663\n",
      "Epoch [9/50], Loss: 0.7196\n",
      "Epoch [9/50], Val Loss: 1.4861\n",
      "Stopping early at epoch 9 (No improvement in 3 epochs)\n",
      "Test Accuracy CNN K Means: 66.57%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "cnn_kmeans_plus = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    cnn_model = EMNIST_CNN(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(cnn_model.parameters(), lr=0.01, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "    def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                m.bias.data.fill_(0.01)\n",
    "    cnn_model.apply(init_weights)\n",
    "\n",
    "    best_val_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = cnn_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        cnn_model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader_cnn:\n",
    "                images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "                outputs = cnn_model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_cnn)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    cnn_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "            outputs = cnn_model(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy CNN K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    cnn_kmeans_plus[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.4375</td>\n",
       "      <td>91.956731</td>\n",
       "      <td>86.4375</td>\n",
       "      <td>82.149038</td>\n",
       "      <td>66.567308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     75000      50000    10000      5000       1000 \n",
       "0  92.4375  91.956731  86.4375  82.149038  66.567308"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_kmeans_plus_df = pd.DataFrame(cnn_kmeans_plus)\n",
    "cnn_kmeans_plus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Lipschitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 97.5875\n",
      "Epoch [1/50], Val Loss: 0.2850\n",
      "Epoch [2/50], Loss: 44.8981\n",
      "Epoch [2/50], Val Loss: 0.2182\n",
      "Epoch [3/50], Loss: 36.7626\n",
      "Epoch [3/50], Val Loss: 0.1898\n",
      "Epoch [4/50], Loss: 32.6282\n",
      "Epoch [4/50], Val Loss: 0.1711\n",
      "Epoch [5/50], Loss: 29.0655\n",
      "Epoch [5/50], Val Loss: 0.1668\n",
      "Epoch [6/50], Loss: 26.2135\n",
      "Epoch [6/50], Val Loss: 0.1503\n",
      "Epoch [7/50], Loss: 23.9570\n",
      "Epoch [7/50], Val Loss: 0.1415\n",
      "Epoch [8/50], Loss: 21.4524\n",
      "Epoch [8/50], Val Loss: 0.1406\n",
      "Epoch [9/50], Loss: 19.6562\n",
      "Epoch [9/50], Val Loss: 0.1407\n",
      "Epoch [10/50], Loss: 18.0470\n",
      "Epoch [10/50], Val Loss: 0.1359\n",
      "Epoch [11/50], Loss: 16.3945\n",
      "Epoch [11/50], Val Loss: 0.1422\n",
      "Epoch [12/50], Loss: 14.8937\n",
      "Epoch [12/50], Val Loss: 0.1500\n",
      "Epoch [13/50], Loss: 13.9249\n",
      "Epoch [13/50], Val Loss: 0.1435\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 92.48%\n",
      "Epoch [1/50], Loss: 80.2956\n",
      "Epoch [1/50], Val Loss: 0.3712\n",
      "Epoch [2/50], Loss: 36.1110\n",
      "Epoch [2/50], Val Loss: 0.2757\n",
      "Epoch [3/50], Loss: 29.0439\n",
      "Epoch [3/50], Val Loss: 0.2355\n",
      "Epoch [4/50], Loss: 24.9178\n",
      "Epoch [4/50], Val Loss: 0.2154\n",
      "Epoch [5/50], Loss: 22.2188\n",
      "Epoch [5/50], Val Loss: 0.1941\n",
      "Epoch [6/50], Loss: 19.7561\n",
      "Epoch [6/50], Val Loss: 0.1948\n",
      "Epoch [7/50], Loss: 17.8152\n",
      "Epoch [7/50], Val Loss: 0.1946\n",
      "Epoch [8/50], Loss: 16.0438\n",
      "Epoch [8/50], Val Loss: 0.1876\n",
      "Epoch [9/50], Loss: 14.2159\n",
      "Epoch [9/50], Val Loss: 0.1873\n",
      "Epoch [10/50], Loss: 12.9103\n",
      "Epoch [10/50], Val Loss: 0.1891\n",
      "Epoch [11/50], Loss: 11.4829\n",
      "Epoch [11/50], Val Loss: 0.1898\n",
      "Epoch [12/50], Loss: 10.4447\n",
      "Epoch [12/50], Val Loss: 0.1791\n",
      "Epoch [13/50], Loss: 9.1544\n",
      "Epoch [13/50], Val Loss: 0.2060\n",
      "Epoch [14/50], Loss: 8.4100\n",
      "Epoch [14/50], Val Loss: 0.1940\n",
      "Epoch [15/50], Loss: 7.4777\n",
      "Epoch [15/50], Val Loss: 0.1958\n",
      "Stopping early at epoch 15 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 91.91%\n",
      "Epoch [1/50], Loss: 36.1468\n",
      "Epoch [1/50], Val Loss: 0.9986\n",
      "Epoch [2/50], Loss: 13.8770\n",
      "Epoch [2/50], Val Loss: 0.7040\n",
      "Epoch [3/50], Loss: 9.5173\n",
      "Epoch [3/50], Val Loss: 0.6172\n",
      "Epoch [4/50], Loss: 7.5540\n",
      "Epoch [4/50], Val Loss: 0.4771\n",
      "Epoch [5/50], Loss: 6.1603\n",
      "Epoch [5/50], Val Loss: 0.4646\n",
      "Epoch [6/50], Loss: 4.9987\n",
      "Epoch [6/50], Val Loss: 0.4515\n",
      "Epoch [7/50], Loss: 4.2564\n",
      "Epoch [7/50], Val Loss: 0.4293\n",
      "Epoch [8/50], Loss: 3.5379\n",
      "Epoch [8/50], Val Loss: 0.4548\n",
      "Epoch [9/50], Loss: 2.9682\n",
      "Epoch [9/50], Val Loss: 0.4687\n",
      "Epoch [10/50], Loss: 2.5923\n",
      "Epoch [10/50], Val Loss: 0.4592\n",
      "Stopping early at epoch 10 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 87.74%\n",
      "Epoch [1/50], Loss: 22.5938\n",
      "Epoch [1/50], Val Loss: 1.6266\n",
      "Epoch [2/50], Loss: 10.4855\n",
      "Epoch [2/50], Val Loss: 1.0686\n",
      "Epoch [3/50], Loss: 6.3642\n",
      "Epoch [3/50], Val Loss: 0.8292\n",
      "Epoch [4/50], Loss: 4.7574\n",
      "Epoch [4/50], Val Loss: 0.7917\n",
      "Epoch [5/50], Loss: 3.8095\n",
      "Epoch [5/50], Val Loss: 0.7204\n",
      "Epoch [6/50], Loss: 3.1608\n",
      "Epoch [6/50], Val Loss: 0.6718\n",
      "Epoch [7/50], Loss: 2.7837\n",
      "Epoch [7/50], Val Loss: 0.6656\n",
      "Epoch [8/50], Loss: 2.1613\n",
      "Epoch [8/50], Val Loss: 0.6437\n",
      "Epoch [9/50], Loss: 1.7567\n",
      "Epoch [9/50], Val Loss: 0.6914\n",
      "Epoch [10/50], Loss: 1.2087\n",
      "Epoch [10/50], Val Loss: 0.6542\n",
      "Epoch [11/50], Loss: 1.0892\n",
      "Epoch [11/50], Val Loss: 0.6766\n",
      "Stopping early at epoch 11 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 83.59%\n",
      "Epoch [1/50], Loss: 6.5291\n",
      "Epoch [1/50], Val Loss: 3.1162\n",
      "Epoch [2/50], Loss: 5.8883\n",
      "Epoch [2/50], Val Loss: 2.6088\n",
      "Epoch [3/50], Loss: 4.2936\n",
      "Epoch [3/50], Val Loss: 1.9124\n",
      "Epoch [4/50], Loss: 3.0944\n",
      "Epoch [4/50], Val Loss: 2.0572\n",
      "Epoch [5/50], Loss: 2.4159\n",
      "Epoch [5/50], Val Loss: 1.8104\n",
      "Epoch [6/50], Loss: 2.0093\n",
      "Epoch [6/50], Val Loss: 1.4163\n",
      "Epoch [7/50], Loss: 1.4566\n",
      "Epoch [7/50], Val Loss: 1.3685\n",
      "Epoch [8/50], Loss: 1.1692\n",
      "Epoch [8/50], Val Loss: 1.4225\n",
      "Epoch [9/50], Loss: 0.8902\n",
      "Epoch [9/50], Val Loss: 1.4592\n",
      "Epoch [10/50], Loss: 0.7646\n",
      "Epoch [10/50], Val Loss: 1.3624\n",
      "Epoch [11/50], Loss: 0.6480\n",
      "Epoch [11/50], Val Loss: 1.3988\n",
      "Epoch [12/50], Loss: 0.4992\n",
      "Epoch [12/50], Val Loss: 1.4421\n",
      "Epoch [13/50], Loss: 0.4156\n",
      "Epoch [13/50], Val Loss: 1.4474\n",
      "Stopping early at epoch 13 (No improvement in 3 epochs)\n",
      "Test Accuracy Lipschitz CNN K Means: 70.03%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "tolerance = 1e-3  # Minimum improvement in loss to continue training\n",
    "patience = 3\n",
    "\n",
    "cnn_lipschitz_kmeans_plus = {}\n",
    "\n",
    "for subset in num_subsets:\n",
    "\n",
    "    accuracy_list = []\n",
    "\n",
    "    prototype_train_data = torch.load(f\"emnist_centroids/emnist_kmeans_plus_centroids_{subset}.pth\", weights_only=False)\n",
    "    prototype_train_labels = torch.tensor(torch.load(f\"emnist_centroids/emnist_kmeans_plus_cluster_labels_{subset}.pth\", weights_only=False))\n",
    "    kmeans_centroids = KMeans(prototype_train_data, prototype_train_labels)\n",
    "\n",
    "    prototype_loader = DataLoader(kmeans_centroids, batch_size=128, shuffle=True)\n",
    "\n",
    "    cnn_model_lipschitz = EMNIST_CNN(num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer_sdg = optim.SGD(cnn_model_lipschitz.parameters(), lr=0.005, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "\n",
    "    def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)  # Set all weights to zero\n",
    "                m.bias.data.fill_(0.01)\n",
    "    cnn_model_lipschitz.apply(init_weights)\n",
    "\n",
    "    best_val_loss = float('inf')  # Track the best loss\n",
    "    epochs_without_improvement = 0  # Counter for early stopping\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model_lipschitz.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in prototype_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer_sdg.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "\n",
    "            L = max(param.grad.abs().max().item() for param in cnn_model_lipschitz.parameters() if param.grad is not None)\n",
    "            adaptive_lr = min(0.01, max(1e-4, 1.0 / (L + 1e-6)))  # Clamped learning rate\n",
    "\n",
    "            # Update optimizer's learning rate\n",
    "            for param_group in optimizer_sdg.param_groups:\n",
    "                param_group['lr'] = adaptive_lr\n",
    "                \n",
    "            optimizer_sdg.step()\n",
    "\n",
    "            total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "        current_loss = total_loss / len(train_loader)  # Compute average loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {current_loss:.4f}\")\n",
    "\n",
    "        # Validation loop\n",
    "        cnn_model_lipschitz.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader_cnn:\n",
    "                images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "\n",
    "                outputs = cnn_model_lipschitz(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader_cnn)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "            f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1} (No improvement in {patience} epochs)\")\n",
    "                break\n",
    "\n",
    "\n",
    "    cnn_model_lipschitz.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.view(-1, 1, 28, 28).to(device), labels.to(device)\n",
    "            outputs = cnn_model_lipschitz(images)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted labels\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy Lipschitz CNN K Means: {accuracy:.2f}%\")\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    cnn_lipschitz_kmeans_plus[subset] = accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>75000</th>\n",
       "      <th>50000</th>\n",
       "      <th>10000</th>\n",
       "      <th>5000</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92.475962</td>\n",
       "      <td>91.913462</td>\n",
       "      <td>87.740385</td>\n",
       "      <td>83.586538</td>\n",
       "      <td>70.028846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       75000      50000      10000      5000       1000 \n",
       "0  92.475962  91.913462  87.740385  83.586538  70.028846"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_lipschitz_kmeans_plus_df = pd.DataFrame(cnn_lipschitz_kmeans_plus)\n",
    "cnn_lipschitz_kmeans_plus_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
